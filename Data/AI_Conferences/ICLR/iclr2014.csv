unique_id,year,title,authors,pdf_url,paper_text,abstract
1312.6184,2014,Do Deep Nets Really Need to be Deep?  ,"['Jimmy Lei Ba', 'Rich Caurana']",https://arxiv.org/pdf/1312.6184.pdf,"Do Deep Nets Really Need to be Deep?  Draft for NIPS 2014 (not camera ready copy)  ***  ***  4 1 0 2    t c O 1 1         ]  G L . s c [      7 v 4 8 1 6  .  2 1 3 1 : v i X r a  Lei Jimmy Ba  University of Toronto  jimmy@psi.utoronto.ca  Rich Caruana  Microsoft Research  rcaruana@microsoft.com  Abstract  Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this paper we empirically demonstrate that shallow feed-forward nets can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models. Moreover, in some cases the shallow neural nets can learn these deep functions using the same number of parameters as the original deep models. On the TIMIT phoneme recognition and CIFAR-10 image recognition tasks, shallow nets can be trained that perform similarly to complex, well-engineered, deeper convolutional architectures.  1  Introduction  You are given a training set with 1M labeled points. When you train a shallow neural net with one fully-connected feed-forward hidden layer on this data you obtain 86% accuracy on test data. When you train a deeper neural net as in [1] consisting of a convolutional layer, pooling layer, and three fully-connected feed-forward layers on the same data you obtain 91% accuracy on the same test set. What is the source of this improvement? Is the 5% increase in accuracy of the deep net over the shallow net because: a) the deep net has more parameters; b) the deep net can learn more complex functions given the same number of parameters; c) the deep net has better bias and learns more interesting/useful functions (e.g., because the deep net is deeper it learns hierarchical representations [5]); d) nets without convolution can’t easily learn what nets with convolution can learn; e) current learning algorithms and regularization methods work better with deep architectures than shallow architectures[8]; f) all or some of the above; g) none of the above? There have been attempts to answer the question above. It has been shown that deep nets coupled with unsupervised layer-by-layer pre-training technique[10] [19] work well. In [8], the authors show that depth combined with pre-training provides a good prior for model weights, thus improving gen- eralization. There is well-known early theoretical work on the representational capacity of neural nets. For example, it was proved that a network with a large enough single hidden layer of sigmoid units can approximate any decision boundary[4]. Empirical work, however, shows that it is difﬁcult to train shallow nets to be as accurate as deep nets. For vision tasks, a recent study on deep con- volutional nets suggests that deeper models are preferred under a parameter budget [7]. In [5], the authors trained shallow nets on SIFT features to classify a large-scale ImageNet dataset and showed that it is challenging to train large shallow nets to learn complex functions. And in [17], the authors show that deeper models are more competitive than shallow models in speech acoustic modeling.  1  In this paper we provide empirical evidence that shallow nets are capable of learning the same function as deep nets, and in some cases with the same number of parameters as the deep nets. We do this by ﬁrst training a state-of-the-art deep model, and then training a shallow model to mimic the deep model. The mimic model is trained using the model compression scheme described in the next section. Remarkably, with model compression we are able to train shallow nets to be as accurate as some deep models, even though we are not able to train these shallow nets to be as accurate as the deep nets when the shallow nets are trained directly on the original labeled training data. If a shallow net with the same number of parameters as a deep net can learn to mimic a deep net with high ﬁdelity, then it is clear that the function learned by that deep net does not really have to be deep.  2 Training Shallow Nets to Mimic Deep Nets  2.1 Model Compression  The main idea behind model compression is to train a compact model to approximate the function learned by a larger, more complex model. For example, in [3], a single neural net of modest size could be trained to mimic a much larger ensemble of models — although the small neural nets contained 1000 times fewer parameters, often they were just as accurate as the ensembles they were trained to mimic. Model compression works by passing unlabeled data through the large, accurate model to collect the scores produced by that model. This synthetically labeled data is then used to train the smaller mimic model. The mimic model is not trained on the original labels—it is trained to learn the function that was learned by the larger model. If the compressed model learns to mimic the large model perfectly it makes exactly the same predictions and mistakes as the complex model. Surprisingly, often it is not (yet) possible to train a small neural net on the original training data to be as accurate as the complex model, nor as accurate as the mimic model. Compression demonstrates that a small neural net could, in principle, learn the more accurate function, but current learning algorithms are unable to train a model with that accuracy from the original training data; instead, we must train the complex intermediate model ﬁrst and then train the neural net to mimic it. Clearly, when it is possible to mimic the function learned by a complex model with a small net, the function learned by the complex model wasn’t truly too complex to be learned by a small net. This suggests to us that the complexity of a learned model, and the size of the representation best used to learn that model, are different things. In this paper we apply model compression to train shallow neural nets to mimic deeper neural nets, thereby demonstrating that deep neural nets may not need to be deep.  2.2 Mimic Learning via Regressing Logit with L2 Loss  On both TIMIT and CIFAR-10 we train shallow mimic nets using data labeled by either a deep net, or an ensemble of deep nets, trained on the original TIMIT or CIFAR-10 training data. The deep models are trained in the usual way using softmax output and cross-entropy cost function. The shallow mimic models, however, instead of being trained with cross-entropy on the 183 p values j ezj output by the softmax layer from the deep model, are trained directly on  where pk = ezk /(cid:80)  the 183 log probability values z, also called logit, before the softmax activation. Training on these logit values makes learning easier for the shallow net by placing emphasis on all prediction targets. Because the logits capture the logarithm relationships between the proba- bility predictions, a student model trained on logits has to learn all of the additional ﬁne detailed relationships between labels that is not obvious in the probability space yet was learned by the teacher model. For example, assume there are three targets that the teacher predicts with probability [2e − 9, 4e − 5, 0.9999]. If we use these probabilities as prediction targets directly to minimize a cross entropy loss function, the student will focus on the third target and easily ignore the ﬁrst and second target. Alternatively, one can extract the logit prediction from the teacher model and obtain our new targets [10, 20, 30]. The student will learn to regress the third target, yet it still learns the ﬁrst and second target along with their relative difference. The logit values provide richer informa- tion to student to mimic the exact behaviours of a teach model. Moreover, consider a second training case where the teacher predicts logits [−10, 0, 10]. After softmax, these logits yield the same pre- dicted probabilities as [10, 20, 30], yet clearly the teacher has learned internally to model these two cases very differently. By training the student model on the logits directly, the student is better able  2  to learn the internal model learned by the teacher, without suffering from the information loss that occurs after passing through the logits to probability space. We formulate the SNN-MIMIC learning objective function as a regression problem given training data {(x(1), z(1)),...,(x(T ), z(T )) }:  L(W, β) =  1 2T  ||g(x(t); W, β) − z(t)||2 2,  (1)  (cid:88)  t  where, W is the weight matrix between input features x and hidden layer, β is the weights from hidden to output units, g(x(t); W, β) = βf (W x(t)) is the model prediction on the tth training data point and f (·) is the non-linear activation of the hidden units. The parameters W and β are updated using standard error back-propagation algorithm and stochastic gradient descent with momentum. We have also experimented with other different mimic loss function, such as minimizing the KL divergence KL(pteacher(cid:107)pstudent) cost function and L2 loss on the probability. Logits regression out- performs all the other loss functions and is one of the key technique for obtaining the results in the rest of this paper. We found that normalizing the logits from the teacher model, by subtracting the mean and dividing the standard deviation of each target across the training set, can improve the L2 loss slightly during training. Normalization is not crucial for obtaining a good student model.  2.3 Speeding-up Mimic Learning by Introducing a Linear Layer  To match the number of parameters in a deep net, a shallow net has to have more non-linear hidden units in a single layer to produce a large weight matrix W . When training a large shallow neural network with many hidden units, we ﬁnd it is very slow to learn the large number of parameters in the weight matrix between input and hidden layers of size O(HD), where D is input feature dimension and H is the number of hidden units. Because there are many highly correlated parameters in this large weight matrix gradient descent converges slowly. We also notice that during learning, shallow nets spend most of the computation in the costly matrix multiplication of the input data vectors and large weight matrix. The shallow nets eventually learn accurate mimic functions, but training to convergence is very slow (multiple weeks) even with a GPU. We found that introducing a bottleneck linear layer with k linear hidden units between the input and the non-linear hidden layer sped up learning dramatically: we can factorize the weight matrix W ∈ RH×D into the product of two low rank matrices, U ∈ RH×k and V ∈ Rk×D, where k << D, H. The new cost function can be written as:  L(U, V, β) =  1 2T  ||βf (U V x(t)) − z(t)||2  2  (2)  (cid:88)  t  The weights U and V can be learnt by back-propagating through the linear layer. This re- parameterization of weight matrix W not only increases the convergence rate of the shallow mimic nets, but also reduces memory space from O(HD) to O(k(H + D)). Factorizing weight matrices has been previously explored in [16] and [20]. While these prior works focus on using matrix factorization in the last output layer, our method is applied between input and hidden layer to improve the convergence speed during training. The reduced memory usage enables us to train large shallow models that were previously infeasible due to excessive memory usage. The linear bottle neck can only reduce the representational power of the network, and it can always be absorbed into a signle weight matrix W .  3 TIMIT Phoneme Recognition  The TIMIT speech corpus has 462 speakers in the training set. There is a separate development set for cross-validation including 50 speakers, and a ﬁnal test set with 24 speakers. The raw waveform audio data were pre-processed using 25ms Hamming window shifting by 10ms to extract Fourier- transform-based ﬁlter-banks with 40 coefﬁcients (plus energy) distributed on a mel-scale, together  3  with their ﬁrst and second temporal derivatives. We included +/- 7 nearby frames to formulate the ﬁnal 1845 dimension input vector. The data input features were normalized by subtracting the mean and dividing by the standard deviation on each dimension. All 61 phoneme labels are represented in tri-state, i.e., 3 states for each of the 61 phonemes, yielding target label vectors with 183 dimensions for training. At decoding time these are mapped to 39 classes as in [13] for scoring.  3.1 Deep Learning on TIMIT  Deep learning was ﬁrst successfully applied to speech recognition in [14]. We follow the same framework and train two deep models on TIMIT, DNN and CNN. DNN is a deep neural net con- sisting of three fully-connected feedforward hidden layers consisting of 2000 rectiﬁed linear units (ReLU) [15] per layer. CNN is a deep neural net consisting of a convolutional layer and max-pooling layer followed by three hidden layers containing 2000 ReLU units [2]. The CNN was trained using the same convolutional architecture as in [6]. We also formed an ensemble of nine CNN models, ECNN. The accuracy of DNN, CNN, and ECNN on the ﬁnal test set are shown in Table 1. The error rate of the convolutional deep net (CNN) is about 2.1% better than the deep net (DNN). The table also shows the accuracy of shallow neural nets with 8000, 50,000, and 400,000 hidden units (SNN-8k, SNN-50k, and SNN-400k) trained on the original training data. Despite having up to 10X as many parameters as DNN, CNN and ECNN, the shallow models are 1.4% to 2% less accurate than the DNN, 3.5% to 4.1% less accurate than the CNN, and 4.5% to 5.1% less accurate than the ECNN.  3.2 Learning to Mimic an Ensemble of Deep Convolutional TIMIT Models  The most accurate single model we trained on TIMIT is the deep convolutional architecture in [6]. Because we have no unlabeled data from the TIMIT distribution, we are forced to use the same 1.1M points in the train set as unlabeled data for compression by throwing away their labels.1 Re-using the train set reduces the accuracy of the mimic models, increasing the gap between the teacher and mimic models on test data: model compression works best when the unlabeled set is much larger than the train set, and when the unlabeled samples do not fall on train points where the teacher model is more likely to have overﬁt. To reduce the impact of the gap caused by performing compression with the original train set, we train the student model to mimic a more accurate ensemble of deep convolutional models. We are able to train a more accurate model on TIMIT by forming an ensemble of 9 deep, convo- lutional neural nets, each trained with somewhat different train sets, and with architectures with different kernel sizes in the convolutional layers. We used this very accurate model, ECNN, as the teacher model to label the data used to train the shallow mimic nets. As described in Section 2.2, the logits (log probability of the predicted values) from each CNN in the ECNN model are averaged and the average logits are used as ﬁnal regression targets to train the mimic SNNs. We trained shallow mimic nets with 8k (SNN-MIMIC-8k) and 400k (SNN-MIMIC-400k) hidden units on the re-labeled 1.1M training points. As described in Section 2.3, both mimic models have 250 linear units between the input and non-linear hidden layer to speed up learning — preliminary experiments suggest that for TIMIT there is little beneﬁt from using more than 250 linear units.  3.3 Compression Results For TIMIT  The bottom of Table 1 shows the accuracy of shallow mimic nets with 8000 ReLUs and 400,000 ReLUs (SNN-MIMIC-8k and -400k) trained with model compression to mimic the ECNN. Surpris- ingly, shallow nets are able to perform as well as their deep counter-parts when trained with model compression to mimic a more accurate model. A neural net with one hidden layer (SNN-MIMIC-8k) can be trained to perform as well as a DNN with a similar number of parameters. Furthermore, if we increase the number of hidden units in the shallow net from 8k to 400k (the largest we could train), we see that a neural net with one hidden layer (SNN-MIMIC-400k) can be trained to perform com- parably to a CNN even though the SNN-MIMIC-400k net has no convolutional or pooling layers.  1That SNNs can be trained to be as accurate as DNNs using only the original training data data highlights  that it should be possible to train accurate SNNs on the original train data given better learning algorithms.  4  SNN-8k  SNN-50k  SNN-400k  DNN  CNN  ECNN  SNN-MIMIC-8k  SNN-MIMIC-400k  Architecture  8k + dropout  trained on original data  50k + dropout  trained on original data 250L-400k + dropout trained on original data  2k-2k-2k + dropout  trained on original data c-p-2k-2k-2k + dropout trained on original data ensemble of 9 CNNs  250L-8k  250L-400k  no convolution or pooling layers  no convolution or pooling layers  # Param. ∼12M ∼100M ∼180M ∼12M ∼13M ∼125M ∼12M ∼180M  # Hidden units  PER  ∼8k ∼50k ∼400k ∼6k ∼10k ∼90k ∼8k ∼400k  23.1%  23.0%  23.6%  21.9%  19.5%  18.5%  21.6%  20.0%  Table 1: Comparison of shallow and deep models: phone error rate (PER) on TIMIT core test set.  Figure 1: Accuracy of SNNs, DNNs, and Mimic SNNs vs. # of parameters on TIMIT Dev (left) and Test (right) sets. Accuracy of the CNN and target ECNN are shown as horizontal lines for reference.  This is interesting because it suggests that a large single hidden¡ layer without a topology custom designed for the problem is able to reach the performance of a deep convolutional neural net that was carefully engineered with prior structure and weight sharing without any increase in the number of training examples, even though the same architecture trained on the original data could not. Figure 1 shows the accuracy of shallow nets and deep nets trained on the original TIMIT 1.1M data, and shallow mimic nets trained on the ECNN targets, as a function of the number of parameters in the models. The accuracy of the CNN and the teacher ECNN are shown as horizontal lines at the top of the ﬁgures. When the number of parameters is small (about 1 million), the SNN, DNN, and SNN- MIMIC models all have similar accuracy. As the size of the hidden layers increases and the number of parameters increases, the accuracy of a shallow model trained on the original data begins to lag behind. The accuracy of the shallow mimic model, however, matches the accuracy of the DNN until about 4 million parameters, when the DNN begins to fall behind the mimic. The DNN asymptotes at around 10M parameters, while the shallow mimic continues to increase in accuracy. Eventually the mimic asymptotes at around 100M parameters to an accuracy comparable to that of the CNN. The shallow mimic never achieves the accuracy of the ECNN it is trying to mimic (because there is not enough unlabeled data), but it is able to match or exceed the accuracy of deep nets (DNNs) having the same number of parameters trained on the original data.  5   76 77 78 79 80 81 82 83 1 10 100Accuracy on TIMIT Dev SetNumber of Parameters (millions)ShallowNetDeepNetShallowMimicNetConvolutional NetEnsemble of CNNs 75 76 77 78 79 80 81 82 1 10 100Accuracy on TIMIT Test SetNumber of Parameters (millions)ShallowNetDeepNetShallowMimicNetConvolutional NetEnsemble of CNNs4 Object Recognition: CIFAR-10  To verify that the results on TIMIT generalize to other learning problems and task domains, we ran similar experiments on the CIFAR-10 Object Recognition Task[12]. CIFAR-10 consists of a set of natural images from 10 different object classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. The dataset is a labeled subset of the 80 million tiny images dataset[18] and is divided into 50,000 train and 10,000 test images. Each image is 32x32 pixels in 3 color channels, yielding input vectors with 3072 dimensions. We prepared the data by subtracting the mean and dividing the standard deviation of each image vector to perform global contrast normalization. We then applied ZCA whitening to the normalized images. This pre-processing is the same used in [9].  4.1 Learning to Mimic a Deep Convolutional Neural Network  Deep learning currently achieves state-of-the-art accuracies on many computer vision problems. The key to this success is deep convolutional nets with many alternating layers of convolutional, pooling and non-linear units. Recent advances such as dropout are also important to prevent over-ﬁtting in these deep nets. We follow the same approach as with TIMIT: An ensemble of deep CNN models is used to label CIFAR-10 images for model compression. The logit predictions from this teacher model are used as regression targets to train a mimic shallow neural net (SNN). CIFAR-10 images have a higher dimension than TIMIT (3072 vs. 1845), but the size of the CIFAR-10 training set is only 50,000 compared to 1.1 million examples for TIMIT. Fortunately, unlike TIMIT, in CIFAR-10 we have access to unlabeled data from a similar distribution by using the super set of CIFAR-10: the 80 million tiny images dataset. We add the ﬁrst 1 million images from the 80 million set to the original 50,000 CIFAR-10 training images to create a 1.05M mimic training (transfer) set. CIFAR-10 images are raw pixels for objects viewed from many different angles and positions, whereas TIMIT features are human-designed ﬁlter-bank features. In preliminary experiments we observed that non-convolutional nets do not perform well on CIFAR-10 no matter what their depth. Instead of raw pixels, the authors in [5] trained their shallow models on the SIFT features. Similarly, [7] used a base convolution and pooling layer to study different deep architectures. We follow the approach in [7] to allow our shallow models to beneﬁt from convolution while keeping the models as shallow as possible, and introduce a single layer of convolution and pooling in our shallow mimic models to act as a feature extractor to create invariance to small translations in the pixel domain. The SNN-MIMIC models for CIFAR-10 thus consist of a convolution and max pooling layer followed by fully connected 1200 linear units and 30k non-linear units. As before, the linear units are there only to speed learning; they do not increase the model’s representational power and can be absorbed into the weights in the non-linear layer after learning. Results on CIFAR-10 are consistent with those from TIMIT. Table 2 shows results for the shallow mimic models, and for much-deeper convolutional nets. The shallow mimic net trained to mimic the teacher CNN (SNN-CNN-MIMIC-30k) achieves accuracy comparable to CNNs with multiple convolutional and pooling layers. And by training the shallow model to mimic the ensemble of CNNs (SNN-ECNN-MIMIC-30k), accuracy is improved an additional 0.9%. The mimic models are able to achieve accuracies previously unseen on CIFAR-10 with models with so few layers. Although the deep convolution nets have more hidden units than the shallow mimic models, because of weight sharing, the deeper nets with multiple convolution layers have fewer parameters than the shallow fully-connected mimic models. Still, it is surprising to see how accurate the shallow mimic models are, and that their performance continues to improve as the performance of the teacher model improves (see further discussion of this in Section 5.2).  5 Discussion  5.1 Why Mimic Models Can Be More Accurate than Training on Original Labels  It may be surprising that models trained on the prediction targets taken from other models can be more accurate than models trained on the original labels. There are a variety of reasons why this can happen:  6  DNN  SNN-30k single-layer feature extraction CNN[11] (no augmentation) CNN[21] (no augmentation)  teacher CNN (no augmentation)  ECNN (no augmentation) SNN-CNN-MIMIC-30k trained on a single CNN SNN-CNN-MIMIC-30k trained on a single CNN SNN-ECNN-MIMIC-30k trained on ensemble  Architecture  2000-2000 + dropout 128c-p-1200L-30k  + dropout input&hidden  4000c-p  followed by SVM  64c-p-64c-p-64c-p-16lc  + dropout on lc  64c-p-64c-p-128c-p-fc  + dropout on fc  and stochastic pooling  + dropout on fc  and stochastic pooling ensemble of 4 CNNs  64c-p-1200L-30k  with no regularization  128c-p-1200L-30k  with no regularization  128c-p-1200L-30k  with no regularization  128c-p-128c-p-128c-p-1000fc  # Param. ∼10M ∼70M ∼125M ∼10k  ∼56k  ∼35k  ∼140k ∼54M ∼70M ∼70M  # Hidden units  Err.  4k  ∼190k ∼3.7B ∼110k  ∼120k  ∼210k  ∼840k ∼110k ∼190k ∼190k  57.8%  21.8%  18.4%  15.6%  15.13%  12.0%  11.0%  15.4%  15.1%  14.2%  Table 2: Comparison of shallow and deep models: classiﬁcation error rate on CIFAR-10. Key: c, convolution layer; p, pooling layer; lc, locally connected layer; fc, fully connected layer  • if some labels have errors, the teacher model may eliminate some of these errors (i.e., cen- sor the data), thus making learning easier for the student: on TIMIT, there are mislabeled frames introduced by the HMM forced-alignment procedure.  • if there are regions in the p(y|X) that are difﬁcult to learn given the features, sample den- sity, and function complexity, the teacher may provide simpler, soft labels to the student. The complexity in the data set has been washed away by ﬁltering the targets through the teacher model.  • learning from the original hard 0/1 labels can be more difﬁcult than learning from the teacher’s conditional probabilities: on TIMIT only one of 183 outputs is non-zero on each training case, but the mimic model sees non-zero targets for most outputs on most training cases. Moreover, the teacher model can spread the uncertainty over multiple outputs when it is not conﬁdent of its prediction. Yet, the teacher model can concentrate the probability mass on one (or few) outputs on easy cases. The uncertainty from the teacher model is far more informative to guiding the student model than the original 0/1 labels. This beneﬁt appears to be further enhanced by training on logits.  The mechanisms above can be seen as forms of regularization that help prevent overﬁtting in the student model. Shallow models trained on the original targets are more prone to overﬁt- ting than deep models—they begin to overﬁt before learning the accurate functions learned by deeper models even with dropout (see Fig- ure 2). If we had more effective regularization methods for shallow models, some of the per- formance gap between shallow and deep mod- els might disappear. Model compression ap- pears to be a form of regularization that is ef- fective at reducing this gap.  Figure 2: Training shallow mimic model prevents overﬁtting. 7  02468101214Number of Epoches74.074.575.075.576.076.577.077.5Phone Recognition AccuracySNN-8kSNN-8k + dropoutSNN-MIMIC-8k5.2 The Capacity and Representational Power of Shallow Models  Figure 3 shows results of an experiment with TIMIT where we trained shallow mimic mod- els of two sizes (SNN-MIMIC-8k and SNN- MIMIC-160k) on teacher models of different accuracies. The two shallow mimic models are trained on the same number of data points. The only difference between them is the size of the hidden layer. The x-axis shows the accuracy of the teacher model, and the y-axis is the accu- racy of the mimic models. Lines parallel to the diagonal suggest that increases in the accuracy of the teacher models yield similar increases in the accuracy of the mimic models. Although the data does not fall perfectly on a diagonal, there is strong evidence that the accuracy of the mimic models continues to increase as the ac- curacy of the teacher model improves, suggest- ing that the mimic models are not (yet) running out of capacity. When training on the same targets, SNN-MIMIC-8k always perform worse than SNN-MIMIC-160K that has 10 times more parameters. Although there is a consistent performance gap between the two models due to the difference in size, the smaller shallow model was eventu- ally able to achieve a performance comparable to the larger shallow net by learning from a better teacher, and the accuracy of both models continues to increase as teacher accuracy increases. This suggests that shallow models with a number of parameters comparable to deep models are likely capable of learning even more accurate functions if a more accurate teacher and/or more unlabeled data became available. Similarly, on CIFAR-10 we saw that increasing the accuracy of the teacher model by forming an ensemble of deep CNNs yielded commensurate increase in the accuracy of the student model. We see little evidence that shallow models have limited capacity or representational power. Instead, the main limitation appears to be the learning and regularization procedures used to train the shallow models.  Figure 3: Accuracy of student models continues to improve as accuracy of teacher models improves.  5.3 Parallel Distributed Processing vs. Deep Sequential Processing  Our results show that shallow nets can be competitive with deep models on speech and vision tasks. One potential beneﬁt of shallow nets is that training them scales well with the modern parallel hardware. In our experiments the deep models usually required 8–12 hours to train on Nvidia GTX 580 GPUs to reach the state-of-the-art performance on TIMIT and CIFAR-10 datasets. Although some of the shallow mimic models have more parameters than the deep models, the shallow models train much faster and reach similar accuracies in only 1–2 hours. Also, given parallel computational resources, at run-time shallow models can ﬁnish computation in 2 or 3 cycles for a given input, whereas a deep architecture has to make sequential inference through each of its layers, expending a number of cycles proportional to the depth of the model. This beneﬁt can be important in on-line inference settings where data parallelization is not as easy to achieve as it is in the batch inference setting. For real-time applications such as surveillance or real-time speech translation, a model that responds in fewer cycles can be beneﬁcial. 6 Future Work  The tiny images dataset contains 80 millions images. We are currently investigating if by labeling these 80M images with a teacher, it is possible to train shallow models with no convolutional or pooling layers to mimic deep convolutional models. This paper focused on training the shallowest-possible models to mimic deep models in order to better understand the importance of model depth in learning. As suggested in Section 5.3, there are practical applications of this work as well: student models of small-to-medium size and depth can be trained to mimic very large, high accuracy deep models, and ensembles of deep models, thus yielding  8   78 79 80 81 82 83 78 79 80 81 82 83Accuracy of Mimic Model on Dev SetAccuracy of Teacher Model on Dev SetMimic with 8k Non-Linear UnitsMimic with 160k Non-Linear Unitsy=x (no student-teacher gap)better accuracy with reduced runtime cost than is currently achievable without model compression. This approach allows one to adjust ﬂexibly the trade-off between accuracy and computational cost. In this paper we are able to demonstrate empirically that shallow models can, at least in principle, learn more accurate functions without a large increase in the number of parameters. The algorithm we use to do this—training the shallow model to mimic a more accurate deep model, however, is awkward. It depends on the availability of either a large unlabeled data set (to reduce the gap between teacher and mimic model) or a teacher model of very high accuracy, or both. Developing algorithms to train shallow models of high accuracy directly from the original data without going through the intermediate teacher model would, if possible, be a signiﬁcant contribution.  7 Conclusions  We demonstrate empirically that shallow neural nets can be trained to achieve performances pre- viously achievable only by deep models on the TIMIT phoneme recognition and CIFAR-10 image recognition tasks. Single-layer fully-connected feedforward nets trained to mimic deep models can perform similarly to well-engineered complex deep convolutional architectures. The results suggest that the strength of deep learning may arise in part from a good match between deep architectures and current training procedures, and that it may be possible to devise better learning algorithms to train more accurate shallow feed-forward nets. For a given number of parameters, depth may make learning easier, but may not always be essential.  Acknowledgements We thank Li Deng for generous help with TIMIT, Li Deng and Ossama Abdel- Hamid for code for the TIMIT convolutional model, Chris Burges, Li Deng, Ran Gilad-Bachrach, Tapas Kanungo and John Platt for discussion that signiﬁcantly improved this work, and Mike Ault- man for help with the GPU cluster.  References [1] Ossama Abdel-Hamid, Abdel-rahman Mohamed, Hui Jiang, and Gerald Penn. Applying con- volutional neural networks concepts to hybrid nn-hmm model for speech recognition. In Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on, pages 4277–4280. IEEE, 2012.  [2] Ossama Abdel-Hamid, Li Deng, and Dong Yu. Exploring convolutional neural network struc-  tures and optimization techniques for speech recognition. Interspeech 2013, 2013.  [3] Cristian Bucilu, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Pro- ceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535–541. ACM, 2006.  [4] George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of  control, signals and systems, 2(4):303–314, 1989.  [5] Yann N Dauphin and Yoshua Bengio. Big neural networks waste capacity. arXiv preprint  arXiv:1301.3583, 2013.  [6] Li Deng, Jinyu Li, Jui-Ting Huang, Kaisheng Yao, Dong Yu, Frank Seide, Michael Seltzer, Geoff Zweig, Xiaodong He, Jason Williams, et al. Recent advances in deep learning for speech research at microsoft. ICASSP 2013, 2013.  [7] David Eigen, Jason Rolfe, Rob Fergus, and Yann LeCun. Understanding deep architectures  using a recursive convolutional network. arXiv preprint arXiv:1312.1847, 2013.  [8] Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and Samy Bengio. Why does unsupervised pre-training help deep learning? The Journal of Machine Learning Research, 11:625–660, 2010.  [9] Ian Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout networks. In Proceedings of The 30th International Conference on Machine Learning, pages 1319–1327, 2013.  [10] G.E. Hinton and R.R. Salakhutdinov. Reducing the dimensionality of data with neural net-  works. Science, 313(5786):504–507, 2006.  9  [11] G.E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R.R. Salakhutdinov.  Im- proving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.  [12] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.  Computer Science Department, University of Toronto, Tech. Rep, 2009.  [13] K-F Lee and H-W Hon. Speaker-independent phone recognition using hidden markov models.  Acoustics, Speech and Signal Processing, IEEE Transactions on, 37(11):1641–1648, 1989.  [14] Abdel-rahman Mohamed, George E Dahl, and Geoffrey Hinton. Acoustic modeling using deep belief networks. Audio, Speech, and Language Processing, IEEE Transactions on, 20(1): 14–22, 2012.  [15] V. Nair and G.E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In Proc. 27th International Conference on Machine Learning, pages 807–814. Omnipress Madison, WI, 2010.  [16] Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Low-rank matrix factorization for deep neural network training with high-dimensional out- put targets. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 6655–6659. IEEE, 2013.  [17] Frank Seide, Gang Li, and Dong Yu. Conversational speech transcription using context-  dependent deep neural networks. In Interspeech, pages 437–440, 2011.  [18] Antonio Torralba, Robert Fergus, and William T Freeman. 80 million tiny images: A large data set for nonparametric object and scene recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 30(11):1958–1970, 2008.  [19] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.A. Manzagol. Stacked denoising autoen- coders: Learning useful representations in a deep network with a local denoising criterion. The Journal of Machine Learning Research, 11:3371–3408, 2010.  [20] Jian Xue, Jinyu Li, and Yifan Gong. Restructuring of deep neural network acoustic models  with singular value decomposition. Proc. Interspeech, Lyon, France, 2013.  [21] Matthew D Zeiler and Rob Fergus. Stochastic pooling for regularization of deep convolutional  neural networks. arXiv preprint arXiv:1301.3557, 2013.  10  ","Currently, deep neural networks are the state of the art on problems such asspeech recognition and computer vision. In this extended abstract, we show thatshallow feed-forward networks can learn the complex functions previouslylearned by deep nets and achieve accuracies previously only achievable withdeep models. Moreover, in some cases the shallow neural nets can learn thesedeep functions using a total number of parameters similar to the original deepmodel. We evaluate our method on the TIMIT phoneme recognition task and areable to train shallow fully-connected nets that perform similarly to complex,well-engineered, deep convolutional architectures. Our success in trainingshallow neural nets to mimic deeper models suggests that there probably existbetter algorithms for training shallow feed-forward nets than those currentlyavailable."
1312.5853,2014,Multi-GPU Training of ConvNets  ,"['Omry Yadan', 'Keith Adams', 'Yaniv Taigman', ""Marc'Aurelio Ranzato""]",https://arxiv.org/pdf/1312.5853.pdf,"Multi-GPU Training of ConvNets  Omry Yadan  Keith Adams  Yaniv Taigman  Marc’Aurelio Ranzato  {omry, kma, yaniv, ranzato}@fb.com  Facebook AI Group  Convolutional neural networks [3] have proven useful in many domains, including computer vi- sion [1, 4, 5], audio processing [6, 7] and natural language processing [8]. These powerful models come at great cost in training time, however. Currently, long training periods make experimentation difﬁcult and time consuming. In this work, we consider a standard architecture [1] trained on the Imagenet dataset [2] for classiﬁ- cation and investigate methods to speed convergence by parallelizing training across multiple GPUs. In this work, we used up to 4 NVIDIA TITAN GPUs with 6GB of RAM. While our experiments are performed on a single server, our GPUs have disjoint memory spaces, and just as in the distributed setting, communication overheads are an important consideration. Unlike previous work [9, 10, 11], we do not aim to improve the underlying optimization algorithm. Instead, we isolate the impact of parallelism, while using standard supervised back-propagation and synchronous mini-batch stochas- tic gradient descent. We consider two basic approaches: data and model parallelism [9]. In data parallelism, the mini- batch is split across several GPUs as shown in ﬁg. 3. Each GPU is responsible for computing gradients with respect to all model parameters, but does so using a subset of the samples in the mini-batch. This is the most straightforward parallelization method, but it requires considerable communication between GPUs, since each GPU must communicate both gradients and parameter values on every update step. Also, each GPU must use a large number of samples to effectively utilize the highly parallel device; thus, the mini-batch size effectively gets multiplied by the number of GPUs, hampering convergence. In our implementation, we ﬁnd a speed-up of 1.5 times moving from 1 to 2 GPUs in the data parallel framework (when using 2 GPUs, each gets assigned a mini- batch of size 128). This experiment used the same architecture, network set up and dataset described in Krizhevsky et al. [1]. Model parallelism, on the other hand, consists of splitting an individual network’s computation across multiple GPUs [9, 12]. An example is shown in ﬁg. 4. For instance, a convolutional layer with N ﬁlters can be run on two GPUs, each of which convolves its input with N/2 ﬁlters. In their seminal work, Krizhevsky et al. [1] further customized the architecture of the network to better leverage model parallelism: the architecture consists of two “columns” each allocated on one GPU.  4 1 0 2     b e F 8 1         ]  G L . s c [      4 v 3 5 8 5  .  2 1 3 1 : v i X r a  Table 1: Comparison of different parallelization schemes. All models use a mini-batch size equal to 256 sam- ples. The network is a convolutional network with the same structure and hyper-parameter setting as described by Krizhevsky et al. [1] (with the only exception of the mini-batch size). The task is classiﬁcation on the Im- ageNet 2012 datasset [2]. All GPUs are NVIDIA TITANs with 6GB of RAM and they all reside on the same server.  Time to complete 100 epochs  Conﬁguration  1 GPU  2 GPUs Model parallelism 2 GPUs Data parallelism 4 GPUs Data parallelism  4 GPUs model + data parallelism  10.5 days 6.6 days 7 days 7.2 days 4.8 days  1  Columns have cross connections only at one intermediate layer and at the very top fully connected layers. While model parallelism is more difﬁcult to implement, it has two potential advantages relative to data parallelism. First, it may requires less communication bandwidth when the cross connections involve small intermediate feature maps. Second, it allows the instantiation of models that are too big for a single GPUs memory. Our implementation of model parallelism, replicating prior work by Krizhevsky et al. [1], achieved a speed-up of 1.6 times moving from 1 to 2 GPUs. Data and model parallelism can also be hybridized, as shown in ﬁg. 5. We consider using 4 GPUs in three possible conﬁgurations: all model parallelism, all data parallelism, and a hybrid of model and data parallelism. In our experiment, we ﬁnd that the hybrid approach yields the fastest convergence. The hybrid approach on 4 GPUs achieves a speed-up of 2.2 times compared to 1 GPU. More detailed results are shown in tab. 1. The convergence curves comparing the most interesting conﬁgurations are shown in ﬁg. 1. In general, not all conﬁgurations could be explored. For instance, on a single NVIDIA TITAN GPU with 6GB of RAM we are unable to ﬁt mini-batches larger than 256 samples. On the other hand, we ﬁnd mini-batches of 64 or fewer samples under-utilize the GPUs cores. This can be seen in tab. 1 which reports the timing of the data parallelism approach using 4 GPUs. These preliminary results show promising speed-up factors by employing a hybrid parallelization strategy. In the future, we plan to extend this work to parallelization across servers by combining data and model parallelism with recent advances in asynchronous optimization methods and local learning algorithms.  Figure 1: Test error on the ImageNet dataset as a function of time using different forms of parallelism. On the y-axis we report the error rate of the test set, on the x-axis time in seconds. All experiments used the same mini-batch size (256) and ran for 100 epochs (here showing only the ﬁrst 10 for clarity of visualization) with the same architecture and the same hyper-parameter setting as in Krizhevsky et al. [1]. If plotted against number of weight updates, all these curves would almost perfectly coincide.  References [1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. ImageNet classiﬁcation with deep convolutional  neural networks. In NIPS, 2012.  [2] J. Deng, W. Dong, R. Socher, L.J. Li, K. Li, and L. Fei-Fei. Imagenet: a large-scale hierarchical image  database. In CVPR, 2009.  2  Figure 2: Diagram of a generic deep network. The number of arrows is proportional to the size of the mini- batch.  Figure 3: Diagram of a generic deep network using two GPUs (data parallelism). Each GPU computes errors and gradients for half of the samples in the mini-batch. Parameters and gradients are communicated across GPUs using PCI-e. The layers computed on a GPU all share the same color in the diagram.  [3] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.  Proceedings of the IEEE, 86(11):2278–2324, 1998.  [4] C. Szegedy, A. Toshev, and D. Erhan. Deep neural networks for object detection. In NIPS, 2013. [5] C. Farabet, L. Couprie, C. amd Najman, and Y. LeCun. Learning hierarchical features for scene labeling.  IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013.  [6] O. Abdel-Hamid, A.R. Mohamed, H. Jiang, and G. Penn. Applying convolutional neural networks con-  cepts to hybrid nn-hmm model for speech recognition. In ICASSP, 2012.  [7] T. Sainath, A.R. Kingsbury, Mohamed, G. Dahl, G. Saon, H. Soltau, T. Beran, A. Aravkin, and B. Ram-  abhadran. Improvements to deep convolutional neural networks for lvcsr. In ASRU, 2013.  [8] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language process-  ing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537, 2011.  [9] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. Le, M. Mao, M. Ranzato, A. Senior, P. Tucker,  K. Yang, and A. Ng. Large scale distributed deep networks. In NIPS, 2012.  [10] S. Zhang, C. Zhang, Z. You, R. Zheng, and B. Xu. Asynchronous stochastic gradient descent for dnn  training. In ICASSP, 2013.  [11] X. Chen, A. Eversole, G. Li, D. Yu, and F. Seide. Pipelined back-propagation for context-dependent deep  neural networks. In Interspeech, 2012.  [12] A. Coates, B. Huval, T. Wang, D.J. Wu, A.Y. Ng, and B. Catanzaro. Deep learning with cots hpc. In  ICML, 2013.  3  Figure 4: Diagram of a generic deep network using two GPUs (model parallelism). The architecture is split into two columns which makes easier to split computation across the two GPUs.  Figure 5: Example of how model and data parallelism can be combined in order to make effective use of 4 GPUs.  4  ",In this work we evaluate different approaches to parallelize computation ofconvolutional neural networks across several GPUs.
1312.6885,2014,Deep learning for class-generic object detection  ,"['Brody Huval', 'Adam Coates', 'Andrew Ng']",https://arxiv.org/pdf/1312.6885.pdf,"Deep learning for class-generic object detection  3 1 0 2   c e D 4 2         ]  V C . s c [      1 v 5 8 8 6  .  2 1 3 1 : v i X r a  Brody Huval Adam Coates Andrew Ng  Abstract  We investigate the use of deep neural networks for the novel task of class-generic object detec- tion. We show that neural networks originally designed for image recognition can be trained to detect objects within images, regardless of their class, including objects for which no bounding box labels have been provided. In addition, we show that bounding box labels yield a 1% per- formance increase on the ImageNet recognition challenge.  1. Introduction The task of separating objects from background is funda- mental for many computer vision tasks. This has led to much research on localizing and classifying objects by us- ing object segmentation, object detection, and region pro- posals. Currently, most detectors are trained individually for each object class, which requires a class label and a bounding box for all images. Unfortunately, in this ap- proach it is difﬁcult to transfer information from previously trained detectors to novel classes where bounding box la- bels may not be available. This situation is common in current datasets, which often have many class labels but incomplete bounding box labels. In this work, we aim to overcome these challenges by training separately from bounding box labels and class labels, enabling our system to learn even when only one of these labels is available. This approach harnesses the notion of object-ness (Endres & Hoiem, 2010; Alexe et al., 2012; Uijlings et al., 2013) to build a deep neural network (Krizhevsky et al., 2012) able to detect novel objects where bounding box labels have not been provided. One successful approach to object detection is to train a sin- gle detector for each class of objects (for example, the De- formable Parts Model (DPM) (Felzenszwalb et al., 2010)). In this approach, one discriminatively trains a set of detec-  Proceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copy- right 2014 by the author(s).  BRODYH@STANFORD.EDU ACOATES@STANFORD.EDU ANG@STANFORD.EDU  tors on each individual class. This strategy generally has proven useful on the Pascal VOC detection challenge due to the limited number of classes, each of which includes many bounding box labels. In other cases, however, where we may have an abundance of class labels, but few or no bounding box labels, it is not clear how to apply this same strategy. For example, the Image-Net dataset has 14 million class labels but only about 7% are labeled with bounding boxes (Deng et al., 2009). Recently, region proposal algorithms have shown good performance in object detection pipelines by proposing class-generic locations for further classiﬁcation (Endres & Hoiem, 2010; Alexe et al., 2012; Girshick et al., 2013; Ui- jlings et al., 2013). They attempt to measure object-ness within an image by training on all bounding boxes labels, regardless of class, in hopes of building a single detector for all classes. While training from only bounding box labels potentially enables a detector to locate novel classes never seen be- fore, it may perform poorly due to having too few train- ing examples and failing to exploit the wealth of class la- bels available in datasets like Image-Net. We propose to train a detector to localize objects while also exploiting ob- ject class labels by separating the recognition and detec- tion problems. We show that by pretraining our detector on class labels and then on object locations, we can increase its performance in detecting previously seen objects, while nearly retaining its ability to localize objects for which we have no bounding box labels.  2. Related works (Szegedy et al., 2013) have used a similar neural network for object detection in Pascal VOC. Like our approach, they avoided the use of sliding windows or region proposals, and instead directly used a deep neural network for predicting object locations. However, their work focuses on only a handful of classes from Pascal VOC, and ﬁve different net- works are trained for each class. In contrast, we train a single network able to provide class-generic object detec- tions. Region proposal algorithms are typically shallow methods  Deep Learning for Class-Generic Object Detection  that focus on high-recall object-ness detection (Endres & Hoiem, 2010; Alexe et al., 2012; Uijlings et al., 2013). Therefore, they return hundreds to thousands of potential bounding boxes for evaluation, which is still a large reduc- tion over the number of evaluations required for the sliding window approach. These potential locations are then input to a stronger classiﬁcation algorithm such as a deep Con- volutional Neural Network (CNN) to identify the object. In our work, we focus on high-precision detection, using non- max supression to reduce our predictions to a set of likely detected objects.  3. Object Detection from Neural Networks 3.1. Model  The Convolutional Neural Network (CNN) we use is simi- lar to that proposed by (Krizhevsky et al., 2012) for object classiﬁcation. The network consists of ﬁve layers of con- volution followed by two densely connected layers. Every layer applies a Rectiﬁed Linear Unit (ReLU) as its non- linearity. Only the ﬁrst, second, and ﬁfth layers use Local Contrast Normalization (LCN) and max pooling. The ﬁnal output is a 4096 dimensional feature vector for the image. See (Krizhevsky et al., 2012) for details.  3.2. Bounding Box Training  In the image classiﬁcation results from (Krizhevsky et al., 2012), the ﬁnal feature vector is input to a softmax layer which provides a probability distribution over class labels. In our work, we instead use a softmax layer to provide a probability distribution over a discretized space of bound- ing boxes. This 4-dimensional space encodes the x-y posi- tion, the scale, and the aspect ratio of a bounding box. Because the softmax layer treats every output label in- dependently, the network will receive the same loss for bounding boxes with high or low overlap with the ground truth, which is not ideal. This also leads to low counts of each label during training. To resolve this, instead of plac- ing a one or zero at each label, we place a Gaussian distri- bution centered at the correct label. The result is a smaller loss when a bounding box similar to the ground truth is pre- dicted. To allow multiple bounding boxes in each image, a Gaussian distribution is placed at each location and the labels are re-normalized to sum to one. During evaluation, multiple boxes are predicted by applying non-max suppres- sion to the resulting probability distribution over bounding boxes.  3.3. Classiﬁcation pretraining  When training a CNN for image recognition, the net- work is learning discriminative ﬁlters that help in detecting  Table 1: AUC after training without 100 classes bounding box classes.  pretrained random  AUC 0.475 0.448  the different classes, while ignoring potentially distracting generic backgrounds. As a result, the task of recognition and detection are related, and information from one task can improve results on the other. To implement this intu- ition, we optionally pretrain our network using the image recognition task. Our results will conﬁrm the usefulness of this intuition: Pretraining on image recognition turns out to increase performance on class-generic object detec- tion. Conversely, pretraining on object detection can in- crease image recognition performance.  4. Experiments To perform our experiments, we use multiple GPUs for training (Coates et al., 2013; Krizhevsky et al., 2012).  4.1. Dataset  All experiments were performed on the Imagenet 2012 Localization Challenge (Deng et al., 2009). This dataset provides 1.2 million classiﬁcation images with 592, 000 including bounding box labels over 1000 different cate- gories/synsets.  4.2. Evaluation  To show that our network is capable of detecting objects, despite never having seen their bounding boxes, we ran- domly chose 100 object classes out of the 1000 ImageNet Challenge classes and train without their bounding boxes. We then evaluate the performance on a validation set only containing the 100 object classes held out during train- ing. This was performed with one network starting from random weights, and another starting from a network pre- trained on image recognition for all 1000 object classes. Our results are shown in Table 1. To measure the drop in performance from not having the bounding boxes from 100 classes available, the performance for both networks, pretrained and random, are reported when trained on all 1000 bounding box classes, shown in Table 2. Precision recall curves are also shown for these four cases in Fig- ure 1. Examples of correct and incorrect detections from our network on held out bounding box classes are shown in Figure 2. Finally, we looked at the impact of training to detect objects (without the corresponding class labels) on image recogni-  Deep Learning for Class-Generic Object Detection  (a)  (b)  (c)  (d)  (e)  (f)  Figure 2: (a)-(c) Correct (d)-(f) Incorrect detections for classes without bounding box labels. True labels shown in green, predictions in blue.  Table 2: AUC after training on all bounding box labels.  pretrained random  AUC 0.545 0.498  Figure 1: Precision Recall curves.  tion performance. We found that by initializing an image recognition network with the weights from a trained object detection network, we reduce our top-5 error by 1%.  5. Conclusion By using a single deep neural network we have investigated a method for object-ness detection, capable of exploiting both class and bounding box labels. Our network is able to generalize to classes for which it has never seen bounding box labels while beneﬁtting from class labels when avail- able. In addition, we have also found that the object-ness detection task yields a modest improvement in the Ima- geNet recognition challenge, which does not involve de- tection.  References Alexe, Bogdan, Deselaers, Thomas, and Ferrari, Vittorio.  Measuring the objectness of image windows. 2012.  Coates, Adam, Huval, Brody, Wang, Tao, Wu, David, Catanzaro, Bryan, and Andrew, Ng. Deep learning with In Proceedings of the 30th Interna- cots hpc systems. tional Conference on Machine Learning (ICML-13), pp. 1337–1345, 2013.  Deng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai, and Fei-Fei, Li. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recog- nition, 2009. CVPR 2009. IEEE Conference on, pp. 248– 255. IEEE, 2009.  Endres, Ian and Hoiem, Derek. Category independent ob- In Computer Vision–ECCV 2010, pp.  ject proposals. 575–588. Springer, 2010.  Felzenszwalb, Pedro F, Girshick, Ross B, McAllester, David, and Ramanan, Deva. Object detection with dis- criminatively trained part-based models. Pattern Analy- sis and Machine Intelligence, IEEE Transactions on, 32 (9):1627–1645, 2010.  Girshick, Ross, Donahue, Jeff, Darrell, Trevor, and Ma- lik, Jitendra. Rich feature hierarchies for accurate ob- ject detection and semantic segmentation. arXiv preprint arXiv:1311.2524, 2013.  Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoff. Im- agenet classiﬁcation with deep convolutional neural net- In Advances in Neural Information Processing works. Systems 25, pp. 1106–1114, 2012.  Szegedy, Christian, Toshev, Alexander, and Erhan, Du- mitru. Deep neural networks for object detection. In Advances in Neural Information Processing Systems, pp. 2553–2561, 2013.  Uijlings, JRR, van de Sande, KEA, Gevers, T, and Smeul- ders, AWM. Selective search for object recognition. In- ternational Journal of Computer Vision, pp. 1–18, 2013.  0.00.20.40.60.81.0Recall0.00.20.40.60.81.0Precisionpretrained, all bounding boxesrandom, all bounding boxespretrained, without bounding boxesrandom, without bounding boxes","We investigate the use of deep neural networks for the novel task of classgeneric object detection. We show that neural networks originally designed forimage recognition can be trained to detect objects within images, regardless oftheir class, including objects for which no bounding box labels have beenprovided. In addition, we show that bounding box labels yield a 1% performanceincrease on the ImageNet recognition challenge."
1312.6186,2014,GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training  ,"['Hailin Jin', 'Thomas Huang', 'Zhe Lin', 'Jianchao Yang', 'Thomas Paine']",https://arxiv.org/pdf/1312.6186.pdf,"3 1 0 2   c e D 1 2         ]  V C . s c [      1 v 6 8 1 6  .  2 1 3 1 : v i X r a  GPU Asynchronous Stochastic Gradient Descent to  Speed Up Neural Network Training  University of Illinois at Urbana-Champaign  Tom Paine  Urbana, IL  paine1@illinois.edu  Hailin Jin  Adobe Research  San Jose, CA  Jianchao Yang Adobe Research  San Jose, CA  Zhe Lin  Adobe Research  San Jose, CA  University of Illinois at Urbana-Champaign  Thomas Huang  Urbana, IL  Abstract  The ability to train large-scale neural networks has resulted in state-of-the-art per- formance in many areas of computer vision. These results have largely come from computational break throughs of two forms: model parallelism, e.g. GPU acceler- ated training, which has seen quick adoption in computer vision circles, and data parallelism, e.g. A-SGD, whose large scale has been used mostly in industry. We report early experiments with a system that makes use of both model paral- lelism and data parallelism, we call GPU A-SGD. We show using GPU A-SGD it is possible to speed up training of large convolutional neural networks useful for computer vision. We believe GPU A-SGD will make it possible to train larger networks on larger training sets in a reasonable amount of time.  1  Introduction  Recently, large convolutional neural networks have achieved state-of-the-art results across many areas of computer vision including: character recognition [3], object recognition [8, 12, 13, 18], and object detection [10]. This is partly the result of larger datasets, e.g. the Imagenet Large Scale Visual Recognition Challenge (ILSVRC) [7] and accelerated training algorithms that can make use of the data. These approaches may be accelerated by using many CPUs [1, 6, 15, 19], or GPUs [2,3,5,12,16], and even many GPUs [4]. We believe accelerating training further will result in more break throughs in computer vision. We present experiments using a new system for accelerating neural network training, using asyn- chronous stochastic gradient descent (A-SGD) with many GPUs, which we call GPU A-SGD. We show that this system can be used to speed up training by several times, and explore how to best use GPU A-SGD to further speed up training. To benchmark our speed up, we use the pipeline found in [12]. We train a convolutional neural network on the ILSVRC 2012 dataset, which has 1000 classes, and 1.2 million images. Like that work, our network uses dropout [11], relu neurons [14], and is trained use data augmentation. We will ﬁrst review neural network training algorithms. And then highlight how our training algo- rithm differs from existing methods.  1  2 Training neural networks  A neural network can be seen as a large parameterized function. The parameters in this function can be learned through gradient descent style algorithms. In traditional gradient descent, the gradient of the objective function needs to be calculated over the entire dataset. The parameters are then updated with this gradient. This is repeated until convergence. There are two main issues with this approach: The dataset may be too large to ﬁt into memory, and the gradient may take too long to compute. When the dataset is too large, stochastic gradient descent (SGD) may be used. Here the gradient of the objective function is calculated over a small random partition of the dataset called a minibatch. The parameters are updated with this minibatch gradient, and a new minibatch is chosen. This process is repeated until convergence. This algorithm can be accelerated in two ways: speeding up the calculation of the minibatch gradient (model parallelism), and parallelization of the stochastic gradient descent steps (data parallelism).  2.1 Model parallelism  In many approaches, the structure of neural network computations is exploited to speed up the calculation of the minibatch gradient. This can be called model parallelism. This can be achieved using GPUs [2, 3, 5, 12, 16], distributed CPU approaches [6], or distributed GPU approaches [4]. The distributed approaches have the added beneﬁt that they can train models that are too big to ﬁt in memory on a single device. In many cases, these models ignore parallelization of SGD, with [6] being the notable exception. It’s DistBelief technique makes use of both model parallelism, and data parallelism, which we will talk about more below. One work [4] is similar to ours in that they experiment with many GPUs in a distributed framework to accelerate computation of very large models. Their work differs from ours because they pri- marily focus on model parallelism to train models too big to ﬁt on a single device, especially for unsupervised pre-training of locally-connected neural networks. They are able to train the bil- lion parameter model of [13], using a signiﬁcantly smaller number of nodes by leveraging consumer off-the-shelf GPUs and high-speed interconnect. While this line of research is very promising, these locally-connected, unsupervised models are not currently the top performing models on common computer vision benchmarks like ILSVRC. We believe our approach is complementary to theirs.  2.2 Data parallelism  Another method for speeding up training of neural networks is using distributed versions of stochas- tic gradient decent [1, 6, 15, 19]. These methods can be called data parallel because they speed up the rate as which the entire dataset contributes to the optimization. The data parallel part of the DistBelief [6] model, (A-SGD) is especially interesting, because it is essentially many neural network models training independently, and occasionally communicating with a central parameter server to synchronize the overall effect for many distributed gradient up- dates. This makes it straight-forward to apply with various model parallel approaches. This model has also proved useful for computer vision problems, achieving state-of-the-art performance on a computer vision benchmark with 14 million images1 [13]. While these methods may outperform single GPU based methods, by leveraging many more parameters, they operate at a very large scale (thousands of CPU cores).  2.3 Our contribution: GPU A-SGD  Our work also exploits both model parallelism, and data parallelism. We use GPUs for model parallelism, and A-SGD for data parallelism. A-SGD is a subset of the DistBelief system described in [6]. Our technique ignores their distributed CPU approach for model parallelism, and instead used GPUs to accelerate gradient computation. Multiple replicas of a model are used to optimize a single objective. Each model replica is trained using a GPU. This is achieved by extending the  1Imagenet Fall 2011 release, not to be confused with the ILSVRC 2012, which is a subset of the Fall release.  2  publicly available cuda-convnet code2 used in [12] to allow several GPU clients to communicate with a server. We use MPI for communication. Each model requests updated parameters every nf etch steps, and sends updated gradient values every npush steps. In the DistBelief paper [6] nf etch = npush = 1. This regime would not work well for GPUs, where the gradients are not usually communicated to the CPU after every minibatch. Typically the parameters are updated on the GPU for nsync steps before being copied to the CPU, where nsync can be large, e.g. 600. This is because there is additional overhead cost for transferring the parameters from the GPU to the CPU. This overhead can reduce the beneﬁt for GPU accelerate gradient calculations. In our experiments we set nf etch = npush = nsync. We experiment with different values of nsync.  3 Experiments  To test GPU A-SGD, we train a convolutional neural network with the same architecture as described in [12] on the ILSVRC 2012 dataset. On a single NVIDIA Tesla K20X GPU this takes about 10.7 days. We performed all experiments on the Blue Water supercomputer. It has over 4000 Nvidia Tesla K20X nodes, and has a Gemini high-speed interconnect. While we make use of a very high- performance machine, [4] notes that GPUs and high speed interconnect are now available off-the- shelf. All of our experiments are performed with 32 or less GPU nodes.  Figure 1: Train set and test set error. Note that test set error reaches an average low around 45% by 22 epochs.  3.1 Experiment 1  Our ﬁrst experiment is to test whether we can achieve similar performance to [12] with GPU A-SGD. We used the same settings we used in the single GPU cases, with nsync = 600. For this experiment we use 8 GPU clients. The resulting learning curves are shown in (ﬁg .1). We get near state of the art performance by epoch 22 which takes 3.3 days, before overﬁtting. This is about a 3.2x speed up. In our experience, the minibatch test set performance is usually 2-3% higher than the overall test set performance after averaging 10 crops as in [12]. That is true here, the checkpoint before over-ﬁtting gets a test error of 42.2%. For the next experiments we want to compare the speed up using varying numbers of GPU clients and varying values of nsync. Since, it is hard to interpret many raw learning curves on a single plot, we smooth each plot using a sliding window of 400 mini batches. Also, we plot only the training error, so that the sliding window doesn’t need to be adjusted for different values of nsync. Since the  2The original cuda-convnet code is available at: https://code.google.com/p/cuda-convnet/  3  Figure 2: Training error with a cold start. Notice early on training with 16 and 32 clients is much slower. Also notice that latter on, the 32 client GPU A-SGU model has the steepest learning curve.  training and testing error are very similar for the early training period we observe, we feel this is indicative of performance.  3.2 Experiment 2  In our second experiment, we examined the effect of a cold start on the learning, as the number of GPU clients increases from 2 to 32 (ﬁg. 2). Each GPU A-SGD instance is run for 24 hours. We observe that as the number of GPUs increase, initial learning becomes much slower. We also observe that later in training, GPU A-SGD instances with more GPU clients learn more rapidly. We hypothesize that early in training, there are many gradient directions that may decrease error. Since each GPU client calculates different gradients, averaging them may slow progress. Later in training gradients become more consistent and averaging them increases the speed of learning. This result suggests that a warm start may be beneﬁcial as suggested in [6]. This may also be improved by methods that explicitly deal with variance in gradients such as adagrad [9] and adadelta [17].  3.3 Experiment 3  In our third experiment, we explore how nsync effects learning with many GPU clients. We try nsync values from 100 to 900 and 1-8 GPU clients (ﬁg. 3). We begin all experiments from a warm start, which we obtained by training the network on a single GPU for 12 hours. With a warm start, the effect of many GPU clients is clearer. When nsync = 100, our error decreases from 70% with a single GPU to 58% with 8 GPUs. Note that as nsync increases, the error curve has jagged artifacts. We believe these are from stale updates. Also note that when nsync = 100, signiﬁcantly fewer minibatches are processed in 24 hours, but the error rate is still lower. This suggests that while there is a cost associated with increased update frequency, it may still be a net win. To emphasize these observations, we plot the learning curves for 8 GPU clients with nsync values from 100 to 900 (ﬁg. 4).  4 Future directions  We plan to explore Adagrad [9] and Adadelta [17] to see if they can further boost performance. We believe GPU A-SGD is a promising direction. Recently [18] showed that larger models can further improve performance on computer vision tasks, and that these larger models begin to over  4  01000020000300004000050000Minibatches0.550.600.650.700.750.800.850.900.951.00Training errorCold startGPU clients: 2GPU clients: 4GPU clients: 8GPU clients: 16GPU clients: 32Figure 3: Training error with a warm start. Increasing the number of GPU client shows a signiﬁcant speed up, across all values of nsync. Note: for nsync = 300, the experiment for GPU cients=2 failed to run in time for this publication and it not included.  Figure 4: Training error with a warm start, using 8 GPU clients. Notice that between nsync = 900 and nsync = 100 there is about a 4% difference in training error.  ﬁt, suggesting they would beneﬁt from more training data. Both larger models, and larger training sets would beneﬁt from faster training times.  5  nsyncnsyncnsyncnsyncsyncsyncsyncsyncAcknowledgments  This material is based upon work supported by the National Science Foundation under Grant No. 392 NSF IIS 13-18971. This research is part of the Blue Waters sustained-petascale computing project, which is supported by the Na- tional Science Foundation (award number OCI 07-25070) and the state of Illinois. Blue Waters is a joint effort of the University of Illinois at Urbana-Champaign and its National Center for Supercomputing Applications.  References [1] A. Agarwal and J. C. Duchi. Distributed delayed stochastic optimization. In Decision and Control (CDC),  2012 IEEE 51st Annual Conference on, pages 5451–5452. IEEE, 2012.  [2] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley, In Proceedings of the Python for  and Y. Bengio. Theano: a cpu and gpu math expression compiler. Scientiﬁc Computing Conference (SciPy), volume 4, 2010.  [3] D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classiﬁcation. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 3642–3649. IEEE, 2012.  [4] A. Coates, B. Huval, T. Wang, D. Wu, B. Catanzaro, and N. Andrew. Deep learning with cots hpc systems. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 1337–1345, 2013.  [5] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A matlab-like environment for machine learning.  In BigLearn, NIPS Workshop, 2011.  [6] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. Le, M. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, and A. Ng. Large scale distributed deep networks. In P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1232–1240. 2012.  [7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248–255. IEEE, 2009.  [8] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolu-  tional activation feature for generic visual recognition. arXiv preprint arXiv:1310.1531, 2013.  [9] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic  optimization. The Journal of Machine Learning Research, 999999:2121–2159, 2011.  [10] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection  and semantic segmentation. arXiv preprint arXiv:1311.2524, 2013.  [11] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov.  Improving neural  networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.  [12] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deep convolutional neural net-  works. In Advances in Neural Information Processing Systems 25, pages 1106–1114, 2012.  [13] Q. V. Le, M. Ranzato, R. Monga, M. Devin, K. Chen, G. S. Corrado, J. Dean, and A. Y. Ng. Building  high-level features using large scale unsupervised learning. arXiv preprint arXiv:1112.6209, 2011.  [14] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In Proceedings  of the 27th International Conference on Machine Learning (ICML-10), pages 807–814, 2010.  [15] F. Niu, B. Recht, C. R´e, and S. J. Wright. Hogwild!: A lock-free approach to parallelizing stochastic  gradient descent. arXiv preprint arXiv:1106.5730, 2011.  [16] R. Raina, A. Madhavan, and A. Y. Ng. Large-scale deep unsupervised learning using graphics processors.  In ICML, volume 9, pages 873–880, 2009.  [17] M. D. Zeiler. Adadelta: An adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012. [18] M. D. Zeiler and R. Fergus. Visualizing and Understanding Convolutional Networks. ArXiv e-prints,  Nov. 2013.  [19] M. Zinkevich, M. Weimer, L. Li, and A. J. Smola. Parallelized stochastic gradient descent. In Advances  in Neural Information Processing Systems, pages 2595–2603, 2010.  6  ","The ability to train large-scale neural networks has resulted instate-of-the-art performance in many areas of computer vision. These resultshave largely come from computational break throughs of two forms: modelparallelism, e.g. GPU accelerated training, which has seen quick adoption incomputer vision circles, and data parallelism, e.g. A-SGD, whose large scalehas been used mostly in industry. We report early experiments with a systemthat makes use of both model parallelism and data parallelism, we call GPUA-SGD. We show using GPU A-SGD it is possible to speed up training of largeconvolutional neural networks useful for computer vision. We believe GPU A-SGDwill make it possible to train larger networks on larger training sets in areasonable amount of time."
1312.6205,2014,Relaxations for inference in restricted Boltzmann machines  ,"['Sida I. Wang', 'Roy Frostig', 'Percy Liang', 'Christopher D. Manning']",https://arxiv.org/pdf/1312.6205.pdf,"4 1 0 2     n a J    2      ] L M  . t a t s [      2 v 5 0 2 6  .  2 1 3 1 : v i X r a  Relaxations for inference in restricted Boltzmann machines  Sida Wang∗ Roy Frostig∗ Percy Liang Christopher D. Manning Computer Science Department, Stanford University, Stanford, CA 94305, USA  sidaw@cs.stanford.edu rf@cs.stanford.edu pliang@cs.stanford.edu manning@stanford.edu  Abstract  We propose a randomized relax-and-round inference algorithm that samples near-MAP conﬁgurations of a binary pairwise Markov random ﬁeld. We experiment on MAP in- ference tasks in several restricted Boltzmann machines. We also use our underlying sam- pler to estimate the log-partition function of restricted Boltzmann machines and compare against other sampling-based methods.  1. Background and setup  A binary pairwise Markov random ﬁeld (MRF) over n variables x ∈ {0, 1}n models a probability distribution p ˜A(x) ∝ exp(xT ˜Ax). The non-diagonal entries of the matrix ˜A ∈ Rn×n encode pairwise potentials between variables while its diagonal entries encode unary po- tentials. The exponentiated linear term xT ˜Ax is the negative energy or simply the score of the MRF. A restricted Boltzmann machine (RBM) is a particular MRF whose variables are split into two classes, visible and hidden, and in which intra-class pairwise poten- tials are disallowed.  Notation We write Symn for the set of symmetric n × n real matrices, and S k to denote the unit sphere {x ∈ Rk : (cid:107)x(cid:107)2 = 1}. All vectors are columns unless stated otherwise.  1.1. Integer quadratic programming  Finding the maximum a posteriori (MAP) value of a discrete pairwise MRF can be cast as an integer quadratic program (IQP) given by  max  x∈{−1,1}n  xTAx  (1)  Note that we have the domain constraint x ∈ {−1, 1}n rather than {0, 1}n. We relate the two in Section 2.3.  2. Relaxations  Solving (1) is NP-hard in general. In fact, the MAX- CUT problem is a special case. Even the cases where A encodes an RBM are NP-hard in general (Alon & Naor, 2006). We can trade oﬀ exactness for eﬃciency and instead optimize a relaxed (indeﬁnite) quadratic program:  max  x∈[−1,1]n  xTAx  (2)  Such a relaxation is tight for positive semideﬁnite A: global optima of the QP and the IQP have equal ob- jective values.1 Therefore (2) is just hard in general as (1), even though it aﬀords optimization by gradient- based methods in place of combinatorial search.  The following semideﬁnite program (SDP) is a looser relaxation of (1) obtained by extending x to higher ambient dimension:  max S∈Symn subject to S (cid:23) 0, diag(S) ≤ 1  tr(AS)  (3)  This relaxation dates back at least to Goemans & Williamson (1995), who use it to give the ﬁrst better- than- 1  2 approximation of MAX-CUT.  Note that, from the problem constraints, if S is feasible for (3) then it must also have a factorization S = XX T where X ∈ Rn×n and the rows of X have Euclidean norm at most 1. Indeed, (3) is a relaxation: we can rewrite the objective as tr(AXX T) = tr(X TAX), then notice that (1) corresponds to a special case where the ﬁrst column of X is in {−1, 1}n and all other entries of X are zero.  International Conference on Learning Representations, Banﬀ, Canada, 2014. ∗Authors contributed equally.  1We can always ensure tightness when A is not PSD, as  in Ravikumar & Laﬀerty (2006).  Relaxations for inference in restricted Boltzmann machines  2.1. Rounding  Given such S = XX T, we round it to a point x fea- sible for the original IQP (1) by drawing a vector g uniformly at random from the unit sphere, projecting the rows of X onto g, and rounding entrywise. For- i g) for i ∈ {1, . . . , n}. Prior mally, we let xi = sign(X T theoretical work (Bri¨et et al., 2010; Nesterov, 1998) shows that when A is positive semideﬁnite this round- ing is not too lossy in expectation. Namely, we have E[xTAx] ≥ 2  π tr(X TAX).  2.2. Low-rank relaxations  The SDP relaxation (3) is appealing primarily because it is a convex optimization problem. Convexity, how- ever, comes at the cost of a loose relaxion, and with it a rounding error that may still be too lossy in practice. What’s more, though convexity begets computational ease in a theoretical sense, the number of variables in the SDP is quadratic in n, whereas in the QP relax- ation (2) it is linear. Even in simple benchmark ap- plications such as modeling the MNIST dataset with an RBM (n ≈ 1.3K), solving a semideﬁnite program of such size takes hours on modern hardware.  We hence interpolate between the QP and SDP re- laxations with a sequence of optimization problems of intermediate size: Deﬁnition 2.1. Let k ∈ {1, . . . , n}. Denote by LRPk the optimization problem:  tr(X TAX)  max X∈Rn×k subject to (cid:107)Xi(cid:107)2 ≤ 1, i = 1, . . . , n  .  (4)  We call k the width of this optimization problem.  Note that LRP1 is equivalent to the QP (2), and that LRPn corresponds to the SDP (3) subject to reparam- eterization by S = XX T. The LRPk objective is gen- erally non-convex; in experiments we typically seek a stationary (locally optimal) point by projected gradi- ent descent. Extensive properties of LRPk are studied in Burer & Monteiro (2005).  2.3. Hypercube constraint reductions  Much of the existing literature considers RBMs over the domain x ∈ {0, 1}n instead of x ∈ {−1, 1}n (Hinton, 2010; Salakhutdinov & Murray, 2008). The two are essentially equivalent under a linear change of variables. Given an IQP as in (1) with objective xTAx over x ∈ {0, 1}n, we can equivalently optimize 2 (˜x + 1)] over ˜x ∈ {−1, 1}n. Conversely, 2 (˜x + 1)]TA[ 1 [ 1 in place of the objective ˜xTA˜x over ˜x ∈ {−1, 1}n, we can optimize (2x − 1)TA(2x − 1) for x ∈ {0, 1}n.  These reductions introduce cross-terms — a linear term (of the form bTx for b ∈ Rn) and a constant term (of the form c ∈ R). For instance, when going from the {0, 1} domain to the {−1, 1} domain, we collect terms:  b =  c =  1 4 1 4  (1TA + A1)  1TA1  (5)  (6)  We may ignore c as it is an additive constant that does not aﬀect optimization. When optimizing over x ∈ {0, 1}n, b can be folded into A in a new matrix  A + diag(b).  (7) When optimizing over x ∈ {−1, 1}n, we can similarly fold b into A by introducing a single auxiliary variable  and augmenting A to(cid:20) 0  (cid:21)  1  2 bT 1 2 b A  .  (8)  A caveat of these reductions is that the objective cross terms (5) that they introduce behave as unary coef- ﬁcients proportional to the sum of rows and columns of A. Empirically, we found that these terms, when large in magnitude, can dominate the objective and reduce the quality of rounded solutions to the original (unreduced) problem.  3. Sampling  For a single relaxed solution X, many samples can be produced by randomized rounding. This yields the randomized relax-and-round (rrr-MAP) algorithm, summarized in Algorithm 1. Given the solution X whose rows are Xi, we have a rounding distribution pX (x) over the corners of the hypercube x ∈ {−1, 1}n with a geometric interpretation as follows. Every vec- tor Xi implicitly deﬁnes a halfspace (points z such i z ≥ 0). A sign vector x ∈ {−1, 1}n describes that X T a volume of points lying within (if xi = 1) or without (if xi = −1) each halfspace, and pX (x) is the propor- tion of the unit sphere boundary that intersects this volume. Formally,  pX (x) = Vol({z ∈ S k : x ◦ Xz ≥ 0})/ Vol(S k). where ◦ denotes Hadamard (entrywise) product. As shown in Figure 1, this sampler produces lower- energy samples than a mixed Gibbs sampler in both the MNIST and random parameter settings.  (9)  Relaxations for inference in restricted Boltzmann machines  Input : Binary pairwise MRF parameters A Output: Samples {x(t)}T near maxx pA(x)  t=1 such that pA(x(t)) is  Take X by optimizing LRPk under A for t ← 1 to T do g ← random vector from unit sphere S k x(t) ← sign(Xg)  end  Algorithm 1: Randomized relax-and-round MAP sampler (rrr-MAP).  Figure 1. Empirical densities obtained by the rrr-MAP sampler (Algorithm 1) and Gibbs sampling from two dif- ferent RBMs. In both cases, 10,000 samples are drawn. Top: the MNIST-trained RBM of Salakhutdinov & Mur- ray (2008). Bottom: a random RBM, with parameters sampled independently from a standard Gaussian.  4. Experiments  Although our techniques are intended for general use with MRFs, we focus entirely on RBMs in exper- iments. Doing so is motivated by special interest (largely owed to uses in feature learning) and for sake of comparison with other sampling-based infer- ence techniques. The bipartite architecture of RBMs also nicely accommodates Gibbs-based samplers. In- deed, the RBM setting proves to be challenging for relaxation and rounding to do better than Gibbs vari- ants. An RBM with visible variables v ∈ {−1, 1}m and hid- den variables h ∈ {−1, 1}p ﬁts into the above MRF framework by taking n = m + p and x = (v, h) ∈  {−1, 1}n. Suppose the RBM score is vTW h + aTv + bTh.  (10)  In MRF notation, we would add a single auxiliary vari- able and take the augmented parameter matrix A as per (8):  0 aT  bT 0 W T a b W 0   .  A =  1 2  (11)  log(cid:80)  Our experiments focus on two inference scenar- ios: (a) approximately and eﬃciently computing the MAP, and (b) estimating the log-partition function v,h pA(v, h). The latter is discussed and moti- vated by Salakhutdinov & Murray (2008), and is gen- erally interesting as it captures the essential theoretical hardness of RBM inference (Long & Servedio, 2010).  4.1. MAP inference  In these benchmarks we attempt to ﬁnd a low-energy conﬁguration x. We run Algorithm 1 to obtain many samples and output the best among them. We com- pare to an annealed Gibbs sampling procedure and to an oﬀ-the-shelf IQP solver (Gurobi) that directly op- timizes (1).2  We compare techniques across the following three RBM instances. Results appear in Table 1, and Fig- ure 2 illustrates convergence.  the {0, 1}n domain,  • MNIST. We downloaded the weights for an RBM over trained by Salakhutdinov & Murray (2008) to model the MNIST dataset distribution. The original model has W ∈ R784×500 and we reduced it to an RBM over the {−1, 1}n domain as per Section 2.3.  • Random. We populate W ∈ R784×500, a, and b with independently random entries sampled from a standard Gaussian.  • Hard. We begin with the same type of random instance, then randomly select three pairs of vari- ables, each of the form (vi, hj) — i.e. one visible and one hidden. We modify Wi,j to be very large (namely, 5000) and take ai and bj to be an order of magnitude smaller (i.e. 500). This construction is intended to impede a Gibbs sampler by intro- ducing local energy minima. If we initialize such a pair at vi = hj = −1, then a Gibbs procedure is discouraged from ever ﬂipping the value of either vi or hj conditioned on the other being −1.  2http://www.gurobi.com/.  5010015020025030035000.0050.010.0150.02density  15020025030035040000.020.04density  rrr−MAPGibbsrrr−MAPGibbsRelaxations for inference in restricted Boltzmann machines  rrr  MNIST 340.29 Random 22309 Hard 40037  AG rrr-AG 377.39 23358 41016  377.47 22175 36236  Gu  319.34 12939 23347  MNIST Random-S Random-L  True - 5127.6 -  AIS 436.37 5127.5 9750.5  rrr-low rrr-IS 438.40 5092.4 9606.7  436.69 5095.7 9547.7  Table 1. RBM scores found by diﬀerent methods: rrr is the rrr-MAP sampler (Algorithm 1); AG is an annealed Gibbs procedure with a linear temperature schedule; rrr- AG is the annealed Gibbs procedure initialized at samples obtained from rrr-MAP; Gu is the Gurobi IQP solver. Ex- ecutions of rrr-MAP use LRP2 as the initial relaxation (i.e. width k = 2). Gurobi is given an execution time limit that is 10x that of rrr.  Table 2. Estimates of the RBM log-partition function log Z(A): True is the true value, when available; AIS is estimation by annealed importance sampling; rrr-low is a lower bound provided by the log-sum-exp of the energy of 10K conﬁgurations obtained by the rrr-MAP sampler; rrr-IS is estimation by importance sampling using the rrr- MAP sampler as a proposal distribution. Random-S indi- cates small W ∈ R784×15. Random-L indicates W of the same size as MNIST (784 × 500). In these trials, AIS was run for just under twice the amount of time as rrr-low.  We compare the following three estimation techniques. Results are shown in Table 2.  • Annealed importance sampling. The proce-  dure of Salakhutdinov & Murray (2008).  • rrr-MAP sampling.  log((cid:80)  samples  {x(t)} t exp(x(t)T  are  taken,  Ax(t))) is reported.  10,000  rrr-MAP and the value This is  a lower bound on the true value of log Z(A).  (cid:88)  Figure 2. RBM score measured across procedure steps. The steps are qualitatively comparable: Gibbs requires a matrix-vector multiplication at every step, and LRPk requires a gradient update (dominated by matrix-vector multiplication) and projection onto the L2 ball (i.e. vector normalization). Black curves show the relaxed objective and the value of the best rounded sample out of a thou- sand. Blue and red curves show annealed Gibbs, where red is annealing starting from a 10x higher temperature.  4.2. Estimating the log-partition function  The goal of these trials is to estimate  log Z(A) = log  exp(vTW h + aTv + bTh).  (12)  v,h  The true log-partition function of large RBM instances (e.g. MNIST) is typically unknown, so we also compare results across small instances, where the true value of log Z(A) can be computed via exhaustive enumeration.  Throughout these benchmarks, we take advantage of the bipartite property of RBMs to perform an analytic summation over one class of variables. For instance, for a ﬁxed v, we can analytically sum out h in linear time:  (cid:89)  (cid:0)1 + exp(vTWi + bi)(cid:1) . (13)  Z(A) =  exp(aTv)  (cid:88)  v  i  • rrr-MAP importance sampling. Importance sampling using rrr-MAP sampler as a proposal distribution. 10,000 rrr-MAP samples are taken and weighted by 1/pX (x) (as in (9)) to approxi- mate Z. That is, we compute  (cid:20) exp(xTAx)  (cid:21)  pX (x)  Z(A) ≈ Ex∼pX  (14)  by estimating the right hand side with an empir- ical mean. Note that (14) is indeed a rough ap- proximation as pX has support that, for smaller k, is very sparse in {−1, 1}n. Using k = 2, pX (x) can be computed in time O(n) after a single O(n log n) preprocessing step.3  It is expected that sampling near the MAP (as in rrr- MAP) would help in this estimation task whenever log Z(A) is dominated by a few near-MAP samples. As results show, this is perhaps a poor assumption, and further research is needed to make rrr-MAP sampling useful to this end. The method does, however, remain simple to implement, relatively eﬃcient, and overall still comparable in estimation quality.  3This procedure is similar to the Graham scan. The preprocessing step sorts the row vectors of X by increasing angle. Then pX (x) is computed for any x by considering the row vectors in order, seeking the two consecutive vec- tors that support the cone {z : x ◦ Xz ≥ 0}. The angle between these two vectors, normalized by 2π, is pX (x).  2040608010012014033.23.43.63.844.24.44.64.85x 104# stepsNegative energy  RelaxationRoundedAGAG−hitempRelaxations for inference in restricted Boltzmann machines  Salakhutdinov, R. and Murray, I. On the quantitative analysis of deep belief networks. In Proceedings of the 25th international conference on Machine learn- ing, pp. 872–879, 2008.  5. Conclusion  We described an approximate MRF inference tech- nique based on relaxation and randomized rounding, and showed that in the RBM setting it fares compara- bly to more common sampling-based methods. When seeking approximate MAP conﬁgurations, it succeeds in settings where annealed Gibbs is impeded by local optima. We showed that rrr-MAP solutions can be used to initialize local search algorithms to yield bet- ter results than either technique ﬁnds alone.  The rrr-MAP algorithm is just as applicable more gen- erally in MRFs, where Gibbs sampling is less eﬃcient than it is in the bipartite (RBM) case. This general setting and its surrounding theory are examined in on- going work.  References  Alon, N. and Naor, A. Approximating the cut-norm SIAM Journal on  via grothendieck’s inequality. Computing, 35(4):787–803, 2006.  Bri¨et, J., de Oliveira Filho, F. M., and Vallentin, F. The positive semideﬁnite grothendieck problem with rank constraint. In Automata, Languages and Pro- gramming, pp. 31–42. Springer, 2010.  Burer, S. and Monteiro, R. Local minima and conver- gence in low-rank semideﬁnite programming. Math- ematical Programming, 103(3):427–444, 2005.  Goemans, M. and Williamson, D. Improved approxi- mation algorithms for maximum cut and satisﬁabil- ity problems using semideﬁnite programming. Jour- nal of the ACM (JACM), 42(6):1115–1145, 1995.  Hinton, G. A practical guide to training restricted boltzmann machines. Technical report, University of Toronto, 2010.  Long, P. and Servedio, R. Restricted boltzmann ma- chines are hard to approximately evaluate or simu- late. In Proceedings of the 27th International Con- ference on Machine Learning, pp. 703–710, 2010.  Nesterov, Y. Semideﬁnite relaxation and nonconvex quadratic optimization. Optimization methods and software, 9(1-3):141–160, 1998.  Ravikumar, P. and Laﬀerty, J. Quadratic program- ming relaxations for metric labeling and markov random ﬁeld map estimation. In Proceedings of the 23rd international conference on Machine learning, pp. 737–744, 2006.  ",We propose a relaxation-based approximate inference algorithm that samplesnear-MAP configurations of a binary pairwise Markov random field. We experimenton MAP inference tasks in several restricted Boltzmann machines. We also useour underlying sampler to estimate the log-partition function of restrictedBoltzmann machines and compare against other sampling-based methods.
1312.6024,2014,Occupancy Detection in Vehicles Using Fisher Vector Image Representation  ,"['Yusuf Artan', 'Peter Paul']",https://arxiv.org/pdf/1312.6024.pdf,"3 1 0 2   c e D 0 2         ]  V C . s c [      1 v 4 2 0 6  .  2 1 3 1 : v i X r a  Occupancy Detection in Vehicles Using Fisher Vector  Image Representation  Yusuf Artan  Xerox Research Center  Webster, NY 14580  Yusuf.Artan@xerox.com  Peter Paul  Xerox Research Center  Webster, NY 14580  Peter.Paul@xerox.com  Abstract  Due to the high volume of trafﬁc on modern roadways, transportation agencies have proposed High Occupancy Vehicle (HOV) lanes and High Occupancy Tolling (HOT) lanes to promote car pooling. However, enforcement of the rules of these lanes is currently performed by roadside enforcement ofﬁcers using visual obser- vation. Manual roadside enforcement is known to be inefﬁcient, costly, potentially dangerous, and ultimately ineffective. Violation rates up to 50%− 80% have been reported, while manual enforcement rates of less than 10% are typical. Therefore, there is a need for automated vehicle occupancy detection to support HOV/HOT lane enforcement. A key component of determining vehicle occupancy is to deter- mine whether or not the vehicle’s front passenger seat is occupied. In this paper, we examine two methods of determining vehicle front seat occupancy using a near infrared (NIR) camera system pointed at the vehicle’s front windshield. The ﬁrst method examines a state-of-the-art deformable part model (DPM) based face detection system that is robust to facial pose. The second method examines state- of-the-art local aggregation based image classiﬁcation using bag-of-visual-words (BOW) and Fisher vectors (FV). A dataset of 3000 images was collected on a pub- lic roadway and is used to perform the comparison. From these experiments it is clear that the image classiﬁcation approach is superior for this problem.  1  Introduction  Intelligent transportation systems (ITS) improve safety and mobility through the integration of sens- ing, computational power, and advanced communications into the transportation infrastructure [1]. Managed lanes, enabled by ITS, combine trafﬁc management, tolling, transit, and carpooling in a multi-purpose roadway, creating new options for agencies and the traveling public, and use con- gestion pricing to more efﬁciently manage demand and generate revenue. High Occupancy Vehicle (HOV) lanes are standard car-pool lanes where either two (2+) or three (3+) vehicle occupants are required in order to use the lane. Due to these requirements, the HOV lanes are typically less con- gested and allow a vehicle to make its journey more rapidly [22]. High Occupancy Tolling (HOT) lanes are a form of managed lanes where single occupant vehicles are allowed to use the HOV lane upon payment of a toll. Typically the toll is dynamically adjusted in real-time to maintain a mini- mum speed on the roadway - often the minimum speed is 45 miles per hour. If the average trafﬁc speed on the HOT lane starts to become lower than 45 mph, the toll prices increases to discourage additional vehicles from entering the HOT lane. However, to realize the congestion reducing beneﬁt  1  (A)  (B)  Figure 1: Overview of the occupancy image database. Part (A) is the passenger side images captured using NIR camera system. Part (B) shows the empty seat images.  of HOV/HOT lanes, the rules of the HOV/HOT lane need to be enforced. To enforce the rules of these lanes, current practice requires dispatching law enforcement ofﬁcers at the roadside to visu- ally examine incoming cars. This method is expensive, difﬁcult, and ultimately ineffective. Typical violation rates can exceed 50%-80%, while manual enforcement rates are typically less than 10% [2]. Currently, transportation authorities are seeking an automatic or semi-automatic approach to replace and/or augment this manual enforcement. Any automatic system would require very high accuracies. A typical automatic license plate reading system, for example, has over 99.9% accuracy while making a decision on over 80% of the vehicles. Image based vehicle occupancy detection sys- tems have been examined by past researchers [21], [20], [22], [3], [18]. For automated occupancy detection, face detection was used in these earlier studies. More recently, studies have proposed methods that extract certain features to count passengers including faces, seats, seat belts, or skin that are visible to the camera to distinguish driver + passenger vs. driver only [19]. In this paper, we examine front seat vehicle occupancy detection for use as part of an automatic or semi-automatic HOV/HOT lane enforcement system. Other parts of the HOV/HOT lane enforce- ment system would include window/windshield locators and rear seat detection. However, in this work we focus on front passenger detection. An empty front passenger seat may indicate a ’candi- date violator’ such that the imagery is further reviewed and processed to determine its ﬁnal violator status. Our experimental setup includes a camera based imaging system to capture still images in the near-infrared (NIR) band through the windshield of an approaching vehicle. NIR imaging is commonly used in transportation imaging systems so that the illumination does not distract drivers. Two detection approaches are examined in this work. As in past studies, the ﬁrst approach uses face detection methods to perform front seat occupancy detection. The second approach uses a machine- learning based classiﬁer that detects front seat occupancy by using a global image representation of the captured images. In [4], authors developed a model that is a novel and simple approach to encode the elastic deformation and 3D structure of an object for face detection and pose estimation. It uses mixtures of trees (poses) with a shared pool of parts deﬁned at each landmark positions. It then uses global mixtures to model topological changes due to view points. That study showed state-of-the-art results for faces larger than 80 × 80 pixels. In this paper, we use this deformable part models to detect front seat passenger faces and compare results to image classiﬁcation based methods. A popular image classiﬁcation method is to describe images with bag-of-visual-words (BoW) his- tograms and to classify them using non-linear support vector machines (SVMs) [5]. In the BoW representation, dense local descriptors (such as SIFT [14]) are extracted from the image and each descriptor is assigned to its closest visual word in a visual codebook obtained by clustering a large set of descriptors with k-means [15], [13]. In recent studies, Fisher vectors (FV) have shown to yield better performance in image categorization tasks than BoW [7], [9]. Therefore, we utilize FV image representation to perform image classiﬁcation in front seat occupancy detection task.  2  The organization of this article as follows. In Section 3, we brieﬂy describe the details of the de- formable part based face detection method and FV descriptors for image classiﬁcation. Evaluation of the methods using real world road images are presented in Section 4. In Section 5, we discuss the advantages & disadvantages of the proposed approach and present the conclusions.  2  Image DataSet:  Dataset: We have collected a large roadway dataset using an NIR imaging system. The distance from camera to cars is on average 60-feet and horizontal ﬁeld of view of the camera is approximately 12-feet at that distance. In these experiments, we use 3000 vehicle front passenger seat images along with their ground truth. Figure 1 illustrates a set of passenger side NIR images with and without passengers from the dataset. Note that there is a large variation in image intensity due to variations in windshield transmission, illuminator state, sun position, cloud position, and other factors. The passenger images show a wide variety of human faces, facial poses, occlusions, and other expected ’within class’ variations. The empty seat images also show a large ’within class’ variation. Face detection is a natural approach for this problem since the vehicle occupants have visible faces. Past researchers have taken this ap- proach. However, we observe, that due to the large variations within the image dataset of real-world vehicle images, face detection is challenged. Further, the front passenger seat detection problem is inherently a binary classiﬁcation problem where either 1 or 0 faces exist in the scene. Thus an image classiﬁcation approach is indicated.  3 Methodology  In this section, we ﬁrst brieﬂy describe the face detection method of [4]. Next, we present the FV representation, which aggregate the d-dimensional local descriptors into a single vector representa- tion.  3.1 Deformable Part Based Face Detection  In a recent study, [4] developed a face detection method based on a mixture of trees with a set of parts V . Every facial part is considered as a node (part) in the tree and a mixture model is used to capture topological changes due to viewpoint. In the tree-structured model, each tree T m = (V m, Em) is linearly parameterized pictorial model, where m indicates the mixture, V m is the set of parts (V m ⊆ V ) and Em is the set of edges between parts. [4] deﬁned a score for a particular conﬁguration of parts L = {li; i ∈ V } for the given image I as shown in Eq. 1,  S(I, L, m) = Appm(I, L) + Shapem(L)  · φ(I, li)  wm i  am ij dx2 + bm  ij dy2 + cm  ij dx + dm  ij dy  (cid:88) (cid:88)  i∈V m  ij∈Em  =  +  where φ(I, li) is the histogram of gradients (HoG) features (for the landmark points) extracted at pixel location li [11], and li = (xi, yi) for the pixel location of part i. App term sums the appearance evidence for placing the ith template, wm i , tuned for mixture m, at location li. Shape term score the spatial arrangement of the set of parts L, where dx (dy) term represents the spatial deformation in x (y) axis between parts i and j. This model can be viewed as a linear classiﬁer [10] with ij} learned during training using latent SVM as shown unknown parameters wm in [10]. During inference, we maximize Eq. 2 over L and m for a given test image I using dynamic programming to ﬁnd the best conﬁguration of parts as shown in [4], [10].  i and {am  ij , dm  ij , cm  ij , bm  (1)  S∗(I) = max  m  L  [max  S(I, L, m)]  (2)  3  3.2  Image Classiﬁcation Based Person Detection  In image classiﬁcation, statistics generated from local invariant descriptors are aggregated into an image level vector signature, which is subsequently passed into a classiﬁer. FV has recently become a standard for the aggregation part [9]. FV has been proposed to incorporate generative models into discriminative classiﬁers [6]. Suppose X = {xt, t = 1...T} denote the set of T local descriptors extracted from a given image. We assume that generation process of local descriptors can be modeled by a probabilistic model p(X|θ) where θ denotes the parameters of the function. [6] proposed to describe X by the gradient vector :  GX  θ =  1 T  ∇θ log p(X|θ)  (3)  , in which the gradient of the log-likelihood describes the contribution of the parameter θ to the generation process. Its dimensionality only depends on the number of parameters in θ. A natural kernel on these gradient vectors is the ﬁsher kernel [6],  K(X, Y ) = GX T where Fθ is the Fisher information matrix of p(X|θ):  θ F −1  θ GY  θ  Fθ = Ex∼p[∇θ log p(X|θ)∇θ log p(X|θ)T ]  (4)  (5)  θ  is symmetric and positive deﬁnite, it has a Cholesky decomposition F −1  where F −1 θ Lθ. Therefore the kernel K(X, Y ) can be written as a dot product between normalized vectors (gθ) shown in Eq. 6.  θ = LT  gX θ = LθGX θ  sume that p(X|θ) is a gaussian mixture model (GMM): p(x|θ) = (cid:80)K  (6) Typically, gX is referred to as ﬁsher vector of X. Similar to the earlier work [7], we as- θ i=1 wipi(x). We denote θ = {wi, µi, σi, i = 1...K} where wi, µi, and σi are respectively the mixture weight, mean vector and variance matrix (assumed diagonal) of Gaussian pi. In this paper, we only consider gradients with respect to the mean. We use the diagonal closed form approximation of the Fisher information matrix of [7] in which case the normalization of the gradient by Lθ is simply a whitening of the dimensions. Let αi(t) be the assignment of the descriptor xt to the ith gaussian:  i denotes the d-dimensional gradient with respect to the mean µi of Gaussian i. Assuming that  Let gX the xt’s are generated independently by p(X|θ), we obtain Eq. 8 after mathematical derivations:  (cid:80)K wipi(xt|θ) j=1 wjpj(xt|θ) T(cid:88) i vectors for i = {1..K} and is K × d dimensional. The ﬁnal vector gX Experiments have been performed for values ranging from K = 32 to K = 512. As noted in earlier studies, bag of words required d times less visual words, however, our exper- iments indicate that FK based learning yields better performance as shown in Section 4. In this paper, we utilize stochastic gradient descent (SGD) SVM classiﬁer to construct the classiﬁcation model [16], [17].  θ is the concatenation of the gX  xt − µi  αi(t) =  gX i =  αt(i)(  √ 1  (8)  (7)  wi  σi  t=1  T  )  4 Experiments  4.1 Face Detection Performance  Before presenting the experimental results using image classiﬁcation techniques, let us ﬁrst present results using state-of-the-art face detector in our application [4]. Face detection method [4] has shown promising performance for faces with arbitrary poses superior to the classical Viola-Jones face detector [12]. However, occlusion, illumination changes and camera conditions results in high  4  miss rate using this technique. In this study, we deﬁne an overlap measure (overlap = area|A ∩ B|/area|A ∪ B|, where A and B are the segmentation output and ground truth, respectively) to quantify an error rate by comparing a candidate face area to the ground truth. If the overlap is larger than 0.6, we declare the (face) detection as true positive. For instance, Figure 2 presents the sample image results obtained using landmark based deformable part models for face detection. Results indicate that face detection works well when details of face region is not occluded, however, it fails in arbitrary view angles and under occlusion. For our test set, we achieve an accuracy of 92.3% using the face detection technique of [4]. This is the baseline approach to compare our image classiﬁcation based results.  4.2 Occupancy Detection:  In this section, we present a comparison of occupancy detection using various local aggregation methods (FV, BoW, and VLAD) as well as state-of-the-art face detection techniques. For local aggregation based methods, we extract features from 24 × 24 pixel patches on regular grids ( every 4 pixels) at 3 scales. We only extract 128-D SIFT descriptors for these image patches. Next, we utilize principal components analysis (PCA) to reduce SIFT dimensions to 64.  4.2.1 Fisher Vectors (FV): In our experiments, we used gaussian mixture models (GMM) with K = {32, 64, 128, 256, 512} gaussians to compute the FVs. The GMM’s are trained using the maximum likelihood (ML) criterion and a standard expectation maximization (EM) algorithm. Similar to [8], we apply the power and L2 normalization to ﬁsher vectors to improve the classiﬁcation performance.  4.2.2 Bag-of-Words (BoW):  The BoW representation is a ubiquitously method to group together descriptors [15], [5]. It requires the deﬁnition of a codebook of K-centroids (visual words) obtained by k-means clustering. Each local descriptor of dimension d from an image is assigned to the closest centroid. BoW is the histogram of the count of visual words assigned to each visual word. Therefore it produces a K- dimensional vector, which is subsequently normalized. In this study, we follow a spatial pyramid based BoW representation in which we create histograms for the original image and spatial grid partitioning of the original image. In our studies, we concatenate histograms from the original image, 2 × 2 and 4 × 4 regular spatial grid.  4.2.3 Vector of Locally Aggregated Descriptors (VLAD):  VLAD is a simpliﬁed non probabilistic version of the Fisher Vector (FV). Similar to BoW, a code- book is generated using k-means clustering [9]. Each local descriptor xt is associated to its nearest visual word in the codebook. For each visual codeword µi, the differences xt − µi of the vectors xt assigned to µi are accumulated to form a vector vi as shown in Eq. 9  vi =  xt − µi  (9)  (cid:88)  xt:N N (xt)=i  The VLAD is formed by the concatenation of the d-dimensional vectors vi. Similar to FV, we apply the power and L2 normalization on VLAD vectors. Once the features are learned (for positive and negative training images) using one of the local ag- gregation methods presented above, we learn a classiﬁer model utilizing a linear SGD-SVM with hinge loss using the stochastic gradient descent techniques [16], [17]. Table 1 presents classiﬁcation accuracy using various local aggregation methods. Fisher vectors out- performs other methods including face detection by achieving an accuracy of approximately 96% using K = 256. Figure 3 shows that performance of FV is consistently better than VLAD if the K value is set above 64. Similar to [9], we have also performed PCA on the FV and VLAD vectors to observe the effect of dimension reduction in the overall classiﬁcation performance (we reduced number of dimensions from K × d to 512 for both FV and VLAD), FV+PCA achieves an accuracy of 93.72 % at K=256. Similarly, FV+PCA consistently performs better than VLAD+PCA for this study. Figure 4 shows the receiver operating characteristics (ROC) curves and Accuracy versus Yield  5  Figure 2: State-of-the-art face detector [4] applied to some test images in our dataset. Face detector misses the faces shown in the last 2 columns.  Figure 3: Comparison of FV and VLAD representations for various number of gaussian components as well as the PCA applied FV and VLAD representations.  Table 1: Classiﬁcation accuracies of Bag-of-words (BoW), Fisher vectors (FV), and VLAD for various number of gaussians (K).  K  32  BoW 0.9376 VLAD 0.9244 0.7513 Fisher  64  0.9412 0.9312 0.9460  128  0.9384 0.9444 0.9540  256  0.9348 0.9496 0.9596  512  0.9464 0.9508 0.9544  1024 0.9504 0.9372  -  2048 0.9528  4096 0.9424  - -  - -  curves for FV, VLAD, BoW and face detection method of [4]. Note that face detector performs poorly compare to the image classiﬁcation based methods. However, if we set the number of gaus- sians in FV too low (such as K = 32), detection rate degrades signiﬁcantly. Note also that at the yield level of 80%, we can achieve over 99% accuracy using the image classiﬁcation based on FV and VLAD representations. Figure 5 presents several samples of the occupancy detection task in HOV/HOT lanes. FV has correctly determined the presence of passengers in Column 1 images while face detection method of [4] failed. Rows 3, 4 and 5 (of Column 1) shows the robustness of this approach even in the presence of the occlusion.  5 Discussion & Conclusion  There are several key advantages of the image based classiﬁcation approach compared to the face detection methods. First, it should be noted that the face does not need to be located within the scene. Face location is a time consuming and computationally costly process that is inherent to face detection methods. Second, non-facial passenger information such as torsos, arms, shoulders, etc. naturally are used in the image classiﬁcation approach, while not taken advantage of in the face detection approach. In order to incorporate non-face related structure to the deformable part model based face detection methods, model complexity has to be increased signiﬁcantly, with the related ground truthing complexity involved. In this paper, we compare face detection and image classiﬁcation for use in the front seat vehicle  6  501001502002503003504004505000.70.750.80.850.90.951Num. of GaussiansAcc.  VLADFVFV−PCAVLAD−PCA(A)  (B)  Figure 4: Part (A) presents comparison of ROC curves for SVM classiﬁers constructed using FV, BoW, VLAD features and face detection method of [4]. Part (B) show the accuracy versus yield for SVM classiﬁers constructed using FV, BoW, VLAD features and face detection [4].  Image  Face Detect. [4]  FV  Image  Face Detect. [4]  FV  Empty  Person  Person  Empty  Empty  Person  Empty  Empty  Empty  Person  Empty  Empty  Empty  Person  Person  Person  Empty  Person  Person  Person  For a given image, we present the face detection [4] versus image classiﬁcation Figure 5: using FV results. Best threshold is chosen for face detection method. Images in Column 1 are contrast enhanced to show the passenger.  occupancy detection task. Experiments using 3000 real-world images captured on a city roadway indicate that the image classiﬁcation approach is far superior for front seat occupancy detection. The state-of-the-art deformable part model based approach for face detection [4] underperforms a Fisher vector based image classiﬁcation approach on this dataset. In further testing across differing roadways for tens of thousands of images, the FV approach has continued to yield accuracy rates above 95% for the front seat vehicle occupancy task.  References [1] http://services.xerox.com/transportation-solutions/transportation-management/enus.html  7  00.10.20.30.40.50.60.70.80.910.60.650.70.750.80.850.90.951FPRTPR  Face Detection [2]FV−32FV−256VLAD−512BoW−20480.10.20.30.40.50.60.70.80.910.920.930.940.950.960.970.980.991YieldAccuracy  Face Detection [4]FV−256VLAD−512BoW−2048[2] S. Schijns, P. Mathews, “A Breakthrough in Automated Vehicle Occupancy Monitoring Systems  for HOV / HOT Facilities”, 12th HOV Systems Conference, April 20, 2005.  [3] J. Billheimer, K. Kaylor, C. Shade, “Use of videotape in HOV lane surveillance and enforce-  ment,” Technical Report, U.S. Department of Transportation, 1990.  [4] X. Zhu, D. Ramanan, “Face Detection, Pose Estimation, and Landmark Localization in the  Wild,” in CVPR, 2012.  [5] G. Csurka, C. Dance, L. Fan, J. Willamowski, C. Bray, “Visual Categorization with bag of  keypoints,” in ECCV SLCV Workshop, 2004.  [6] T. Jaakkola, D. Haussler, “Exploiting generative models in discriminative classiﬁers,” in NIPS,  [7] F. Perronnin, C. R. Dance, “Fisher kernels on visual vocabularies for image categorization,” in  [8] F. Perronnin, J. Sanchez, T. Mensink, “Improving Fisher Kernels for Large-Scale Image Classi-  Dec. 1998.  CVPR, June 2007  ﬁcation,” in ECCV, 2010.  [9] H. Jegou, F. Perronin, M. Douze, J. Sanchez, P. Perez, C. Schmid,“Aggregating local image  descriptors into compact codes,” IEEE PAMI, 2011.  [10] P. F. Felzenszwalb, R. B. Grishick, D. McAllester, D. Ramanan, “Object detection with dis-  criminatively trained part based models,” IEEE PAMI , 2009.  [11] N. Dalal, B. Triggs, “Histograms of oriented gradients for human detection,” in CVPR, 2005. [12] M. Jones, P. Viola, “Fast multi-view face detection,” in CVPR, 2003. [13] H. Jegou, M. Douze, C. Schmid, “Improving bag-of-features for large scale image search,”  International Journal of Computer Vision, vol. 87, pp. 316-336, Feb. 2010.  [14] D. Lowe, “Distinctive image features from scale invariant from scale-invariant keypoints,”  International Journal of Computer Vision, vol. 60, no. 2, pp. 91-110, 2004.  [15] J. Sivic, A. Zisserman, “Video Google: A text retrieval approach to object matching in videos,”  in ICCV, pp. 1470-1477, Oct. 2003.  [16] F. Perronnin, Z. Akata, Z. Harchaoui, C. Schmid, “Towards good practice in large-scale learn-  ing for image classiﬁcation,” in CVPR, 2012.  [17] L. Bottou, “Stochastic Learning,” Advanced Lectures on Machine Learning, pp. 146-168,  2003.  [18] E. N. Dalal., P. Paul, L. K. Mestha, A. S. Islam, “Vehicle OccupancyDetection via single band  infrared imaging,” United States Patent Application, 20130141574, 2013.  [19] Z. Fan, A. S. Islam, P. Paul, B. Xu, L. K. Mestha, “Front seat vehicle occupancy detection via  seat pattern recognition,” United States Patent Application, 20130051625, 2013.  [20] X. Hao, H. Chen, J. Li, “An Automatic Vehicle Occupant Counting Algorithm Based on Face  Detection,” in Int. Conf. on Sign. Proc., vol 3, 2006.  [21] P. M. Birch, R. C. D. Young, F. Claret-Tournier, C. R. Chatwin, “Automated Vehicle Occupancy  Monitoring,” Optical Engineering, vol 43, no. 8, pp. 1828-1832, 2004.  [22] W. Daley, O. Arif, J. Stewart, J. Wood, C. Usher, E. Hanson, J. Turgeson, D. Britton, “Sensiting system development for HOV (High Occupancy Vehicle) Lane Monitoring,” Georgia Department of Transportation Technical Report, 2011.  8  ","Due to the high volume of traffic on modern roadways, transportation agencieshave proposed High Occupancy Vehicle (HOV) lanes and High Occupancy Tolling(HOT) lanes to promote car pooling. However, enforcement of the rules of theselanes is currently performed by roadside enforcement officers using visualobservation. Manual roadside enforcement is known to be inefficient, costly,potentially dangerous, and ultimately ineffective. Violation rates up to50%-80% have been reported, while manual enforcement rates of less than 10% aretypical. Therefore, there is a need for automated vehicle occupancy detectionto support HOV/HOT lane enforcement. A key component of determining vehicleoccupancy is to determine whether or not the vehicle's front passenger seat isoccupied. In this paper, we examine two methods of determining vehicle frontseat occupancy using a near infrared (NIR) camera system pointed at thevehicle's front windshield. The first method examines a state-of-the-artdeformable part model (DPM) based face detection system that is robust tofacial pose. The second method examines state-of- the-art local aggregationbased image classification using bag-of-visual-words (BOW) and Fisher vectors(FV). A dataset of 3000 images was collected on a public roadway and is used toperform the comparison. From these experiments it is clear that the imageclassification approach is superior for this problem."
1312.6169,2014,Learning Information Spread in Content Networks  ,"['Cédric Lagnier', 'Ludovic Denoyer', 'Sylvain Lamprier', 'Simon Bourigault', 'patrick gallinari']",https://arxiv.org/pdf/1312.6169.pdf,"000 001 002 003 004 005 006 007 008 009 010 011 012 4 013 1 014 0 015 2 016   b 017 e 018 F 019   2 020     021 ] G 022 L 023 024 . s 025 c [ 026     027 2 028 v 9 029 6 030 1 031 6 032 . 2 033 1 034 3 035 1 036 : v 037 i X 038 r 039 a 040 041 042 043 044 045 046 047 048 049 050 051 052 053 054  Predict Information Diﬀusion using a Latent Representation Space  Abstract  2. Notations  Information propagation is a hard task where the goal is to predict users behavior. We in- troduce an extension of a model which make use of a kernel to modelize diﬀusion in a latent space. This extension introduce a threhsold to diﬀerentiate if users are contam- inated or not.  1. Introduction  The emergence of Social Networks and Social Media sites has motivated a large amount of recent research. Diﬀerent generic tasks, such as Social Network Analy- sis, Social Network annotation, Community Detection or Link Prediction, are currently studied. Another active research topic is the study of temporal prop- agation of information through this type of media. It aims at studying how interactions between users, like sharing a link on facebook or retweeting something on Twitter, eﬀects the spread of items such as pictures, videos or gossip on the internet. While the study of this word-of-mouth phenomenon pre-dates the devel- opment of computer science, the amount of data made available by the growth of online social networks of- fers an unprecedented ﬁeld of study and enabled new developments. Propagation models aim at predicting and understanding the dynamic of observed propaga- tion.  In this paper, we propose a new diﬀusion model based on the heat diﬀusion. It aims to project users in a latent space where propagation occurs like the heat diﬀusion. This projection is based on the order in which users have been infected in cascades of the train dataset. In order to be able to ﬁnd which users have been infected and not only who is the most likely to be infected, we deﬁne a threshold to split users in two groups: infected or not. This model is an extension of the CDK model (Content Diﬀusion Kernel) presented in (Bourigault et al., 2014) where no threshold was deﬁned.  Preliminary work. Under review by the International Con- ference on Machine Learning (ICML). Do not distribute.  Traditionally, diﬀusion on networks is represented with the notion of cascade. A cascade is a sequence of users infected by some information (for instance, it could be the list of users who ”liked” a speciﬁc YouTube video). A cascade describes to whom and when an item spreads through the network, but not how diﬀu- sion happens: while it is easy to know when a user got infected by some content, it is usually not possible to know who infected him.  Given a social network composed of a set of N users U = (u1, ...., uN ), cascades correspond to sets of users infected by the propagated information. Depending on the kind of network and the task in concern, the propagated information can for instance correspond to a given topic, a particular url, a speciﬁc tag, etc... In the following, we consider C as a set of cascades over a given network, and two sets of distinct cascades: C(cid:96) ⊆ C the set of training cascades and Ct ⊆ C the set of testing cascades. A cascade c ∈ C is deﬁned as:  • A source sc ∈ U which is the user at the source of the cascade - i.e, the ﬁrst user that published the item concerned by the diﬀusion.  • A set of contaminated users Sc ⊂ U such that ui ∈ Sc means that ui has participated to the cascade c ¯Sc is the set of users who have not participated in c.  • A contamination timestamp function deﬁned over Sc such that tc(ui) corresponds to the timestamp at which ui ∈ Sc has ﬁrst participated in the cas- cade. We consider that the contamination times- tamp of the source is equal to 0.  3. Model  The proposed model aims at predicting information diﬀusion. The central idea of this model is to map the observed information diﬀusion process into a heat dif- fusion process in a continuous (euclidean) space. To perform this, we learn diﬀusion kernels that capture the dynamics of diﬀusion from a set of training cas- cades. Let us denote Z = Rn an euclidean space of dimension n - also called latent space. Learning such  055 056 057 058 059 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 100 101 102 103 104 105 106 107 108 109  Predict Information Diﬀusion using a Latent Representation Space  110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164  a diﬀusion kernel comes down in our case to learning a mapping of each node of the network to a particular location in Z such that, for a given metric, the latent space explains the contamination timestamps observed in the training cascades.  Learning using a diﬀusion kernel We deﬁne a diﬀusion kernel K(t, y, x) such that K : R+×X ×X → R which models the heat diﬀusion in a latent space. It corresponds here to the contamination propensity of a node x at time t given a particular information source y. For learning the kernel function, there is however no full supervision available - this would correspond to a continuous time function giving the heat evolution at any point. The observations only provide the contam- ination time of the diﬀerent nodes in a cascade. This partial supervision will be used to constrain the kernel to contaminate the diﬀerent nodes in their actual temporal order of infection.  In practice, we will use the following constraints:  • Given two nodes ui and uj such that ui and uj are contaminated during cascade c - i.e ui ∈ Sc and uj ∈ Sc - and respecting tc(ui) < tc(uj), KZ must be deﬁned such that ∀t, KZ(t, sc, ui) > KZ(t, sc, uj)  • We deﬁne a heat threshold hτ which determine the heat users have to reach to be contaminated. Thus:  – Given a node ui such that ui is contaminated during a cascade c, KZ must be deﬁned such that ∃t, KZ(t, sc, ui) > hτ  – Given a node ui such that ui is not contami- nated during a cascade c, KZ must be deﬁned such that ∀t, KZ(t, sc, ui) < hτ  These constraints basically aim at ﬁnding embeddings such that users who are contaminated ﬁrst are closer to the source of the contamination than users contami- nated later (or not contaminated at all). hτ is a unique heat threshold which split users in two groups in or- der to determine which users will be contaminated and not only an order of contamination. Based on the heat equation, we can thus easily rewrite these three con- straints as: ∀ui ∈ Sc, ∀ui ∈ ¯Sc, ∀(ui, uj) ∈ Sc × Sc  ||zsc − zui||2 < τ τ < ||zsc − zui||2  tc(ui) < tc(uj) ⇒ ||zsc − zui||2 < ||zsc − zuj||2  (1)  where τ is a distance threshold. It correponds to the distance from the source of the diﬀusion beyond which users are not contaminated: their heat never reach hτ .  By the use of classical hinge loss functions, these con- straints can be handled by deﬁning a ranking objective ∆rank such as:  ∆rank(KZ(., sc, .), c, τ ) =  (cid:88) max(0, 1 − (τ − ||zsc − zui||2)) (cid:88) max(0, 1 − (||zsc − zui||2 − τ )) (cid:88)  ui∈ ¯Sc  ui∈Sc  +  +  ui,uj∈Sc×Sc tc (ui)<tc (uj )  max(0, 1 − (||zsc − zuj||2 − ||zsc − zui||2))  (2)  Learning Algorithm The ﬁnal training objective is:  Lrank(Z, τ ) =  ∆rank(KZ(., sc, .), c, τ )  (3)  (cid:88)  c∈C(cid:96)  We name this model ”Content Diﬀusion Kernel with Threshold” (CDKT). Diﬀerent methods can be used to optimize the objective function. We propose to use a classical stochastic gradient descent method, which it- erates until having reached a stop criterion (typically a number of iterations without signiﬁcant improvement of the global loss). After having randomly initialized1 all embeddings for nodes in U, the algorithm samples at each iteration a cascade c from the training set C(cid:96) and two nodes ui and uj with uj a node that is either non-infected, or contaminated after ui in the diﬀusion process described by cascade c. If constraints deﬁned in equation 1 are not respected with a suﬃcient mar- gin2 for this cascade c and the nodes ui and uj, em- beddings zui, zuj , zsc and τ are modiﬁed towards their respective steepest gradient direction with a learning rate α which is a decreasing function of the number of iterations. The learning process is illustrated in algo- rithm 1.  4. Experimentations  Datasets We tested our model on several datasets from various online sources: ICWSM (Burton et al., 2009), Memetracker (Leskovec et al., 2009) and Digg. The ﬁrst two datasets are sets of blog posts crawled from the web. We deﬁne a cascade as a set of posts  1Diﬀerent initialization strategies can be adopted.  In our experiments, we used an uniform initialization between -1 and 1.  2As deﬁned by the hinge loss function, see equation 2.  165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219  Predict Information Diﬀusion using a Latent Representation Space  220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274  Algorithm 1 Stochastic gradient descent algorithm 1: procedure SGD Rank Diffusion Kernel  u ← random  Learning t ← 0 ∀u ∈ U, z(t) τ (t) ← random while t < T do Sample c ∈ C(cid:96) Sample ui ∈ Sc Sample uj ∈ U , tc(ui) < tc(uj) or uj ∈ ¯Sc di ← ||z(t) ui ||2 sc − z(t) dj ← ||z(t) sc − z(t) uj ||2 if uj ∈ Sc then  if (dj − di) < 1 then  ui ← z(t) z(t+1) uj ← z(t) z(t+1) sc ← z(t) z(t+1)  ui + α(t) × 2(z(t) uj + α(t) × 2(z(t) sc + α(t) × 2(z(t)  sc − z(t) ui ) uj − z(t) sc ) ui − z(t) uj )  end if  end if if (τ (t) − di) < 1 then ui ← z(t) z(t+1) τ (t+1) ← τ (t) − α(t) end if if (dj − τ (t)) < 1 then uj ← z(t) z(t+1) τ (t+1) ← τ (t) + α(t)  ui + α(t) × 2(z(t)  sc − z(t) ui )  uj + α(t) × 2(z(t)  uj − z(t) sc )  2: 3: 4: 5: 6: 7: 8:  9:  10: 11: 12:  13:  14:  15: 16: 17: 18:  19: 20: 21: 22:  23: 24: 25: 26: 27: 28: 29: end procedure  end if t ← t + 1 end while Z ← Z (t)  linked together by hyperlinks. Digg est a plateform where users can share news stories with each other. A cascade is thus the set of users who have share the same story. We ﬁltered the users of each dataset to keep about 5000 users with the most posts.  Quality of the ranking In order to test the qual- ity of this model, we compared it to several baselines using the same protocol we used in (Bourigault et al., 2014). The goal is to compute the average precision the model obtains on all cascades. We show the results of 3 baselines IC (Saito et al., 2008), Netrate (Gomez- Rodriguez et al., 2011) and Heat Diﬀusion (Ma et al., 2008) and the 2 latent models CDK and CDKT. IC obtains better results than other baselines.  CDK obtains slightly better results than CDKT. They outperform baselines on both ICWSM and Digg while IC obtains better results on Memetracker.  Model  Memetracker  CDK-500 CDKT-500  IC  Netrate  Heat Diﬀ.  0.363 0.324 0.372 0.287 0.374  ICWSM Digg 0.280 0.233 0.197 0.162 0.082  0.773 0.746 0.712 0.187 0.483  Table 1. MAP on 3 real datasets: Memetracker, ICWSM and Digg. Results of CDK and CDKT are given for a latent space of 500 dimensions.  Learning and Inference complexity Let T be the number of iterations. The learning complexity is O(T × n), where n is the size of the latent space. Once Z has been learned, the inference process is simple. For a cascade c, we just compute the distance between the user sc and every other user in U. The inference complexity for every cascade is then O(N × n), where N is the number of users. Considering that n (cid:28) N , this turns out to be much smaller than the complex- ity of most alternative discrete methods. For instance, the inference step of the very famous Independant Cas- cade model(IC), which is a probabilistic model where diﬀusion propabilities are deﬁned on edges of the net- work’s graph, requires to consider at each time step of the diﬀusion t every possible infection situation at pre- vious time t − 1, which quickly becomes untractable. In practice, inference of graphical models is done by employing a Monte-Carlo approximation that consists in performing a high amount of simulations of the dif- fusion process starting from the source of the cascade and following the diﬀusion probabilities on links of the graph. The inference complexity of this approximation of IC is O(r × ˆSuccs × ˆ|Sc|), where ˆ|Sc| is the aver- age number of infected nodes in the performed simu- ˆSuccs is their average outdegree and r is the lations, number of simulations used for the MCMC approxi- mation. The weaker the probabilities deﬁned on links are, the greater r must be set to obtain a correct ap- proximation of the distribution of ﬁnal infection prob- abilities.  Comparison between CDK and CDKT We compare here the two kernel models in the task of pre- dicting which users will be contaminated at the end of the diﬀusion. The main problem to achieve this task with the CDK model is that all cascades are not on the same scale and it is very diﬃcult to ﬁnd a unique threshold which properly split data in two clusters: contaminated or not. For this reason we don’t have any threshold for the CDK model and thus couldn’t compare clusters made.  We use the following protocol: after predicting a score  275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329  Predict Information Diﬀusion using a Latent Representation Space  330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384  Memetracker  CDK-50 CDKT-50 CDK-500 CDKT-500  0.0001 0.297 0.625 0.417  ICWSM Digg 0.450 0.543 0.067 0.862  1.0 1.0 0.794 1.0  Table 2. P@50 on 3 real datasets: Memetracker, ICWSM and Digg. Results of CDK and CDKT are given for two values of n, the dimension of the latent space Z (50 and 500 dimensions).  Figure 1. Precision/Recall curve for models using a latent space of 500 dimensions on the Digg dataset.  of contamination for each user for each cascade, we group all cascades in a unique set. The goal is to see if the models can ﬁnd users contaminated by any cascade in this set. If there is a unique threshold, they should be able to do so.  Table 2 shows the precision at rank 50 (P@50) on all datasets for the two models using two latent spaces with diﬀerent dimension. It corresponds to their abil- ity to ﬁnd 50 contaminated users. We see that both CDK and CDKT obtain better results on ICWSM. They also obtain a better MAP. This is because the ICWSM dataset is easier than the 2 others. While CDKT is obtain better resultats than CDK on most of the datasets and dimension spaces.  As the P@50 doesn’t show all information, ﬁgure 1 shows the precision/recall curve for the Digg datasets in 500 dimensions spaces. As for the P@50, the curve shows that CDKT is better than CDK.  5. Conclusion  The CDK model use the phenomenon of heat diﬀu- sion to modelize the propagation of content in a latent  space. This model is based on a ranking of users and because of the diﬀerent scale of each representation, there is no easy way to ﬁnd which user will be con- taminated. We proposed in this article an extension of this model CDKT which learns a threshold to split users in two groups: contaminated or not. On sev- eral real datasets, we showed that this model is bet- ter to ﬁnd contaminated users. Our next step with this model will be to understand in which contexts (low/hight diﬀusion, network type, etc) it outperfoms CDK.  References  Bourigault, Simon, Lagnier, C´edric, Lamprier, Syl- vain, Denoyer, Ludovic, and Gallinari, Patrick. Learning social network embeddings for predicting information diﬀusion. In Proceedings of the Sixth ACM International Conference on Web Search and Data Mining, WSDM ’14. ACM, 2014.  Burton, Kevin, Java, Akshay, and Soboroﬀ, Ian. The icwsm 2009 spinn3r dataset. In Proceedings of the Third Annual Conference on Weblogs and Social Media, May 2009.  Gomez-Rodriguez, Manuel, Balduzzi, David, and Sch¨olkopf, Bernhard. Uncovering the temporal dy- namics of diﬀusion networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), ICML ’11, pp. 561–568. ACM, 2011.  Leskovec, Jure, Backstrom, Lars, and Kleinberg, Jon. Meme-tracking and the dynamics of the news cy- cle. In Proceedings of the 15th ACM SIGKDD in- ternational conference on Knowledge discovery and data mining, KDD ’09, pp. 497–506, New York, NY, USA, 2009. ACM.  Ma, Hao, Yang, Haixuan, Lyu, Michael R., and King, Irwin. Mining social networks using heat diﬀusion processes for marketing candidates selection. In Pro- ceedings of the 17th ACM conference on Information and knowledge management, CIKM ’08, pp. 233– 242. ACM, 2008.  Saito, Kazumi, Nakano, Ryohei,  and Kimura, Masahiro. Prediction of information diﬀusion prob- abilities for independent cascade model. In Pro- ceedings of the 12th international conference on Knowledge-Based Intelligent Information and En- gineering Systems, Part III, KES ’08, pp. 67–75. Springer-Verlag, 2008.  385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439  ","We introduce a model for predicting the diffusion of content information onsocial media. When propagation is usually modeled on discrete graph structures,we introduce here a continuous diffusion model, where nodes in a diffusioncascade are projected onto a latent space with the property that theirproximity in this space reflects the temporal diffusion process. We focus onthe task of predicting contaminated users for an initial initial informationsource and provide preliminary results on differents datasets."
1312.6002,2014,Stochastic Gradient Estimate Variance in Contrastive Divergence and Persistent Contrastive Divergence  ,"['Mathias Berglund', 'Tapani Raiko']",https://arxiv.org/pdf/1312.6002.pdf,"Stochastic Gradient Estimate Variance in Contrastive Divergence  and Persistent Contrastive Divergence  4 1 0 2     b e F 4 1         ] E N . s c [      3 v 2 0 0 6  .  2 1 3 1 : v i X r a  Mathias Berglund Aalto University, Finland Tapani Raiko Aalto University, Finland  Abstract  Contrastive Divergence (CD) and Persistent Con- trastive Divergence (PCD) are popular methods for training the weights of Restricted Boltzmann Machines. However, both methods use an ap- proximate method for sampling from the model distribution. As a side effect, these approxima- tions yield signiﬁcantly different biases and vari- ances for stochastic gradient estimates of indi- It is well known that CD vidual data points. yields a biased gradient estimate. In this pa- per we however show empirically that CD has a lower stochastic gradient estimate variance than exact sampling, while the mean of subsequent PCD estimates has a higher variance than exact sampling. The results give one explanation to the ﬁnding that CD can be used with smaller mini- batches or higher learning rates than PCD.  1. Introduction Popular methods to train Restricted Boltzmann Machines (Smolensky, 1986) include Contrastive Divergence (Hin- ton, 2002; Hinton & Salakhutdinov, 2006) and Persistent Contrastive Divergence1 (Younes, 1989; Tieleman, 2008). Although some theoretical research has focused on the properties of these two methods (Bengio & Delalleau, 2009; Carreira-Perpinan & Hinton, 2005; Tieleman, 2008), both methods are still used in similar situations, where the choice is often based on intuition or heuristics. One known feature of Contrastive Divergence (CD) learn- ing is that it yields a biased estimate of the gradient (Bengio  1PCD is also known as Stochastic Maximum Likelihood  Preliminary work. Under review.  MATHIAS.BERGLUND@AALTO.FI  TAPANI.RAIKO@AALTO.FI  & Delalleau, 2009; Carreira-Perpinan & Hinton, 2005). On the other hand, it is known to be fast for reaching good re- sults (Carreira-Perpinan & Hinton, 2005; Tieleman, 2008). In addition to the computationally light sampling proce- dure in CD, it is claimed to beneﬁt from a low variance of the gradient estimates (Hinton, 2002; Carreira-Perpinan & Hinton, 2005). However, the current authors are not aware of any rigorous research on whether this claim holds true, and what the magnitude of the effect is2. On the other hand, Persistent Contrastive Divergence (PCD) has empirically been shown to require a lower learn- ing rate and longer training than CD3 (Tieleman, 2008). The authors propose that the low learning rate is required since the model weights are updated while the Markov chain runs, which means that in order to sample from a distribution close to the stationary distribution the weight cannot change too rapidly. However, for similar reasons that CD updates are assumed to have low variance, subse- quent PCD updates are likely to be correlated leading to a possibly undesirable ”momentum” in the updates. This be- havior would effectively increase the variance of the mean of subsequent updates, requiring either larger minibatches or smaller learning rates. In this paper we explore the variances of CD, PCD and ex- act stochastic gradient estimates. By doing so, we hope to shed light on the observed fast speed of CD learning, and on the required low learning rate for PCD learning com- pared to CD learning. Thereby we hope to contribute to the understanding of the difference between CD and PCD beyond the already well documented bias of CD.  2The topic has been covered in e.g.  (Williams & Agakov, 2002), although for a Boltzmann machine with only one visible and hidden neuron.  3There are however tricks to be able to increase the learning  rate of PCD, see e.g. (Swersky et al., 2010)  Gradient Estimate Variance in CD and PCD  2. Contrastive Divergence and Persistent  Contrastive Divergence  log P (x) = log ((cid:80)  A restricted Boltzmann machine (RBM) is a Boltzmann machine where each visible neuron xi is connected to all hidden neurons hj and each hidden neuron to all visible neurons, but there are no edges between the same type of neurons. An RBM deﬁnes an energy of each state (x, h) by −E(x, h | θ) = b(cid:62)x + c(cid:62)h + x(cid:62)Wh, and assigns the following probability to the state via the Boltzmann distri- Z(θ) exp{−E (x, h | θ)} , where bution: p(x, h | θ) = 1 θ = {b, c, W} is a set of parameters and Z(θ) normalizes the probabilities to sum up to one. The log likelihood of one training data point is hence φ = h exp{−E (x, h | θ)}) − log Z(θ) = φ+ − φ−. Sampling the positive phase of the gradient of the log likelihood ∂φ+ ∂W is easy, but sampling the negative phase ∂φ− A popular method to solve sampling of the negative phase is Contrastive Divergence (CD). In CD, the negative parti- cle is sampled only approximately by running a Markov Chain a limited number of steps (often only one step) from the positive particle (Hinton, 2002). Another method, called Persistent Contrastive Divergence (PCD) solves the sampling with a related method, only that the negative par- ticle is not sampled from the positive particle, but rather from the negative particle from the last data point (Tiele- man, 2008).  ∂W is intractable.  3. Experiments In order to examine the variance of CD and PCD gradient estimates, we use an empirical approach. We train an RBM and evaluate the variance of gradient estimates from dif- ferent sampling strategies at different stages of the training process. The sampling strategies are CD-k with k rang- ing from 1 to 10, PCD, and CD-1000 that is assumed to correspond to an almost unbiased stochastic gradient. In addition, we test CD-k with independent samples (I-CD), where the negative particle is sampled from a random train- ing example. The variance of I-CD separates the effect of the negative particle being close to the data distribution in general, and the effect of the negative particle being close to the positive particle in question. We use three different data sets. The ﬁrst is a reduced size MNIST (LeCun et al., 1998) set with 14 × 14 pixel images of the ﬁrst 1 000 training set data points of each digit, total- ing 10 000 data points. The second data set are the center 14 × 14 pixels of the ﬁrst 10 000 CIFAR (Krizhevsky & Hinton, 2009) images converted into gray scale. The third are the Caltech 101 Silhouettes (Marlin et al., 2010), with 8 641 16 × 16 pixel black and white images. We binarize  Figure 1. CD-k gradient estimate variance for different values of k compared to CD-1000 after 10 and 500 epochs of training. Error bars indicate standard deviation between iterations.  the grayscale images by sampling the visible units with ac- tivation probabilities equal to the pixel intensity. We set the number of hidden neurons equal to the number of visible neurons. The biases are initialized to zero, while √ the weights are initially sampled from a zero-mean normal distribution with standard deviation 1/ nv + nh where nv and nh are the number of visible and hidden neurons, re- spectively. We train the model with CD-1, and evaluate the variance of the gradient estimates after 10, and 500 epochs. We use Adaptive learning rate (Cho et al., 2011) with an initial learning rate of 0.01. We do not use weight decay. In all of the gradient estimates, the ﬁnal sampling step for the probabilities of the hidden unit activations is omitted. The gradient estimate is therefore based on sampled binary visible unit activations, but continuous hidden unit activa- tion probabilities conditional on these visible unit activa- tions. This process is called Rao-Blackwellisation (Swer- sky et al., 2010), and is often used in practice. The variance is calculated on individual gradient estimates based on only one positive and negative particle each. In practice, the gra- dient is usually estimated by averaging over a mini-batch of N independent samples, which diminishes the variance N- fold. We ignore the bias gradient estimates. When analyzing subsequent PCD gradient estimates, the negative particles of the ﬁrst estimate are sampled 1 000 steps from a random training example. Subsequent k es- timates are then averaged, where the positive particle is randomly sampled from the data for each step while the negative particle is sampled from the previous negative particle. No learning occurs between the subsequent es- timates. We can therefore disentangle the effects of weight  123456789100.10.20.30.40.50.60.70.80.911.1CD stepsVariance compared to exact sampling  10 epochs MNIST500 epochs MNIST10 epochs CIFAR500 epochs CIFAR10 epochs Silhouettes500 epochs SilhouettesGradient Estimate Variance in CD and PCD  Figure 2. The PCD vs CD-1000 ratio of the variance for the mean of k subsequent estimates after 10 and 500 epochs of training. Error bars indicate standard deviation between iterations.  Figure 3. I-CD-k gradient estimate variance for different values of k compared to CD-1000 after 10 and 500 epochs of training. Error bars indicate standard deviation between iterations.  updates from the effect of correlation between subsequent estimates. We iterate all results for 10 different random initializations of the weights, and evaluate the variance by sampling gra- dient estimates of individual training examples 10 times for each training example in the data set. The variance is cal- culated for each weight matrix element separately, and the variances of the individual weights are then averaged.  4. Results As we can see from Figure 1, the variance of Contrastive Divergence is indeed smaller than for exact sampling of the negative particle. We also see that the variance of CD estimates quickly increases with the number of CD steps. However, this effect is signiﬁcant only in later stages of training. This phenomenon is expected, as the model is ex- pected not to mix as well in later stages of training as when the weights are close to the small initial random weights. If we sample the negative particle from a different train- ing example than the positive particle (I-CD), in Figure 3 we see that the variance is similar or even larger compared to the variance with exact sampling. Although it is triv- ial that the variance of the I-CD estimates is higher than for CD, the interesting result is that I-CD loses all of the variance advantage against exact sampling. The result sup- ports the hypothesis that the low variance of CD precisely stems from the fact that the negative particle is sampled from the positive particle, and not from that the negative particle is sampled only a limited number of steps from a random training example.  For subsequent PCD updates, we see in Figure 2 that the variance indeed is considerably higher than for independent sampling. Again, as expected this effect is stronger the later during training the evaluation is done. When looking at the magnitude of the variance difference, we see that for CD-1, the mean of 10 subsequent updates have a multiple times smaller variance than PCD. In effect, this means that ignoring any other effects and the effect of weight updates, PCD would need considerably smaller learning rates or larger minibatches to reach the same vari- ance per minibatch. This magnitude is substantial, and might explain the empirical ﬁnding that PCD performs best with smaller learning rates than CD.  5. Conclusions Contrastive Divergence or Persistent Contrastive Diver- gence are often used for training the weights of Restricted Boltzmann machines. Contrastive Divergence is claimed to beneﬁt from low variance of the gradient estimates when using stochastic gradients. Persistent Contrastive Diver- gence could on the other hand suffer from high correlation between subsequent gradient estimates due to poor mixing of the Markov chain estimating the model distribution. In this paper, we have empirically conﬁrmed both of these ﬁndings. In experiments on three data sets, we ﬁnd that the variance of CD-1 gradient estimates are considerably lower than when independently sampling with many steps from the model distribution. Conversely, the variance of the mean of subsequent gradient estimates using PCD is signiﬁcantly higher than with independent sampling. This  24681012141618200123456789Subsequent PCD stepsVariance compared to exact sampling  10 epochs MNIST500 epochs MNIST10 epochs CIFAR500 epochs CIFAR10 epochs Silhouettes500 epochs Silhouettes123456789100.90.9511.051.11.151.21.251.31.351.4I−CD stepsVariance compared to exact sampling  10 epochs MNIST500 epochs MNIST10 epochs CIFAR500 epochs CIFAR10 epochs Silhouettes500 epochs SilhouettesGradient Estimate Variance in CD and PCD  mann machine learning. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 509–516, 2010.  Smolensky, Paul. Information processing in dynamical sys-  tems: Foundations of harmony theory. 1986.  Swersky, Kevin, Chen, Bo, Marlin, Ben, and de Freitas, Nando. A tutorial on stochastic approximation algo- rithms for training restricted boltzmann machines and In Information Theory and Applica- deep belief nets. tions Workshop (ITA), 2010, pp. 1–10. IEEE, 2010.  Tieleman, Tijmen. Training restricted Boltzmann machines using approximations to the likelihood gradient. In Pro- ceedings of the 25th international conference on Ma- chine learning, pp. 1064–1071. ACM, 2008.  Williams, Christopher KI and Agakov, Felix V. An analy- sis of contrastive divergence learning in Gaussian Boltz- mann machines. Institute for Adaptive and Neural Com- putation, 2002.  Younes, Laurent. Parametric inference for imperfectly ob- served gibbsian ﬁelds. Probability Theory and Related Fields, 82(4):625–645, 1989.  effect is mainly observable towards the end of training. In effect, this indicates that from a variance perspective, PCD would require considerably lower learning rates or larger minibatches than CD. As CD is known to be a biased esti- mator, it therefore seems that the choice between CD and PCD is a trade-off between bias and variance. Although the results in this paper are practically signiﬁcant, the approach in this paper is purely empirical. Further the- oretical analysis of the variance of PCD and CD gradient estimates would therefore be warranted to conﬁrm these ﬁndings. In addition, we intend to repeat the experiments with a larger data set, and train the models and replace the CD-1000 baseline with better approximations of exact sam- pling using e.g. Enhanced gradient (Cho et al., 2011) and parallel tempering (Cho et al., 2010).  References Bengio, Yoshua and Delalleau, Olivier. Justifying and gen- eralizing contrastive divergence. Neural Computation, 21(6):1601–1621, 2009.  Carreira-Perpinan, Miguel A and Hinton, Geoffrey E. On contrastive divergence learning. In Artiﬁcial Intelligence and Statistics, volume 2005, pp. 17, 2005.  Cho, KyungHyun, Raiko, Tapani, and Ilin, Alexander. Par- allel tempering is efﬁcient for learning restricted boltz- mann machines. In IJCNN, pp. 1–8. Citeseer, 2010.  Cho, KyungHyun, Raiko, Tapani, and Ilin, Alexander. En- hanced gradient and adaptive learning rate for train- In Proceedings of ing restricted Boltzmann machines. the 28th International Conference on Machine Learning (ICML-11), pp. 105–112, 2011.  Hinton, Geoffrey E. Training products of experts by min- imizing contrastive divergence. Neural computation, 14 (8):1771–1800, 2002.  Hinton, Geoffrey E and Salakhutdinov, Ruslan R. Reduc- ing the dimensionality of data with neural networks. Sci- ence, 313(5786):504–507, 2006.  Krizhevsky, Alex and Hinton, Geoffrey. Learning multi- ple layers of features from tiny images. Master’s thesis, Department of Computer Science, University of Toronto, 2009.  LeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and Haffner, Patrick. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278– 2324, 1998.  Marlin, Benjamin M, Swersky, Kevin, Chen, Bo, and Fre- itas, Nando D. Inductive principles for restricted Boltz-  ","Contrastive Divergence (CD) and Persistent Contrastive Divergence (PCD) arepopular methods for training the weights of Restricted Boltzmann Machines.However, both methods use an approximate method for sampling from the modeldistribution. As a side effect, these approximations yield significantlydifferent biases and variances for stochastic gradient estimates of individualdata points. It is well known that CD yields a biased gradient estimate. Inthis paper we however show empirically that CD has a lower stochastic gradientestimate variance than exact sampling, while the mean of subsequent PCDestimates has a higher variance than exact sampling. The results give oneexplanation to the finding that CD can be used with smaller minibatches orhigher learning rates than PCD."
1312.5940,2014,Generic Deep Networks with Wavelet Scattering  ,"['Edouard Oyallon', 'Stéphane Mallat', 'Laurent Sifre']",https://arxiv.org/pdf/1312.5940.pdf,"4 1 0 2    r a     M 0 1      ]  V C . s c [      3 v 0 4 9 5  .  2 1 3 1 : v i X r a  Generic Deep Networks with Wavelet Scattering  Edouard Oyallon Ecole Normale Sup´erieure, 45 rue d’Ulm, 75005 Paris, France  St´ephane Mallat Ecole Normale Sup´erieure, 45 rue d’Ulm, 75005 Paris, France  Laurent Sifre Ecole Normale Sup´erieure, 45 rue d’Ulm, 75005 Paris, France  EDOUARD.OYALLON@ENS.FR  STEPHANE.MALLAT@ENS.FR  LAURENT.SIFRE@POLYTECHNIQUE.EDU  Abstract  We introduce a two-layer wavelet scattering net- work, for object classiﬁcation. This scattering transform computes a spatial wavelet transform on the ﬁrst layer and a new joint wavelet trans- form along spatial, angular and scale variables in the second layer. Numerical experiments demon- strate that this two layer convolution network, which involves no learning and no max pooling, performs efﬁciently on complex image data sets such as CalTech, with structural objects variabil- ity and clutter. It opens the possibility to simplify deep neural network learning by initializing the ﬁrst layers with wavelet ﬁlters.  and  2013)  (Bruna & Mallat, rotation-translations (Sifre & Mallat, 2012) have previously been applied to digit recognition and texture discrimination. The main source of variability of these images are due to deformations and stationary stochastic variability. This paper applies a scattering transform to CalTech-101 and CalTech-256 datasets, which include much more complex structural variability of objects and clutter, with good classiﬁcation results. The scattering transform is adapted to the important source of variability of these images, by applying wavelet transforms along spatial, rotation, and scaling variables, with separable convolutions which is computationally efﬁcient and by considering YUV color channels independently.  1. Introduction  Supervised training of deep convolution networks (LeCun et al., 1998) is clearly highly effective for im- age classiﬁcation, as shown by results on ImageNet (Krizhevsky et al., 2012). The ﬁrst layers of the networks trained on ImageNet also perform very well to classify im- ages in very different databases, which indicates that these layers capture generic image information (Zeiler & Fergus, 2013; Girshick et al., 2013; Donahue et al., 2013). This paper shows that such generic properties can be captured by a scattering transform, which has the capability to build invariants to afﬁne transformations. Scattering transforms compute hierarchical invariants along groups of transformations by cascading wavelet convolutions and modulus non-linearities, along the group variables (Mallat, 2012).  Invariant  scattering  transforms  to  translations  Submitted to the workshop track of International Conference on Learning Representations (ICLR), 2014  It differs to most deep network in two respects. No ad-hoc renormalization is added within the network and no max pooling is performed. All poolings are average pooling, which guarantees the mathematical stability of the repre- sentation. It does not affect the quality of results when applied to wavelets, even in cluttered environments. This study concentrates on two layers because adding a third layer of wavelet coefﬁcients did not reduce classiﬁcation errors. Beyond the ﬁrst two layers, which take care of trans- lation, rotation and scaling variability, seems necessary to learn the ﬁlters involved in the third and next layers to im- prove classiﬁcation performances.  2. Scattering along Translations, Rotations  and Scales  A two-layer scattering transform is computed by cascading wavelet transforms and modulus non-linearities. The ﬁrst wavelet transform W1 ﬁlters the image x with a low-pass ﬁlter and complex wavelets which are scaled and rotated. The low-pass ﬁlter outputs an averaged image S0x and the modulus of each complex coefﬁcients deﬁnes the ﬁrst scat- tering layer U1x. A second wavelet transform W2 applied  Generic Deep Networks with Wavelet Scattering  x  / |W1|  U1x  / |W2|  U2x  / S2x  S0x  S1x  Figure1.A scattering representation is computed by successively computing the modulus of wavelet coefﬁcients with |W1|, |W2|, followed by an average pooling φJ .  to U1x computes an average S1x and the next layer U2x. A ﬁnal averaging computes second order scattering coefﬁ- cients S2x, as illustrated in Figure 1. Higher order scatter- ing coefﬁcients are not computed.  The ﬁrst wavelet transform is deﬁned from a mother wavelet ψ1(u), which is a complex Morlet function (Bruna & Mallat, 2013) well localized in the image plane and a gaussian lowpass ﬁlter φJ . The wavelet is scaled by 2j1, where j1 is an integer or half-integer, and rotated by θ1 = 2kπ/K1 for 0 ≤ k < K1:  ψ1  w1 (u) = 2−2j1 ψ1(2−2j1 rθ1 u)  where w1 = (θ1, j1). The averaging ﬁlter is φJ (u) = 2−2J φ(2−J u).  This wavelet transform ﬁrst computes the average S0x = x⋆φJ of x(u) and we compute the modulus of the complex wavelet coefﬁcients:  U1x(u, w1) = |x ⋆ ψ1  w1(u)|  = (cid:12)(cid:12)(cid:12)  X  v  x(u)ψ1  w1 (u − v)(cid:12)(cid:12)(cid:12)  .  The spatial variable u is subsampled by 2j1−1. We write u1 = (u, w1) the aggregated variable which indexes these ﬁrst layer coefﬁcients.  The next layer is computed with a second wavelet trans- form which convolves U1x(u1) with separable wavelets along the spatial, rotation and scale variables u1 = (u, θ1, j1)  ψ2  w2 (u1) = ψ1  θ2,j2 (u) ψb  k2 (θ1) ψc  l2 (j1) .  The index w2 = (θ2, j2, k2, l2) speciﬁes the angle of rota- tion θ2 = 2kπ/K2, 0 ≤ k ≤ K2, the scales 2j2, 2k2 and 2l2 of these wavelets. We choose this wavelet family so that it deﬁnes a tight frame, and hence an invertible linear oper- ator which preserves the norm. The wavelet family in an- gle and scales also includes the necessary averaging ﬁlters. The next layer of coefﬁcients are deﬁned for u1 = (u, w1) and w2 = (θ2, j2, k2, l2) by  U2x(u1, w2) = |U1x ⋆ ψ2  w2 (u1)|  = (cid:12)(cid:12)(cid:12)  X  v1  U1x(v1) ψ2  w2 (u1 − v1)(cid:12)(cid:12)(cid:12)  .  First order order scattering coefﬁcients are given by S1x = U1x ⋆ φJ and the second order scattering coefﬁcients are computed with only a low-pass ﬁltering S2x = U2x ⋆ φJ from the second layer. As opposed to almost all deep networks (Krizhevsky et al., 2012), we found that no non- linear normalization was needed to obtain good classiﬁca- tion results with a scattering transform. As usual, at the classiﬁcation stage, all coefﬁcients are standardized by set- ting their variance to 1.  A locally invariant translation scattering representation is obtained by concatenating the scattering coefﬁcients of dif- ferent orders S0x, S1x, S2x:  Sx = {S0x, S1x, S2x}.  Each spatial averaging by φJ is subsampled at intervals 2J −1.  3. Numerical Classiﬁcation Results  The classiﬁcation performance of this double layer wavelet scattering representation is evaluated on the Caltech databases. All images are ﬁrst renormalized to a ﬁxed size of 128 by 128 pixels by a linear rescaling. Each YUV channel of each image is computed separately and their scattering coefﬁcients are concatenated. The ﬁrst wavelet transform W1 is computed with Morlet wavelets (Bruna & Mallat, 2013), over 5 octaves 1 ≤ 2j1 ≤ 25 with K1 = 8 angles θ1. The second wavelet transform W2 is still computed with Morlet wavelets ψ1 over a range of spa- tial scales 2j1 ≤ 2j2 ≤ 25 and K2 = 8 angles θ2. We also use a Morlet wavelet ψb over the angle variable, calculated over 3 octaves 1 ≤ 2l2 ≤ 23. In this implementation, we did not use a wavelet along the scale variable j1.  The ﬁnal scattering coefﬁcients Sx are computed with a spatial pooling at a scale 2J = 32, as opposed to the max- ima selection used in most convolution networks. These coefﬁcients are renormalized by a standardization which subtracts their mean and sets their variance to 1. The mean and variance are computed on the training databases. Stan- dardized scattering coefﬁcients are then provided to a linear SVM classiﬁer.  on Caltech-101  and Caltech-256, with  Almost state of the art classiﬁcation results are ob- tained a ConvNet(Zeiler & Fergus, 2013) pretrained on Ima- geNet. Table 1 shows that with 7 layers it has an 85.5% accuracy on Caltech-101 and 72.6% accuracy on Caltech- 256, using respectively 30 and 60 training images. The classiﬁcation is performed with a linear SVM. In this work, we concentrate on the ﬁrst two layers. With only two layers, the ConvNet performances drop to 66.2% on Caltech-101 and 39.6% on Caltech-256, and progressively increase as the number of layers increases. A scattering  / / / (cid:15) (cid:15) / / / (cid:15) (cid:15) / Generic Deep Networks with Wavelet Scattering  Table1.Classiﬁcation accuracies of convolution networks on Caltech-101 and Caltech-256 using respectively 30 and 60 sam- ples per class, depending upon the number of layers, for a scattering transform and a network trained on ImageNet (Zeiler & Fergus, 2013).  DATASET  LAYERS  CALT.-101  CALT.-256  SCATTERING IMAGENET CCN SCATTERING IMAGENET CCN IMAGENET CCN IMAGENET CCN  1 1 2 2 3 7  51.2±0.8 44.8 ± 0.7 68.8± 0.5 66.2± 0.5 72.3± 0.4 85.5± 0.4  19.3±0.2 24.6±0.4 34.6±0.2 39.6±0.3 46.0±0.3 72.6±0.2  transform has similar performances as a ConvNet when restricted to 1 and 2 layers, as shown by Table 1. It indicates that major sources of classiﬁcation improvements over these ﬁrst two layers can be obtained with wavelet convolutions over spatial variables on the ﬁrst layer, and joint spatial and rotation variables on the second layer. Color improves by 1.5% our results on Caltech-101 but it has not been tried yet on Caltech-256, whose results are given with gray-level images. More improvements can potentially be obtained by adjusting the wavelet ﬁltering along scale variables.  Locality-constrained Linear Coding(LLC) (Wang et al., 2010) are examples different two layer architectures, with a ﬁrst layer computing SIFT feature vectors and a second layer which uses an unsupervised dictionary optimization. This algorithm performs a max-pooling followed by an SVM. LLC yields performances up to 73.4% on Caltech- 101 and 47.7% on Caltech-256 (Wang et al., 2010). These results are better than the one obtained with ﬁxed archi- tectures such as the one presented in this paper, because the second layer performs an unsupervised optimization adapted to the data set.  In this work we observed that average-pooling can pro- vide competitive and even better results that max-polling. According to some analysis and numerical experiments performed on sparse features (Boureau et al., 2010), max- pooling should perform better than average-pooling. Cal- tech images are piecewise regular so we do have sparse wavelets coefﬁcients. It however seems that using the modulus on complex wavelet coefﬁcients does improves results obtained with average pooling compared to max- pooling. When using real wavelets and an absolute value non-linearity, average and max pooling achieve similar per- formances. The max-pooling was implemented with and without overlapping windows. Using squared windows of length 25, 26, 27, with a max-pooling we obtained 60.5% of accuracy on Caltech-101 without overlapping windows  and 62.0% with overlapping windows. This accuracy is well below the average pooling results presented in Table 1. With real Haar wavelets, we observed that max-pooling and average pooling perform similarly with respectively 66.9% and 67.0% accuracy on Caltech-101. These difference be- haviors with real and complex wavelets are still not well understood.  4. Conclusion  We showed that a two layer scattering convolution network, which involves no learning, provides similar accuracy on the Caltech databases, as double layers neural network pre- trained on ImageNet. This scattering transform linearizes the variability relatively to translations and rotations and provides invariants to translations with an average pooling. It involves no inner renormalization but a standardisation of output coefﬁcients.  Many further improvements can still be brought to these ﬁrst scattering layers, in particular by optimizing the scal- ing invariance. These preliminary experiments indicate that wavelet scattering transforms provide a good approach to understand the ﬁrst two layers of convolution networks for complex image classiﬁcation, and an efﬁcient initialization of these two layers.  References Boureau, Y-Lan, Ponce, Jean, and LeCun, Yann. A theo- retical analysis of feature pooling in visual recognition. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 111–118, 2010.  Bruna, J. and Mallat, S.  Invariant scattering convolution network. IEEE Trans. on PAMI, 35(8):1872–1886, 2013.  Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. Decaf: A deep convolutional activation feature for generic visual recognition. arXiv preprint arXiv:1310.1531, 2013.  Girshick, R., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object detection and se- mantic segmentation. arXiv preprint arXiv:1311.2524, 2013.  Krizhevsky, A., Sutskever, I., and Hinton, G.  Imagenet classiﬁcation with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pp. 1106–1114, 2012.  LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient- based learning applied to document recognition. Pro- ceedings of the IEEE, 86(11):2278–2324, 1998.  Generic Deep Networks with Wavelet Scattering  Mallat, S. Group invariant scattering. Communications on Pure and Applied Mathematics, 65(10):1331–1398, 2012.  Sifre, L. and Mallat, S. Combined scattering for rotation In European Symposium on  invariant texture analysis. Artiﬁcial Neural Networks, 2012.  Wang, Jinjun, Yang, Jianchao, Yu, Kai, Lv, Fengjun, Huang, Thomas, and Gong, Yihong. Locality- constrained linear coding for image classiﬁcation. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pp. 3360–3367. IEEE, 2010.  Zeiler, M. D. and Fergus, R. Visualizing and under- standing convolutional neural networks. arXiv preprint arXiv:1311.2901, 2013.  ","We introduce a two-layer wavelet scattering network, for objectclassification. This scattering transform computes a spatial wavelet transformon the first layer and a new joint wavelet transform along spatial, angular andscale variables in the second layer. Numerical experiments demonstrate thatthis two layer convolution network, which involves no learning and no maxpooling, performs efficiently on complex image data sets such as CalTech, withstructural objects variability and clutter. It opens the possibility tosimplify deep neural network learning by initializing the first layers withwavelet filters."
1312.6042,2014,Learning States Representations in POMDP  ,"['Gabriella Contardo', 'Ludovic Denoyer', 'Thierry Artieres', 'patrick gallinari']",https://arxiv.org/pdf/1312.6042.pdf,"Learning States Representations in POMDP  4 1 0 2     n u J    7 1      ]  G L . s c [      4 v 2 4 0 6  .  2 1 3 1 : v i X r a  Gabriella Contardo Ludovic Denoyer Thierry Artieres Patrick Gallinari Sorbonne Universites, UPMC Univ Paris 06, UMR 7606, LIP6, F-75005, Paris, France CNRS, UMR 7606, LIP6, F-75005, Paris, France  gabriella.contardo@lip6.fr ludovic.denoyer@lip6.fr thierry.artieres@lip6.fr patrick.gallinari@lip6.fr  Abstract  We propose to deal with sequential processes where only partial observations are available by learning a latent representation space on which policies may be accurately learned.  1. Introduction  We consider a Markov decision process (MDP) deﬁned by possible states s ∈ S, possible actions a ∈ A, transition between states P (s(cid:48)|s, a) and reward func- tion r(s, a). Standard reinforcement learning (RL) approaches make the assumption that the input pro- vided to the model (i.e. the state of the system) con- tains enough information to learn an optimal policy (i.e a function π(s) ∈ A that chooses which action to take in a state in order to maximize the expected discounted reward). When facing approximated rein- forcement learning problems, the input consists in a feature vector - called observation - which is assumed to fully characterize the current state of the process, thus allowing for an optimal action choice. However, this assumption is unrealistic in real-life applications where the observation is only a partial view of the cur- rent state provided by limited sensors1. For example, it is the case in visual reinforcement learning prob- lems where the input is a camera-based picture of the environment, which can not provide second-order in- formations such as the moving speed of the diﬀerent elements of the system. Without this information, a policy learned from such an observation will probably give low quality results.  We propose to address this problem by switching from  1This more general case is also known as partially ob-  servable Markov decision processes or POMDP  the original observation space to a latent representa- tion space, which we expect to be more informative, and then learn policies in this new space. The model operates in two steps: (i) First, it learns how to ﬁnd good representations on a set of randomly collected trajectories. This unsupervised operation is used to learn the system only once, and may be used to tackle diﬀerent tasks sharing the same dynamical process. (ii) The model then infers new representations for any new trajectory, these representations being then used for discovering an optimal policy for a particular re- ward function.  Our approach is transductive in the sense that when- ever a new observation occurs, the system has to re- compute all the previous representations so that it best matches the whole observation sequence. Although this process could be expensive in terms of computa- tion, we show how to perform fast Monte-Carlo simu- lations in the representation space resulting in a high- speed algorithm. In comparison to common repre- sentation learning algorithms which directly compute the representation given the observation, our approach does the opposite, considering that a good representa- tion is a representation from which the observation can be computed.  2. Model  Let us denote st the state of the process at step t and ot the observation corresponding to this state. An ob- servation is a feature vector of size m: ot ∈ Rm. Note that the learning agent only accesses the observation and does not know in which exact state the process is. The latent representation of a state st will be de- noted zt ∈ Rn. Finding an optimal policy at the rep- resentation level, π(zt), can be made using standard Reinforcement Learning techniques.  Our model is based on two ideas: (i) The latent repre-  Learning States Representations in POMDP  sentation zt of a state st should contain enough infor- mation to compute the corresponding observation ot. We thus consider a decoder function dθ : Rn → Rm that aims at computing the observation given the representation of the current state. (ii) The rep- resentation zt of a state st should contain informa- tion about the dynamics of the system, allowing to compute the representation of the next state. This is handled through the use of a dynamical function mγ : Rn × A → Rn such that mγ(zt, at) aims at com- puting zt+1.  We address now the two steps of this approach: unsu- pervised learning from randomly sampled tra- jectories which will consist in learning the decoder and the dynamical function, and inferring represen- tations on new states which will consist in ﬁnding the latent representation sequence from a new obser- vation sequence.  2.1. Unsupervised Learning  Given a sequence of observations and actions (o1, a1, ..., ot, at), we deﬁne the following loss function:  L(z, θ, γ) =  ∆dec(dθ(zt), ot)  ∆dyn(mγ(zt, at), zt+1)  (1)  (cid:88) (cid:88)  t  +  t  where ∆dec measures the quality of the decoder, ∆dyn measures the quality of the dynamical model and z is the sequence of latent representations. The value of this loss function directly reﬂects the ability of z, θ and γ to explain the observations. Given a set of Q trajectories, learning resumes to:  z∗, θ∗, γ∗ = argmin  L(zq, θ, γ)  (2)  (cid:88)  q∈[1;Q]  where zq is the sequence of representations computed for trajectory number q.  Learning produces both the optimal decoder dθ∗ and the dynamical function mγ∗ , together with the repre- sentation sequences corresponding to the Q trajecto- ries.  Input Model Nb. Dim.  FO PO PO PO PO PO PO PO PO PO PO  FObs FObs FLat FLat FLat FDyn FDyn FDyn FPar FPar FPar  - - 2 3 5 2 3 5 2 3 5  Perf. 0.943 0.579 0.86 0.861 0.912 0.769 0.797 0.81 0.882 0.892 0.91  Table 1. Average Reward obtained over 5 runs  Exact Inference Given a new observation ot+1, the ﬁrst method consists in solving:  z∗ = argminz1,..,zt+1L(z, θ∗, γ∗)  (3)  This produces the optimal representation zt+1 while revising previously computed representations z1 to zt. This characteristic of our model can be intepreted as a thinking process since it means that any new infor- mation gathered by the system will make it revise the whole representation sequence. The drawback of such an inference schema is its high complexity : ﬁnding a new representation may be slow and the optimization must be performed at each step.  Fast Inference The second method consists in us- ing the dynamical function to directly compute the next representation through zt+1 = mγ∗ (zt, at). In that case, the new representation can be produced di- rectly from zt without requiring the observation ot+1. The advantages are twofold. First the computation of mγ∗ (zt, at) is fast, which makes this inference method particularly adapted for processes where data acquisi- tion is slow, such as moving robots for example. More- over, the dynamical function can be used as a learned simulator which allows one to compute Monte-Carlo simulations directly in the latent representation space, and thus to discover an optimal policy without needing to compute new real-world trajectories.  3. Experiments  2.2. Inferring new representations  Knowing dθ∗ and mγ∗ , the next question is to com- pute representations for new trajectories. Consider that at time t, given a sequence of observations (o1, a1, ..., ot, at), the t ﬁrst representations z1 to zt have already been computed. We propose two meth- ods to compute the representation zt+1:  We present preliminary results obtained on a classical toy example of the domain: mountain car (Sutton & Barto, 1998). This problem has been studied in many diﬀerent articles with many diﬀerent variants. In this article, we consider that: (i) A state s = (x, ˙x) is de- ﬁned by the position of the car x and its speed ˙x. (ii) The initial state s1 is generated by uniformly sampling x and ˙x and following a random policy during 5 steps.  Learning States Representations in POMDP  (iii) Each trajectory has a limited size T - in these ex- eriments T = 100. (iv) When the car reaches the goal, or when the maximum trajectory size is reached, the episode stops. The reward function measures the aver- age number of sucessful trajectories. We consider two settings: The full observation (FO) setting where the entire knowledge of the current state is given to the system i.e ot = (xt, ˙xt) and the partial obser- vation (PO) setting where observations only contain the position of the car at time step t i.e ot = (xt).  In order to learn an optimal policy, we use the RCPI algorithm (Dimitrakakis & Lagoudakis, 2008) using a linear classiﬁcation model with a hinge-loss. At each iteration of RCPI, we sample 1000 states and simulate the current policy using only 1 trajectory per state. The base model we are using is a combination of an L2 regularized linear decoder with a linear+hyperbolic- tangent dynamical model. The ∆ losses are L1 norms since using an L2 norms gives lower performances.  We consider four models: The From observation model (FObs.) directly considers that the represen- tation zt of a state st is the observation i.e zt = ot. This corresponds to the classical Approximated Re- inforcement Learning context. The From latent model (FLat.) computes the zt value by minimiz- ing the objective function for each new state. The From dynamical model (FDyn.) computes the representation of the initial state z1 by minimizing the loss, using the 5-sized trajectory that generated s1, and then use the fast inference method. At last, the partial (FPar.) model computes the representation of a state zt by randomly choosing between FDyn and FLat.  Experimental results are illustrated in Table 1 which shows the performance - the expected reward - ob- tained by the policy found after 10 iterations of RCPI (averaged on 5 runs). The baseline corresponds to the ﬁrst line where the full observation of the state is provided to the system (i.e speed and position). In that case, almost 95% of the trajectories are sucess- ful. With our method, the best performance is ob- tained with the FLat model in a latent space of size 5 - 91% of success. Note that using alternative inference methods (FDyn and FPar) allows one to obtain good performance. Particularly, the FDyn results show the ability of the model to directly learn from the dynami- cal model, without acquiring observations and thus at a very high speed. An illustration of the learned la- tent space is given in Figure 1. As may be seen, states corresponding to diﬀerent speeds are projected in dif- ferent areas of the latent space, meaning the missing information has been recovered.  Figure 1. Plot of the 2D latent representations z learned in the PO setting (the observation only includes x), the color indicates the speed value.  4. Related work  Eﬃcient approaches have been proposed to extract high-level representations using deep-learning (Bengio, 2009) but few studies have proposed extension to deal with sequential processes. A formal analysis has been proposed in (Ryabko, 2013). Models concerning partially observable sequential pro- cesses have been proposed in the context of controlling tasks problems. For example, (Schmidhuber, 1990) and (Cuccu et al., 2011) present models using recur- rent neural networks (RNN) to learn a controller for a given task. In these approaches, informative rep- resentations are constructed by the RNN, but these representations are driven by the task to solve. Some unsupervised approaches have been recently proposed. In that case, a representation learning model is learned over the observations, without needing to deﬁne a re- ward function. The policy is learned afterward using these representations, by usually using classical RL al- gorithms. For instance, (Gissl´en et al., 2011) propose a model based on a recurrent auto-associative memory with history of arbitrary depth, while (Duell et al., 2012) present an extension of RNN for unsupervised learning. In comparison to these models, our transductive ap- proach is simultaenously based on unsupervised tra- jectories, and also allows us to choose which action to take even if observations are missing, by learning a dynamic model in the latent space.  5. Conclusion  We proposed a novel approach to learn representations on sequential processes when only partial observations are given. The model is unsupervised and transduc- tive. It can be used for both inferring new represen- tations, but also as a simulator, to predict what can happen in the future. Experiments on more realistic  Learning States Representations in POMDP  domains are currently under investigation.  Acknowledgements  This work was performed within the Labex SMART supported by French state funds managed by the ANR within the Investissements d’Avenir programme un- der reference ANR-11-LABX-65 and by the Lampada project ANR-09-EMER-007.  References  Bengio, Yoshua. Learning deep architectures for ai. Foundations and trends R(cid:13) in Machine Learning, 2 (1):1–127, 2009.  Cuccu, Giuseppe, Luciw, Matthew, Schmidhuber, J¨urgen, and Gomez, Faustino. Intrinsically moti- vated neuroevolution for vision-based reinforcement learning. In Development and Learning (ICDL), 2011 IEEE International Conference on, volume 2, pp. 1–7. IEEE, 2011.  Dimitrakakis, Christos and Lagoudakis, Michail G. Rollout sampling approximate policy iteration. Ma- chine Learning, 72(3):157–171, 2008.  Duell, Siegmund, Udluft, Steﬀen, and Sterzing, Volk- mar. Solving partially observable reinforcement learning problems with recurrent neural networks. In Neural Networks: Tricks of the Trade, pp. 709– 733. Springer, 2012.  Gissl´en, Linus, Luciw, Matt, Graziano, Vincent, and Schmidhuber, J¨urgen. Sequential constant size com- pressors for reinforcement learning. In Artiﬁcial General Intelligence, pp. 31–40. Springer, 2011.  Ryabko, Daniil. Unsupervised model-free representa- tion learning. arXiv preprint arXiv:1304.4806, 2013.  Schmidhuber, J¨urgen. An on-line algorithm for dy- namic reinforcement learning and planning in reac- tive environments. In Neural Networks, 1990., 1990 IJCNN International Joint Conference on, pp. 253– 258. IEEE, 1990.  Sutton, Richard S and Barto, Andrew G. Reinforce- ment learning: An introduction, volume 1. Cam- bridge Univ Press, 1998.  ",We propose to deal with sequential processes where only partial observationsare available by learning a latent representation space on which policies maybe accurately learned.
1312.5783,2014,Unsupervised Feature Learning by Deep Sparse Coding  ,"['Yunlong He', 'Arthur Szlam', 'Yanjun Qi', 'Yun Wang', 'Koray Kavukcuoglu']",https://arxiv.org/pdf/1312.5783.pdf,"3 1 0 2    c e D 0 2         ]  G L . s c [      1 v 3 8 7 5  .  2 1 3 1 : v i X r a  Unsupervised Feature Learning by Deep Sparse  Coding  Yunlong He∗  Koray Kavukcuoglu†  Yun Wang‡  Arthur Szlam §  Yanjun Qi¶  Abstract  In this paper, we propose a new unsupervised feature learning framework, namely Deep Sparse Coding (DeepSC), that extends sparse coding to a multi-layer architecture for vi- sual object recognition tasks. The main innovation of the framework is that it connects the sparse-encoders from different layers by a sparse-to-dense module. The sparse-to-dense module is a composition of a local spatial pooling step and a low-dimensional embedding process, which takes advantage of the spatial smoothness information in the image. As a result, the new method is able to learn several levels of sparse representation of the im- age which capture features at a variety of abstraction levels and simultaneously preserve the spatial smoothness between the neighboring image patches. Combining the feature representations from multiple layers, DeepSC achieves the state-of-the-art performance on multiple object recognition tasks.  Introduction  1 Visual object recognition is a major topic in computer vision and machine learning. In the past decade, people have realized that the central problem of object recognition is to learn meaningful representations (features) of the image/videos. A large amount of focus has been put on constructing effective learning architecture that combines modern machine learning methods and in the mean- while considers the characteristics of image data and vision problems. In this work, we combine the power of deep learning architecture and the bag-of-visual-words (BoV) pipeline to construct a new unsupervised feature learning architecture for learning image represen- tations. Compared to the single-layer sparse coding (SC) framework, our method can extract feature hierarchies at the different levels of abstraction. The sparse codes at the same layer keeps the spatial smoothness across image patches and different SC hierarchies also capture different spatial scopes of the representation abstraction. As a result, the method has richer representation power and hence has better performance on object recognition tasks. Compared to deep learning methods, our method beneﬁts from effective hand-crafted features, such as SIFT features, as the input. Each module of our architecture has sound explanation and can be formulated as explicit optimization problems with promising computational performance. The method shows superior performance over the state-of- the-art methods in multiple experiments. In the rest of this section, we review the technical background of the new framework, including the pipeline of using bag-of-visual-words for object recognition and a low-dimensional embedding method called DRLIM.  1.1 Bag-of-visual-words pipeline for object recognition  We now review the bag-of-visual-words pipeline consisting of hand-crafted descriptor computing, bag-of-visual-words representation learning, spatial pyramid pooling and ﬁnally a classiﬁer.  ∗heyunlong@gatech.edu, Georgia Institute of Technology †koray@deepmind.com, DeepMind Technologies ‡yunwang@princeton.edu, Princeton University §aszlam@ccny.cuny.edu ,The City College of New York ¶yanjun@virginia.edu, University of Virginia  1  Figure 1: The bag-of-visual-words pipeline.  The ﬁrst step of the pipeline is to exact a set of overlapped image patches from each image with ﬁxed patch size, while the spacing between the centers of two adjacent image patches is also ﬁxed. Then a D-dimensional hand-crafted feature descriptor (e.g. 128-dimensional SIFT descriptor) is computed from each image patch. Now let X (i) denote the set of Mi feature descriptors, which are converted from Mi overlapped image patches extracted from the i-th image (e.g. size 300 × 300), i.e.,  X (i) = [x(i)  1 ,··· , x(i)  Mi  ] ∈ RD×Mi,  is the feature descriptor of the j-th patch in the i-th image.  where x(i) j Let X = [X (1), X (2) ··· , X (N )] ∈ RD×M , where M = M1 + M2 + ··· + MN , denote the set of all feature descriptors from all N training images. The second step of the pipeline consists of a dictionary learning process and a bag-of-visual-words representation learning process. In the case of using sparse coding to learn the bag-of-visual-words representation, the two processes can be uniﬁed as the following problem.  (1)  (cid:107)X − V Y (cid:107)2  F + α(cid:107)Y (cid:107)1,1  min V,Y  M(cid:88)  (cid:107)xm − V ym(cid:107)2  = s.t.(cid:107)vk(cid:107) ≤ 1, ∀k = 1,··· , K  2 + α(cid:107)ym(cid:107)1  m=1  Mi  1 ,··· , y(i)  where V = [v1,··· , vK] ∈ RD×K denotes the dictionary of visual-words, and columns of Y = [y1,··· , yM ] ∈ RK×M are the learned sparse codes, and α is the parameter that controls sparsity of the code. We should note, however, other sparse encoding methods such as vector quantization and LLC could be used to learn the sparse representations (see [6] for review and comparisons). Moreover, the dictionary learning process of ﬁnding V in (1) is often conducted in an online style [14] and then the feature descriptors of the i-th image stored in X (i) are encoded as the bag-of-visual-words representations stored in Y (i) = [y(i) ] in the K-dimensional space (K >> D). Intuitively speaking, the components of the bag-of-visual-words representation are less correlated compared to the components of dense descriptors. Therefore, compared to the dense feature descriptors, the high-dimensional sparse representations are more favorable for the classiﬁcation tasks. In the third stage of the pipeline, the sparse bag-of-visual-words representations of all image patches from each image are pooled together to obtain a single feature vector for the image based on the histogram statistics of the visual-words. To achieve this, each image is divided into three levels of pooling regions as suggested by the spatial pyramid matching (SPM) technique [13]. The ﬁrst level of pooling region is the whole image. The second level is consist of 4 pooling regions which are 4 quadrants of the whole image. The third level consist of 16 pool regions which are quadrants of the second level pooling regions. In this way, we obtain 21 overlapped pooling regions. Then for each pooling region, a max-pooling operator is applied to all the sparse codes whose associating image patch center locates in this pooling region, and we obtain a single feature vector as the result. The max-pooling operator maps any number of vectors that have the same dimensionality to a single vector, whose components are the maximum value of the corresponding components in the mapped vectors. Formally, given the descriptors y1,··· , yn ∈ RK that are in the same pooling region, we calculate  y = opmax(y1,··· , yn) := max{y1,··· , yn} ∈ RK,  (2) where max is operated component-wisely. From the second stage of the framework, we know that the nonzero elements in a sparse code imply the appearance of corresponding visual-words in the image patch. Therefore, the max-pooling operator is actually equivalent to calculating the histogram statistics of the visual-words in a pooling region. Finally, the pooled bag-of-visual-words represen- tations from 21 pooling regions are concatenated to obtain a single feature vector, which is regarded  2  as the representation for the image and linear SVM is then used for training and testing on top of this representation. Since the labels of the training images are not used until the ﬁnal training of SVM, the whole pipeline is regarded as an unsupervised method. For the rest of this paper, we focus on the version of the pipeline where the feature (bag-of-visual-words representation) learning part is performed by a sparse coding step as in (1).  1.2 Dimensionality reduction by learning an invariant mapping  We now review a method called dimensionality reduction by learning an invariant mapping (DRLIM, see [12]), which is the base model for our new method in Subsection 2.3. Different from traditional unsupervised dimensionality reduction methods, DRLIM relies not only on a set of training instances y1, y2,··· , yn ∈ RK, but also on a set of binary labels {lij : (i, j) ∈ I}, where I is the set of index pairs such that (i, j) ∈ I if the label for the corresponding instance pair (yi, yj) is available. The binary label lij = 0 if the pair of training instances yi and yj are similar instances, and lij = 1 if yi and yj are known to be dissimilar. Notice that the similarity indicated by lij is usually from extra resource instead of the knowledge that can be learned from data instances y1, y2,··· , yn directly. DRLIM learns a parametric mapping  A : y ∈ RK (cid:55)→ z ∈ RD,  such that the embeddings of similar instances attract each other in the low-dimensional space while the embeddings of dissimilar instances push each other away in the low-dimensional space. In this spirit, the exact loss function of DRLIM is as follows:  L(A) =  (1 − lij)  (cid:107)A(yi) − A(yj)(cid:107)2  1 2  (3)  (max(0, β − (cid:107)A(yi) − A(yj)(cid:107))2,  (cid:88)  (i,j)∈I 1 2  + lij  where β > 0 is the parameter for the contrastive loss term which decides the extent to which we want to push the dissimilar pairs apart. Since the parametric mapping A is assumed to be decided by some parameter. DRLIM learn the mapping A by minimizing the loss function in (3) with respect to the parameters of A. The mapping A could be either linear or nonlinear. For example, we can assume A is a two-layer fully connected neural network and then minimize the loss function (3) with respect to the weight. Finally, for any new data instance ynew, its low-dimensional embedding is represented by A(ynew) without knowing its relationship to the training instances.  2 Deep sparse learning framework  2.1 Overview  Recent progress in deep learning [2] has shown that the multi-layer architecture of deep learning sys- tem, such as that of deep belief networks, is helpful for learning feature hierarchies from data, where different layers of feature extractors are able to learn feature representations of different scopes. This results in more effective representations of data and beneﬁts a lot of further tasks. The rich representation power of deep learning methods motivate us to combine deep learning with the bag- of-visual-words pipeline to achieve better performance on object recognition tasks. In this section, we introduce a new learning framework, named as deep sparse coding (DeepSC), which is built of multiple layers of sparse coding. Before we introduce the details of the DeepSC framework, we ﬁrst identify two difﬁculties in de- signing such a multi-layer sparse coding architecture.  • First of all, to build the feature hierarchies from bottom-level features, it is important to take advantage of the spatial information of image patches such that a higher-level feature is a composition of lower-level features. However, this issue is hardly addressed by simply stacking sparse encoders. • Second, it is well-known (see [16, 10]) that sparse coding is not “smooth”, which means a small variation in the original space might lead to a huge difference in the code space. For  3  Figure 2: A three-layer deep sparse coding framework. Each of the three layers contains three modules. The ﬁrst module converts the input (image patches at the the ﬁrst layer and sparse codes at other layers) to dense codes. The second module is a sparse encoder converting the dense codes to sparse codes. The sparse codes are then sent to the next layer, and simultaneously to a spatial pyramid pooling module. The outputs of the spatial pyramid pooling modules can be used for further tasks such as classiﬁcation.  instance, if two overlapped image patches have similar SIFT descriptors, their correspond- ing sparse codes can be very different. If another sparse encoder were applied to the two sparse codes, they would lost the afﬁnity which was available in the SIFT descriptor stage. Therefore, stacking sparse encoders would only make the dimensionality of the feature higher and higher without gaining new informations.  Based on the two observations above, we propose the deep sparse coding (DeepSC) framework as follows. The ﬁrst layer of DeepSC framework is exactly the same as the bag-of-visual-words pipeline introduced in Subsection 1.1. Then in each of the following layer of the framework, there is a sparse-to-dense module which converts the sparse codes obtained from the last layer to dense codes, which is then followed by a sparse coding module. The output sparse code of the sparse coding module is the input of the next layer. Furthermore, the spatial pyramid pooling step is conducted at every layer such that the sparse codes of current layer are converted to a single feature vector for that layer. Finally, we concatenate the feature vectors from all layers as the input to the classiﬁer. We summarize the DeepSC framework in Figure 2. It is important to emphasis that the whole framework is unsupervised until the ﬁnal classiﬁer. The sparse-to-dense module is the key innovation of the DeepSC framework, where a “pooling function” is proposed to tackle the aforementioned two concerns. The pooling function is the com- position of a local spatial pooling step and a low-dimensional embedding step, which are introduced in Subsection 2.2 and Subsection 2.3 respectively. On one hand, the local spatial pooling step en- sures the higher-level features are learned from a collection of nearby lower-level features and hence exhibit larger scopes. On the other hand, the low-dimensional embedding process is designed to take into account the spatial afﬁnities between neighboring image patches such that the spatial smooth- ness information is not lost during the dimension reduction process. As the combination of the two steps, the pooling function ﬁlls the gaps between the sparse coding modules, such that the power of sparse coding and spatial pyramid pooling can be fully expressed in a multi-layer fashion.  2.2 Learning the pooling function  In this subsection, we introduce the details of designing the local spatial pooling step, which per- forms as the ﬁrst part of the pooling function. First of all, we deﬁne the pooling function as a map from a set of sparse codes on a sampling grid to a set of dense codes on a new sampling grid. Assume that G is the sampling grid that includes M sampling points on a image, where the  4  Figure 3: The ﬁrst, second and third level sampling grids are consists of sampling points in blue, green and red colors, respectively. The local spatial pooling step is performed on the local 4 × 4 grid.  any two adjacent sampling points have ﬁxed spacing (number of pixels) between them. As in- troduced in Subsection 1.1, each sampling point corresponds to the center of a image patch. Let Y = [y1,··· , yM ] ∈ RK×M be the sparse codes on the sampling grid G, where each yi is as- sociated with a sampling point on G according to its associated image patch. Mathematically, the pooling function is deﬁned as the map:  f : (Y, G) (cid:55)→ (Z, G(cid:48)),  where G(cid:48) is the new sampling grid with M(cid:48) sampling points and Z = [z1,··· , zM(cid:48)] ∈ RD×M(cid:48) stores the D-dimensional dense codes (D < K 1) associated with the sampling points on the new sampling grid G(cid:48). As the feature representations learned in the new layer are expected have larger scope than those in the previous layer, we enforce each of the sampling points on new grid G(cid:48) to cover a larger area in the image. To achieve this, we take the center of 4 × 4 neighboring sampling points in G and let it be the new sampling points in G(cid:48). By taking the center of every other 4 × 4 neighboring sampling points, the spacing between neighboring sampling points in G(cid:48) is twice of that in G. As a result, we map G to a coarser grid G(cid:48) such that M(cid:48) ≈ M/4 (see Figure 3). Once the new sampling grid G(cid:48) is determined, we ﬁnish the local spatial pooling step by applying the max-pooling operator (deﬁned in (2)) to the subsets of M sparse codes {y1,··· , yM} and obtain M(cid:48) pooled sparse codes associated with the new sampling grid G(cid:48). More speciﬁcally, let ¯yi denote the pooled sparse codes associated with the i-th sampling point in G(cid:48), where i ∈ {1,··· , M(cid:48)}. We have (4) where {i1, i2,··· , i16} are the indices of the 16 sampling points in G that are most close to the i-th sampling point in G(cid:48).  ¯yi := opmax(yi1, yi2,··· , yi16 ),  2.3 Dimensionality reduction with spatial information  In this subsection, we introduce the details of combining the DRLIM method [12] with the spatial information of image patches to learn a low-dimensional embedding A such that  zi := A(¯yi).  (5) As the feature vector is transformed by A to lower-dimensional space, part of its information is discarded while some is preserved. As introduced in Subsection 1.2, DRLIM is trained on a collec- tion of data instance pairs (¯yi, ¯yj), each of which is associated with a binary label indicating their relationship. Therefore, it provides the option to incorporate prior knowledge in the dimensionality reduction process by determining the binary labels of training pairs based on the prior knowledge. In the case of object recognition, the prior knowledge that we want to impose on the system is that if a image patch is shifted by a few pixels, it still contains the same object. Therefore, we constructed  1For simplicity, we let D be the same as the dimensionality of SIFT features.  5  the collection of training pairs for DRLIM as follows. We extract training pairs such that there always exist overlapped pixels between the two corresponding patches. Let ¯yi and ¯yj be the pooled sparse codes corresponding to two image patches that have overlapped pixels and dij be the distance (in terms of pixels) between them, which is calculated based on the coordinate of the image patch centers. Given a thresholding σ, we set  (cid:26) 0  1  lij =  dij < σ dij > σ  (6)  Generated this way, lij = 0 indicates the two image patches are mostly overlapped, while lij = 1 indicates that the two image patch are only partially overlapped. This process of generating training pairs ensures that the training of the transformation A is focused on the most difﬁcult pairs. Ex- periments shows that if we instead take the pooled sparse codes of far-apart image patches as the negative pairs (lij = 1), DRLIM suffers downgrading in performance. The sensitivity of the system to the thresholding parameter σ is demonstrated in Table 7. Let the linear transformation A be deﬁned by the transformation matrix W ∈ RD×K such that  and then the loss function with respect to the pair (¯yi, ¯yj) is  A(¯yi) = W ¯yi,  Lij(W ) = (1 − lij)  (cid:107)W ¯yi − W ¯yj(cid:107)2  (7)  1 2  + lijmax(0, β − (cid:107)W ¯yi − W ¯yj(cid:107))2.  Let I be the set of index pairs for training pairs collected from all training images, W is then obtained by minimizing the loss with respect to all training pairs, i.e., solving  (cid:88)  min W  (i,j)∈I  Lij  s.t. (cid:107)wk(cid:107) ≤ 1, ∀k = 1,··· , K.  3 Experiments  In this section, we evaluate the performance of DeepSC framework for image classiﬁcation on three data sets: Caltech-101 [7] , Caltech-256 [11] and 15-Scene. Caltech-101 data set contains 9144 images belonging to 101 classes, with about 40 to 800 images per class. Most images of Caltech- 101 are with medium resolution, i.e., about 300× 300. Caltech-256 data set contains 29, 780 images from 256 categories. The collection has higher intra-class variability and object location variability than Caltech-101. The images are of similar size to Caltech-101. 15-Scene data set is compiled by several researchers [8, 13, 15], contains a total of 4485 images falling into 15 categories, with the number of images per category ranging from 200 to 400. The categories include living room, bedroom, kitchen, highway, mountain, street and et al. For each data set, the average per-class recognition accuracy is reported. Each reported number is the average of 10 repeated evaluations with random selected training and testing images. For each image, following [4], we sample 16 × 16 image patches with 4-pixel spacing and use 128 dimensional SIFT feature as the basic dense feature descriptors. The ﬁnal step of classiﬁcation is performed using one-vs-all SVM through LibSVM toolkit [5]. The parameters of DRLIM and the parameter to control sparsity in the sparse coding are selected layer by layer through cross- validation. In the following, we present a comprehensive set of experimental results, and discuss the inﬂuence of each of the parameters independently. In the rest of this paper, DeepSC-2 indicates two-layer DeepSC system; DeepSC-3 represents three-layer DeepSC system, and SPM-SC means the one layer baseline, i.e. the BoV pipeline with sparse coding plus spatial pyramid pooling.  3.1 Effects of Number of DeepSC Layers  As shown in Figure 2, the DeepSC framework utilizes multiple-layers of feature abstraction to get a better representation for images. Here we ﬁrst check the effect of varying the number of layers  6  utilized in our framework. Table 1 shows the average per-class recognition accuracy on three data sets when all using 1024 as dictionary size. The number of training images per class for the three data sets is set as 30 for Caltech-101, 60 for Caltech-256, and 100 for 15-Scene respectively. The second row shows the results when we have only one layer of the sparse coding, while the third row and the fourth row describe the results when we have two layers in DeepSC or three layers in DeepSC. Clearly the multi-layer structured DeepSC framework has superior performance on all three data sets compared to the single-layer SPM-SC system. Moreover, the classiﬁcation accuracy improves as the number of layers increases.  SPM-SC DeepSC-2 DeepSC-3  Caltech-101 Caltech-256 43.04±0.34 75.66±0.59 77.41±1.06 46.02±0.57 78.24±0.76 47.00±0.45  15-Scene 80.83±0.59 82.57±0.72 82.71±0.68  Table 1: Average per-class recognition accuracy (shown as percentage) on three data sets using 1024 as dictionary size. The number of training images per class for the three data sets are 30 for Caltech-101, 60 for Caltech-256, and 100 for 15-Scene respectively. DeepSC-2/3: two/three layers of deep sparse coding. SPM-SC: the normal BoV pipeline with one layer of sparse coding plus spatial pyramid pooling.  3.2 Effects of SC Dictionary Size  We examine how performance of the proposed DeepSC framework changes when varying the dic- tionary size of the sparse coding. On each of the three data sets, we consider three settings where the dimension of the sparse codes K is 1024, 2048 and 4096. The number of training images per class for these experiments is set as 30 for Caltech-101, 60 for Caltech-256, and 100 for 15-Scene respectively. We report the results for the three data sets in Table 2, Table 3 and Table 4 respectively. Clearly, when increasing the dictionary size of sparse coding K from 1024 to 4096, the accuracy of the system improves for all three data sets. We can observe that the performance of DeepSC is always improved with more layers, while in the case of K = 4096 the performance boost in term of accuracy is not so signiﬁcant. This probably is due to that the parameter space in this case is already very large for the limited training data size. Another observation we made from Table 2, Table 3 and Table 4 is that DeepSC-2 (K=1024) always performs better than SPM-SC (K=2048), and DeepSC-2 (K=2048) always performs better than SPM-SC (K=4096). These two comparisons demonstrate that simply increasing the dimension of sparse codes doesn’t give the same performance boost as increasing the number of layers, and therefore DeepSC framework indeed beneﬁts from the feature hierarchies learned from the image.  Caltech-101  SPM-SC DeepSC-2 DeepSC-3  K = 1024 75.66±0.59 77.41±1.06 78.24±0.76  K = 2048 76.34±0.58 78.27±0.6 78.43±0.72  K = 4096 77.21±0.7 78.3±0.9 78.41±0.74  Table 2: Effect of dictionary size used in sparse coding on recognition accuracy (shown as percent- age). data set: Caltech-101; number of training images per class: 30  Caltech-256  SPM-SC DeepSC-2 DeepSC-3  K = 1024 43.04±0.34 46.02±0.57 47.0±0.45  K = 2048 45.66±0.53 48.04±0.44 48.85±0.42  K = 4096 47.8±0.63 49.29±0.50 49.91±0.39  Table 3: Effect of dictionary size used in sparse coding on recognition accuracy (shown as percent- age). data set: Caltech-256; number of training images per class: 60  7  15-Scene SPM-SC DeepSC-2 DeepSC-3  K = 1024 80.83±0.59 82.57±0.72 82.71±0.68  K = 2048 82.11±0.61 83.58±0.71 83.58±0.61  K = 4096 82.88±0.82 83.76±0.72 83.8±0.73  Table 4: Effect of varying sparse coding dictionary size on recognition accuracy (shown as percent- age). data set: 15-Scene; number of training images per class: 100  3.3 Effects of Varying Training Set Size  Furthermore, we check the performance change when varying the number of training images per class on two Caltech data sets. Here we ﬁx the dimension of the sparse codes K as 2048. On Caltech-101, we compare two cases: randomly select 15 or 30 images per category respectively as training images and test on the rest. On Caltech-256, we randomly select 60, 30 and 15 images per category respectively as training images and test on the rest. Table 5 and Table 6 show that with the smaller set of training images, DeepSC framework still continues to improve the accuracy with more layers.  Caltech-101  SPM-SC DeepSC-2 DeepSC-3  30  76.34±0.58 78.27±0.6 78.43±0.72  15  69.94±0.61 71.53±0.53 71.86±0.55  Table 5: Effect of varying training set size on averaged recognition accuracy. data set: Caltech-101; Dictionary Size: 2048  Caltech-256  SPM-SC DeepSC-2 DeepSC-3  60  45.66±0.53 48.04±0.44 48.80±0.42  30  39.86±0.24 41.86±0.28 42.33±0.29  15  33.44±0.15 35.10±0.19 35.28±0.27  Table 6: Effect of varying training set size on averaged recognition accuracy. data set: Caltech-256; Dictionary Size: 2048  3.4 Effects of varying parameters of DRLIM  In table 7, we report the performance variations when tuning the parameters for DRLIM. The param- eter σ is the threshold for selecting positive and negative training pairs (see (6)) and the parameter β in the hinge loss (see (7)) of DRLIM model is for controlling penalization for negative pairs. We can see that it is important to choose the proper thresholding parameter σ such that the transformation learned by DRLIM can differentiate mostly overlapped image pairs and partially overlapped image pairs.  3.5 Comparison with other methods  We then compare our results with other algorithms in Table 8. The most direct baselines 2 for DeepSC to compare are the sparse coding plus SPM framework (ScSPM) [17], LLC[16], and SSC[1]. Table 8 shows the comparison of our DeepSC versus the ScSPM and SSC. We can see that our results are comparable to SSC, with a bit lower accuracy on the 15-Scene data (the std of SSC is much higher than ours). For the LLC method proposed from [16], it reported to achieve 73.44% for Caltech-101 when using K = 2048 and 47.68% when using K = 4096. Our DeepSC-3 has achieved 78.43% for Caltech-101 when using K = 2048 and 49.91% when using K = 4096. Overall our system achieves the state-of-the-art performance on all the three data sets.  2We are also aware of that some works achieve very high accuracy based on adaptive pooling step [9] or  multiple-path system that utilizes image patches of multiple sizes [3].  8  σ \ β 8 16 24  1  76.5 74.93 73.95  2  77.41 76.55 75.43  3  77.07 76.87 76.18  4  76.71 76.97 76.42  5  76.24 76.43 76.53  6  75.81 75.83 76.45  Table 7: The effect of tuning DRLIM parameters on recognition accuracy for DeepSC-2. data set: Caltech-101; dictionary size: 1024; the number of training images per class: 30.  ScSPM  SSC  DeepSC-3  Caltech-101 Caltech-256 40.14±0.91 73.2±0.54 77.54±2.59 78.24±0.76 47.04±0.45  −  15-Scene 80.28±0.93 84.53±2.57 82.71±0.68  Table 8: Comparison of results with other image recognition algorithms: ScSPM[17], LLC[16], and SSC[1]. Dictionary size K = 1024. Number of training images are 30, 60, and 100 for Caltech-101, Caltech-256 and 15-Scene respectively.  References [1] K. Balasubramanian, K. Yu, and G. Lebanon. Smooth sparse coding via marginal regression for learning  sparse representations. In ICML, 2013.  [2] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. arXiv  preprint arXiv:1206.5538, 2012.  [3] L. Bo, X. Ren, and D. Fox. Multipath sparse coding using hierarchical matching pursuit. CVPR, 2013. [4] Y.-L. Boureau, F. Bach, Y. LeCun, and J. Ponce. Learning mid-level features for recognition. In CVPR,  pages 2559–2566. IEEE, 2010.  [5] C.-C. Chang and C.-J. Lin. Libsvm: a library for support vector machines. ACM Transactions on  Intelligent Systems and Technology (TIST), 2(3):27, 2011.  [6] A. Coates and A. Y. Ng. The importance of encoding versus training with sparse coding and vector  quantization. In ICML, volume 8, page 10, 2011.  [7] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: an incremental bayesian approach tested on 101 object categories. In CVPR, Workshop on Generative-Model Based Vision., 2004.  [8] L. Fei-Fei and P. Perona. A bayesian hierarchical model for learning natural scene categories. In CVPR,  2005.  [9] J. Feng, B. Ni, Q. Tian, and S. Yan. Geometric p-norm feature pooling for image classiﬁcation. In CVPR,  pages 2609–2704. IEEE, 2011.  [10] S. Gao, I. W. Tsang, L.-T. Chia, and P. Zhao. Local features are not lonely–laplacian sparse coding for  image classiﬁcation. In CVPR, pages 3555–3561. IEEE, 2010.  [11] G. Grifﬁn, A. Holub, and P. Perona. Caltech-256 object category dataset. 2007. [12] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduction by learning an invariant mapping. In  CVPR, volume 2, pages 1735–1742. IEEE, 2006.  [13] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for recognizing  natural scene categories. In CVPR, volume 2, pages 2169–2178. IEEE, 2006.  [14] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online dictionary learning for sparse coding. In Proceedings  of the 26th Annual International Conference on Machine Learning, pages 689–696. ACM, 2009.  [15] A. Oliva and A. Torraba. Modeling the shape of the scene: A holistic representation of the spatial envelop.  In IJCV, 2001.  [16] J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and Y. Gong. Locality-constrained linear coding for image  classiﬁcation. In CVPR, pages 3360–3367. IEEE, 2010.  [17] J. Yang, K. Yu, Y. Gong, and T. Huang. Linear spatial pyramid matching using sparse coding for image  classiﬁcation. In CVPR, pages 1794–1801. IEEE, 2009.  9  ","In this paper, we propose a new unsupervised feature learning framework,namely Deep Sparse Coding (DeepSC), that extends sparse coding to a multi-layerarchitecture for visual object recognition tasks. The main innovation of theframework is that it connects the sparse-encoders from different layers by asparse-to-dense module. The sparse-to-dense module is a composition of a localspatial pooling step and a low-dimensional embedding process, which takesadvantage of the spatial smoothness information in the image. As a result, thenew method is able to learn several levels of sparse representation of theimage which capture features at a variety of abstraction levels andsimultaneously preserve the spatial smoothness between the neighboring imagepatches. Combining the feature representations from multiple layers, DeepSCachieves the state-of-the-art performance on multiple object recognition tasks."
1312.6034,2014,Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps  ,"['Karen Simonyan', 'Andrea Vedaldi', 'Andrew Zisserman']",https://arxiv.org/pdf/1312.6034.pdf,"4 1 0 2    r p A 9 1         ]  V C . s c [      2 v 4 3 0 6  .  2 1 3 1 : v i X r a  Deep Inside Convolutional Networks: Visualising Image Classiﬁcation Models and Saliency Maps  Karen Simonyan  Andrea Vedaldi  Andrew Zisserman  Visual Geometry Group, University of Oxford  {karen,vedaldi,az}@robots.ox.ac.uk  Abstract  This paper addresses the visualisation of image classiﬁcation models, learnt us- ing deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The ﬁrst one generates an image, which maximises the class score [5], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, speciﬁc to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classiﬁcation ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [13].  Introduction  1 With the deep Convolutional Networks (ConvNets) [10] now being the architecture of choice for large-scale image recognition [4, 8], the problem of understanding the aspects of visual appearance, captured inside a deep model, has become particularly relevant and is the subject of this paper. In previous work, Erhan et al. [5] visualised deep models by ﬁnding an input image which max- imises the neuron activity of interest by carrying out an optimisation using gradient ascent in the image space. The method was used to visualise the hidden feature layers of unsupervised deep ar- chitectures, such as the Deep Belief Network (DBN) [7], and it was later employed by Le et al.[9] to visualise the class models, captured by a deep unsupervised auto-encoder. Recently, the problem of ConvNet visualisation was addressed by Zeiler et al.[13]. For convolutional layer visualisation, they proposed the Deconvolutional Network (DeconvNet) architecture, which aims to approximately reconstruct the input of each layer from its output. In this paper, we address the visualisation of deep image classiﬁcation ConvNets, trained on the large-scale ImageNet challenge dataset [2]. To this end, we make the following three contributions. First, we demonstrate that understandable visualisations of ConvNet classiﬁcation models can be ob- tained using the numerical optimisation of the input image [5] (Sect. 2). Note, in our case, unlike [5], the net is trained in a supervised manner, so we know which neuron in the ﬁnal fully-connected clas- siﬁcation layer should be maximised to visualise the class of interest (in the unsupervised case, [9] had to use a separate annotated image set to ﬁnd out the neuron responsible for a particular class). To the best of our knowledge, we are the ﬁrst to apply the method of [5] to the visualisation of ImageNet classiﬁcation ConvNets [8]. Second, we propose a method for computing the spatial support of a given class in a given image (image-speciﬁc class saliency map) using a single back-propagation pass through a classiﬁcation ConvNet (Sect. 3). As discussed in Sect. 3.2, such saliency maps can be used for weakly supervised object localisation. Finally, we show in Sect. 4 that the gradient-based visualisation methods generalise the deconvolutional network reconstruction procedure [13].  ConvNet implementation details. Our visualisation experiments were carried out using a single deep ConvNet, trained on the ILSVRC-2013 dataset [2], which includes 1.2M training images, labelled into 1000 classes. Our ConvNet is similar to that of [8] and is implemented using their  1  cuda-convnet toolbox1, although our net is less wide, and we used additional image jittering, based on zeroing-out random parts of an image. Our weight layer conﬁguration is: conv64-conv256- conv256-conv256-conv256-full4096-full4096-full1000, where convN denotes a convolutional layer with N ﬁlters, fullM – a fully-connected layer with M outputs. On ILSVRC-2013 validation set, the network achieves the top-1/top-5 classiﬁcation error of 39.7%/17.7%, which is slightly better than 40.7%/18.2%, reported in [8] for a single ConvNet. 2 Class Model Visualisation In this section we describe a technique for visualising the class models, learnt by the image clas- siﬁcation ConvNets. Given a learnt classiﬁcation ConvNet and a class of interest, the visualisation method consists in numerically generating an image [5], which is representative of the class in terms of the ConvNet class scoring model. More formally, let Sc(I) be the score of the class c, computed by the classiﬁcation layer of the ConvNet for an image I. We would like to ﬁnd an L2-regularised image, such that the score Sc is high:  arg max  I  Sc(I) − λ(cid:107)I(cid:107)2 2,  (1)  returned by the soft-max layer: Pc = exp Sc(cid:80)  where λ is the regularisation parameter. A locally-optimal I can be found by the back-propagation method. The procedure is related to the ConvNet training procedure, where the back-propagation is used to optimise the layer weights. The difference is that in our case the optimisation is performed with respect to the input image, while the weights are ﬁxed to those found during the training stage. We initialised the optimisation with the zero image (in our case, the ConvNet was trained on the zero-centred image data), and then added the training set mean image to the result. The class model visualisations for several classes are shown in Fig. 1. It should be noted that we used the (unnormalised) class scores Sc, rather than the class posteriors, . The reason is that the maximisation of the class posterior can be achieved by minimising the scores of other classes. Therefore, we optimise Sc to ensure that the optimisation concentrates only on the class in question c. We also experimented with optimising the posterior Pc, but the results were not visually prominent, thus conﬁrming our intuition. 3 In this section we describe how a classiﬁcation ConvNet can be queried about the spatial support of a particular class in a given image. Given an image I0, a class c, and a classiﬁcation ConvNet with the class score function Sc(I), we would like to rank the pixels of I0 based on their inﬂuence on the score Sc(I0). We start with a motivational example. Consider the linear score model for the class c:  Image-Speciﬁc Class Saliency Visualisation  c exp Sc  c I + bc,  Sc(I) = wT  (2) where the image I is represented in the vectorised (one-dimensional) form, and wc and bc are respec- tively the weight vector and the bias of the model. In this case, it is easy to see that the magnitude of elements of w deﬁnes the importance of the corresponding pixels of I for the class c. In the case of deep ConvNets, the class score Sc(I) is a highly non-linear function of I, so the reasoning of the previous paragraph can not be immediately applied. However, given an image I0, we can approximate Sc(I) with a linear function in the neighbourhood of I0 by computing the ﬁrst-order Taylor expansion:  where w is the derivative of Sc with respect to the image I at the point (image) I0:  Sc(I) ≈ wT I + b,  (cid:12)(cid:12)(cid:12)(cid:12)I0  .  w =  ∂Sc ∂I  (3)  (4)  Another interpretation of computing the image-speciﬁc class saliency using the class score deriva- tive (4) is that the magnitude of the derivative indicates which pixels need to be changed the least  1http://code.google.com/p/cuda-convnet/  2  Figure 1: Numerically computed images, illustrating the class appearance models, learnt by a ConvNet, trained on ILSVRC-2013. Note how different aspects of class appearance are captured in a single image. Better viewed in colour.  3  dumbbell cup dalmatian bell pepper lemon husky washing machine computer keyboard kit fox goose limousine ostrich to affect the class score the most. One can expect that such pixels correspond to the object location in the image. We note that a similar technique has been previously applied by [1] in the context of Bayesian classiﬁcation. 3.1 Class Saliency Extraction Given an image I0 (with m rows and n columns) and a class c, the class saliency map M ∈ Rm×n is computed as follows. First, the derivative w (4) is found by back-propagation. After that, the saliency map is obtained by rearranging the elements of the vector w. In the case of a grey-scale image, the number of elements in w is equal to the number of pixels in I0, so the map can be computed as Mij = |wh(i,j)|, where h(i, j) is the index of the element of w, corresponding to the image pixel in the i-th row and j-th column. In the case of the multi-channel (e.g. RGB) image, let us assume that the colour channel c of the pixel (i, j) of image I corresponds to the element of w with the index h(i, j, c). To derive a single class saliency value for each pixel (i, j), we took the maximum magnitude of w across all colour channels: Mij = maxc |wh(i,j,c)|. It is important to note that the saliency maps are extracted using a classiﬁcation ConvNet trained on the image labels, so no additional annotation is required (such as object bounding boxes or segmentation masks). The computation of the image-speciﬁc saliency map for a single class is extremely quick, since it only requires a single back-propagation pass. We visualise the saliency maps for the highest-scoring class (top-1 class prediction) on randomly se- lected ILSVRC-2013 test set images in Fig. 2. Similarly to the ConvNet classiﬁcation procedure [8], where the class predictions are computed on 10 cropped and reﬂected sub-images, we computed 10 saliency maps on the 10 sub-images, and then averaged them. 3.2 Weakly Supervised Object Localisation The weakly supervised class saliency maps (Sect. 3.1) encode the location of the object of the given class in the given image, and thus can be used for object localisation (in spite of being trained on image labels only). Here we brieﬂy describe a simple object localisation procedure, which we used for the localisation task of the ILSVRC-2013 challenge [12]. Given an image and the corresponding class saliency map, we compute the object segmentation mask using the GraphCut colour segmentation [3]. The use of the colour segmentation is motivated by the fact that the saliency map might capture only the most discriminative part of an object, so saliency thresholding might not be able to highlight the whole object. Therefore, it is important to be able to propagate the thresholded map to other parts of the object, which we aim to achieve here using the colour continuity cues. Foreground and background colour models were set to be the Gaussian Mixture Models. The foreground model was estimated from the pixels with the saliency higher than a threshold, set to the 95% quantile of the saliency distribution in the image; the background model was estimated from the pixels with the saliency smaller than the 30% quantile (Fig. 3, right-middle). The GraphCut segmentation [3] was then performed using the publicly available implementation2. Once the image pixel labelling into foreground and background is computed, the object segmentation mask is set to the largest connected component of the foreground pixels (Fig. 3, right). We entered our object localisation method into the ILSVRC-2013 localisation challenge. Consid- ering that the challenge requires the object bounding boxes to be reported, we computed them as the bounding boxes of the object segmentation masks. The procedure was repeated for each of the top-5 predicted classes. The method achieved 46.4% top-5 error on the test set of ILSVRC-2013. It should be noted that the method is weakly supervised (unlike the challenge winner with 29.9% error), and the object localisation task was not taken into account during training. In spite of its simplicity, the method still outperformed our submission to ILSVRC-2012 challenge (which used the same dataset), which achieved 50.0% localisation error using a fully-supervised algorithm based on the part-based models [6] and Fisher vector feature encoding [11]. 4 Relation to Deconvolutional Networks In this section we establish the connection between the gradient-based visualisation and the DeconvNet architecture of [13]. As we show below, DeconvNet-based reconstruction of the n-th layer input Xn is either equivalent or similar to computing the gradient of the visualised neuron ac-  2http://www.robots.ox.ac.uk/˜vgg/software/iseg/  4  Image-speciﬁc class saliency maps for the top-1 predicted class in ILSVRC-2013 Figure 2: test images. The maps were extracted using a single back-propagation pass through a classiﬁcation ConvNet. No additional annotation (except for the image labels) was used in training.  5  Figure 3: Weakly supervised object segmentation using ConvNets (Sect. 3.2). Left: images from the test set of ILSVRC-2013. Left-middle: the corresponding saliency maps for the top-1 predicted class. Right-middle: thresholded saliency maps: blue shows the areas used to compute the foreground colour model, cyan – background colour model, pixels shown in red are not used for colour model estimation. Right: the resulting foreground segmentation masks.  6  tivity f with respect to Xn, so DeconvNet effectively corresponds to the gradient back-propagation through a ConvNet. For the convolutional layer Xn+1 = Xn (cid:63)Kn, the gradient is computed as ∂f /∂Xn = ∂f /∂Xn+1 (cid:63)  (cid:99)Kn, where Kn and (cid:99)Kn are the convolution kernel and its ﬂipped version, respectively. The convo- a DeconvNet: Rn = Rn+1 (cid:63) (cid:99)Kn.  lution with the ﬂipped kernel exactly corresponds to computing the n-th layer reconstruction Rn in  For the RELU rectiﬁcation layer Xn+1 = max(Xn, 0), the sub-gradient takes the form: ∂f /∂Xn = ∂f /∂Xn+1 1 (Xn > 0), where 1 is the element-wise indicator function. This is slightly different from the DeconvNet RELU reconstruction: Rn = Rn+1 1 (Rn+1 > 0), where the sign indicator is computed on the output reconstruction Rn+1 instead of the layer input Xn. Finally, consider a max-pooling layer Xn+1(p) = maxq∈Ω(p) Xn(q), where the element p of the output feature map is computed by pooling over the corresponding spatial neighbourhood Ω(p) of the input. The sub-gradient is computed as ∂f /∂Xn(s) = ∂f /∂Xn+1(p) 1(s = arg maxq∈Ω(p) Xn(q)). Here, arg max corresponds to the max-pooling “switch” in a DeconvNet. We can conclude that apart from the RELU layer, computing the approximate feature map recon- struction Rn using a DeconvNet is equivalent to computing the derivative ∂f /∂Xn using back- propagation, which is a part of our visualisation algorithms. Thus, gradient-based visualisation can be seen as the generalisation of that of [13], since the gradient-based techniques can be applied to the visualisation of activities in any layer, not just a convolutional one. In particular, in this paper we visualised the class score neurons in the ﬁnal fully-connected layer. It should be noted that our class model visualisation (Sect. 2) depicts the notion of a class, memo- rised by a ConvNet, and is not speciﬁc to any particular image. At the same time, the class saliency visualisation (Sect. 3) is image-speciﬁc, and in this sense is related to the image-speciﬁc convolu- tional layer visualisation of [13] (the main difference being that we visualise a neuron in a fully connected layer rather than a convolutional layer). 5 Conclusion In this paper, we presented two visualisation techniques for deep classiﬁcation ConvNets. The ﬁrst generates an artiﬁcial image, which is representative of a class of interest. The second computes an image-speciﬁc class saliency map, highlighting the areas of the given image, discriminative with respect to the given class. We showed that such saliency map can be used to initialise GraphCut- based object segmentation without the need to train dedicated segmentation or detection models. Finally, we demonstrated that gradient-based visualisation techniques generalise the DeconvNet reconstruction procedure [13]. In our future research, we are planning to incorporate the image- speciﬁc saliency maps into learning formulations in a more principled manner. Acknowledgements This work was supported by ERC grant VisRec no. 228180. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research. References [1] D. Baehrens, T. Schroeter, S. Harmeling, M. Kawanabe, K. Hansen, and K.-R. M¨uller. How to explain  individual classiﬁcation decisions. JMLR, 11:1803–1831, 2010.  [2] A. Berg, J. Deng, and L. Fei-Fei. Large scale visual recognition challenge (ILSVRC), 2010. URL  http://www.image-net.org/challenges/LSVRC/2010/.  [3] Y. Boykov and M. P. Jolly. Interactive graph cuts for optimal boundary and region segmentation of objects  in N-D images. In Proc. ICCV, volume 2, pages 105–112, 2001.  [4] D. C. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classiﬁcation.  In Proc. CVPR, pages 3642–3649, 2012.  [5] D. Erhan, Y. Bengio, A. Courville, and P. Vincent. Visualizing higher-layer features of a deep network.  Technical Report 1341, University of Montreal, Jun 2009.  [6] P. Felzenszwalb, D. Mcallester, and D. Ramanan. A discriminatively trained, multiscale, deformable part  model. In Proc. CVPR, 2008.  7  [7] G. E. Hinton, S. Osindero, and Y. W. Teh. A fast learning algorithm for deep belief nets. Neural Compu-  tation, 18(7):1527–1554, 2006.  [8] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classiﬁcation with deep convolutional neural  networks. In NIPS, pages 1106–1114, 2012.  [9] Q. Le, M. Ranzato, R. Monga, M. Devin, K. Chen, G. Corrado, J. Dean, and A. Ng. Building high-level  features using large scale unsupervised learning. In Proc. ICML, 2012.  [10] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.  Proceedings of the IEEE, 86(11):2278–2324, 1998.  [11] F. Perronnin, J. S´anchez, and T. Mensink. Improving the Fisher kernel for large-scale image classiﬁcation.  In Proc. ECCV, 2010.  [12] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep Fisher networks and class saliency maps for ob- In ILSVRC workshop, 2013. URL http://image-net.org/  ject classiﬁcation and localisation. challenges/LSVRC/2013/slides/ILSVRC_az.pdf.  [13] M. D. Zeiler and R. Fergus.  abs/1311.2901v3, 2013.  Visualizing and understanding convolutional networks.  CoRR,  8  ","This paper addresses the visualisation of image classification models, learntusing deep Convolutional Networks (ConvNets). We consider two visualisationtechniques, based on computing the gradient of the class score with respect tothe input image. The first one generates an image, which maximises the classscore [Erhan et al., 2009], thus visualising the notion of the class, capturedby a ConvNet. The second technique computes a class saliency map, specific to agiven image and class. We show that such maps can be employed for weaklysupervised object segmentation using classification ConvNets. Finally, weestablish the connection between the gradient-based ConvNet visualisationmethods and deconvolutional networks [Zeiler et al., 2013]."
1312.5559,2014,Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds  ,"['Irina Sergienya', 'Hinrich Schütze']",https://arxiv.org/pdf/1312.5559.pdf,"4 1 0 2     b e F 8 1         ] L C . s c [      3 v 9 5 5 5  .  2 1 3 1 : v i X r a  Distributional Models and Deep Learning  Embeddings: Combining the Best of Both Worlds  Irina Sergienya and Hinrich Sch¨utze  Center for Information and Language Processing  University of Munich  Germany  irina@cis.lmu.de  Abstract  There are two main approaches to the distributed representation of words: low- dimensional deep learning embeddings and high-dimensional distributional mod- els, in which each dimension corresponds to a context word. In this paper, we combine these two approaches by learning embeddings based on distributional- model vectors – as opposed to one-hot vectors as is standardly done in deep learn- ing. We show that the combined approach has better performance on a word relatedness judgment task.  1 Introduction  The standard approach to inducing deep learning embeddings is to represent each word of the in- put vocabulary as a one-hot vector (e.g., Turian et al. (2010), Collobert et al. (2011), Mikolov et al. (2013a)). There is no usable information available in this initial representation; in this sense, the standard way of inducing embedding is a form of learning from scratch.  In this paper, we explore the question of whether it can be advantageous to use a more informative initial representation for inducing embeddings. Speciﬁcally, we will test distributional-model repre- sentations for this purpose – where we deﬁne a distributional-model or distributional representation of a target word w as a vector of dimensionality |V |; the value on the dimension corresponding to v ∈ V records a measure of the signiﬁcance of the dimension word v for w. In the simplest case, this measure of signiﬁcance is a weighted cooccurrence count of w and v (e.g., Sch¨utze (1992), Lund and Burgess (1996), Baroni and Lenci (2010)).  Distributional representations have been successfully used for a wide variety of tasks in natural language processing, like synonym detection (Landauer and Dumais, 1997), concept categorization (Almuhareb and Poesio, 2004), metaphorical sense identiﬁcation (Turney et al., 2011) and sentiment analysis (Turney and Littman, 2003). For this reason, it is natural to ask whether they can also improve the quality of embeddings. We will realize this idea by presenting distributional vectors to the neural network that learns embeddings, instead of presenting one-hot vectors. We will refer to these two modes of learning embeddings as distributional initialization and one-hot initialization.  Our expectation is that distributional initialization will be particularly helpful for rare words that occur in only a few contexts. It is difﬁcult for one-hot initialization to learn good embeddings for rare words. In contrast, distributional initialization should make the learning task easier. For example, the distributional signatures of two rare legal words will be similar since they occur in similar contexts. Thus, distributional initialization will make it easier to learn similar embeddings for them. In one-hot initialization, the embeddings for the two words are initialized randomly and it will be difﬁcult for them to converge to close points in the embedding space during learning if each only occurs in a few contexts.  1  2 Method  As we just discussed, we would expect distributional initialization to be beneﬁcial mostly for rare words. Conversely, it is likely that distributional initialization will actually hurt the quality of em- beddings learned for frequent words. The reason is that distributional initialization puts constraints on the relationship between different embeddings. This is a good thing for rare words as it enforces similarity of the embeddings of similar rare words. But it can be harmful for frequent words that of- ten have idiosyncratic properties. For frequent words, it is better to use one-hot initialization, which in principle does not impose any constraints on the type of embedding that can be learned.  Based on this motivation, we propose a hybrid initalization scheme: all words with a frequency f > θ are initialized with one-hot vectors, all words with a frequency f ≤ θ are initialized with distributional vectors, where the frequency threshold θ is a parameter. We test two versions of this hybrid initalization: separate and mixed. Let n be the dimensionality of the distributional vectors, i.e., the number of words that we use as dimension words, and k the number of words with frequency f > θ. In the separate scheme, the input representation for a word is the concatentation of a k-dimensional vector and an n-dimensional vector. For a frequent word, the k-dimensional vector is a one-hot vector and the n-dimensional vector is zero. For a rare word, the k-dimensional vector is zero and the n-dimensional vector is its distributional vector. In the mixed scheme, the input representation for a word is an n-dimensional vector. It is a one-hot vector for a frequent word and a distributional vector for a rare word.  In addition to the two separate and mixed hybrid schemes, we also test non-hybrid distributional intialization. In that case, the input representation for a word is an n-dimensional distributional vector for frequent words as well as for rare words.  3 Experimental setup  As training set for the word embeddings, we use parts 02 to 21 of the Wall Street Journal (Marcus et al., 1993), a corpus of about one million tokens and roughly 35,000 word types. We used two word relatedness data sets for evaluation: MEN1 (Bruni et al., 2012) and WordSim3532 (Finkelstein et al., 2001). The two data sets contain pairs of words with human-assigned similarity scores. We only evaluate on the 2186 MEN pairs (of a total of 3000) and 303 WordSim353 pairs (of a total of 353) that are covered by our data set, i.e., both words occurred in WSJ. We added to the continuous skip gram model (Mikolov et al., 2013a) of word2vec3 both one-hot and distributional initialization. We use hierarchical softmax, set the size of the context window to 11, min-count to 0 (do not discard words because of their low frequency), sample to 1e-3 (discard the words in the training set with probability P (w) = 1 − pt/f (w), where f (w) is the frequency of word w and t is a chosen threshold (Mikolov et al., 2013b)) and embedding size to 100. We use a simple binary distributional model: Entry 1 ≤ i ≤ n in the distributional vector of w is set to 1 iff vi and w cooccur at a distance of at most ten words in the corpus and to 0 otherwise. We test the methods for frequency thresholds θ ∈ {1, 2, 5, 10, 20, 50, 100, 1000}. For each initializaton condition, we train 10 models. We measure Spearman correlation of the gold standard – human-assigned similarity scores – with cosine similarity scores between word embeddings generated by our models, and report correlation averages for each initialization setup.  4 Results and discussion  Table 1 gives averaged Spearman correlation coefﬁcients between human and embedding-based similarity judgments on MEN and WordSim. Embeddings are produced by skip gram models with one-hot, hybrid (mixed or separate) and distributional initialization. The threshold is varied for the two hybrid models (column “θ”). Correlation coefﬁcients are given in the last two columns.  1http://clic.cimec.unitn.it/ elia.bruni/MEN 2http://www.cs.technion.ac.il/ gabr/resources/data/wordsim353/wordsim353.html 3https://code.google.com/p/word2vec/  2  Table 1: Averaged Spearman correlation coefﬁcients between human and embedding-based similar- ity judgments on MEN and WordSim. Embeddings are produced by skip gram models with one-hot, hybrid (mixed or separate) and distributional initialization. The threshold θ is varied for the hybrid models. The best correlation in each column is bold. Correlations signiﬁcantly better than one-hot initialization are marked with a *.  initialization one-hot  mixed  separate  distributional  θ  1 2 5 10 20 50 100 1000 1 2 5 10 20 50 100 1000  MEN WordSim 17.31 10.58 *18.02 *19.63 *20.36 *19.05 16.86 *15.93 16.39 11.43 11.47 10.95 8.22 3.06 6.62 11.52 6.76 9.21 12.06 *18.46 *15.90 13.58 *18.05 *16.98 17.76 7.01 12.63 7.46 8.61 5.75 8.95 5.25 7.19 7.12 5.69 4.82  The main result is that the two hybrid initializations outperform one-hot initialization signiﬁcantly4 on both MEN and WordSim data sets for low values of θ. This result is evidence that a hybrid initialization scheme can be superior to one-hot initialization for words with very few occurrences.  Hybrid initialization only does well for low values of θ. In general, as θ increases, performance goes down. This trend reaches its endpoint for distributional initialization, which can be interpreted as hybrid initialization with θ = ∞. The correlations for distributional initialization are at the low end of the range of performance numbers and are lower than one-hot initialization.  The fact that distributional initialization performs worse than hybrid initialization conﬁrms our initial hypothesis that frequent and rare words should be treated differently. Distributional initialization for all words – including frequent words – imposes harmful constraints on the embeddings of frequent words; it is probably also harmful because it links the embeddings of rare words to those of frequent words, which makes it harder for the skip gram model to learn embeddings for rare words.  5 Related work  The problem of word embedding initialization was also addressed by Le et al. (2010). They pro- pose three initialization schemes. Two of them, re-initialization and iterative re-initialization, use vectors from prediction space to initialize the context space during training. This approach is both more complex and less efﬁcient than ours. The third initialization scheme, one vector initialization, initializes all word embeddings with the same random vector: this helps to keep rare words close to each other because vectors of rare words are rarely updated. However, this approach is also less efﬁcient than ours since the initial embedding is much denser than in our approach.  6 Conclusion  We have proposed to use a hybrid initialization for learning embeddings, an initialization that com- bines the standardly used one-hot initialization with a distributional initialization for rare words.  4Student’s t-test, two-tailed, p < .05  3  Experimental results on a word relatedness task provide tentative evidence that hybrid initialization produces better embeddings than one-hot initialization.  Our results are not directly comparable with prior research on modeling word relateness judgments, partly because the corpus we use has low coverage of the words in the evaluation sets. We also use a simple binary distributional vector representation, which is likely to have a negative effect on the performance of the embeddings.  In future work, we will test our models on larger corpora and look at a wider range of distributional models to produce results that are directly comparable with other work on word relatedness. Acknowledgments. We would like to thank Sebastian Ebert for his help with the code.  References Almuhareb, A., Poesio, M., 2004. Attribute-based and value-based clustering: An evaluation. In:  EMNLP. pp. 158–165.  Baroni, M., Lenci, A., 2010. Distributional memory: A general framework for corpus-based seman-  tics. Computational Linguistics 36 (4), 673–721.  Bruni, E., Boleda, G., Baroni, M., Tran, N. K., 2012. Distributional semantics in technicolor. In:  ACL. pp. 136–145.  Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., Kuksa, P., 2011. Natural lan- guage processing (almost) from scratch. Journal of Machine Learning Research 12, 2493–2537. Finkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E., Solan, Z., Wolfman, G., Ruppin, E., 2001.  Placing search in context: The concept revisited. In: WWW. pp. 406–414.  Landauer, T. K., Dumais, S. T., 1997. Solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge. Psychological Review 104 (2), 211–240.  Le, H. S., Allauzen, A., Wisniewski, G., Yvon, F., 2010. Training continuous space language mod-  els: Some practical issues. In: EMNLP. pp. 778–788.  Lund, K., Burgess, C., 1996. Producing high-dimensional semantic spaces from lexical co-  occurrence. Behavior Research Methods, Instruments, & Computers 28 (2), 203–208.  Marcus, M. P., Marcinkiewicz, M. A., Santorini, B., 1993. Building a large annotated corpus of  English: The Penn treebank. Computational Linguistics 19 (2), 313–330.  Mikolov, T., Chen, K., Corrado, G., Dean, J., 2013a. Efﬁcient estimation of word representations in  vector space. In: Workshop at ICLR.  Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., Dean, J., 2013b. Distributed representations of words and phrases and their compositionality. In: Advances in Neural Information Processing Systems 26. pp. 3111–3119.  Sch¨utze, H., 1992. Dimensions of meaning. In: ACM/IEEE Conference on Supercomputing. pp.  787–796.  Turian, J., Ratinov, L.-A., Bengio, Y., 2010. Word representations: A simple and general method for  semi-supervised learning. In: ACL. pp. 384–394.  Turney, P. D., Littman, M. L., 2003. Measuring praise and criticism: Inference of semantic orienta-  tion from association. ACM TOIS 21 (4), 315–346.  Turney, P. D., Neuman, Y., Assaf, D., Cohen, Y., 2011. Literal and metaphorical sense identiﬁcation  through concrete and abstract context. In: EMNLP. pp. 680–690.  4  ","There are two main approaches to the distributed representation of words:low-dimensional deep learning embeddings and high-dimensional distributionalmodels, in which each dimension corresponds to a context word. In this paper,we combine these two approaches by learning embeddings based ondistributional-model vectors - as opposed to one-hot vectors as is standardlydone in deep learning. We show that the combined approach has betterperformance on a word relatedness judgment task."
1310.1811,2014,End-to-End Text Recognition with Hybrid HMM Maxout Models  ,"['Ouais Alsharif', 'Joelle Pineau']",https://arxiv.org/pdf/1310.1811.pdf,"End-to-End Text Recognition with Hybrid HMM Maxout Models  Ouais Alsharif Reasoning and Learning Laboratory, School of Computer Science, McGill University, Montreal, QC, Canada Joelle Pineau Reasoning and Learning Laboratory, School of Computer Science, McGill University, Montreal, QC, Canada  OUAIS.ALSHARIF@MAIL.MCGILL.CA  JPINEAU@CS.MCGILL.CA  3 1 0 2    t c O 7         ]  V C . s c [      1 v 1 1 8 1  .  0 1 3 1 : v i X r a  Abstract  The problem of detecting and recognizing text in natural scenes has proved to be more challenging than its counterpart in documents, with most of the previous work focusing on a single part of the problem. In this work, we propose new solutions to the character and word recognition problems and then show how to combine these solutions in an end-to-end text-recognition system. We do so by leveraging the recently introduced Maxout networks along with hybrid HMM models that have proven useful for voice recognition. Using these elements, we build a tunable and highly ac- curate recognition system that beats state-of-the- art results on all the sub-problems for both the ICDAR 2003 and SVT benchmark datasets.1  1. Introduction Recognizing text in natural images is a challenging prob- lem with many promising applications. Natural attributes such as lighting, shadows, styles, fonts and backgrounds that affect the perception of textual information is the main culprit that shifts this problem away from document text recognition, and closer to a combination of object and handwriting recognition. The end-to-end text recognition problem can be decom- posed into three natural sub-problems: text detection, char- acter recognition and word recognition. The ﬁrst problem is to identify text locations in a natural image. The second problem requires identifying characters in cropped image patches; this is a classiﬁcation problem with high confu- sion due to upper-case/lower-case and letter/number con- fusion. The third problem is a sequencing problem, given an image of a word, to output the most likely word corre-  1Code for this paper will be provided with the ﬁnal version.  sponding to that image. These problems clearly overlap as character recognition is a sub-problem of word recognition, which itself is a sub-problem of text detection. The end-to-end problem presents technical challenges on multiple levels. On the character level, the main challenge is to achieve high recognition accuracy. On the word level, the word recognizer needs to be accurate, fast, and scal- able with lexicon size. On the end-to-end level, the system needs to balance precision, recall, complexity and speed. In our work, we aim for a highly accurate character rec- ognizer, a fast, accurate and scalable word recognizer and for the end-to-end system, we aim for fast performance at query time, and high F-measure. To achieve the goals speciﬁed above, we dissect the end- to-end recognition problem, exploring new solutions for all three subproblems. More speciﬁcally, we leverage a deep, convolutional variant of the recently introduced Max- out networks which make heavy use of dropout to beat the state-of-the-art on the character recognition task with min- imal preprocessing. Also, inspired by the recent break- throughs in voice recognition (Hinton et al., 2012a), we propose to treat the word recognition problem in a way sim- ilar to the phone-sequence recognition problem. More con- cretely, we create a hybrid HMM/Maxout architecture that is able to sequence words into their corresponding charac- ters. The proposed model allows for simple integration of a lexicon’s higher order n-grams, resulting in a method that is fast, accurate and highly tunable, while taking constant time relative to lexicon size. We then show how to integrate these parts in a novel end-to-end recognition system that achieves state-of-the-art F-measure on the ICDAR 2003 (Lucas et al., 2003) and SVT (Wang & Belongie, 2010) datasets. Due to the hierarchical dependency of the problem, where word recognition is a sub-module of text detection and character recognition is a sub-module of word recognition, we present the modules starting with character recognition in section 4, proceeding on to word recognition in section 5, and then joining the previous parts in the end-to-end sys-  End-to-End Text Recognition with Hybrid HMM Maxout Models  tem in section 6, with a pipeline of the end-to-end system in Figure 6.  2. Related Work The text recognition problem has been addressed in the lit- erature on multiple levels: character recognition (Saidane & Garcia, 2007; Coates et al., 2011), word recognition (Novikova et al., 2012; Mishra et al., 2012) and text detec- tion (Hanif et al., 2008; Chen et al., 2011). Most previous works focused on a single stage of the pipeline, with few looking into the end-to-end systems, namely (Wang et al., 2011; Neumann & Matas, 2011; Wang et al., 2012). The character recognition problem is a classiﬁcation prob- lem that is generally addressed with the use of strong clas- siﬁers such as Convolutional Neural Nets (CNNs) (Wang et al., 2011; 2012), deformable parts models (Shi et al., 2013) or manually-engineered feature-extraction followed by a classiﬁer (de Campos et al., 2009). The word recognition problem is, much like phone recog- nition and handwriting recognition, a sequence recognition problem. Previous works have addressed this problem us- ing CNNs (Wang et al., 2012), Conditional Random Fields (CRFs) (Mishra et al., 2012; Novikova et al., 2012) and Pictorial Structures (PS) (Wang et al., 2011). Most of the work in this area has relied on segmentation-free lexicon- dependent approaches. The use of the lexicons helps tackle the high confusion inherent in the text recognition prob- lem. However, despite the argument for the validity of task- speciﬁc lexicon use in (Wang et al., 2011), it is clear that we ultimately wish to recognize text with a very general lexicon. To do so, we require word recognizers that scale well in the size of the lexicon. The works of (Neumann & Matas, 2011; Mishra et al., 2012; Novikova et al., 2012) are the only works we know of that show how their meth- ods scale with lexicon size. The text detection problem is deﬁned such that, given a natural image, the goal is to output bounding boxes on all words in the image. Abstractly speaking, the problem is an instance of the object detection problem, followed by segmenting text regions into their constituent words. Pre- vious works investigated different approaches for text de- tection, typically trading off precision, recall, training time and time consumed for manually designing features. Pre- trained CNNs (Coates et al., 2011; Wang et al., 2012) ap- plied in a multi-scale sliding window fashion are highly accurate but very time consuming. Viola-Jones style clas- siﬁers remedy the slowness in CNNs, but have long train- ing times and require manually-engineered features (Hanif et al., 2008; Chen & Yuille, 2011). Alternative methods that cleverly exploit the nature of text such as Maximally Stable Extremal Regions (MSERs) (Matas et al., 2004) and  Stroke Width Transform (Epshtein et al., 2010) generally have lower accuracy but are fast to compute. Such meth- ods were used successfully to detect text as in (Neumann & Matas, 2011; Chen et al., 2011; Wang et al., 2011).  3. Technical Background This section provides technical background on some of the techniques used in the proposed system.  3.1. Convolutional Neural Networks  Convolutional Neural Networks (CNNs) (LeCun et al., 1998) are discriminativly trained neural networks that al- ternate convolution layers and pooling layers, with the last layer being usually a softmax or RBF layer. In a convolution layer, an input is convolved with multiple learned ﬁlters leading to multiple maps which then are pooled together through a pooling scheme. Combined with regularization and pretraining techniques, these neu- ral nets achieve state-of-the-art results on many datasets (Krizhevsky et al., 2012; Goodfellow et al., 2013b).  3.2. Dropout  Dropout (Hinton et al., 2012b) is a simple and efﬁcient technique that can be used to reduce overﬁtting in neural networks. The main idea of dropout is to stochasticly omit some of the units from the network during learning. In- tuitively, dropout adds robustness to the network by intro- ducing noise on all levels of the architecture. Another way to view dropout is as a way to do model averaging over exponentially-many models with shared parameters.  3.3. Maxout Networks  A Maxout network (Goodfellow et al., 2013b) is a multi- layer perceptron that makes heavy use of dropout to regu- larize the neural net, thereby reducing overﬁtting. It also uses a max activation function that produces a sparse gra- dient. Speciﬁcally, in these networks, for an input x ∈ Rd, every hidden layer implements the function:  where  hi(x) = max zij,  zij = xT W...ij + bij  W ∈ Rd×m×k, b ∈ Rm×k.  Maxout networks have produced state-of-the-art results on benchmark datasets without any pre-training (Goodfellow et al., 2013b).  End-to-End Text Recognition with Hybrid HMM Maxout Models  3.4. Hybrid HMM models  HMMs have long been among the main tools used for se- quence modelling in voice recognition (Rabiner, 1989) and hand-writing recognition (Hu et al., 1996). Hybrid mod- els (Morgan & Bourlard, 1995) extend HMMs with a sim- ple idea, that is, instead of using Gaussian Mixture Models (GMMs) to model the HMMs observation model, hybrid models use Bayes rule and implicitly model the observation model using a probabilistic classiﬁer. Concretely, let O be a sequence of observations and let Q be a state sequence, p(Q|O). the purpose of the HMM is to produce argmax In a standard setting, to train an HMM, we require an ob- servation model p(o|q) where o is an observation and q is an HMM state. In the hybrid model, we approximate the observation model through Bayes rule with a probabilistic classiﬁer that computes the posterior p(q|o) distribution on HMM states q given an input o. Concretely:  Q  p(o|q) = p(q|o)  p(o) p(q)  ∝ p(q|o)  p(q)  ,  (1)  with p(o) assumed to be equal for all observations. Such hybrid models are usually trained with the embedded Viterbi algorithm (Bourlard & Morgan, 1998) to maximize the likelihood of the data. In other variants of the model, hybrid models are discriminatively trained to optimize seg- mentation accuracy directly (Bengio et al., 1992; 1995). Combined with deep architectures, these models have in- creased accuracies on challenging sequencing tasks primar- ily in voice-recognition (Hinton et al., 2012a).  4. Character Recognition The character recognition problem involves building a character recognizer that when presented with a character image, produces a probability distribution over all charac- ters. In our case, the recognizer classiﬁes characters into 62 classes (26 upper-case, 26 lower-case and 10 digits). The dataset we use for this task is the ICDAR 2003 charac- ter recognition dataset (Lucas et al., 2003) which consists of 6114 training samples and 5379 test samples after re- moving all non-alphanumeric characters as in (Wang et al., 2012). We augment the training dataset with 75,495 char- acter images from the Chars74k English dataset (de Cam- pos et al., 2009) and 50,000 synthetic characters generated by (Wang et al., 2012) making the total size of the training set 131,609 tightly cropped character images. The architecture we use for this task is a ﬁve-layer convo- lutional Maxout network with the ﬁrst three layers being convolution-pooling Maxout layers, the fourth a Maxout layer and ﬁnally a softmax layer on top. The ﬁrst three  layers have respectively 48, 128, 128 ﬁlters of sizes 8-by-8 for the ﬁrst two and 5-by-5 for the third, pooling over re- gions of sizes 4-by-4, 4-by-4 and 2-by-2 respectively, with 2 linear pieces per Maxout unit and a 2-by-2 stride. The 4th layer has 400 units and 5 linear pieces per Maxout unit, fully connected with the softmax output layer. We train the proposed network on 32-by-32 grey-scale character image patches with a simple preprocessing stage of subtracting the mean of every patch and dividing by its standard deviation + (cid:15). Similar to (Goodfellow et al., 2013b), we train this network using stochastic gradient de- scent with momentum and dropout to maximize log p(y|x). Training was done on GPUs using Theano (Bergstra et al., 2010) and pylearn (Goodfellow et al., 2013a). The resulting character recognizer achieves state-of-the-art recognition rates on the ICDAR 2003 character test set with an accuracy of 85.5% on the 62-way case-sensitive bench- mark and 89.9% on the case-insensitive 36-way bench- mark. When we use the Maxout network as a feature ex- tractor and feed the features from the penultimate layer into an SVM with an RBF kernel, the recognition accuracy in- creases to 86% on the 62-way benchmark while it remains roughly the same (89.8%) on the 36-way benchmark. Table 1 compares our results to other works on this dataset. Over- all, the performance of the Maxout networks is slightly su- perior to that of the CNNs used in previous approaches. As a side note, we found that different forms of binarization (otsu, random walkers, grabcuts) and preprocessing meth- ods, such as ZCA as used in (Wang et al., 2012), do not enhance the test accuracy and in some cases, decrease it.  Table 1. Character recognition accuracy on ICDAR 2003 test set. All methods use the same augmented training dataset.  Work  (Coates et al., 2011) (Wang et al., 2012)  this work this work  Method  pre-trained CNNs pre-trained CNNs  Result 81.7 83.9 85.5 Conv-Maxout + SVM 86.0  Conv-Maxout  5. Word Recognition The purpose of the word recognition module is to transcribe a word image into text. Our approach for word recognition is a segmentation-based, lexicon-free approach that could easily incorporate a lexicon during inference or as a post- inference processing tool. As it currently stands, it is difﬁcult to recognize words with high accuracy without any language model due to the char- acter confusion problem, therefore, all of the previous sys- tems rely on lexicons to better the results. However, since lexicons can be very large, we make the distinction in our approach between where query time is linear in the size of  End-to-End Text Recognition with Hybrid HMM Maxout Models  by-2 stride.  the lexicon and those approaches where it is constant. To recognize a word, we ﬁrst segment it into possible char- acters using a Hybrid HMM/Maxout model (Sec.5.1), then use the resulting segmentation to construct a cascade of po- tential characters (Sec.5.2), after which we apply a variant of the Viterbi algorithm that trades off speed and accuracy to compute a list of candidate results (Sec.5.3). Figure 1 depicts the pipeline for the full word recognition module.  Figure 2. Word-to-character Hybrid HMM Maxout module. The Maxout network’s fourth layer is coupled with the HMM.  After creating the dataset, we train the Hybrid HMM model using embedded Viterbi training as deﬁned in (Bourlard & Morgan, 1998). Somewhat surprisingly, we ﬁnd that a sin- gle iteration of the embedded Viterbi is sufﬁcient for the hybrid model to learn to segment; this is likely because the Maxout component is learning a good initial segmentation. After the model is trained, we use it to produce the seg- P (Q|O) with the standard mentation Q such that argmax  Viterbi algorithm.  Q  5.2. Constructing the Cascade  The segmentation produced by the hybrid model suf- fers from two natural shortcomings: over- and under- segmentation. Over-segmentation arises because some characters are composed by concatenating other characters, e.g. V V instead of W . Under-segmentation is more often observed in cases of difﬁcult fonts, blurry images and com- plex background noise. To ﬁlter out instances of over-segmentation, we train a 4-layer convolutional Maxout network with the same ar- chitecture as the Maxout used in Sec.5.1, to predict the probability of over and under-segmentation. This net- work (called Segmentation Correction Maxout in Fig.1) is trained on correct, over-, and under-segmentations gener- ated from the ICDAR 2003 training dataset. We create a new interval from every two adjacent intervals if the joined interval has a higher probability of being a correct segmen- tation than both of its constituents under the learned net- work. As for under-segmentation, we simply divide every resulting interval into two intervals by cutting the resulting intervals in the middle. This operation produces what we call a cascade. Every cas- cade induces an adjacency graph that we use later for infer- ring the corresponding word. Figure 1 depicts a cascade  Figure 1. Flowchart for the word recognition module. Pentagons represent learned modules. The character recognition Maxout module was described in Sec.4. Note that the lexicon can be used either as a post-processing tool, or during inference after a lan- guage model is constructed.  5.1. Hybrid HMM Maxout Model  The use of a hybrid HMM Maxout model for text seg- mentation was inspired by works in voice recognition (Re- nals et al., 1994; Hinton et al., 2012a). Whereas in voice recognition the hybrid model is used to directly sequence phonemes, here we use it to segment word images into character/inter-character regions. The dataset we use for training the model is made from the ﬁrst 500 words in the ICDAR 2003 training-set. Speciﬁ- cally, for every word in the dataset, we create a segmen- tation into character/inter-character regions as follows: for every word, for every character pair that are adjacent, we deﬁne an inter-character region as the region stretching 10% into the left character and 10% into the right character. In the hybrid model we use, depicted in Figure 2, the HMM has four states for each of the character/inter-character re- gions. For the classiﬁer, we use a four-layer convolu- tional Maxout net with the ﬁrst three layers being convo- lution/pooling layers with 48 ﬁlters each, where the ﬁlters were of size 8-by-8 for the ﬁrst two layers and 5-by-5 for the third and a softmax layer on top. The ﬁrst three layers had 2, 2 and 4 linear pieces per Maxout unit respectively and pooling was done on regions of size 4-by-4 with a 2-  }HMMMaxout}End-to-End Text Recognition with Hybrid HMM Maxout Models  for the word “JFC” where the middle row is the segmen- tation from the hybrid HMM/Maxout model along with its induced graph.  Figure 3. A sample cascade with its induced graph. The middle row is the output of the Hybrid HMM Maxout module. The letter J in the top level was produced as an output of the Segmentation Correction Maxout module. The lower row is produced by sys- tematic splitting of all the nodes in the middle row.  5.3. Word Inference  Computing the most likely word given a cascade is equiv- alent to computing the most likely path from the beginning of the cascade to its end. This problem can be solved us- ing dynamic programming in a way similar to the Viterbi algorithm, except where both nodes and edges in the graph incur a cost. Let the alphabet consist of K characters, also, let ci be character with index i, let vk be an interval in the cascade with index k where the cascade intervals are sorted by their left-most point. We deﬁne S(ci, vk) to be the probability of the most likely sequence ending in interval vk and charac- ter ci. We also deﬁne N (vk) to be the set of intervals that immediately precede vk. S can be computed optimally us- ing a Viterbi style algorithm in two cases: with no language model, or with a bigram language model. In the ﬁrst case S becomes:  S(ci, sk) = p(ci|sk)p(sk) max  j,q  S(cj, sq),  (2)  while in the second:  S(ci, sk) = p(ci|sk)p(sk) max  j,q  p(ci|cj)S(cj, sq),  (3)  such that q ∈ N (vk). Computing S takes O(K 2V + V log(V )) where V is the number of intervals in the cascade assuming that for any vk, |N (vk)| = O(1). The most likely word can be found by tracing back the largest S(ci, vk) for all intervals whose  end is the end of the cascade2. While we can obtain p(c|v) as the posterior from the ﬁve- layer character recognition Maxout network (from section 4), we obtain p(v) from the Segmentation Correction Max- out network speciﬁed in Sec.5.2. As for the language model, p(ci|cj), we obtain it from a predeﬁned lexicon. The straight forward generalization of equation (3) to n- gram language models incurs a large time penalty on the order of O(K n). To side step that penalty while allowing for higher order language models we propose an algorithm that trades off accuracy with inference time in a way simi- lar to Beam Search (Russell et al., 1995); keeping the top B candidates for every interval. We call this Cascade Beam Search (see Algorithm 1). Here, p(c|lm, w) is the proba- bility of a character given an n-gram language model lm and a sequence of characters w that come before it, costv is the visual cost of an interval character pair, costl is the linguistic cost of ending in an interval s with a character c, and (cid:107) is the string concatenation operation.  Algorithm 1 Cascade Beam Search  Input: intervals si, language model lm , Beam width B for i = 1 to V do for j ∈ N (vi) do  for ck ∈ Alphabet do  for every word w in Qj do ˆw = w (cid:107) ck costv = p(ck|vi) ∗ p(vi) costl = p(ck|lm, w) cost ˆw = costv ∗ costl ∗ costw Add ( ˆw, cost ˆw) to Qi if size(Qi) > B then  remove word with lowest cost from Qi  end if end for  end for  end for  end for return all w ∈ Qj sorted decreasingly by their costs, such that vj is at the end of the word  5.4. Word Recognition Results  We test our word recognition subsystem on word images from the ICDAR 2003 (Lucas et al., 2003) and SVT (Wang & Belongie, 2010) word recognition test sets. The ICDAR  2One issue with the optimization problem in equation 3 is that it is comparing sequences on different probability spaces such that the sequence length prior is induced by the hybrid model’s seg- mentation and the cascade construction. This particular issue was not directly studied by any works that we know of. Other works in the area have handled it in other ways, for references, consult (Bengio et al., 1995; LeCun et al., 1998)  seEnd-to-End Text Recognition with Hybrid HMM Maxout Models  Table 2. Word recognition accuracies on ICDAR 2003 and SVT datasets  Work  (Wang et al., 2012) (Mishra et al., 2012) (Novikova et al., 2012)  (Wang et al., 2011)  Method CNNs CRFs CRFs PS  This work, 5-gram language model HMM/Maxout HMM/Maxout  This work, edit-distance  90.0 81.8 82.8 76.0 90.1 93.1  84.0 67.8  -  62.0 87.3 88.6  - - - -  83.0 85.1  SVT 70.0 73.2 72.9 57.0 67.0 74.3  W-Small W-Medium W-large  2003 test set consists of images of tightly cropped words. The SVT test set is a harder benchmark with more loosely cropped words and case-wise incorrect labellings. Similar to (Wang et al., 2011; 2012), all of our tests are on words that do not contain non-alphanumeric characters and that are of length greater than 2, leaving 860 and 647 test words for the ICDAR 2003 and SVT datasets respectively. For the ICDAR 2003 test set, we test the recognizer under three scenarios that vary by lexicon size: small, medium and large. In the case of small lexicons, an image’s lexi- con contains the ground truth word in the image in addi- tion to 50 distractor words provided by (Wang et al., 2011). In the medium lexicon case, the lexicon contains all the words in the test set. For the large lexicon case, we use Hunspell’s spell checking dictionary3 that contains almost 50,000 words, in addition to all the words in the test set. We call these scenarios respectively W-Small, W-Medium and W-Large. As for the SVT test set, as in (Wang et al., 2011; 2012), we test the recognizer under a single setting in which for every word, we use other distractor words provided in the dataset. Moreover, since the SVT dataset’s lexions contain only capitalized words, we collapse the classiﬁer’s result p(ck|vi) by setting the probability of a character to the sum of its upper-case and lower-case probabilities. We also test the system in two modes, the ﬁrst is where the system takes constant time in lexicon size per query, while in the second we permit the query to post-process the result with a lexicon. In the ﬁrst mode, we test the system with language mod- els constructed from task-speciﬁc lexicons. While in the second mode, we do not use a language model at all and instead, we use the resulting list of words from the cascade beam search algorithm and consider the recognition result to be the most likely resultant word that exists in the lex- icon, or in case none of the resulting words were in the lexicon, we use the word with the least edit distance to any word in the lexicon. Table 2 compares our results on the benchmarks W-Small,  3available here: http://wordlist.sourceforge.net/  W-Medium, W-Large and SVT to previous published re- sults under the two modes speciﬁed above. All experiments were run with a beam width B = 100. Without the use of either a language model or a lexicon, the module reaches an accuracy of 55.6%. As is shown in table 2, our proposed algorithm outperforms previous state-of-the-art algorithms on the speciﬁed bench- marks. On the large lexicon benchmark, we couldn’t ﬁnd works that were directly comparable to ours. However, we note that when we increase the lexicon size a 1000-fold, we get an accuracy of 85.1% which compares favourably with 78% achieved by (Novikova et al., 2012) when they increase their lexicon size 90-fold.  5.4.1. EFFECT OF BEAM WIDTH  Since the complexity of the cascade beam search algorithm is O(KV B log(B) + V log(V )), we could trade the accu- racy of the algorithm with its speed through the parameter B. Figure 4 shows the effect of the beam width on recog- nition accuracy and on recognition speed on the W-Small task. As shown in the ﬁgure, a small beam width does not lead to a great decrease in accuracy, and permits a great in- crease in recognition speed, making the word recognition module almost 15 times faster.  Figure 4. Beam Width vs. Accuracy on the ICDAR 2003 word recognition dataset under a small lexicon scenario  5.4.2. EFFECT OF LANGUAGE MODEL ORDER  As noted earlier, the Cascade Beam Search algorithm also allows for integration of higher order language models di-  020406080100beam width86889092949698100accuracy (%)012345speed (seconds per query)accuracyspeedEnd-to-End Text Recognition with Hybrid HMM Maxout Models  rectly through the inference stage. This would be most helpful in the cases of very large lexicons, since the infer- ence process takes a constant time in lexicon size after the initial stage of encoding the lexicon by its n-grams. Figure 5 depicts how accuracy changes with the language model’s order for different lexicon sizes. The Small, Medium and Large curves correspond to using the Small, Medium and Large lexicons speciﬁed in section 5.4. The Large* curve corresponds to using the same large lexicon but without adding the ground truth words in the lexicon; this is the only scenario done on case-insensitive words. The high- est accuracy reached under the Large* scenario is 67.0%. It is notable that in the Large* scenario, higher orders of language models cause overﬁtting and thereby reduce the recognition accuracy.  candidate line-level bounding boxes. After we obtain the line-level bounding boxes, we segment these lines using the Line-to-Word Hybrid HMM/Maxout trained to segment lines to words from the ICDAR 2003 scene training set. We then add words by gradually in- troducing gaps from the segmentation one at a time in de- scending size order. After this, we threshold the resulting word bounding boxes using the Word Detection Maxout, a four-layer convolutional Maxout network with the same architecture as the one used in Sec.5.1 on word/non-text images extracted from ICDAR 2003 scene training dataset. We also threshold words on the costv score resulting from the word recognition module and the edit-distance value.  Figure 5. Language model order vs. accuracy by lexicon size on the ICDAR 2003 test set with beam width B = 100. Note that the Small, Medium and Large curves are tested on case-sensitive words while the large* is on case-insensitive words  6. End-to-End Text Recognition In this section, we show how the previous parts can be in- tegrated into a full end-to-end text recognition system. The main issue, unaddressed by the previous sections, is to lo- calize text patches in natural images.  Figure 6. End-to-end pipeline. Pentagons represent learned mod- ules. The word recognition module shown here represents the full system from Fig.1.  We follow this pipeline by doing a non-max suppression (NMS) (Neubeck & Van Gool, 2006) on word boxes that overlap by 30% of the area of their bounding box according to the visual cost costv from the word recognition module.  6.1. End-to-End Pipeline  6.2. End-to-End Results  To extract text locations from an image, we start by getting possible text candidates using Maximally Stable Extremal Regions (MSERs). MSERs are deﬁned to be regions in the image that are either maximas or minimas of image in- tensities with respect to their surroundings. While being highly imprecise text detectors, they can be computed very quickly (Nist´er & Stew´enius, 2008). The use of MSERs allows us to sidestep the enormous time penalty incurred by applying a costly recognizer on multiple scales of the image as in (Wang et al., 2012), thereby allowing our sys- tem to become much more efﬁcient. Since MSERs would ideally correspond to character regions, we form candidate line boxes by clustering the character candidates with DB- SCAN (Ester et al., 1996) using multiple distances to obtain  We tested the above system on both the ICDAR 2003 and SVT end-to-end scene text-recognition test sets. Each of the datasets contain 249 scene images. More speciﬁcally, for the ICDAR 2003 dataset, we conduct tests under ﬁve scenarios, where for the ﬁrst three, the lexicons consist if {5,20,50} distractor words per image in addition to the ground truth words for that image, in the fourth scenario all the test words are included in the lexicon and in the ﬁfth scenario, we use the same large lexicon we used to test the word recognition module (Sec.5.4). We label these scenar- ios I-5, I-20, I-50, I-Full and I-Large respectively. The lex- icons were provided by the authors of (Wang et al., 2011). As for the SVT dataset, we conduct the tests using the lex- icons provided with the dataset. All tests were done with  2345678language model order556065707580859095accuracy (%)SmallMediumLargeLarge*End-to-End Text Recognition with Hybrid HMM Maxout Models  Table 3. End-to-end F-measures on the ICDAR 2003 and SVT datasets  Work  (Wang et al., 2011) (Wang et al., 2012)  This work  I-5 72 76 80  I-20 70 74 79  I-50 68 72 77  I-Full  51 67 70  I-Large  - - 63  SVT 38 46 48  Figure 7. Samples from the end-to-end results, the purple boxes represent the ground truth and the green boxes represent the predictions  the text-recognition module in the edit-distance mode. We test the end-to-end system using the standard preci- sion/recall metrics under the benchmarks speciﬁed in (Lu- cas et al., 2003), where a prediction is considered a hit when the area of the overlap between the predicted box and the target box is greater than 50% of the bounding box area and the predicted text matches exactly. Table 3 compares our results to other results in the ﬁeld. Despite our use of a simple method with low accuracy like MSERs to extract possible text regions, our end-to-end sys- tem is able to outperform previous state-of-the-art end-to- end systems and produce reasonable results for large lexi- cons. Figure 8 shows the precision/recall curves for all the tasks on the ICDAR 2003 dataset and Figure 7 shows a few sample outputs from our system.  Figure 8. Precision/recall curves for the end-to-end system on the ICDAR 2003 dataset under different lexicon sizes  7. Discussion In this section, we discuss possible ways to increase the accuracy of both the word recognition module and the end- to-end system, and possible ways to make the entire system operate under real-time constraints. For the word recognition module, using learned edit- distances (Ristad & Yianilos, 1998; McCallum et al., 2012) would help boost the module’s accuracy. Beyond that, most of the loss in accuracy comes from segmentations created by the hybrid HMM model. Designing a neural net to fac- tor in context information into the hybrid HMM while com- puting posterior probabilities should help reduce that loss in accuracy. To increase the F-measures on the end-to-end system, we should seek to boost recall. As pointed out in (Neumann & Matas, 2011) MSERs do not offer high recall for char- acter location extraction. The alternative of using a time- consuming but highly accurate classiﬁer as in (Wang et al., 2012) is not practical to make the end-to-end system work in real-time. In our opinion, a promising solution would be to develop a Viola-Jones-style cascade (Viola & Jones, 2001) coupled with feature-learning. Such an approach could offer a fast, accurate, easy to train and feature- engineering-free text detector that would increase recall.  8. Conclusion In this work, we presented a novel end-to-end text recogni- tion system. We proposed novel solutions to each subprob-  0.20.30.40.50.60.70.80.91.0precision0.450.500.550.600.650.700.75recallI-5I-20I-50I-FullI-LargeEnd-to-End Text Recognition with Hybrid HMM Maxout Models  lem in the end-to-end system. Speciﬁcally, we leveraged convolutional Maxout networks to beat the state-of-the-art on character recognition. We showed how to use the char- acter recognizer in a word recognizer that is fast, tunable, highly accurate, and scales elegantly with lexicon size. We then constructed an end-to-end text recognition system us- ing the previous modules in addition to other, relatively simple constructs. The proposed system outperforms previ- ous works on end-to-end text recognition tasks on standard challenging benchmarks.  9. Acknowledgements We would like to thank Yoshua Bengio, Aaron Courville and Ian Goodfellow for their insightful comments and dis- cussions. We would also like to thank Paul Kry and Shel- don Andrews for providing the GPUs. Members of the RL lab for helpful discussions. Financial support for this re- search was provided by the NSERC Discovery grant.  References Bengio, Y, De Mori, R, Flammia, G, and Kompe, R. Global op- timization of a neural network-hidden markov model hybrid. Neural Networks, IEEE Transactions on, 3(2):252–259, 1992.  Bengio, Y, LeCun, Y, Nohl, C, and Burges, C. Lerec: A nn/hmm hybrid for on-line handwriting recognition. Neural Computa- tion, 7(6):1289–1303, 1995.  Bergstra, J, Breuleux, O, Bastien, F, Lamblin, P, Pascanu, R, Des- jardins, G, Turian, J, Warde-Farley, D, and Bengio, Y. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Conference (SciPy), 2010.  Bourlard, H and Morgan, N. Hybrid hmm/ann systems for speech recognition: Overview and new research directions. In Adap- tive Processing of Sequences and Data Structures. 1998.  Chen, H, Tsai, S S, Schroth, G, Chen, D M, Grzeszczuk, R, and Girod, B. Robust text detection in natural images with edge- enhanced maximally stable extremal regions. In ICIP, 2011.  Chen, X and Yuille, A. Adaboost learning for detecting and read-  ing text in city scenes. 2011.  Coates, A, Carpenter, B, Case, C, Satheesh, S, Suresh, B, Wang, T, Wu, D J, and Ng, A Y. Text detection and character recog- nition in scene images with unsupervised feature learning. In ICDAR, 2011.  de Campos, T E, Babu, B R, and Varma, M. Character recognition  in natural images. 2009.  Epshtein, B, Ofek, E, and Wexler, Y. Detecting text in natural  scenes with stroke width transform. In CVPR, 2010.  Ester, M, Kriegel, H, Sander, J, and Xu, X. A density-based al- gorithm for discovering clusters in large spatial databases with noise. 1996.  Goodfellow,  I, Warde-Farley, D, Lamblin, P, Dumoulin, V, Mirza, M, Pascanu, R, Bergstra, J, Bastien, F, and Bengio, Y. Pylearn2: a machine learning research library. arXiv:1308.421, 2013a.  Goodfellow, I J., Warde-Farley, D, Mirza, M, Courville, A, and  Bengio, Y. Maxout networks. In ICML, 2013b.  Hanif, S, Prevost, L, and Negri, P. A cascade detector for text  detection in natural scene images. In ICPR, 2008.  Hinton, G, Deng, L, Yu, D, Dahl, G E, Mohamed, A, Jaitly, N, Se- nior, A, Vanhoucke, V, Nguyen, P, Sainath, Tara N, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. Signal Processing Magazine, IEEE, 29(6):82–97, 2012a.  Hinton, G E, Srivastava, N, Krizhevsky, A, Sutskever, I, and Salakhutdinov, R R. Improving neural networks by preventing co-adaptation of feature detectors. arXiv:1207.0580, 2012b.  Hu, J, Brown, M K, and Turin, W. Hmm based online handwriting recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 18(10):1039–1045, 1996.  Krizhevsky, A, Sutskever, I, and Hinton, G. Imagenet classiﬁca- tion with deep convolutional neural networks. In NIPS, 2012.  End-to-End Text Recognition with Hybrid HMM Maxout Models  LeCun, Y, Bottou, L, Bengio, Y, and Haffner, P. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.  Lucas, S, Panaretos, A, Sosa, L, Tang, A, Wong, S, and Young, R.  Icdar 2003 robust reading competitions. 2003.  Matas, J, Chum, O, Urban, M, and Pajdla, T. Robust wide- baseline stereo from maximally stable extremal regions. Image and vision computing, 22(10):761–767, 2004.  McCallum, A, Bellare, K, and Pereira, F. A conditional ran- dom ﬁeld for discriminatively-trained ﬁnite-state string edit distance. arXiv:1207.1406, 2012.  Mishra, A, Alahari, Karteek, Jawahar, CV, et al. Scene text recog-  nition using higher order language priors. 2012.  Morgan, N and Bourlard, H. Continuous speech recognition. Sig-  nal Processing Magazine, IEEE, 12(3):24–42, 1995.  Neubeck, A and Van Gool, L. Efﬁcient non-maximum suppres-  sion. In ICPR, 2006.  Neumann, L and Matas, J. A method for text localization and  recognition in real-world images. In ACCV. 2011.  Nist´er, D and Stew´enius, H. Linear time maximally stable ex-  tremal regions. In ECCV. 2008.  Novikova, T, B, Olga, K, Pushmeet, and L, Victor. Large- lexicon attribute-consistent text recognition in natural images. In ECCV 2012. 2012.  Rabiner, L. A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257–286, 1989.  Renals, S, Morgan, N, Bourlard, H, Cohen, M, and Franco, H. Connectionist probability estimators in hmm speech recogni- tion. Speech and Audio Processing, IEEE Transactions on, 2 (1):161–174, 1994.  Ristad, E and Yianilos, P. Learning string-edit distance. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 20 (5):522–532, 1998.  Russell, S J, Norvig, P, Canny, J F, Malik, J M, and Edwards, D D. Artiﬁcial intelligence: a modern approach, volume 74. Prentice hall Englewood Cliffs, 1995.  Saidane, Z and Garcia, C. Automatic scene text recognition using  a convolutional neural network. 2007.  Shi, C, Wang, C, Xiao, B, Zhang, Y, Gao, S, and Zhang, Z. Scene text recognition using part-based tree-structured character de- tection. 2013.  Viola, P and Jones, M. Rapid object detection using a boosted In Computer Vision and Pattern cascade of simple features. Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on, volume 1, pp. I–511. IEEE, 2001.  Wang, K and Belongie, S. Word spotting in the wild. In ECCV.  2010.  Wang, K, Babenko, B, and Belongie, S. End-to-end scene text  recognition. In ICCV, 2011.  Wang, T, Wu, D, Coates, A, and Ng, A. End-to-end text recogni-  tion with convolutional neural networks. In ICPR, 2012.  ","The problem of detecting and recognizing text in natural scenes has proved tobe more challenging than its counterpart in documents, with most of theprevious work focusing on a single part of the problem. In this work, wepropose new solutions to the character and word recognition problems and thenshow how to combine these solutions in an end-to-end text-recognition system.We do so by leveraging the recently introduced Maxout networks along withhybrid HMM models that have proven useful for voice recognition. Using theseelements, we build a tunable and highly accurate recognition system that beatsstate-of-the-art results on all the sub-problems for both the ICDAR 2003 andSVT benchmark datasets."
1312.4314,2014,Learning Factored Representations in a Deep Mixture of Experts  ,"['David Eigen', ""Marc'Aurelio Ranzato"", 'Ilya Sutskever']",https://arxiv.org/pdf/1312.4314.pdf,"4 1 0 2    r a  M 9         ]  G L . s c [      3 v 4 1 3 4  .  2 1 3 1 : v i X r a  Learning Factored Representations in a  Deep Mixture of Experts  David Eigen 1,2 Marc’Aurelio Ranzato 1 ∗  Ilya Sutskever 1  1 Google, Inc.  2 Dept. of Computer Science, Courant Institute, NYU  deigen@cs.nyu.edu  ranzato@fb.com  ilyasu@google.com  Abstract  Mixtures of Experts combine the outputs of several “expert” networks, each of which specializes in a different part of the input space. This is achieved by train- ing a “gating” network that maps each input to a distribution over the experts. Such models show promise for building larger networks that are still cheap to compute at test time, and more parallelizable at training time. In this this work, we ex- tend the Mixture of Experts to a stacked model, the Deep Mixture of Experts, with multiple sets of gating and experts. This exponentially increases the number of effective experts by associating each input with a combination of experts at each layer, yet maintains a modest model size. On a randomly translated version of the MNIST dataset, we ﬁnd that the Deep Mixture of Experts automatically learns to develop location-dependent (“where”) experts at the ﬁrst layer, and class-speciﬁc (“what”) experts at the second layer. In addition, we see that the different combi- nations are in use when the model is applied to a dataset of speech monophones. These demonstrate effective use of all expert combinations.  1  Introduction  Deep networks have achieved very good performance in a variety of tasks, e.g. [10, 5, 3]. However, a fundamental limitation of these architectures is that the entire network must be executed for all inputs. This computational burden imposes limits network size. One way to scale these networks up while keeping the computational cost low is to increase the overall number of parameters and hidden units, but use only a small portion of the network for each given input. Then, learn a computationally cheap mapping function from input to the appropriate portions of the network. The Mixture of Experts model [7] is a continuous version of this: A learned gating network mixes the outputs of N “expert” networks to produce a ﬁnal output. While this model does not itself achieve the computational beneﬁts outlined above, it shows promise as a stepping stone towards networks that can realize this goal. In this work, we extend the Mixture of Experts to use a different gating network at each layer in a multilayer network, forming a Deep Mixture of Experts (DMoE). This increases the number of effective experts by introducing an exponential number of paths through different combinations of experts at each layer. By associating each input with one such combination, our model uses different subsets of its units for different inputs. Thus it can be both large and efﬁcient at the same time. We demonstrate the effectiveness of this approach by evaluating it on two datasets. Using a jittered MNIST dataset, we show that the DMoE learns to factor different aspects of the data representation at each layer (speciﬁcally, location and class), making effective use of all paths. We also ﬁnd that all combinations are used when applying our model to a dataset of speech monophones.  ∗Marc’Aurelio Ranzato currently works at the Facebook AI Group.  1  N(cid:88) N(cid:88)  i=1  i=1  2 Related Work  A standard Mixture of Experts (MoE) [7] learns a set of expert networks fi along with a gating network g. Each fi maps the input x to C outputs (one for each class c = 1, . . . , C), while g(x) is a distribution over experts i = 1, . . . , N that sums to 1. The ﬁnal output is then given by Eqn. 1  FMoE(x) =  =  gi(x)softmax(fi(x))  p(ei|x)p(c|ei, x) = p(c|x)  (1)  (2)  This can also be seen as a probability model, where the ﬁnal probability over classes is marginalized over the selection of expert: setting p(ei|x) = gi(x) and p(c|ei, x) = softmax(fi(x)), we have Eqn. 2. A product of experts (PoE) [6] is similar, but instead combines log probabilities to form a product:  FPoE(x) ∝ N(cid:89)  N(cid:89)  softmax(fi(x)) =  pi(c|x)  (3)  i=1  i=1  Also closely related to our work is the Hierarchical Mixture of Experts [9], which learns a hierarchy of gating networks in a tree structure. Each expert network’s output corresponds to a leaf in the tree; the outputs are then mixed according to the gating weights at each node. Our model differs from each of these three models because it dynamically assembles a suitable expert combination for each input. This is an instance of the concept of conditional computation put forward by Bengio [1] and examined in a single-layer stochastic setting by Bengio, Leonard and Courville [2]. By conditioning our gating and expert networks on the output of the previous layer, our model can express an exponentially large number of effective experts.  3 Approach  i ) and j ), along with a ﬁnal linear layer f 3 (see Fig. 1). The ﬁnal output is produced by composing  To extend MoE to a DMoE, we introduce two sets of experts with gating networks (g1, f 1 (g2, f 2 the mixtures at each layer:  N(cid:88) M(cid:88)  i=1  z1 =  z2 =  g1 i (x)f 1  i (x)  g2 j (z1)f 2  j (z1)  F (x) = z3 = softmax(f 3(z2))  j=1  i to two layers of linear maps with  i to a single linear map with rectiﬁcation, and each gl  We set each f l rectiﬁcation (but with few hidden units); f 3 is a single linear layer. See Section 4 for details. We train the network using stochastic gradient descent (SGD) with an additional constraint on gating assignments (described below). SGD by itself results in a degenerate local minimum: The experts at each layer that perform best for the ﬁrst few examples end up overpowering the remaining experts. This happens because the ﬁrst examples increase the gating weights of these experts, which in turn causes them to be selected with high gating weights more frequently. This causes them to train more, and their gating weights to increase again, ad inﬁnitum. To combat this, we place a constraint on the relative gating assignments to each expert during train- i(xt(cid:48)) be the running total assignment to expert i of layer l at step t, and ing. Let Gl i(t) be their mean (here, xt(cid:48) is the training example at step t(cid:48)). Then for each let ¯Gl(t) = 1 N i(t) − ¯Gl(t) > m for a margin threshold m, and renormalize the expert i, we set gl  i(t) =(cid:80)t (cid:80)N  i=1 Gl i(xt) = 0 if Gl  t(cid:48)=1 gl  2  (a)  (b)  Figure 1: (a) Mixture of Experts; (b) Deep Mixture of Experts with two layers.  distribution gl(xt) to sum to 1 over experts i. This prevents experts from being overused initially, resulting in balanced assignments. After training with the constraint in place, we lift it and further train in a second ﬁne-tuning phase.  4 Experiments  i x + b1  i and f 2  i and f 2 j  Jittered MNIST  i ), and similarly for f 2  to one-layer linear models with rectiﬁcation, f 1 i (x) = j . We set f 3 to a linear layer, f 3(z2) = W 3z2 + b3. j between 20 and 100. The ﬁnal output  4.1 We trained and tested our model on MNIST with random uniform translations of ±4 pixels, resulting in grayscale images of size 36 × 36. As explained above, the model was trained to classify digits into ten classes. For this task, we set all f 1 max(0, W 1 We varied the number of output hidden units of f 1 from f 3 has 10 units (one for each class). The gating networks g1 and g2 are each composed of two linear+rectiﬁcation layers with either 50 or 20 hidden units, and 4 output units (one for each expert), i.e. g1(x) = softmax(B · max(0, Ax + a) + b), and similarly for g2. We evaluate the effect of using a mixture at the second layer by comparing against using only a single ﬁxed expert at the second layer, or concatenating the output of all experts. Note that for a mixture with h hidden units, the corresponding concatenated model has N · h hidden units. Thus we expect the concatenated model to perform better than the mixture, and the mixture to perform better than the single network. It is best for the mixture to be as close as possible to the concatenated-experts bound. In each case, we keep the ﬁrst layer architecture the same (a mixture). We also compare the two-layer model against a one-layer model in which the hidden layer z1 is mapped to the ﬁnal output through linear layer and softmax. Finally, we compare against a fully- connected deep network with the same total number of parameters. This was constructed using the same number of second-layer units z2, but expanding the number ﬁrst layer units z1 such that the total number of parameters is the same as the DMoE (including its gating network parameters).  3  x	 f11(x)	 f21(x)	 fN1(x)	 g1(x)	 z1	 . . .	 x	 f11(x)	 f21(x)	 fN1(x)	 g1(x)	 z1	 . . .	 f12(x)	 f22(x)	 fM2(x)	 g2(x)	 z2	 . . .	 z3	 4.2 Monophone Speech  In addition, we ran our model on a dataset of monophone speech samples. This dataset is a random subset of approximately one million samples from a larger proprietary database of several hundred hours of US English data collected using Voice Search, Voice Typing and read data [8]. For our experiments, each sample was limited to 11 frames spaced 10ms apart, and had 40 frequency bins. Each input was fed to the network as a 440-dimensional vector. There were 40 possible output phoneme classes. We trained a model with 4 experts at the ﬁrst layer and 16 at the second layer. Both layers had 128 hidden units. The gating networks were each two layers, with 64 units in the hidden layer. As before, we evaluate the effect of using a mixture at the second layer by comparing against using only a single expert at the second layer, or concatenating the output of all experts.  5 Results  5.1  Jittered MNIST  Table 1 shows the error on the training and test sets for each model size (the test set is the MNIST test set with a single random translation per image). In most cases, the deeply stacked experts performs between the single and concatenated experts baselines on the training set, as expected. However, the deep models often suffer from overﬁtting: the mixture’s error on the test set is worse than that of the single expert for two of the four model sizes. Encouragingly, the DMoE performs almost as well as a fully-connected network (DNN) with the same number of parameters, even though this network imposes fewer constraints on its structure. In Fig. 2, we show the mean assignment to each expert (i.e. the mean gating output), both by input translation and by class. The ﬁrst layer assigns experts according to translation, while assignment is uniform by class. Conversely, the second layer assigns experts by class, but is uniform according to translation. This shows that the two layers of experts are indeed being used in complementary ways, so that all combinations of experts are effective. The ﬁrst layer experts become selective to where the digit appears, regardless of its membership class, while the second layer experts are selective to what the digit class is, irrespective of the digit’s location. Finally, Fig. 3 shows the nine test examples with highest gating value for each expert combination. First-layer assignments run over the rows, while the second-layer runs over columns. Note the translation of each digit varies by rows but is constant over columns, while the opposite is true for the class of the digit. Furthermore, easily confused classes tend to be grouped together, e.g. 3 and 5.  Model 4 × 100 − 4 × 100 4 × 100 − 4 × 20 4 × 100 − 4 × 20 4 × 50 − 4 × 20 4 × 100 (one layer)  Model 4 × 100 − 4 × 100 4 × 100 − 4 × 20 4 × 100 − 4 × 20 4 × 50 − 4 × 20 4 × 100 (one layer)  Gate Hids 50 − 50 50 − 50 50 − 20 20 − 20 50  Gate Hids 50 − 50 50 − 50 50 − 20 20 − 20 50  Test Set Error: Jittered MNIST  Single Expert DMoE Concat Layer2 DNN 1.30 1.41 1.40 1.67  1.33 1.58 1.41 1.63 2.86  0.85 1.05 1.04 1.60 2.99  1.42 1.50 1.39 1.77 1.72  0.91 0.96 0.98 1.41 1.78  1.30 1.30 1.30 1.50 1.69  0.77 0.85 0.87 1.33 1.59  Training Set Error: Jittered MNIST  Single Expert DMoE Concat Layer2 DNN 0.60 0.90 0.87 1.32  –  –  Table 1: Comparison of DMoE for MNIST with random translations, against baselines (i) using only one second layer expert, (ii) concatenating all second layer experts, and (iii) a DNN with same total number of parameters. For both (i) and (ii), experts in the ﬁrst layer are mixed to form z1. Models are annotated with “# experts × # hidden units” for each layer.  4  Jittered MNIST: Two-Layer Deep Model by Translation  by Class  Layer 1  Layer 2  1-Layer MoE without jitters  —  Figure 2: Mean gating output for the ﬁrst and second layers, both by translation and by class. Color indicates gating weight. The distributions by translation show the mean gating assignment to each of the four experts for each of the 9× 9 possible translations. The distributions by class show the mean gating assignment to each of the four experts (rows) for each of the ten classes (columns). Note the ﬁrst layer produces assignments exclusively by translation, while the second assigns experts by class. For comparison, we show assignments by class of a standard MoE trained on MNIST without jitters, using 5 experts × 20 hidden units.  5.2 Monophone Speech Table 2 shows the error on the training and test sets. As was the case for MNIST, the mixture’s error on the training set falls between the two baselines. In this case, however, test set performance is about the same for both baselines as well as the mixture. Fig. 4 shows the 16 test examples with highest gating value for each expert combination (we show only 4 experts at the second layer due to space considerations). As before, ﬁrst-layer assignments run over the rows, while the second-layer runs over columns. While not as interpretable as for MNIST, each expert combination appears to handle a distinct portion of the input. This is further bolstered by Fig. 5, where we plot the average number of assignments to each expert combination. Here, the choice of second-layer expert depends little on the choice of ﬁrst-layer expert.  Model 4 × 128 − 16 × 128 4 × 128 (one layer)  Test Set Phone Error: Monophone Speech Gate Hids 64 − 64 64  0.55 0.58  0.55 0.55  Single Expert Mixed Experts Concat Layer2  0.56 0.55  Model 4 × 128 − 16 × 128 4 × 128 (one layer)  Gate Hids 64 − 64 64  Training Set Phone Error: Monophone Speech  Single Expert Mixed Experts Concat Layer2  0.47 0.56  0.42 0.50  0.40 0.50  Table 2: Comparison of DMoE for monophone speech data. Here as well, we compare against baselines using only one second layer expert, or concatenating all second layer experts.  5  (cid:79)(cid:68)(cid:92)(cid:72)(cid:85)(cid:20)(cid:3)(cid:68)(cid:86)(cid:86)(cid:76)(cid:74)(cid:81)(cid:80)(cid:72)(cid:81)(cid:87)(cid:86)(cid:29)(cid:79)(cid:68)(cid:92)(cid:72)(cid:85)(cid:21)(cid:3)(cid:68)(cid:86)(cid:86)(cid:76)(cid:74)(cid:81)(cid:80)(cid:72)(cid:81)(cid:87)(cid:86)(cid:29)(cid:79)(cid:68)(cid:92)(cid:72)(cid:85)(cid:20)(cid:3)(cid:68)(cid:86)(cid:86)(cid:76)(cid:74)(cid:81)(cid:80)(cid:72)(cid:81)(cid:87)(cid:86)(cid:29)(cid:79)(cid:68)(cid:92)(cid:72)(cid:85)(cid:21)(cid:3)(cid:68)(cid:86)(cid:86)(cid:76)(cid:74)(cid:81)(cid:80)(cid:72)(cid:81)(cid:87)(cid:86)(cid:29)(cid:77)(cid:82)(cid:76)(cid:81)(cid:87)(cid:3)(cid:76)(cid:81)(cid:76)(cid:87)(cid:3)(cid:85)(cid:68)(cid:81)(cid:71)(cid:82)(cid:80)(cid:74)(cid:68)(cid:87)(cid:72)(cid:3)(cid:87)(cid:85)(cid:68)(cid:76)(cid:81)(cid:72)(cid:71)(cid:69)(cid:68)(cid:86)(cid:72)(cid:79)(cid:76)(cid:81)(cid:72)(cid:3)(cid:20)(cid:69)(cid:79)(cid:82)(cid:70)(cid:78)(cid:3)(cid:79)(cid:68)(cid:92)(cid:72)(cid:85)(cid:21)(cid:69)(cid:68)(cid:86)(cid:72)(cid:79)(cid:76)(cid:81)(cid:72)(cid:3)(cid:87)(cid:68)(cid:85)(cid:74)(cid:72)(cid:87)(cid:23)(cid:91)(cid:20)(cid:19)(cid:19)(cid:3)(cid:16)(cid:3)(cid:23)(cid:91)(cid:20)(cid:19)(cid:19)(cid:3)(cid:69)(cid:68)(cid:79)(cid:68)(cid:81)(cid:70)(cid:72)(cid:71)(cid:85)(cid:28)(cid:28)(cid:25)(cid:85)(cid:28)(cid:28)(cid:26)(cid:3)(cid:11)(cid:23)(cid:91)(cid:20)(cid:19)(cid:19)(cid:16)(cid:20)(cid:91)(cid:20)(cid:19)(cid:19)(cid:12)(cid:85)(cid:28)(cid:28)(cid:27)(cid:3)(cid:11)(cid:23)(cid:91)(cid:20)(cid:19)(cid:19)(cid:16)(cid:20)(cid:91)(cid:23)(cid:19)(cid:19)(cid:12)(cid:179)(cid:3)(cid:73)(cid:76)(cid:81)(cid:72)(cid:87)(cid:88)(cid:81)(cid:72)(cid:85)(cid:20)(cid:19)(cid:22)(cid:26)(cid:85)(cid:20)(cid:19)(cid:22)(cid:28)(cid:85)(cid:20)(cid:19)(cid:23)(cid:19)(cid:23)(cid:91)(cid:20)(cid:19)(cid:19)(cid:3)(cid:16)(cid:3)(cid:23)(cid:91)(cid:21)(cid:19)(cid:3)(cid:69)(cid:68)(cid:79)(cid:68)(cid:81)(cid:70)(cid:72)(cid:71)(cid:85)(cid:20)(cid:19)(cid:25)(cid:23)(cid:85)(cid:20)(cid:19)(cid:25)(cid:24)(cid:85)(cid:20)(cid:19)(cid:25)(cid:25)(cid:179)(cid:3)(cid:73)(cid:76)(cid:81)(cid:72)(cid:87)(cid:88)(cid:81)(cid:72)(cid:85)(cid:20)(cid:19)(cid:26)(cid:27)(cid:85)(cid:20)(cid:19)(cid:26)(cid:28)(cid:85)(cid:20)(cid:19)(cid:27)(cid:19)(cid:23)(cid:91)(cid:20)(cid:19)(cid:19)(cid:3)(cid:16)(cid:3)(cid:23)(cid:91)(cid:21)(cid:19)(cid:3)(cid:74)(cid:68)(cid:87)(cid:72)(cid:21)(cid:19)(cid:3)(cid:69)(cid:68)(cid:79)(cid:68)(cid:81)(cid:70)(cid:72)(cid:71)(cid:85)(cid:20)(cid:19)(cid:25)(cid:26)(cid:85)(cid:20)(cid:19)(cid:25)(cid:27)(cid:85)(cid:20)(cid:19)(cid:25)(cid:28)(cid:179)(cid:3)(cid:73)(cid:76)(cid:81)(cid:72)(cid:87)(cid:88)(cid:81)(cid:72)(cid:85)(cid:20)(cid:19)(cid:27)(cid:20)(cid:85)(cid:20)(cid:19)(cid:27)(cid:21)(cid:85)(cid:20)(cid:19)(cid:27)(cid:22)(cid:23)(cid:91)(cid:24)(cid:19)(cid:3)(cid:16)(cid:3)(cid:23)(cid:91)(cid:21)(cid:19)(cid:3)(cid:74)(cid:68)(cid:87)(cid:72)(cid:21)(cid:19)(cid:3)(cid:69)(cid:68)(cid:79)(cid:68)(cid:81)(cid:70)(cid:72)(cid:71)(cid:85)(cid:20)(cid:19)(cid:26)(cid:19)(cid:85)(cid:20)(cid:19)(cid:26)(cid:20)(cid:85)(cid:20)(cid:19)(cid:26)(cid:21)(cid:179)(cid:3)(cid:73)(cid:76)(cid:81)(cid:72)(cid:87)(cid:88)(cid:81)(cid:72)(cid:85)(cid:20)(cid:19)(cid:27)(cid:23)(cid:85)(cid:20)(cid:19)(cid:27)(cid:24)(cid:85)(cid:20)(cid:19)(cid:27)(cid:25)(cid:23)(cid:91)(cid:21)(cid:19)(cid:3)(cid:16)(cid:3)(cid:23)(cid:91)(cid:21)(cid:19)(cid:3)(cid:74)(cid:68)(cid:87)(cid:72)(cid:21)(cid:19)(cid:16)(cid:21)(cid:19)(cid:3)(cid:69)(cid:68)(cid:79)(cid:85)(cid:20)(cid:19)(cid:26)(cid:22)(cid:85)(cid:20)(cid:19)(cid:26)(cid:23)(cid:85)(cid:20)(cid:19)(cid:26)(cid:24)(cid:179)(cid:3)(cid:73)(cid:76)(cid:81)(cid:72)(cid:87)(cid:88)(cid:81)(cid:72)(cid:85)(cid:20)(cid:19)(cid:27)(cid:26)(cid:85)(cid:20)(cid:19)(cid:27)(cid:27)(cid:85)(cid:20)(cid:19)(cid:27)(cid:28)(cid:23)(cid:91)(cid:21)(cid:19)(cid:3)(cid:16)(cid:3)(cid:27)(cid:91)(cid:21)(cid:19)(cid:3)(cid:74)(cid:68)(cid:87)(cid:72)(cid:21)(cid:19)(cid:16)(cid:21)(cid:19)(cid:3)(cid:69)(cid:68)(cid:79)(cid:85)(cid:20)(cid:19)(cid:26)(cid:25)(cid:85)(cid:20)(cid:19)(cid:26)(cid:23)(cid:85)(cid:20)(cid:19)(cid:26)(cid:26)(cid:179)(cid:3)(cid:73)(cid:76)(cid:81)(cid:72)(cid:87)(cid:88)(cid:81)(cid:72)(cid:85)(cid:20)(cid:19)(cid:28)(cid:19)(cid:85)(cid:20)(cid:19)(cid:27)(cid:27)(cid:85)(cid:20)(cid:19)(cid:28)(cid:20)(cid:23)(cid:91)(cid:20)(cid:19)(cid:19)(cid:3)(cid:16)(cid:3)(cid:27)(cid:91)(cid:21)(cid:19)(cid:3)(cid:74)(cid:68)(cid:87)(cid:72)(cid:24)(cid:19)(cid:21)(cid:19)(cid:3)(cid:69)(cid:68)(cid:79)(cid:85)(cid:20)(cid:20)(cid:23)(cid:20)(cid:85)(cid:20)(cid:20)(cid:23)(cid:21)(cid:85)(cid:20)(cid:20)(cid:23)(cid:22)(cid:23)(cid:91)(cid:20)(cid:19)(cid:19)(cid:3)(cid:16)(cid:3)(cid:20)(cid:25)(cid:91)(cid:21)(cid:19)(cid:3)(cid:74)(cid:68)(cid:87)(cid:72)(cid:24)(cid:19)(cid:21)(cid:19)(cid:3)(cid:69)(cid:68)(cid:79)(cid:85)(cid:20)(cid:20)(cid:23)(cid:23)(cid:85)(cid:20)(cid:20)(cid:23)(cid:21)(cid:85)(cid:20)(cid:20)(cid:23)(cid:24)(cid:73)(cid:76)(cid:85)(cid:86)(cid:87)(cid:3)(cid:87)(cid:85)(cid:68)(cid:76)(cid:81)(cid:3)(cid:83)(cid:68)(cid:86)(cid:86)(cid:15)(cid:3)(cid:69)(cid:82)(cid:87)(cid:75)(cid:3)(cid:79)(cid:68)(cid:92)(cid:72)(cid:85)(cid:86)(cid:3)(cid:69)(cid:68)(cid:79)(cid:68)(cid:81)(cid:70)(cid:72)(cid:71)(cid:3)(cid:11)(cid:69)(cid:72)(cid:73)(cid:82)(cid:85)(cid:72)(cid:3)(cid:73)(cid:76)(cid:81)(cid:72)(cid:87)(cid:88)(cid:81)(cid:76)(cid:81)(cid:74)(cid:12)(cid:29)(cid:77)(cid:82)(cid:76)(cid:81)(cid:87)(cid:3)(cid:76)(cid:81)(cid:76)(cid:87)(cid:3)(cid:85)(cid:68)(cid:81)(cid:71)(cid:82)(cid:80)(cid:74)(cid:68)(cid:87)(cid:72)(cid:3)(cid:87)(cid:85)(cid:68)(cid:76)(cid:81)(cid:72)(cid:71)(cid:69)(cid:68)(cid:86)(cid:72)(cid:79)(cid:76)(cid:81)(cid:72)(cid:3)(cid:20)(cid:69)(cid:79)(cid:82)(cid:70)(cid:78)(cid:3)(cid:79)(cid:68)(cid:92)(cid:72)(cid:85)(cid:21)(cid:69)(cid:68)(cid:86)(cid:72)(cid:79)(cid:76)(cid:81)(cid:72)(cid:3)(cid:87)(cid:68)(cid:85)(cid:74)(cid:72)(cid:87)(cid:23)(cid:91)(cid:20)(cid:19)(cid:19)(cid:3)(cid:16)(cid:3)(cid:23)(cid:91)(cid:20)(cid:19)(cid:19)(cid:3)(cid:69)(cid:68)(cid:79)(cid:68)(cid:81)(cid:70)(cid:72)(cid:71)(cid:85)(cid:28)(cid:28)(cid:25)(cid:85)(cid:28)(cid:28)(cid:26)(cid:3)(cid:11)(cid:23)(cid:91)(cid:20)(cid:19)(cid:19)(cid:16)(cid:20)(cid:91)(cid:20)(cid:19)(cid:19)(cid:12)(cid:85)(cid:28)(cid:28)(cid:27)(cid:3)(cid:11)(cid:23)(cid:91)(cid:20)(cid:19)(cid:19)(cid:16)(cid:20)(cid:91)(cid:23)(cid:19)(cid:19)(cid:12)(cid:179)(cid:3)(cid:73)(cid:76)(cid:81)(cid:72)(cid:87)(cid:88)(cid:81)(cid:72)(cid:85)(cid:20)(cid:19)(cid:22)(cid:26)(cid:85)(cid:20)(cid:19)(cid:22)(cid:28)(cid:85)(cid:20)(cid:19)(cid:23)(cid:19)(cid:23)(cid:91)(cid:20)(cid:19)(cid:19)(cid:3)(cid:16)(cid:3)(cid:23)(cid:91)(cid:21)(cid:19)(cid:3)(cid:69)(cid:68)(cid:79)(cid:68)(cid:81)(cid:70)(cid:72)(cid:71)(cid:85)(cid:20)(cid:19)(cid:25)(cid:23)(cid:85)(cid:20)(cid:19)(cid:25)(cid:24)(cid:85)(cid:20)(cid:19)(cid:25)(cid:25)(cid:179)(cid:3)(cid:73)(cid:76)(cid:81)(cid:72)(cid:87)(cid:88)(cid:81)(cid:72)(cid:85)(cid:20)(cid:19)(cid:26)(cid:27)(cid:85)(cid:20)(cid:19)(cid:26)(cid:28)(cid:85)(cid:20)(cid:19)(cid:27)(cid:19)(cid:23)(cid:91)(cid:20)(cid:19)(cid:19)(cid:3)(cid:16)(cid:3)(cid:23)(cid:91)(cid:21)(cid:19)(cid:3)(cid:74)(cid:68)(cid:87)(cid:72)(cid:21)(cid:19)(cid:3)(cid:69)(cid:68)(cid:79)(cid:68)(cid:81)(cid:70)(cid:72)(cid:71)(cid:85)(cid:20)(cid:19)(cid:25)(cid:26)(cid:85)(cid:20)(cid:19)(cid:25)(cid:27)(cid:85)(cid:20)(cid:19)(cid:25)(cid:28)(cid:179)(cid:3)(cid:73)(cid:76)(cid:81)(cid:72)(cid:87)(cid:88)(cid:81)(cid:72)(cid:85)(cid:20)(cid:19)(cid:27)(cid:20)(cid:85)(cid:20)(cid:19)(cid:27)(cid:21)(cid:85)(cid:20)(cid:19)(cid:27)(cid:22)(cid:23)(cid:91)(cid:24)(cid:19)(cid:3)(cid:16)(cid:3)(cid:23)(cid:91)(cid:21)(cid:19)(cid:3)(cid:74)(cid:68)(cid:87)(cid:72)(cid:21)(cid:19)(cid:3)(cid:69)(cid:68)(cid:79)(cid:68)(cid:81)(cid:70)(cid:72)(cid:71)(cid:85)(cid:20)(cid:19)(cid:26)(cid:19)(cid:85)(cid:20)(cid:19)(cid:26)(cid:20)(cid:85)(cid:20)(cid:19)(cid:26)(cid:21)(cid:179)(cid:3)(cid:73)(cid:76)(cid:81)(cid:72)(cid:87)(cid:88)(cid:81)(cid:72)(cid:85)(cid:20)(cid:19)(cid:27)(cid:23)(cid:85)(cid:20)(cid:19)(cid:27)(cid:24)(cid:85)(cid:20)(cid:19)(cid:27)(cid:25)(cid:23)(cid:91)(cid:21)(cid:19)(cid:3)(cid:16)(cid:3)(cid:23)(cid:91)(cid:21)(cid:19)(cid:3)(cid:74)(cid:68)(cid:87)(cid:72)(cid:21)(cid:19)(cid:16)(cid:21)(cid:19)(cid:3)(cid:69)(cid:68)(cid:79)(cid:85)(cid:20)(cid:19)(cid:26)(cid:22)(cid:85)(cid:20)(cid:19)(cid:26)(cid:23)(cid:85)(cid:20)(cid:19)(cid:26)(cid:24)(cid:179)(cid:3)(cid:73)(cid:76)(cid:81)(cid:72)(cid:87)(cid:88)(cid:81)(cid:72)(cid:85)(cid:20)(cid:19)(cid:27)(cid:26)(cid:85)(cid:20)(cid:19)(cid:27)(cid:27)(cid:85)(cid:20)(cid:19)(cid:27)(cid:28)(cid:23)(cid:91)(cid:21)(cid:19)(cid:3)(cid:16)(cid:3)(cid:27)(cid:91)(cid:21)(cid:19)(cid:3)(cid:74)(cid:68)(cid:87)(cid:72)(cid:21)(cid:19)(cid:16)(cid:21)(cid:19)(cid:3)(cid:69)(cid:68)(cid:79)(cid:85)(cid:20)(cid:19)(cid:26)(cid:25)(cid:85)(cid:20)(cid:19)(cid:26)(cid:23)(cid:85)(cid:20)(cid:19)(cid:26)(cid:26)(cid:179)(cid:3)(cid:73)(cid:76)(cid:81)(cid:72)(cid:87)(cid:88)(cid:81)(cid:72)(cid:85)(cid:20)(cid:19)(cid:28)(cid:19)(cid:85)(cid:20)(cid:19)(cid:27)(cid:27)(cid:85)(cid:20)(cid:19)(cid:28)(cid:20)(cid:23)(cid:91)(cid:20)(cid:19)(cid:19)(cid:3)(cid:16)(cid:3)(cid:27)(cid:91)(cid:21)(cid:19)(cid:3)(cid:74)(cid:68)(cid:87)(cid:72)(cid:24)(cid:19)(cid:21)(cid:19)(cid:3)(cid:69)(cid:68)(cid:79)(cid:85)(cid:20)(cid:20)(cid:23)(cid:20)(cid:85)(cid:20)(cid:20)(cid:23)(cid:21)(cid:85)(cid:20)(cid:20)(cid:23)(cid:22)(cid:23)(cid:91)(cid:20)(cid:19)(cid:19)(cid:3)(cid:16)(cid:3)(cid:20)(cid:25)(cid:91)(cid:21)(cid:19)(cid:3)(cid:74)(cid:68)(cid:87)(cid:72)(cid:24)(cid:19)(cid:21)(cid:19)(cid:3)(cid:69)(cid:68)(cid:79)(cid:85)(cid:20)(cid:20)(cid:23)(cid:23)(cid:85)(cid:20)(cid:20)(cid:23)(cid:21)(cid:85)(cid:20)(cid:20)(cid:23)(cid:24)(cid:73)(cid:76)(cid:85)(cid:86)(cid:87)(cid:3)(cid:87)(cid:85)(cid:68)(cid:76)(cid:81)(cid:3)(cid:83)(cid:68)(cid:86)(cid:86)(cid:15)(cid:3)(cid:69)(cid:82)(cid:87)(cid:75)(cid:3)(cid:79)(cid:68)(cid:92)(cid:72)(cid:85)(cid:86)(cid:3)(cid:69)(cid:68)(cid:79)(cid:68)(cid:81)(cid:70)(cid:72)(cid:71)(cid:3)(cid:11)(cid:69)(cid:72)(cid:73)(cid:82)(cid:85)(cid:72)(cid:3)(cid:73)(cid:76)(cid:81)(cid:72)(cid:87)(cid:88)(cid:81)(cid:76)(cid:81)(cid:74)(cid:12)(cid:29)(cid:80)(cid:82)(cid:72)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:85)(cid:72)(cid:79)(cid:88)(cid:3)(cid:76)(cid:81)(cid:3)(cid:72)(cid:68)(cid:70)(cid:75)(cid:3)(cid:72)(cid:91)(cid:83)(cid:72)(cid:85)(cid:87)(cid:15)(cid:3)(cid:87)(cid:85)(cid:68)(cid:76)(cid:81)(cid:18)(cid:87)(cid:72)(cid:86)(cid:87)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:86)(cid:82)(cid:73)(cid:87)(cid:80)(cid:68)(cid:91)(cid:29)(cid:85)(cid:68)(cid:81)(cid:71)(cid:82)(cid:80)(cid:3)(cid:68)(cid:86)(cid:86)(cid:76)(cid:74)(cid:81)(cid:80)(cid:72)(cid:81)(cid:87)(cid:86)(cid:3)(cid:72)(cid:86)(cid:86)(cid:72)(cid:81)(cid:87)(cid:76)(cid:68)(cid:79)(cid:79)(cid:92)(cid:3)(cid:71)(cid:82)(cid:76)(cid:81)(cid:74)(cid:3)(cid:80)(cid:82)(cid:71)(cid:72)(cid:79)(cid:3)(cid:68)(cid:89)(cid:72)(cid:85)(cid:68)(cid:74)(cid:76)(cid:81)(cid:74)(cid:3)(cid:69)(cid:72)(cid:87)(cid:90)(cid:72)(cid:72)(cid:81)(cid:3)(cid:24)(cid:3)(cid:20)(cid:16)(cid:79)(cid:68)(cid:92)(cid:72)(cid:85)(cid:3)(cid:85)(cid:72)(cid:79)(cid:88)(cid:3)(cid:81)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78)(cid:86)(cid:86)(cid:72)(cid:68)(cid:85)(cid:70)(cid:75)(cid:66)(cid:79)(cid:82)(cid:86)(cid:86)(cid:3)(cid:68)(cid:86)(cid:86)(cid:76)(cid:74)(cid:81)(cid:80)(cid:72)(cid:81)(cid:87)(cid:86)(cid:3)(cid:79)(cid:82)(cid:82)(cid:78)(cid:3)(cid:81)(cid:82)(cid:87)(cid:3)(cid:86)(cid:82)(cid:3)(cid:69)(cid:68)(cid:71)(cid:3)(cid:16)(cid:16)(cid:3)(cid:69)(cid:88)(cid:87)(cid:3)(cid:87)(cid:75)(cid:72)(cid:92)(cid:3)(cid:68)(cid:83)(cid:83)(cid:72)(cid:68)(cid:85)(cid:3)(cid:87)(cid:82)(cid:3)(cid:73)(cid:76)(cid:87)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:87)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)(cid:86)(cid:72)(cid:87)(cid:3)(cid:87)(cid:82)(cid:82)(cid:3)(cid:80)(cid:88)(cid:70)(cid:75)(cid:3)(cid:83)(cid:72)(cid:85)(cid:75)(cid:68)(cid:83)(cid:86)(cid:34)(cid:86)(cid:72)(cid:68)(cid:85)(cid:70)(cid:75)(cid:66)(cid:79)(cid:82)(cid:86)(cid:86)(cid:3)(cid:68)(cid:86)(cid:86)(cid:76)(cid:74)(cid:81)(cid:80)(cid:72)(cid:81)(cid:87)(cid:86)(cid:17)(cid:3)(cid:3)(cid:87)(cid:85)(cid:68)(cid:76)(cid:81)(cid:30)(cid:3)(cid:72)(cid:89)(cid:68)(cid:79)(cid:66)(cid:87)(cid:72)(cid:86)(cid:87)(cid:15)(cid:3)(cid:72)(cid:89)(cid:68)(cid:79)(cid:66)(cid:87)(cid:85)(cid:68)(cid:76)(cid:81)Figure 3: The nine test examples with highest gating value for each combination of experts, for the jittered mnist dataset. First-layer experts are in rows, while second-layer are in columns.  6 Conclusion  The Deep Mixture of Experts model we examine is a promising step towards developing large, sparse models that compute only a subset of themselves for any given input. We see precisely the gating assignments required to make effective use of all expert combinations: for jittered MNIST, a factorization into translation and class, and distinctive use of each combination for monophone speech data. However, we still use a continuous mixture of the experts’ outputs rather than restricting to the top few — such an extension is necessary to fulﬁll our goal of using only a small part of the model for each input. A method that accomplishes this for a single layer has been described by Collobert et al. [4], which could possibly be adapted to our multilayer case; we hope to address this in future work.  Acknowledgements  The authors would like to thank Matthiew Zeiler for his contributions on enforcing balancing con- straints during training.  6  (cid:73)(cid:73)(cid:3)(cid:81)(cid:72)(cid:87)(cid:86)(cid:15)(cid:3)(cid:86)(cid:76)(cid:81)(cid:74)(cid:79)(cid:72)(cid:3)(cid:69)(cid:79)(cid:82)(cid:70)(cid:78)(cid:29)(cid:86)(cid:76)(cid:81)(cid:74)(cid:79)(cid:72)(cid:3)(cid:79)(cid:68)(cid:92)(cid:72)(cid:85)(cid:29)Figure 4: The 16 test examples with highest gating value for each combination of experts for the monophone speech data. First-layer experts are in rows, while second-layer are in columns. Each sample is represented by its 40 frequency values (vertical axis) and 11 consecutive frames (horizontal axis). For this ﬁgure, we use four experts in each layer.  Monophone Speech: Conditional Assignments  Figure 5: Joint assignment counts for the monophone speech dataset. Here we plot the average product of ﬁrst and second layer gating weights for each expert combination. We normalize each row, to produce a conditional distribution: This shows the average gating assignments in the second layer given a ﬁrst layer assignment. Note the joint assignments are well mixed: Choice of second layer expert is not very dependent on the choice of ﬁrst layer expert. Colors range from dark blue (0) to dark red (0.125).  7  (cid:45)(cid:82)(cid:76)(cid:81)(cid:87)(cid:3)(cid:36)(cid:86)(cid:86)(cid:76)(cid:74)(cid:81)(cid:80)(cid:72)(cid:81)(cid:87)(cid:3)(cid:40)(cid:91)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:86)(cid:47)(cid:68)(cid:92)(cid:72)(cid:85)(cid:3)(cid:20)(cid:47)(cid:68)(cid:92)(cid:72)(cid:85)(cid:3)(cid:21)(cid:23)(cid:3)(cid:72)(cid:91)(cid:83)(cid:72)(cid:85)(cid:87)(cid:86)(cid:69)(cid:82)(cid:87)(cid:75)(cid:3)(cid:79)(cid:68)(cid:92)(cid:72)(cid:85)(cid:86)(cid:40)(cid:91)(cid:83)(cid:72)(cid:85)(cid:87)(cid:86)(cid:3)(cid:36)(cid:86)(cid:86)(cid:76)(cid:74)(cid:81)(cid:80)(cid:72)(cid:81)(cid:87)(cid:86)(cid:47)(cid:68)(cid:92)(cid:72)(cid:85)(cid:3)(cid:20)(cid:51)(cid:72)(cid:85)(cid:3)(cid:39)(cid:68)(cid:87)(cid:68)(cid:83)(cid:82)(cid:76)(cid:81)(cid:87)(cid:47)(cid:68)(cid:92)(cid:72)(cid:85)(cid:3)(cid:21)(cid:37)(cid:92)(cid:3)(cid:47)(cid:68)(cid:69)(cid:72)(cid:79)(cid:45)(cid:82)(cid:76)(cid:81)(cid:87)(cid:29)(cid:3)(cid:90)(cid:72)(cid:79)(cid:79)(cid:3)(cid:80)(cid:76)(cid:91)(cid:72)(cid:71)(cid:70)(cid:82)(cid:79)(cid:82)(cid:85)(cid:3)(cid:86)(cid:70)(cid:68)(cid:79)(cid:72)(cid:62)(cid:3)(cid:19)(cid:17)(cid:19)(cid:3)(cid:17)(cid:17)(cid:3)(cid:19)(cid:17)(cid:20)(cid:21)(cid:24)(cid:3)(cid:64)(cid:70)(cid:82)(cid:79)(cid:82)(cid:85)(cid:3)(cid:86)(cid:70)(cid:68)(cid:79)(cid:72)(cid:62)(cid:3)(cid:19)(cid:17)(cid:19)(cid:3)(cid:17)(cid:17)(cid:3)(cid:20)(cid:17)(cid:19)(cid:3)(cid:64)2  References [1] Y. Bengio. Deep learning of representations: Looking forward. CoRR, abs/1305.0445, 2013.  [2] Y. Bengio, N. L´eonard, and A. C. Courville. Estimating or propagating gradients through  stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. 2  [3] D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, and J. Schmidhuber. Flexible, high  performance convolutional neural networks for image classiﬁcation. In IJCAI, 2011. 1  [4] R. Collobert, Y. Bengio, and S. Bengio. Scaling large learning problems with hard parallel mixtures. International Journal on Pattern Recognition and Artiﬁcial Intelligence (IJPRAI), 17(3):349–365, 2003. 6  [5] A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural net-  works. In ICASSP, 2013. 1  [6] G. E. Hinton. Products of experts. ICANN, 1:1–6, 1999. 2 [7] R. A. Jacobs, M. I. Jordan, S. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts.  Neural Computation, 3:1–12, 1991. 1, 2  [8] N. Jaitly, P. Nguyen, A. Senior, and V. Vanhoucke. Application of pretrained deep neural  networks to large vocabulary speech recognition. Interspeech, 2012. 4  [9] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural  Computation, 6:181–214, 1994. 2  [10] A. Krizhevsky, I. Sutskever, and G.E. Hinton. Imagenet classiﬁcation with deep convolutional  neural networks. In NIPS, 2012. 1  8  ","Mixtures of Experts combine the outputs of several ""expert"" networks, each ofwhich specializes in a different part of the input space. This is achieved bytraining a ""gating"" network that maps each input to a distribution over theexperts. Such models show promise for building larger networks that are stillcheap to compute at test time, and more parallelizable at training time. Inthis this work, we extend the Mixture of Experts to a stacked model, the DeepMixture of Experts, with multiple sets of gating and experts. Thisexponentially increases the number of effective experts by associating eachinput with a combination of experts at each layer, yet maintains a modest modelsize. On a randomly translated version of the MNIST dataset, we find that theDeep Mixture of Experts automatically learns to develop location-dependent(""where"") experts at the first layer, and class-specific (""what"") experts atthe second layer. In addition, we see that the different combinations are inuse when the model is applied to a dataset of speech monophones. Thesedemonstrate effective use of all expert combinations."
1312.5198,2014,Learning Semantic Script Knowledge with Event Embeddings  ,"['Ashutosh Modi', 'ivan titov']",https://arxiv.org/pdf/1312.5198.pdf,"Learning Semantic Script Knowledge with Event Embeddings  4 1 0 2    r p A 5 2         ]  G L . s c [      4 v 8 9 1 5  .  2 1 3 1 : v i X r a  Ashutosh Modi Saarland University, Saarbr¨ucken, Germany Ivan Titov University of Amsterdam, Amsterdam, the Netherlands  ASHUTOSH@COLI.UNI-SB.DE  TITOV@UVA.NL  Abstract  Induction of common sense knowledge about prototypical sequences of events has recently re- ceived much attention (e.g., (Chambers & Juraf- sky, 2008; Regneri et al., 2010)). Instead of in- ducing this knowledge in the form of graphs, as in much of the previous work, in our method, dis- tributed representations of event realizations are computed based on distributed representations of predicates and their arguments, and then these representations are used to predict prototypical event orderings. The parameters of the com- positional process for computing the event rep- resentations and the ranking component of the model are jointly estimated from texts. We show that this approach results in a substantial boost in ordering performance with respect to previous methods.  1. Introduction It is generally believed that natural language understanding systems would beneﬁt from incorporating common-sense knowledge about prototypical sequences of events and their participants. Early work focused on structured representa- tions of this knowledge (called scripts (Schank & Abelson, 1977)) and manual construction of script knowledge bases. However, these approaches do not scale to complex do- mains (Mueller, 1998; Gordon, 2001). More recently, auto- matic induction of script knowledge from text have started to attract attention: these methods exploit either natural texts (Chambers & Jurafsky, 2008; 2009) or crowdsourced data (Regneri et al., 2010), and, consequently, do not re- quire expensive expert annotation. Given a text corpus, they extract structured representations (i.e. graphs), for ex- ample chains (Chambers & Jurafsky, 2008) or more gen-  Accepted at the workshop track of International Conference on Learning Representations (ICLR), 2014  eral directed acyclic graphs (Regneri et al., 2010). These graphs are scenario-speciﬁc, nodes in them correspond to events (and associated with sets of potential event men- tions) and arcs encode the temporal precedence relation. These graphs can then be used to inform NLP applica- tions (e.g., question answering) by providing information whether one event is likely to precede or succeed another. In this work we advocate constructing a statistical model which is capable of “answering” at least some of the ques- tions these graphs can be used to answer, but doing this without explicitly representing the knowledge as a graph. In our method, the distributed representations (i.e. vec- tors of real numbers) of event realizations are computed based on distributed representations of predicates and their arguments, and then the event representations are used in a ranker to predict the expected ordering of events. Both the parameters of the compositional process for computing the event representation and the ranking component of the model are estimated from data. In order to get an intuition why the embedding approach may be attractive, consider a situation where a prototypi- cal ordering of events the bus disembarked passengers and the bus drove away needs to be predicted. An approach based on frequency of predicate pairs (Chambers & Juraf- sky, 2008), is unlikely to make a right prediction as driv- ing usually precedes disembarking. Similarly, an approach which treats the whole predicate-argument structure as an atomic unit (Regneri et al., 2010) will probably fail as well, as such a sparse model is unlikely to be effectively learn- able even from large amounts of data. However, our em- bedding method would be expected to capture relevant fea- tures of the verb frames, namely, the transitive use for the predicate disembark and the effect of the particle away, and these features will then be used by the ranking component to make the correct prediction. In previous work on learning inference rules (Berant et al., 2011), it has been shown that enforcing transitivity con- straints on the inference rules results in signiﬁcantly im- proved performance. The same is true for the event order-  Learning Semantic Script Knowledge with Event Embeddings  Algorithm 1 Learning Algorithm  Notation w : ranking weight vector Ek : kth sequence of events in temporal order tk : array of model scores for events in Ek γ : ﬁxed global margin for ranking  Figure 1. Computation of an event representation (the bus disem- barked passengers).  ing task, as scripts have largely linear structure, and ob- serving that a ≺ b and b ≺ c is likely to imply a ≺ c. In- terestingly, in our approach we implicitly learn the model which satisﬁes transitivity constraints, without the need for any explicit global optimization on a graph. The approach is evaluated on crowdsourced dataset of Reg- neri et al. (2010) and we demonstrate that using our model results in the 13.5% absolute improvement in F 1 on event ordering with respect to their graph induction method (84% vs. 71%).  2. Model In this section we describe the model we use for computing event representations as well as the ranking component of our model.  2.1. Event Representation  Learning and exploiting distributed word representations (i.e. vectors of real values, also known as embeddings) have been shown to be beneﬁcial in many NLP applica- tions (Bengio et al., 2001; Turian et al., 2010; Collobert et al., 2011). These representations encode semantic and syntactic properties of a word, and are normally learned in the language modeling setting (i.e. learned to be predictive of local word context), though they can also be specialized by learning in the context of other NLP applications such as PoS tagging or semantic role labeling (Collobert et al., 2011). More recently, the area of distributional composi- tional semantics have started to emerge (Baroni & Zam- parelli, 2011; Socher et al., 2012), they focus on induc- ing representations of phrases by learning a compositional model. Such a model would compute a representation of a phrase by starting with embeddings of individual words in the phrase, often this composition process is recursive and guided by some form of syntactic structure.  LearnWeights() for epoch = 1 to T for k = 1 to K  for i = 1 to |Ek|  [over event sequences] [over events in the seq]  Compute embedding xei for event ei Calculate score sei = wT xei  end for Collect scores in tk = [se1, . . . , sei, . . .] error = RankingError(tk) back-propagate error update all embedding parameters and w  end for  end for  RankingError(tk) err = 0 for rank = 1, . . . , l  for rankBef ore = 1, . . . , rank  if (tk[rankBef ore] − tk[rank]) < γ  err = err + 1  end if end for for rankAf ter = rank + 1, . . . , l  if (tk[rank] − tk[rankAf ter]) < γ  err = err + 1  end if end for  end for return err  In our work, we use a simple compositional model for rep- resenting semantics of a verb frame (i.e. the predicate and its arguments). The model is shown in Figure 1. Each word wi in the vocabulary is mapped to a real vector based on the corresponding lemma (the embedding function C). The hidden layer is computed by summing linearly transformed predicate and argument embeddings and passing it through the logistic sigmoid function.1 We use different transfor- mation matrices for arguments and predicates, T and R, respectively. The event representation x is then obtained by applying another linear transform (matrix A) followed by another application of the sigmoid function. These event representations are learned in the context of  1Only syntactic heads of arguments are used in this work. If an argument is a coffee maker, we will use only the word maker.  disembarkedpassengersbuspredicate embedding event embeddingarg embeddingTa1RpTa2xa1=C(bus)a2=C(passenger)p=C(disembark)arg embeddinghidden layerhAhLearning Semantic Script Knowledge with Event Embeddings  Precision (%)  EEverb MSA 80.0 81.9 70.0 73.7 53.0 81.0 48.0 94.1 80.1 78.0 47.0 79.2 67.0 71.4 48.0 76.2 83.0 87.8 87.3 84.0 65.8 81.3  BS 76.0 68.0 97.0 87.0 87.0 91.0 77.0 85.0 92.0 90.0 85.0  EE 85.1 69.5 90.0 92.4 86.9 82.9 80.7 80.0 87.5 84.2 83.9  BL 71.3 72.6 65.1 68.6 67.3 63.4 68.0 62.5 62.8 60.6 66.2  Recall (%) EEverb MSA 80.0 75.8 78.0 75.1 81.0 79.1 75.0 91.4 69.8 72.0 83.0 62.8 64.0 67.7 82.0 80.0 86.0 87.9 87.6 85.0 78.6 77.2  BS 76.0 57.0 65.0 72.0 69.0 74.0 59.0 84.0 87.0 74.0 71.7  BL 70.1 70.1 69.9 74.0 73.4 72.6 72.7 62.2 67.6 66.4 69.9  EE 91.9 71.0 87.9 89.7 80.2 90.3 76.9 84.3 89.0 81.9 84.3  BL 70.7 71.3 67.4 71.0 70.2 67.7 70.3 62.3 65.1 63.3 68.0  F1 (%) EEverb MSA 80.0 78.8 74.0 74.4 64.0 80.0 58.0 92.8 69.8 75.0 60.0 70.0 66.0 69.5 61.0 78.1 84.0 87.8 84.9 84.0 70.6 79.1  BS 76.0 62.0 78.0 79.0 77.0 82.0 67.0 85.0 89.0 81.0 77.6  EE 88.4 70.2 88.9 91.0 83.4 86.4 78.7 82.1 88.2 88.2 84.1  Scenario  Bus Coffee Fastfood Ret. Food  Iron  Microwave Scr. Eggs Shower Telephone Vending Average  Table 1. Results on the crowdsourced data for the verb-frequency baseline (BL), the verb-only embedding model (EEverb), Regneri et al. (2010) (MSA), Frermann et al. (2014)(BS) and the full model (EE).  event ranking: the transformation parameters as well as representations of words are forced to be predictive of the temporal order of events. However, one important charac- teristic of neural network embeddings is that they can be in- duced in a multitasking scenario, and consequently can be learned to be predictive of different types of contexts pro- viding a general framework for inducing different aspects of (semantic) properties of events, as well as exploiting the same representations in different applications.  2.2. Learning to Order  The task of learning stereotyped order of events naturally corresponds to the standard ranking setting. Here, we as- sume that we are provided with sequences of events, and our goal is to capture this order. We discuss how we obtain this learning material in the next section. We learn a linear ranker (characterized by a vector w) which takes an event representation and returns a ranking score. Events are then ordered according to the score to yield the model predic- tion. Note that during the learning stage we estimate not only w but also the event representation parameters, i.e. matrices T , R and A, and the word embedding C. Note that by casting the event ordering task as a global rank- ing problem we ensure that the model implicitly exploits transitivity of the temporal relation, the property which is crucial for successful learning from ﬁnite amount of data, as we argued in the introduction and will conﬁrm in our experiments. We use an online ranking algorithm based on the Percep- tron Rank (PRank, (Crammer & Singer, 2001)), or, more accurately, its large-margin extension. One crucial differ- ence though is that the error is computed not only with re- spect to w but also propagated back through the structure of the neural network. The learning procedure is sketched in Algorithm 1. Additionally, we use a Gaussian prior on weights, regularizing both the embedding parameters and the vector w. We initialize word representations using the  SENNA embeddings (Collobert et al., 2011).2  3. Experiments We evaluate our approach on crowdsourced data collected for script induction by Regneri et al. (2010), though, in principle, the method is applicable in arguably more gen- eral setting of Chambers & Jurafsky (2008).  3.1. Data and task  Regneri et al. (2010) collected short textual descriptions (called event sequence descriptions, ESDs) of various types of human activities (e.g., going to a restaurant, ironing clothes) using crowdsourcing (Amazon Mechanical Turk), this dataset was also complemented by descriptions pro- vided in the OMICS corpus (Gupta & Kochenderfer, 2004). The datasets are fairly small, containing 30 ESDs per ac- tivity type in average (we will refer to different activities as scenarios), but the collection can easily be extended given the low cost of crowdsourcing. The ESDs are written in a bullet-point style and the annotators were asked to follow the temporal order in writing. Consider an example ESD for the scenario prepare coffee : {go to coffee maker} → {ﬁll water in coffee maker} → {place the ﬁlter in holder} → {place coffee in ﬁlter} → {place holder in coffee maker} → {turn on coffee maker} Though individual ESDs may seem simple, the learning task is challenging because of the limited amount of train- ing data, variability in the used vocabulary, optionality of events (e.g., going to the coffee machine may not be men- tioned in a ESD), different granularity of events and vari- ability in the ordering (e.g., coffee may be put in a ﬁlter before placing it in a coffee maker).  2When we kept the word representations ﬁxed to the SENNA embeddings and learned only matrices T , R and A, we obtained similar results (0.3% difference in the average F1 score).  Learning Semantic Script Knowledge with Event Embeddings  Unlike our work, Regneri et al. (2010) relies on WordNet to provide extra signal when using the Multiple Sequence Alignment (MSA) algorithm. As in their work, each de- scription was preprocessed to extract a predicate and heads of argument noun phrases to be used in the model. The methods are evaluated on human annotated scenario- speciﬁc tests: the goal is to classify event pairs as appearing in a given stereotypical order or not (Regneri et al., 2010).3 The model was estimated as explained in Section 2.2 with the order of events in ESDs treated as gold standard. We used 4 held-out scenarios to choose model parameters, no scenario-speciﬁc tuning was performed, and the 10 test scripts were not used to perform model selection. When testing, we predicted that the event pair (e1,e2) is in the stereotypical order (e1 ≺ e2) if the ranking score for e1 exceeded the ranking score for e2  3.2. Results and discussion  In our experiments, we compared our event embedding model (EE) against three baseline systems (BL , MSA) and BSMSA is the system of Regneri et al. (2010). BS is a a hierarchical Bayesian system of Frermann et al. (2014). BL chooses the order of events based on the preferred or- der of the corresponding verbs in the training set: (e1, e2) is predicted to be in the stereotypical order if the number of times the corresponding verbs v1 and v2 appear in this or- der in the training ESDs exceeds the number of times they appear in the opposite order (not necessary at adjacent po- sitions); a coin is tossed to break ties (or if v1 and v2 are the same verb). We also compare to the version of our model which uses only verbs (EEverbs). Note that EEverbs is conceptually very similar to BL, as it essentially induces an ordering over verbs. However, this ordering can beneﬁt from the im- plicit transitivity assumption used in EEverbs (and EE), as we discussed in the introduction. The results are presented in Table 1. The ﬁrst observation is that the full model improves sub- stantially over the baseline and the previous methods (MSA and BS) (13.5% and 6.5% improvement over MSA and BS respectively in F1), this improvement is largely due to an increase in the recall but the precision is not negatively af- fected. We also observe a substantial improvement in all metrics from using transitivity, as seen by comparing the results of BL and EEverb (11.3% improvement in F1). This simple approach already outperforms the pipelined MSA system. These results seem to support our hypothesis in  3The unseen event pairs are not coming from the same ESDs making the task harder: the events may not be in any temporal relation. This is also the reason for using the F1 score rather than the accuracy, both in Regneri et al. (2010) and in our work.  the introduction that inducing graph representations from scripts may not be an optimal strategy from the practical perspective.  References Baroni, Marco and Zamparelli, Robert. Nouns are vec- tors, adjectives are matrices: Representing adjective- In Proceedings noun constructions in semantic space. of EMNLP, 2011.  Bengio, Yoshua, Ducharme, R´ejean, and Vincent, Pascal. A neural probabilistic language model. In Proceedings of NIPS, 2001.  Berant, Jonathan, Dagan, Ido, and Goldberger, Jacob. Global learning of typed entailment rules. In Proceed- ings of ACL, 2011.  Chambers, Nathanael and Jurafsky, Dan. Unsupervised learning of narrative schemas and their participants. In Proceedings of ACL, 2009.  Chambers, Nathanael and Jurafsky, Daniel. Unsupervised In Proceedings of  learning of narrative event chains. ACL, 2008.  Collobert, R., Weston,  J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. Natural language pro- Journal of Machine cessing (almost) from scratch. Learning Research, 12:2493–2537, 2011.  Crammer, Koby and Singer, Yoram. Pranking with ranking.  In Proceedings of NIPS, 2001.  Frermann, Lea, Titov, Ivan, and Pinkal, Manfred. A hi- erarchical bayesian model for unsupervised induction of script knowledge. In EACL, Gothenberg, Sweden, 2014.  Gordon, Andrew. Browsing image collections with repre- sentations of common-sense activities. JAIST, 52(11), 2001.  Gupta, Rakesh and Kochenderfer, Mykel J. Common sense data acquisition for indoor mobile robots. In Proceed- ings of AAAI, 2004.  Mueller, Erik T.  Natural Language Processing with  Thought Treasure. Signiform, 1998.  Regneri, Michaela, Koller, Alexander, and Pinkal, Man- fred. Learning script knowledge with web experiments. In Proceedings of ACL, 2010.  Schank, R. C and Abelson, R. P. Scripts, Plans, Goals, and Understanding. Lawrence Erlbaum Associates, Po- tomac, Maryland, 1977.  Learning Semantic Script Knowledge with Event Embeddings  Socher, Richard, Huval, Brody, Manning, Christopher D., and Ng, Andrew Y. Semantic compositionality through In Proceedings of recursive matrix-vector spaces. EMNLP, 2012.  Turian, Joseph, Ratinov, Lev, and Bengio, Yoshua. Word representations: A simple and general method for semi- supervised learning. In Proceedings of ACL, 2010.  ","Induction of common sense knowledge about prototypical sequences of eventshas recently received much attention. Instead of inducing this knowledge in theform of graphs, as in much of the previous work, in our method, distributedrepresentations of event realizations are computed based on distributedrepresentations of predicates and their arguments, and then theserepresentations are used to predict prototypical event orderings. Theparameters of the compositional process for computing the event representationsand the ranking component of the model are jointly estimated from texts. Weshow that this approach results in a substantial boost in ordering performancewith respect to previous methods."
1312.5242,2014,Unsupervised feature learning by augmenting single images  ,"['Alexey Dosovitskiy', 'Jost Tobias Springenberg', 'Thomas Brox']",https://arxiv.org/pdf/1312.5242.pdf,"4 1 0 2     b e F 6 1         ]  V C . s c [      3 v 2 4 2 5  .  2 1 3 1 : v i X r a  Unsupervised feature learning by augmenting single  images  Alexey Dosovitskiy, Jost Tobias Springenberg and Thomas Brox  Department of Computer Science  University of Freiburg  79110, Freiburg im Breisgau, Germany  {dosovits,springj,brox}@cs.uni-freiburg.de  Abstract  When deep learning is applied to visual object recognition, data augmentation is often used to generate additional training data without extra labeling cost. It helps to reduce overﬁtting and increase the performance of the algorithm. In this paper we investigate if it is possible to use data augmentation as the main component of an unsupervised feature learning architecture. To that end we sample a set of random image patches and declare each of them to be a separate single-image surrogate class. We then extend these trivial one-element classes by applying a variety of transformations to the initial ’seed’ patches. Finally we train a convolu- tional neural network to discriminate between these surrogate classes. The feature representation learned by the network can then be used in various vision tasks. We ﬁnd that this simple feature learning algorithm is surprisingly successful, achiev- ing competitive classiﬁcation results on several popular vision datasets (STL-10, CIFAR-10, Caltech-101).  1  Introduction  Deep convolutional neural networks trained via backpropagation have recently been shown to per- form well on image classiﬁcation tasks containing millions of images and thousands of categories [17, 24]. While deep convolutional neural networks have been known to yield good results on su- pervised image classiﬁcation tasks such as MNIST for a long time [18], the recent successes are made possible through optimized implementations, efﬁcient model averaging and data augmenta- tion techniques [17]. The feature representation learned by these networks achieves state of the art performance not only on the classiﬁcation task the network is trained for, but also on various other computer vision tasks, for example: classiﬁcation on Caltech-101 [24, 7], Caltech-256 [24], Caltech-UCSD birds dataset [7], SUN-397 scene recognition database [7]; detection on PASCAL VOC dataset [9]. This capability to generalize to new datasets indicates that supervised discrimi- native learning is currently the best known algorithm for visual feature learning. The downside of this approach is the need for expensive labeling, as the amount of required labels grows quickly the larger the model gets. For this reason unsupervised learning, although currently underperforming, remains an appealing paradigm, since it can make use of raw unlabeled images and videos which are readily available in virtually inﬁnite amounts. In this work we aim to combine the power of discriminative supervised learning with the simplicity of unsupervised data acquisition. The main novelty of our approach is the way we obtain training data for a convolutional network in an unsupervised manner. In the standard supervised setting there exists a large set of labeled images, which may be further augmented by small translations, rotations or color variations to generate even more (and more diverse) training data.  1  In contrast, our method does not require any labeled data at all: we use the augmentation step alone to create surrogate training data from a set of unlabeled images. We start with trivial surrogate classes consisting of one random image patch each, and then augment the data by applying a ran- dom set of transformations to each patch. After that we train a convolutional neural network to classify these surrogate classes. The feature representation learned by the network is, by construc- tion, discriminative and at the same time invariant to typical data transformations. Nevertheless it is not immediately clear: Would the feature representation learned from this surrogate task perform well on general image classiﬁcation problems? Our experiments show that, indeed, this simple unsupervised feature learning algorithm achieves competitive or state of the art results on several benchmarks. By performing image augmentation we provide prior knowledge about natural image distribution to the training algorithm. More precisely, by assigning the same label to all transformed versions of an image patch we force the learned feature representation to be invariant to the transformations applied. This can be seen as an indirect form of supervision: our algorithm needs some expert knowledge about which transformations the features should be invariant to. However, similar ex- pert knowledge is used in most other unsupervised feature learning algorithms. Features are usu- ally learned from small image patches, which assumes translational invariance. Turning images to grayscale assumes invariance to color changes. Whitening or contrast normalization assumes invariance to contrast changes and, largely, color variations.  1.1 Related work  Our approach is related to a large body of work on unsupervised learning and convolutional neural networks. In contrast to our method, most unsupervised learning approaches, e.g. [13, 14, 23, 6, 25], rely on modeling the input distribution explicitly – often via a reconstruction error term – rather than training a discriminative model and thus cannot be used to jointly train multiple layers of a deep neural network in a straightforward manner. Among these unsupervised methods, most similar to our approach are several studies on learning invariant representations from transformed input samples, for example [22, 25, 15]. Our proposed method can be related to work on metric learning, for example [10, 12]. However, instead of enforcing a metric on the feature representation directly, as in [12], we only implicitly force the representation of transformed images to be mapped close together through the introduced surrogate labels. This enables us to use discriminative training for learning a feature representation which performs well in classiﬁcation tasks. Learning invariant features with a discriminative objective was previously considered in early work on tangent propagation [21], which aims to learn features invariant to small predeﬁned transforma- tions by directly penalizing the derivative of the network output with respect to the parameters of the transformation. In contrast to their work, our algorithm does not rely on labeled data and is less dependent on a small magnitude of the applied transformations. Tangent propagation has been successfully combined with an unsupervised feature learning algorithm in [20] to build a classiﬁer exploiting information about the manifold structure of the learned representation. This, however, again comes with the disadvantages of reconstruction-based training. Loosely related to our work is research on using unlabeled data for regularizing supervised algo- rithms, for example self-training [2] or entropy regularization [11, 19]. In contrast to these semi- supervised methods, our training procedure, as mentioned before, does not make any use of labeled data. Finally, the idea of creating a pseudo-task to improve the performance of a supervised algo- rithm is used in [1].  2 Learning algorithm  Here we describe in detail our feature learning pipeline. The two main stages of our approach are generating the surrogate training data and training a convolutional neural network using this data.  2  Figure 1: Random patches sampled from the STL-10 unlabeled dataset which are later augmented by various transformation to ob- tain surrogate classes for the neural network training.  Figure 2: Random transformations applied to one of the patches extracted from the STL-10 unlabeled dataset. Original patch is in the top left corner.  2.1 Data acquisition  The input to our algorithm is a set of unlabeled images, which come from roughly the same distribu- tion as the images we later aim to classify. We randomly sample N ∈ [50, 32000] random patches of size 32 × 32 pixels from different images, at varying positions and scales. We only sample from regions with considerable gradient energy to avoid getting uniformly colored patches. Then we ap- ply K ∈ [1, 100] random transformations to each of the sampled patches. Each of these random transformations is a composition of four random ’elementary’ transformations from the following list:  horizontally.  • Translation: translate the patch by a distance within 0.25 of the patch size vertically and • Scale: multiply the scale of the patch by a factor between 0.7 and 1.4. • Color: multiply the projection of each patch pixel onto the principal components of the set of all pixels by a factor between 0.5 and 2 (factors are independent for each principal component and the same for all pixels within a patch). • Contrast: raise saturation and value (S and V components of the HSV color representation)  of all pixels to a power between 0.25 and 4 (same for all pixels within a patch).  We do not apply any preprocessing to the obtained patches other than subtracting the mean of each pixel over the whole training dataset. Examples of patches sampled from the STL-10 unlabeled dataset are shown in Fig. 1. Examples of transformed versions of one patch are shown in Fig. 2.  2.2 Training As a result of the procedure described above, to each patch xi ∈ X from the set of initially sampled patches X = {x1, . . . xN} we apply a set of transformations Ti = {T 1 i } and get a set of i xi| T j its transformed versions Sxi = Tixi = {T j i ∈ Ti}. We then declare each of these sets to be (cid:88) (cid:88) a class by assigning label i to the class Sxi and train a convolutional neural network to discriminate between these surrogate classes. Formally, we minimize the following loss function:  i , . . . , T K  (1)  L(X) =  l(i, T j  i xi),  xi∈X  i ∈Ti T j  i xi) is the loss on the sample T j  where l(i, T j i xi with (surrogate) true label i. We use a convolutional neural network with cross entropy loss on top of the softmax output layer of the network, hence in our case  i xi)), CE(y, f ) = −(cid:88)  l(i, T j  i xi) = CE(ei, f (T j  yk log fk,  (2)  k  3  where f denotes the function computing the values of the output layer of the neural network given the input data, and ei is the ith standard basis vector. For training the network we use an implementation based on the fast convolutional neural network code from [17], modiﬁed to support dropout. We use a ﬁxed network architecture in all experiments: 2 convolutional layers with 64 ﬁlters of size 5 × 5 each followed by 1 fully connected layer of 128 neurons with dropout and a softmax layer on top. We perform 2× 2 max-pooling after convolutional layers and do not perform any contrast normalization between layers. We start with a learning rate of 0.01 and gradually decrease the learning rate during training. That is, we train until there is no improvement in validation error, then decrease the learning rate by a factor of 3, and repeat this procedure several times until there is no more signiﬁcant improvement in validation error.  2.2.1 Pre-training  In some of our experiments, in which the number of surrogate classes is large relative to the number of training samples per surrogate class, we observed that during the training process the training error does not signiﬁcantly decrease compared to initial chance level. To alleviate this problem, before training the network on the whole surrogate dataset we pre-train it on a subset with fewer surrogate classes, typically 100. We stop the pre-training as soon as the training error starts falling, indicating that the optimization found a direction towards a good local minimum. We then use the weights learned by this pre-training phase as an initialization for training on the whole surrogate dataset.  2.3 Testing  When the training procedure is ﬁnished, we apply the learned feature representation to classiﬁcation tasks on ’real’ datasets, consisting of images which may differ in size from the surrogate training images. To extract features from these new images, we convolutionally compute the responses of all the network layers except the top softmax and form a 3-layer spatial pyramid of them. We then train a linear support vector machine (SVM) on these features. We select the hyperparameters of the SVM via crossvalidation.  3 Experiments  We report our classiﬁcation results on the STL-10, CIFAR-10 and Caltech-101 datasets, approaching or exceeding state of the art for unsupervised algorithms on each of them. We also evaluate the effects of the number of surrogate classes and the number of training samples per surrogate class in the training data. For training the network in all our experiments we generate a surrogate dataset using patches extracted from the STL-10 unlabeled dataset. For STL-10 we use the usual testing protocol of averaging the results over 10 pre-deﬁned folds of training data and report the mean and the standard deviation. For CIFAR-10 we report two results: ’CIFAR-10’ means training on the whole CIFAR-10 training set and ’CIFAR-10-reduced’ means the average over 10 random selections of 400 training samples per class. For Caltech-101 we follow the usual protocol with selecting 30 random samples per class for training and not more than 50 training samples per class for testing, repeated 10 times.  3.1 Classiﬁcation results  In Table 1 we compare our classiﬁcation results to other recent work. Our network is trained on a surrogate dataset with 8000 surrogate classes containing 150 samples each. We remind that for extracting features during test time we use the ﬁrst 3 layers of the network with 64, 64 and 128 ﬁlters respectively. The feature representation is hence considerably more compact than in most competing approaches. We do not list the results of supervised methods on CIFAR-10 (the best of which currently exceed 90% accuracy), since those are not directly comparable to our unsupervised feature learning method. As can be seen in the table, our results are comparable to state of the art on CIFAR-10 and exceed the performance of many unsupervised algorithms on Caltech-101. On STL-10 for which the image  4  K-means [6] Multi-way local pooling [5] Slowness on videos [25] Receptive ﬁeld learning [16] Hierarchical Matching Pursuit (HMP) [3] Multipath HMP [4] Sum-Product Networks [8] View-Invariant K-means [15] This paper  STL-10 60.1 ± 1  — 61.0 —  64.5 ± 1  —  62.3 ± 1  63.7  67.4 ± 0.6  CIFAR-10-reduced  70.7 ± 0.7  — — — — — —  72.6 ± 0.7 69.3 ± 0.4  CIFAR-10  Caltech-101  82.0 — —  [83.11]1  — —  [83.96]1 81.9 77.5  —  77.3 ± 0.6  74.6  75.3 ± 0.7  —  82.5 ± 0.5  — —  76.6 ± 0.7 2  Table 1: Classiﬁcation accuracy on several popular datasets (in %).  1As mentioned, we do not compare to the methods which use supervised information for learning features  on the full CIFAR-10 dataset  2There are two ways to compute the accuracy on Caltech-101: simply averaging the accuracy over the whole test set or calculating the accuracy for each class separately and then averaging these values. These methods differ because for many classes less than 50 test samples are available. It seems that most researchers in the machine learning ﬁeld use the ﬁrst method, which is what we report in the table. When using the second method, our performance drops to 74.1% ± 0.6%  distribution of the test dataset is closest to the surrogate samples our algorithm reaches 67.4%±0.6% accuracy outperforming all other approaches by a large margin.  3.2  Inﬂuence of the data acquisition on classiﬁcation performance  Our pipeline lets us easily vary the number of surrogate classes in the training data and the number of training samples per surrogate class. We use this to measure the effect of these factors on the quality of the resulting features. We vary the number of surrogate classes between 50 and 32000 and the number of training samples per surrogate class between 1 and 100. The results are shown in Fig. 3 and 4. In Fig. 4 we also show, as a baseline, the classiﬁcation performance of random ﬁlters (all weights are sampled from a normal distribution with standard deviation 0.001, all biases are set to zero). Initializing the random ﬁlters does not require any training data and can hence be seen as using 0 samples per surrogate class. Error bars in Fig. 3 show the standard deviations computed when testing on 10 folds of the STL-10 dataset. An apparent trend in Fig. 3 is that increasing the number of surrogate classes results in an increase in classiﬁcation accuracy until it reaches an optimum at around 8000 surrogate classes. When the number of surrogate classes is further increased the classiﬁcation results do not change or slightly decrease. One explanation for this behavior is that the larger the number of surrogate classes be- comes, the more these classes overlap. As a result of this overlap the classiﬁcation problem becomes more difﬁcult and adapting the network to the surrogate task no longer succeeds. To check the valid- ity of this explanation we also plot in Fig. 3 the classiﬁcation error on the validation set (taken from the surrogate data) computed after training the network. It rapidly grows as the number of surrogate classes increases, supporting the claim that the task quickly becomes more difﬁcult as the number of surrogate classes increases. Fig. 4 shows that classiﬁcation accuracy increases with increasing number of samples per surrogate class and saturates around 100 samples. It can also be seen that when training with small numbers of samples per surrogate class, there is no clear indication that having more classes lead to better performance. We hypothesize that the reason may be that with few training samples per class the surrogate classiﬁcation problem is too simple and hence the network can severely overﬁt, which results in poor and unstable generalization to real classiﬁcation tasks. However, starting from around 8− 16 samples per surrogate class, the surrogate task gets sufﬁciently complicated and the networks with more diverse training data (more surrogate classes) perform consistently better.  5  Figure 3: Dependence of classiﬁcation accu- racy on STL-10 on the number of surrogate classes in the training data. For reference, the error on validation surrogate data is also shown. Note the different scales for the two graphs.  Figure 4: Dependence of classiﬁcation accu- racy on STL-10 on the number of samples per surrogate class. Standard deviations not shown to avoid clutter.  4 Discussion  We proposed a simple unsupervised feature learning approach based on data augmentation that shows good results on a variety of classiﬁcation tasks. While our approach sets the state of the art on STL-10 it remains to be seen whether this success can be translated into consistently better performance on other datasets. The performance of our method saturates when the number of surrogate classes increases. One probable reason for this is that the surrogate task we use is relatively simple and does not allow the network to learn complex invariances such as 3D viewpoint invariance or inter-instance invariance. We hypothesize that our unsupervised feature learning method could learn more powerful higher- level features if the surrogate data were more similar to real-world labeled datasets. This could be achieved by using extra weak supervision provided for example by video data or a small number of labeled samples. Another possible way of obtaining richer surrogate training data would be (unsupervised) merging of similar surrogate classes. We see these as interesting directions for future work.  Acknowledgements  We acknowledge funding by the ERC Starting Grant VideoLearn (279401).  References  [1] A. Ahmed, K. Yu, W. Xu, Y. Gong, and E. Xing. Training hierarchical feed-forward visual recognition  models using transfer learning from pseudo-tasks. In ECCV (3), pages 69–82, 2008.  [2] M.-R. Amini and P. Gallinari. Semi supervised logistic regression. In ECAI, pages 390–394, 2002. [3] L. Bo, X. Ren, and D. Fox. Unsupervised Feature Learning for RGB-D Based Object Recognition. In  ISER, June 2012.  [4] L. Bo, X. Ren, and D. Fox. Multipath sparse coding using hierarchical matching pursuit. In CVPR, pages  660–667, 2013.  [5] Y. Boureau, N. Le Roux, F. Bach, J. Ponce, and Y. LeCun. Ask the locals: multi-way local pooling for  image recognition. In Proc. International Conference on Computer Vision (ICCV’11). IEEE, 2011.  [6] A. Coates and A. Y. Ng. Selecting receptive ﬁelds in deep networks. In NIPS, pages 2528–2536, 2011. [7] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolu-  tional activation feature for generic visual recognition. 2013. pre-print, arXiv:1310.1531v1 [cs.CV].  [8] R. Gens and P. Domingos. Discriminative learning of sum-product networks. In NIPS, pages 3248–3256,  2012.  6  50100250500100020004000800016000320005456586062646668Number of classes (log scale)Classification accuracy on STL−10  Classificationon STL (± σ)Validation error onsurrogate data020406080100Error on validation data1248163264100455055606570Number of samples per class (log scale)Classification accuracy on STL−10  1000 classes2000 classes4000 classesrandom filters[9] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection  and semantic segmentation. 2013. pre-print, arXiv:1311.2524v1 [cs.CV].  [10] J. Goldberger, S. T. Roweis, G. E. Hinton, and R. Salakhutdinov. Neighbourhood components analysis.  In NIPS, 2004.  [11] Y. Grandvalet and Y. Bengio. Entropy regularization. In O. Chapelle, B. Sch¨olkopf, and A. Zien, editors,  Semi-Supervised Learning, pages 151–168. MIT Press, 2006.  [12] R. Hadsell, S. Chopra, and Y. Lecun. Dimensionality reduction by learning an invariant mapping. In In  Proc. Computer Vision and Pattern Recognition Conference (CVPR06, 2006.  [13] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural Comput.,  18(7):1527–1554, July 2006.  [14] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science,  313(5786):504–507, July 2006.  [15] K. Y. Hui. Direct modeling of complex invariances for visual object features.  In S. Dasgupta and D. Mcallester, editors, Proceedings of the 30th International Conference on Machine Learning (ICML- 13), volume 28, pages 352–360. JMLR Workshop and Conference Proceedings, May 2013.  [16] Y. Jia, C. Huang, and T. Darrell. Beyond spatial pyramids: Receptive ﬁeld learning for pooled image  features. In CVPR, pages 3370–3377. IEEE, 2012.  [17] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural  networks. In NIPS, pages 1106–1114, 2012.  [18] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.  Proceedings of the IEEE, 86(11):2278–2324, November 1998.  [19] D.-H. Lee. Pseudo-label : The simple and efﬁcient semi-supervised learning method for deep neural  networks. In Workshop on Challenges in Representation Learning, ICML, 2013.  [20] S. Rifai, Y. N. Dauphin, P. Vincent, Y. Bengio, and X. Muller. The manifold tangent classiﬁer. In Advances  in Neural Information Processing Systems 24 (NIPS). 2011.  [21] P. Simard, B. Victorri, Y. LeCun, and J. S. Denker. Tangent prop - a formalism for specifying selected invariances in an adaptive network. In Advances in Neural Information Processing Systems 4, (NIPS), 1992.  [22] K. Sohn and H. Lee. Learning invariant representations with local transformations. In ICML, 2012. [23] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th International Conference on Machine Learning, ICML ’08, pages 1096–1103, New York, NY, USA, 2008. ACM.  [24] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. 2013. pre-print,  arXiv:1311.2901v3 [cs.CV].  [25] W. Y. Zou, A. Y. Ng, S. Zhu, and K. Yu. Deep learning of invariant features via simulated ﬁxations in  video. In NIPS, pages 3212–3220, 2012.  7  ","When deep learning is applied to visual object recognition, data augmentationis often used to generate additional training data without extra labeling cost.It helps to reduce overfitting and increase the performance of the algorithm.In this paper we investigate if it is possible to use data augmentation as themain component of an unsupervised feature learning architecture. To that end wesample a set of random image patches and declare each of them to be a separatesingle-image surrogate class. We then extend these trivial one-element classesby applying a variety of transformations to the initial 'seed' patches. Finallywe train a convolutional neural network to discriminate between these surrogateclasses. The feature representation learned by the network can then be used invarious vision tasks. We find that this simple feature learning algorithm issurprisingly successful, achieving competitive classification results onseveral popular vision datasets (STL-10, CIFAR-10, Caltech-101)."
1312.5129,2014,Deep Learning Embeddings for Discontinuous Linguistic Units  ,"['Wenpeng Yin', 'Hinrich Schütze']",https://arxiv.org/pdf/1312.5129.pdf,"3 1 0 2   c e D 9 1         ] L C . s c [      2 v 9 2 1 5  .  2 1 3 1 : v i X r a  Deep Learning Embeddings for Discontinuous  Linguistic Units  Wenpeng Yin and Hinrich Sch¨utze  Center for Information and Language Processing  University of Munich  Germany  wenpeng@cis.lmu.de  Abstract  Deep learning embeddings have been successfully used for many natural language processing problems. Embeddings are mostly computed for word forms although a number of recent papers have extended this to other linguistic units like mor- phemes and phrases. In this paper, we argue that learning embeddings for discon- tinuous linguistic units should also be considered. In an experimental evaluation on coreference resolution, we show that such embeddings perform better than word form embeddings.  1 Motivation  One advantage of recent work in deep learning on natural language processing (NLP) is that lin- guistic units are represented by rich and informative embeddings. These embeddings support better performance on a variety of NLP tasks (Collobert et al., 2011) than symbolic linguistic represen- tations that do not directly represent information about similarity and other linguistic properties. Embeddings are mostly derived for word forms although a number of recent papers have extended this to other linguistic units like morphemes (Luong et al., 2013) and phrases (Mikolov et al., 2013). Thus, an important question is: what are the basic linguistic units that should be represented by embeddings in a deep learning NLP system? In this paper, we argue that certain discontinuous linguistic units should also have embeddings. We will restrict ourselves to the arguably simplest possible type of discontinuity: two noncontinous words. For example, in the sentence “this tea helped me to relax”, “helped*to” is one of several such two-word discontinuities. We will refer to discontinuous linguistic units like “helped*to” as minimal contexts (MC) for reasons that will become clear presently.  We can approach the question of what basic linguistic units should have representations from a practical as well as from a cognitive point of view. In practical terms, we want representations to be optimized for good generalization. There are many situations where a particular task involving a phrase cannot be solved based on the phrase itself, but it can be solved by analyzing the context of the phrase. For example, if a coreference resolution system needs to determine whether the unknown word “Xiulan” (a Chinese ﬁrst name) in “he helped Xiulan to ﬁnd a ﬂat” refers to an animate or an inanimate entity, then the minimal context “helped*to” is a good indicator for the animacy of the unknown word – whereas the unknown word itself provides no clue.  From a cognitive point of view, it can be argued that many basic units that the human cognitive system uses are also discontinuous. Particularly convincing examples for such units are phrasal verbs in English, which frequently occur discontinuously. It is implausible to suppose that we retrieve atomic representations for, say, “keep”, “up”, “under” and “in” and then combine them to form the meanings of phrases like “keep him up”, “keep them under”, “keep it in”. Rather, it is more plausible  1  that we recognize “keep up”, “keep under” and “keep in” as relevant basic linguistic units in these contexts and that the human cognitive systems represents them as units.  This paper presents an initial study of minimal context embeddings and shows that they are better suited for a classiﬁcation task needed for coreference resolution than word embeddings. Our con- clusion is that minimal contexts (as well as inﬂected word forms, morphemes and phrases) should be considered as basic units that we need to learn embeddings for.  2 Experimental setup  2.1 Embedding learning  With English Gigaword Corpus, we use the skip-gram model as implemented in word2vec1 (Mikolov et al., 2013) to induce embeddings. To be able to use word2vec directly without code changes, we represent the corpus as a sequence of sentences, each consisting of two tokens: an MC (written as the two enclosing words separated by a star) and a word that occurs between the two enclosing words. The distance k between the two enclosing words can be varied. In our ex- periments, we use either distance k = 2 or distance 2 ≤ k ≤ 3. For example, for k = 2, the trigram wi−1 wi wi+1 generates the single sentence “wi−1*wi+1 wi”; and for 2 ≤ k ≤ 3, the fourgram wi−2 wi−1 wi wi+1 generates the four sentences “wi−2*wi wi−1”, “wi−1*wi+1 wi”, “wi−2*wi+1 wi−1” and “wi−2*wi+1 wi”. Note that the reformated corpus enables word2vec to learn embeddings for single words and MCs simultaneously, we discard the word embeddings, and yet compute standard word embeddings on the original corpus using word2vec skip-gram model. In experiments, embedding size is set to 200.  2.2 Markable classiﬁcation task  A markable is a linguistic expression that refers to an entity in the real world or another linguistic expression. Examples of markables include noun phrases (“the man”), named entities (“Peter”) and nested noun phrases (“their”). We address the task of animacy classiﬁcation of markables: classi- fying them as animate/inanimate. This feature is useful for coreference resolution systems because only animate markables can be referred to using masculine and feminine pronouns in English like “him” and “she”. Thus, this is an important clue for automatically clustering the markables of a document into correct coreference chains.  To create training and test sets, we extract all 39,689 coreference chains from the CoNLL2012 OntoNotes corpus.2 We label chains that contain one of the markables “she”, “her”, “he”, “him” or “his” as animate and chains that contain one of “it” or “its” as inanimate.  We extract 39,942 markables and their corresponding MCs from the 10,361 animate and inanimate chains where an MC simply is the pair of the two words occurring to the left and right of the markable. The gold label of a markable and its MC is the animacy status of its chain: either animate or inanimate. We divide all MCs having received an embedding in the embedding learning phase into a training set of 11,301 (8097 animate, 3204 inanimate) and a balanced test set of 4036. We use LIBLINEAR3 for classiﬁcation, with penalty factors 3 and 1 for inanimate and animate classes, respectively, because the training data are unbalanced.  3 Experimental results  We compare the following representations for animacy classiﬁcation of markables. (i) MC: minimal context embeddings with k = 2 and 2 ≤ k ≤ 3; (ii) concatenation: concatenation of the embeddings of the two enclosing words where the embeddings are either standard word2vec embeddings (see Section 2.1) or the embeddings published by Collobert et al. (2011);4 (iii) the bag-of-words (BOW)  1https://code.google.com/p/word2vec/ 2http://conll.cemantix.org/2012/data.html 3https://github.com/bwaldvogel/liblinear-java 4http://metaoptimize.com/projects/wordreprs/  2  representation of a minimal context: the concatentation of two one-hot vectors of dimensionality V where V is the size of the vocabulary. The ﬁrst (resp. second) vector is the one-hot vector for the left (resp. right) word of the MC. Experimental results are shown in Table 1.  representation k = 2 2 ≤ k ≤ 3 skip-gram model C&W  MC  concatenation  BOW  accuracy 0.703 0.700 0.668*† 0.662*† 0.638*†  Table 1: Classiﬁcation accuracy. Mark “*” means signiﬁcantly lower than MC, k = 2; “†” means signiﬁcantly lower than MC, 2 ≤ k ≤ 3.  The results show that MC embeddings have an obvious advantage in this classiﬁcation task, both for k = 2 and 2 ≤ k ≤ 3. This validates our hypothesis that learning embeddings for discontinuous linguistic units is promising. In our error analysis, we found two types of frequent errors. (i) Unspeciﬁc MCs. Many MCs are equally appropriate for animate and inanimate markables. Examples of such MCs include “take*in”, “keep*alive” and “then*goes”. (ii) Untypical use of speciﬁc MCs. Even MCs that are speciﬁc with respect to what type of markable they enclose sometimes occur with the “wrong” type of markable. For example, most markables occurring in the MC “of*whose” are animate because “whose” usually refers to an animate markable. However, in the context “. . . the southeastern area of Fujian whose economy is the most active” the enclosed markable is Fujian, a province of China. This example shows that “whose” occasionally refers to an inanimate entity even though these cases are infrequent.  4 Related work  Most work on embeddings has focused on word forms with a few exceptions, notably embeddings for stems and morphemes (Luong et al., 2013) and for phrases (Mikolov et al., 2013). To the best of our knowledge, our work is the ﬁrst to learn embeddings for discontinuous linguistic units.  An alternative to learning an embedding for a linguistic unit is to calculate its distributed repre- sentation from the distributed representations of its parts; the best known work along those lines is (Socher et al., 2012, 2010, 2011). This approach is superior for units that are compositional, i.e., whose properties are systematically predictable from their parts. Our approach (as well as similar work on continuous phrases) only makes sense for noncompositional units.  5 Conclusion and Future Work  We have argued that discontinuous linguistic units are part of the inventory of linguistic units that we should compute embeddings for and we have shown that such embeddings are superior to word form embeddings in a coreference resolution task.  It is obvious that we cannot and do not want to compute embeddings for all possible discontinuous linguistic units. Similarly, the subset of phrases that embeddings are computed for should be care- fully selected. In future work, we plan to address the question of how to select a subset of linguistic units – e.g., those that are least compositional – when inducing embeddings.  References Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., Kuksa, P., 2011. Natural lan- guage processing (almost) from scratch. The Journal of Machine Learning Research 12, 2493– 2537.  Luong, M.T., Socher, R., Manning, C.D., 2013. Better word representations with recursive neural networks for morphology, in: Proceedings of the Conference on Computational Natural Language Learning, pp. 104–113.  3  Mikolov, T., Sutskever, I., Chen, K., Corrado, G., Dean, J., 2013. Distributed representations of  words and phrases and their compositionality. arXiv preprint arXiv:1310.4546 .  Socher, R., Huval, B., Manning, C.D., Ng, A.Y., 2012. Semantic compositionality through recursive matrix-vector spaces, in: Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 1201–1211.  Socher, R., Manning, C.D., Ng, A.Y., 2010. Learning continuous phrase representations and syn- tactic parsing with recursive neural networks, in: Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop, pp. 1–9.  Socher, R., Pennington, J., Huang, E.H., Ng, A.Y., Manning, C.D., 2011. Semi-supervised recur- sive autoencoders for predicting sentiment distributions, in: Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 151–161.  4  ","Deep learning embeddings have been successfully used for many naturallanguage processing problems. Embeddings are mostly computed for word formsalthough a number of recent papers have extended this to other linguistic unitslike morphemes and phrases. In this paper, we argue that learning embeddingsfor discontinuous linguistic units should also be considered. In anexperimental evaluation on coreference resolution, we show that such embeddingsperform better than word form embeddings."
1401.0509,2014,Zero-Shot Learning and Clustering for Semantic Utterance Classification  ,"['Yann N. Dauphin', 'Gokhan Tur', 'Dilek Hakkani-Tur', 'Larry Heck']",https://arxiv.org/pdf/1401.0509.pdf,"4 1 0 2    r a  M 7         ] L C . s c [      3 v 9 0 5 0  .  1 0 4 1 : v i X r a  Zero-Shot Learning for Semantic Utterance  Classiﬁcation  Yann N. Dauphin1 Gokhan Tur2 Dilek Hakkani-T¨ur2 Larry Heck2  1University of Montreal, Montreal, Canada  2Microsoft Research, Mountain View, CA, USA  Abstract  We propose a novel zero-shot learning method for semantic utterance classiﬁ- cation (SUC). It learns a classiﬁer f : X → Y for problems where none of the semantic categories Y are present in the training set. The framework un- covers the link between categories and utterances through a semantic space. We show that this semantic space can be learned by deep neural networks trained on large amounts of search engine query log data. What’s more, we propose a novel method that can learn discriminative semantic features without supervision. It uses the zero-shot learning framework to guide the learning of the semantic features. We demonstrate the effectiveness of the zero-shot semantic learning algorithm on the SUC dataset collected by (Tur et al., 2012). Furthermore, we achieve state-of- the-art results by combining the semantic features with a supervised method.  1  Introduction  Conversational understanding systems aim to automatically classify user requests into predeﬁned semantic categories and extract related parameters (Tur and Mori, 2011). For instance, such a system might classify the natural language query “I want to ﬂy from San Francisco to New York next Sunday” into the semantic domain ﬂights. This is known as semantic utterance classiﬁcation (SUC). Typically, these systems use supervised classiﬁcation methods such as Boosting (Schapire and Singer, 2000), support vector machines (SVMs) (Haffner et al., 2003), or maximum entropy models (Yaman et al., 2008). These methods can produce state-of-the-art results but they require signiﬁcant amounts of labelled data. This data is mostly obtained through manual labor and becomes costly as the number of semantic domains increases. This limits the applicability of these methods to problems with relatively few semantic categories. We consider two problems here. First, we examine the problem of predicting the semantic domain of utterances without having seen examples of any of the domains. Formally, the goal is to learn a classiﬁer f : X → Y without any values of Y in the training set. In constrast to traditional SUC systems, adding a domain is as easy as including it in the set of domains. This is a form of zero-shot learning (Palatucci et al., 2009) and is possible through the use of a knowledge base of semantic properties of the classes to extrapolate to unseen classes. Typically this requires seeing examples of at least some of the semantic categories. Second, we consider the problem of easing the task of supervised classiﬁers when there are only few examples per domain. This is done by augmenting the input with a feature vector H for a classiﬁer f : (X, H) → Y . The difﬁculty is that H must be learned without any knowledge of the semantic domains Y . In this paper, we introduce a zero-shot learning framework for SUC where none of the classes have been seen. We propose to use a knowledge base which can output the semantic properties of both the input and the classes. The classiﬁer matches the input to the class with the best matching semantic features. We show that a knowledge-base of semantic properties can be learned automatically for SUC by deep neural networks using large amounts of data. The recent advances in deep learning  1  have shown that deep networks trained at large scale can reach state-of-the-art results. We use the Bing search query click logs, which consists of user queries and associated clicked URLs. We hy- pothesize that the clicked URLs reﬂect high level meaning or intent of the queries. Surprinsingly, we show that is is possible to learn semantic properties which are discriminative of our unseen classes without any labels. We call this method zero-shot discriminative embedding (ZDE). It uses the zero-shot learning framework to provide weak supervision during learning. Our experiments show that the zero-shot learning framework for SUC yields competitive results on the tasks considered. We demonstrate that zero-shot discriminative embedding produces more discriminative semantic properties. Notably, we reach state-of-the-art results by feeding these features to an SVM. In the next section, we formally deﬁne the task of semantic utterance classiﬁcation. We provide a quick overview of zero-shot learning in Section 3. Sections 4 and 5 present the zero-shot learning framework and a method for learning semantic features using deep networks. Section 6 introduces the zero-shot discriminative embedding method. We review the related work on this task in Section 7 In Section 8 we provide experimental results.  2 Semantic Utterance Classiﬁcation  The semantic utterance classiﬁcation (SUC) task aims at classifying a given speech utterance Xr into one of M semantic classes, ˆCr ∈ C = {C1, . . . , CM} (where r is the utterance index). Upon the observation of Xr, ˆCr is chosen so that the class-posterior probability given Xr, P (Cr|Xr), is maximized. More formally, ˆCr = arg maxCr P (Cr|Xr). Semantic classiﬁers need to allow signiﬁcant utterance variations. A user may say “I want to ﬂy from San Francisco to New York next Sunday” and another user may express the same information by saying “Show me weekend ﬂights between JFK and SFO”. Not only is there no a priori con- straint on what the user can say, these systems also need to generalize well from a tractably small amount of training data. On the other hand, the command “Show me the weekend snow forecast” should be interpreted as an instance of another semantic class, say, “Weather.” In order to do this, the selection of the feature functions fi(C, W ) aims at capturing the relation between the class C and word sequence W . Typically, binary or weighted n-gram features, with n = 1, 2, 3, to capture the likelihood of the n-grams, are generated to express the user intent for the semantic class C (Tur and Deng, 2011). Once the features are extracted from the text, the task becomes a text classiﬁca- tion problem. Traditional text categorization techniques devise learning methods to maximize the probability of Cr, given the text Wr; i.e., the class-posterior probability P (Cr|Wr).  3 Zero-shot learning In general, zero-shot learning (Palatucci et al., 2009) is concerned with learning a classiﬁer f : X → Y that can predict novel values of Y not present in the training set. It is an important problem setting for tasks where the set of classes is large and in cases where the cost of labelled examples is high. It has found application in vision where the number of classes can be very large (Frome et al., 2013). A zero-shot learner uses semantic knowledge to extrapolate to novel classes. Instead of predicting the classes directly, the learner predicts semantic properties or features of the input. Thanks to a knowledge-base of semantic features for the classes it can match the inputs to the classes. The semantic feature space is a euclidean space of d dimensions. Each dimension encodes a se- mantic property. In vision for instance, one dimension might encode the size of the object, another the color. The knowledge base K stores a semantic feature vector H for each of the classes. The zero-shot classiﬁer f = m ◦ n is the composition of two classiﬁers. The ﬁrst classiﬁer m : X → H predicts the semantic properties of the input. The training set is found by replacing the class values in the training set by their semantic features. The second classiﬁer n : H → Y matches the semantic code to the class. This can be done by a k-NN classiﬁer. In applying zero-shot learning to semantic utterance classiﬁcation there are several challenges. The framework described by (Palatucci et al., 2009) requires some of the classes to be present in the training data in order to train the m classiﬁer. We are interested in the setting where none of classes have training data. Furthermore, an adequate knowledge-base must be found for SUC.  2  4 Zero-Shot Learning for Semantic Utterance Classiﬁcation  In this section, we introduce a zero-shot learning framework for SUC where none of the classes are seen during training. It is based on the observation that in SUC both the semantic categories and the inputs reside in the same semantic space. In this framework, classiﬁcation can be done by ﬁnding the best matching semantic category for a given input. Semantic utterance classiﬁcation is concerned with ﬁnding the semantic category for a natural lan- guage utterance. Traditionally, conversational systems learn this task using labelled data. This over- looks the fact that classiﬁcation would be much easier in a space that reveals the semantic meaning of utterances. Interestingly, the semantics of language can be discovered without labelled data. What’s more, the name of semantic classes are not chosen randomly. They are in the same language as the sentences and are often chosen because they describe the essence of the class. These two facts can easily be used by humans to classify without task-speciﬁc labels. For instance, it is easy to see that the utterance the accelerator has exploded belongs more to the class physics than outdoors. This is the very human ability that we wish to replicate here.  Figure 1: Visualization of the 2d semantic space learned by a deep neural net. We see that the two axis differentiate between phrases relating to hotels and movies. More details in Section 8.  We propose a framework called zero-shot semantic learning (ZSL) that leverages these observations. In this framework, the knowledge-base K is a function which can output the semantic properties of any sentence. The classiﬁcation procedure can be done in one step because both the input and the categories reside in the same space. The zero-shot classiﬁer ﬁnds the category which best matches the input. More formally, the zero-shot classiﬁer is given by  where Z = (cid:80)  P (Cr|Xr) =  e−|K(Xr)−K(Cr)|  (1) C e−|K(Xr)−K(C)| and |x − y| is a distance measure like the euclidean distance. The knowledge-base maps the input K(Xr) and the category K(Xr) in a space that reveals their meaning. An example 2d semantic space is given in Figure 1 which maps sentences relating to movies close to each other and those relating to hotels further away. In this space, given the cate- gories hotel and movies, the sentence motels in aurora colorado will be classiﬁed to hotel because K(motels in aurora colorado) is closer to K(hotel). This framework will classify properly if  1 Z  • The semantics of the language are properly captured by K. In other words, utterances are • The class name Cr describes the semantic core of the class well. Meaning that K(Cr)  clustered according to their meaning.  resides close to the semantic representation of sentences of that class.  The success of this framework rests on the quality of the knowledge-base K. Following the success of learning methods with language, we are interested in learning this knowledge-base from data.  3  Unsupervised learning methods like LSA, and LDA have had some success but it is hard to ensure that the semantic properties will be useful for SUC.  5 Learning Semantic Features for SUC using Deep Nets  In this section, we describe a method for learning a semantic features for SUC using deep networks trained on Bing search query click logs. We use the query click logs to deﬁne a task that makes the networks learn the meaning or intent behind the queries. The semantic features are found at the last hidden layer of the deep neural network. Query Click Logs (QCL) are logs of unstructured text including both the users queries sent to a search engine and the links that the users clicked on from the list of sites returned by that search engine. Some of the challenges in extracting useful information from QCL is that the feature space is very high dimensional (there are thousands of url clicks linked to many queries), and there are millions of queries logged daily. We make the mild hypothesis that the website clicked following a query reveals the meaning or intent behind a query. The queries which have similar meaning or intent will map to the same website. For example, it is easy to see that queries associated with the website imdb.com share a semantic connection to movies.  Figure 2: Depiction of the deep network from queries to URLs.  We train the network with the query as input and the website as the output (see Figure 2). This learning scheme is inspired by the neural language models (Bengio, 2008) who learn word embed- dings by learning to predict the next word in a sentence. The idea is that the last hidden layer of the network has to learn an embedding space which is helpful to classiﬁcation. To do this, it will map similar inputs in terms of the classiﬁcation task close in the embedding space. The key dif- ference with word embeddings methods like (Bengio, 2008) is that we are learning sentence-level embeddings. We train deep neural networks with softmax output units and rectiﬁed linear hidden units. The inputs Xr are queries represented in bag-of-words format. The labels Yr are the index of the website that was clicked. We train the network to minimize the negative log-likelihood of the data L(X, Y ) = − log P (Yr|Xr). The network has the form  P (Y = i|Xr) =  (cid:80)  eW n+1 j eW n+1  i H n(Xr)+bn+1  i  j H n(Xr)+bn+1  j  The latent representation function H n is composed on n hidden layers H n(Xr) = max(0, W nH n−1(Xr) + bn) H 1(Xr) = max(0, W 1Xr + b1)  We have a set of weight matrices W and biases b for each layer giving us the parameters θ = {W 1, b1, . . . , W n+1, bn+1} for the full network. We train the network using stochastic gradient descent with minibatches.  4  The knowledge-base function is given by the last hidden layer K = H n(Xr). In this scheme, the embeddings are used as the semantic properties of the knowledge-base. However, it is not clear that the semantic space will be discriminative of the semantic categories we care about for SUC.  6 Learning Discriminative Semantic Features without Supervision  We introduce a novel regularization that encourages deep networks to learn discriminative semantic features for the SUC task without labelled data. More precisely, we deﬁne a clustering measure for the semantic classes using the zero-shot learning framework of Section 4. We hypothesize the classes are well clustered hence we minimize this measure. In the past section, we have described a method for learning semantic features using query click logs. The features are given by ﬁnding the best semantic space for the query click logs task. In general, there might be a mismatch between what qualiﬁes as a good semantic space for the QCL and SUC tasks. For example, the network might learn an embedding that clusters sentences of the category movies and events close together because they both relate to activities. In this case the features would have been more discriminative if the sentences were far from each other. However, there is no pressure for the network to do that because it doesn’t know about the SUC task.  Figure 3: Visualization of an actual 2d embedding space learned by a DNN (left) and DNN trained with ZDE (right). The points are sentences with different colors for each class and the arrows point to the location of the class name in the embedding space. ZDE signiﬁcantly improves the clustering of the classes. More details in Section 8.  This problem could have been addressed by multi-task or semi-supervised learning methods if we had access to labelled data. Research has shown adding even a little bit of supervision is often helpful (Larochelle et al., 2009). The simplest solution would be to train the network on the QCL and SUC task simultaneously. In other words, we would train the network to minimize the sum of the QCL objective − log P (Y |X) and the SUC objective − log P (C|X). This would allow the model to leverage the large amount of QCL data while learning a better representation for SUC. We cannot miminize − log P (C|X) but we can minimize a similar measure which does not require labels. We can measure the overlap of the semantic categories using the conditional entropy  H(P (Cr|Xr)) = E[I(P (Cr|Xr))]  = E[−(cid:88)  P (Cr = i|Xr) log P (Cr = i|Xr)].  (2)  i  The measure is lowest when the overlap is small. Interestingly, calculating the entropy does not require labelled data. We can recover a zero-shot classiﬁer P (C|X) from the semantic space using Equation 1. The entropy H(P (Cr|Xr)) of this classiﬁer measures the clustering of the categories in the semantic space. Spaces with the lowest entropy are those where the examples K(Xr) cluster around category names K(Cr) and where the categories have low-overlap in the semantic space.  5  Figure 3 illustrates a semantic space with high conditional entropy on the left, and one with a low entropy on the right side. Zero-shot Discriminative Embedding (ZDE) combines the embedding method of Section 5 with the minimization of the entropy of a zero-shot classiﬁer on that embedding. The objective has the form (3) The variable X is the input, Y is the website that was clicked, C is a semantic class. The hyper- parameter λ controls the strength of entropy objective in the overall objective. We ﬁnd this value by cross-validation.  L(X, Y ) = − log P (Y |X) + λH(P (C|X)).  7 Related work  Early work on spoken utterance classiﬁcation has been done mostly for call routing or intent deter- mination system, such as the AT&T How May I Help You? (HMIHY) system (Gorin et al., 1997), relying on salience phrases, or the Lucent Bell Labs vector space model (Chu-Carroll and Carpenter, 1999). Typically word n-grams are used as features after preprocessing with generic entities, such as dates, locations, or phone numbers. Because of the very large dimensions of the input space, large margin classiﬁers such as SVMs (Haffner et al., 2003) or Boosting (Schapire and Singer, 2000) were found to be very good candidates. Deep learning methods have ﬁrst been used for semantic utterance classiﬁcation by Sarikaya et al. (Sarikaya et al., 2011). Deep Convex Networks (DCNs) (Tur et al., 2012) and Kernel DCNs (K-DCNs) (Deng et al., 2012) have also been applied to SUC. K-DCNs allow the use of kernel functions during training, combining the power of kernel based methods and deep learning. While both approaches resulted in performances better than a Boosting-based baseline, K-DCNs have shown signiﬁcantly bigger performance gains due to the use of query click features. Entropy minimization (Grandvalet and Bengio, 2005) is a semi-supervised learning framework which also uses the conditional entropy. In this framework, both labelled and unlabelled data are available, which is an important difference with ZDE. In (Grandvalet and Bengio, 2005), a classiﬁer is trained to minimize its conditional likelihood and its conditional entropy. ZDE avoids the need for labels by minimizing the entropy of a zero-shot classiﬁer. (Grandvalet and Bengio, 2005) shows that this approach produces good results especially when generative models are mispeciﬁed.  8 Experiments  In this section, we evaluate the zero-shot semantic learning framework and the zero-shot discrimi- native embedding method proposed in the previous sections.  8.1 Setup  We have gathered a month of query click log data from Bing to learn the embeddings. We restricted the websites to the the 1000 most popular websites in this log. The words in the bag-of-words vocabulary are the 9521 found in the supervised SUC task we will use. All queries containing only unknown words were ﬁltered out. We found that using a list of stop-words improved the results. After these restrictions, the dataset comprises 620,474 different queries. We evaluate the performance of the methods for SUC on the dataset gathered by (Tur et al., 2012). It was compiled from utterances by users of a spoken dialog system. There are 16,000 training utterances, 2000 utterances for validation and 2000 utterances for testing. Each utterance is labelled with one of 25 domains. The hyper-parameters of the models are tuned on the validation set. The learning rate parameter of gradient descent is found by grid search with {0.1, 0.01, 0.001}. The number of layers is between 1 and 3. The number of hidden units is kept constant through layers and is found by sampling a random number from 300 to 800 units. We found that it was helpful to regularize the networks using dropout (Hinton et al., 2012). We sample the dropout rate randomly between 0% dropout and 20%. The λ of the zero-shot embedding method is found through grid-search with {0.1, 0.01, 0.001}. The models are trained on a cluster of computers with double quad-core Intel(R) Xeon(R) CPUs with  6  2.33GHz and 8Gb of RAM. Training either the ZDE method on the QCL data requires 4 hours of computation time.  8.2 Results  First, we want to see what is learned by the embedding method described in Section 5. A ﬁrst step is to look at the nearest neighbor of words in the embedding space. Table 1 shows the nearest neighbours of speciﬁc words in the embedding space. We observe that the neighbors of the words al share the semantic domain of the word. This conﬁrms that the network learns some semantics of the language.  Restaurant steakhouse  diner seafood tavern  Hotel suites hyatt resorts ramada  Flight airline airfaire plane baggage  Events festivals upcoming ﬁreworks happening  Transportation  distributing  dfw  petroleum hospitality  Table 1: Nearest neighbours in the embedding space. Each column displays the 5 nearest neigh- bours of the word at the top. We can see that the embedding captures the semantics of the words.  We can better visualize the embedding space using a network with a special architecture. Following (Hinton and Salakhutdinov, 2006), we train deep networks where the last hidden layer contains only 2 dimensions. The depth allows the network to progressively reduce the dimensionality of the data. This approach enables us to visualize exactly what the network has learned. Figure 1 shows the embedding a deep network with 3 layers (with size 200-10-2) trained on the QCL task. We observe that the embedding distinguishes between sentences related to movies and hotels. In Figure 3, we compare the embedding spaces of a DNN trained on the QCL (left) and a DNN trained using ZDE (right) both with hidden layers of sizes 200-10-2. The comparison suggests that minimizing the conditional entropy of the zero-shot classiﬁer successfully improves the clustering.  Method ZSL with Bag-of-words ZSL with p(Y |X) (LR) ZSL with p(Y |X) (DNN) ZSL with DNN Embedding ZSL with ZDE Embedding Representative URL heuristic (DNN)  Restaurant Hotel 0.641 0.821 0.862 0.935 0.940 0.892  0.616 0.779 0.838 0.858 0.863 0.798  Flight Events Transportation 0.683 0.457 0.46 0.870 0.906 0.769  0.559 0.677 0.631 0.727 0.841 0.707  0.5 0.472 0.503 0.667 0.826 0.577  Table 2: Comparison of several zero-shot semantic learning methods for 5 semantic classes. Our proposed zero-shot learning system with DNN embeddings outperforms other approaches.  Second, we want to conﬁrm that good classiﬁcation results can be achieved using zero-shot semantic learning. To do this, we evaluate the classiﬁcation results of our method on the SUC task. Our results are given in Table 2. The performance is measured using the AUC (Area under the curve of the precision-recall curve) for which higher is better. We compare our ZDE method against various means of obtaining the semantic features H. We compare with using the bag-of-words representation (denoted ZSL with Bag-of-words) as semantic features. ZSL with p(Y |X) (LR) and ZSL with p(Y |X) (DNN) are models trained from the QCL to predict the website associated with queries. The semantic features are the vector of probability that each website is associated with the query. ZSL with p(Y |X) (LR) is a logistic regression model, ZSL with p(Y |X) (DNN) is a DNN model. We also compare with a sensible heuristic method denoted Representative URL heuristic. For this heuristic, we associate each semantic category with a representative website (i.e. ﬂights with expedia.com, movies with imdb.com). We train a DNN using the QCL to predict which of these websites is clicked given an utterance. The semantic category distribution P (C|X) is the probability that each associated website was clicked. Table 2 shows that the proposed zero-shot learning method with ZDE achieves the best results. In particular, ZDE improves performance by a wide margin for hard categories like transportation. These results conﬁrm the hypothesis behind both ZSL and the ZDE method.  7  Figure 4: Comparison between the proposed zero-shot learning method and an SVM trained with increasing amount of examples. The curve shows that ZSL compares favorably with SVMs when there are few labels.  We also compare the zero-shot learning system with a supervised SUC system. We compare ZSL with a linear SVM. The task is identify utterances of the restaurant semantic class. Figure 4 shows the performance of the linear SVM as the number of labelled training examples increases. The performance of ZSL is shown as a straight line because it does not use labelled data. Predictably, the SVM achieves better results when the labelled training set is large. However, ZSL achieves better performance in the low-data regime. This conﬁrms that ZSL can be useful in cases where labelled data is costly, or the number of classes is large.  QCL features (Hakkani-T¨ur et al., 2011)  Features  Bag-of-words  DNN urls  DNN embeddings ZDE embeddings  Kernel DCN  9.52% 5.94%  SVM 10.09% 6.36% 6.88% 6.2% 5.73%  Table 3: Test error rate of various methods on the SUC task. The best results are achieved with by augmenting the input with ZDE embeddings.  Finally, we consider the problem of using semantic features H to increase the performance of a classiﬁer f : (X, H) → Y . The input X is a bag-of-words representation of the utterances. We compare with state-of-the-art approaches in Table 3. The state-of-the-art method is the Kernel DCN on QCL features with 5.94% test error. However, we train using the more scalable linear SVM which leads to 6.36% with the same input features. The linear SVM is better to compare features because it cannot non-linearly transform the input by itself. Using the embeddings learned from the QCL data as described in Section 4 yields 6.2% errors. Using zero-shot discriminative embedding further reduces the error t 5.73%.  9 Conclusion  We have introduced a zero-shot learning framework for SUC. The proposed method learns a knowledge-base using deep networks trained on large amounts of search engine query log data. We have proposed a novel way to learn embeddings that are discriminative without access to labelled data. Finally, we have shown experimentally that these methods are effective.  8  References Bengio, Y. (2008). Neural net language models. Scholarpedia, 3(1), 3881. Chu-Carroll, J. and Carpenter, B. (1999). Vector-based natural language call routing. Computational  Linguistics, 25(3), 361–388.  Deng, L., Tur, G., He, X., and Hakkani-T¨ur, D. (2012). Use of kernel deep convex networks and In In Prooceedings of the IEEE SLT  end-to-end learning for spoken language understanding. Workshop, Miami, FL.  Frome, A., Corrado, G., Shlens, J., Bengio, S., Dean, J., Ranzato, M., and Mikolov, T. (2013). De- vise: A deep visual-semantic embedding model. In Advances In Neural Information Processing Systems, NIPS.  Gorin, A. L., Riccardi, G., and Wright, J. H. (1997). How May I Help You? Speech Communication,  23, 113–127.  Grandvalet, Y. and Bengio, Y. (2005). Semi-supervised learning by entropy minimization. In L. K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17 (NIPS’04), pages 529–236. Cambridge, MA.  Haffner, P., Tur, G., and Wright, J. (2003). Optimizing SVMs for complex call classiﬁcation. In  Proceedings of the ICASSP, Hong Kong.  Hakkani-T¨ur, D., Heck, L., and Tur, G. (2011). Exploiting query click logs for utterance domain detection in spoken language understanding. In Proceedings of the ICASSP, Prague, Czech Re- public.  Hinton, G. E. and Salakhutdinov, R. (2006). Reducing the Dimensionality of Data with Neural  Networks. Science, 313, 504–507.  Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2012).  Im- proving neural networks by preventing co-adaptation of feature detectors. Technical report, arXiv:1207.0580.  Larochelle, H., Bengio, Y., Louradour, J., and Lamblin, P. (2009). Exploring strategies for training  deep neural networks. Journal of Machine Learning Research, 10, 1–40.  Palatucci, M., Pomerleau, D., Hinton, G. E., and Mitchell, T. M. (2009). Zero-shot learning with semantic output codes. In Advances in neural information processing systems, pages 1410–1418. Sarikaya, R., Hinton, G. E., and Ramabhadran, B. (2011). Deep belief nets for natural language  call-routing. In Proceedings of the ICASSP, Prague, Czech Republic.  Schapire, R. E. and Singer, Y. (2000). Boostexter: A boosting-based system for text categorization.  Machine Learning, 39(2/3), 135–168.  Tur, G. and Deng, L. (2011). Intent Determination and Spoken Utterance Classiﬁcation, Chpater 3  in Book: Spoken Language Understanding. John Wiley and Sons, New York, NY.  Tur, G. and Mori, R. D., editors (2011). Spoken Language Understanding: Systems for Extracting  Semantic Information from Speech. John Wiley and Sons, New York, NY.  Tur, G., Deng, L., Hakkani-T¨ur, D., and He, X. (2012). Towards deeper understanding deep convex  networks for semantic utterance classiﬁcation. In In Proceedings of the ICASSP, Kyoto, Japan.  Yaman, S., Deng, L., Yu, D., Wang, Y.-Y., and Acero, A. (2008). An integrative and discriminative technique for spoken utterance classiﬁcation. IEEE Transactions on Audio, Speech, and Language Processing, 16(6), 1207–1214.  9  ","We propose a novel zero-shot learning method for semantic utteranceclassification (SUC). It learns a classifier $f: X \to Y$ for problems wherenone of the semantic categories $Y$ are present in the training set. Theframework uncovers the link between categories and utterances using a semanticspace. We show that this semantic space can be learned by deep neural networkstrained on large amounts of search engine query log data. More precisely, wepropose a novel method that can learn discriminative semantic features withoutsupervision. It uses the zero-shot learning framework to guide the learning ofthe semantic features. We demonstrate the effectiveness of the zero-shotsemantic learning algorithm on the SUC dataset collected by (Tur, 2012).Furthermore, we achieve state-of-the-art results by combining the semanticfeatures with a supervised method."
1312.7335,2014,Correlation-based construction of neighborhood and edge features  ,['Balázs Kégl'],https://arxiv.org/pdf/1312.7335.pdf,"4 1 0 2     b e F 6 1         ]  V C . s c [      2 v 5 3 3 7  .  2 1 3 1 : v i X r a  Correlation-based construction of neighborhood and edge features  Bal´azs K´egl LAL/LRI, University of Paris-Sud, CNRS, 91898 Orsay, France  balazs.kegl@gmail.com  Abstract  Motivated by an abstract notion of low- level edge detector ﬁlters, we propose a simple method of unsupervised feature con- struction based on pairwise statistics of fea- tures. In the ﬁrst step, we construct neigh- borhoods of features by regrouping features that correlate. Then we use these sub- sets as ﬁlters to produce new neighbor- hood features. Next, we connect neighbor- hood features that correlate, and construct edge features by subtracting the correlated neighborhood features of each other. To validate the usefulness of the constructed features, we ran AdaBoost.MH on four multi-class classiﬁcation problems. Our most signiﬁcant result is a test error of 0.94% on MNIST with an algorithm which is essentially free of any image-speciﬁc pri- ors. On CIFAR-10 our method is subopti- mal compared to today’s best deep learn- ing techniques, nevertheless, we show that the proposed method outperforms not only boosting on the raw pixels, but also boost- ing on Haar ﬁlters.  1. Introduction  In this paper we propose a simple method of un- supervised feature construction based on pairwise statistics of features. In the ﬁrst step, we con- struct neighborhoods of features by regrouping fea- tures that correlate. Then we use these subsets of features as ﬁlters to produce new neighborhood fea- tures. Next, we connect neighborhood features that correlate, and construct edge features by subtracting the correlated neighborhood features of each other. The method was motivated directly by the notion of low-level edge detector ﬁlters. These ﬁlters work well in practice, and they are ubiquitous in the ﬁrst layer of both biological and artiﬁcial systems that learn on  natural images. Indeed, the four simple feature con- struction steps are directly based on an abstract no- tion of Haar or Gabor ﬁlters: homogeneous, locally connected patches of contrasting intensities. In a more high-level sense, the technique is also inspired by a, perhaps naive, notion of how natural neural networks work: in the ﬁrst layer they pick up cor- relations in stimuli, and settle in a “simple” theory of the world. Next they pick up events when the correlations are broken, and assign a new units to the new “edge” features. Yet another direct motiva- tion comes from (Le Roux et al., 2007) where they show that pixel order can be recovered using only feature correlations. Once pixel order is recovered, one can immediately apply algorithms that explic- itly use neighborhood-based ﬁlters. From this point of view, in this paper we show that it is possible to go from pixel correlations to feature construction, without going through the explicit mapping of the pixel order.  To validate the usefulness of the constructed fea- tures, we ran AdaBoost.MH (Schapire and Singer, 1999) on multi-class classiﬁcation problems. Boost- ing is one of the best “shallow” multi-class classi- ﬁers, especially when the goal is to combine simple classiﬁers that act on small subsets a large set of possibly useful features. Within multi-class boost- ing algorithms, AdaBoost.MH is the state of the art. On this statement we are at odds with recent re- sults on multiclass boosting. On the two UCI data sets we use in this paper (and on others reported in (K´egl and Busa-Fekete, 2009)), AdaBoost.MH with Hamming trees and products (Benbouzid et al., 2012) clearly outperforms SAMME (Zhu et al., 2009), ABC-Boost (Li, 2009), and most impor- tantly, the implementation of AdaBoost.MH in (Zhu et al., 2009), suggesting that SAMME was compared to a suboptimal implementation of Ada- Boost.MH in (Zhu et al., 2009).  Our most signiﬁcant result comes on the MNIST set where we achieve a test error of 0.94% with an algo- rithm which is essentially free of any image-speciﬁc  priors (e.g., pixel order, preprocessing). On CIFAR- 10, our method is suboptimal compared to today’s best deep learning techniques, reproducing basically the results of the earliest attempts on the data set (Ranzato et al., 2010). The main point in these ex- periments is that the proposed method outperforms not only boosting on the raw pixels, but also boost- ing on Haar ﬁlters. We also tried the technique on two relatively large UCI data sets. The results here are essentially negative: the lack of correlations be- tween the features do not allow us to improve signif- icantly on “shallow” AdaBoost.  The paper is organized as follows. In Section 2 we formally describe the method. In Section 3 we show experimental results, and we conclude with a discus- sion on future research in Section 4.  (cid:8)(x1, y1), . . . , (xn, yn)(cid:9) the training data, where  2. Constructing the representation For the formal description of the method, let D = i ) ∈ Rd are the input vectors and xi = (x1 yi ∈ {1, . . . , K} the labels. We will denote the input matrix and its elements by X = [xj i ]j=1,...,d i=1,...,n, and its n) ∈ raw feature (column) vectors by xj = (xj Rn. The algorithm consists of the following steps.  i , . . . , xd  1, . . . , xj  1. We construct neighborhoods J(xj) for each fea- ture vector xj, that is, sets of features1 that are correlated with xj. Formally, J(xj) = {j(cid:48) : xj(cid:48) ∈ X : ρ(xj, xj(cid:48)  ) < ρN}, where  (cid:110) xj, xj(cid:48)(cid:111) (cid:112)Var {xj}Var {xj(cid:48)}  Cov  ρ(xj, xj(cid:48)  ) =  is the correlation of two feature vectors and ρN is a hyperparameter of the algorithm.  2. We construct neighborhood features by using the  neighborhoods as (normalized) ﬁlters, that is,  (cid:88)  zj i =  1  |J(xj)|  xj(cid:48)  i  j(cid:48)∈J(xj )  for all i = 1, . . . , n and j = 1 . . . , d.  3. We construct edges between neighborhoods by connecting correlated neighborhood features. Formally, L = {(j1, j2) : 1 ≤ j1, j2 ≤ 1In what follows, we use the word feature for any real- valued representation of the input, so both the input of the feature-building algorithm and its ouput. It is an intended terminology to emphasize that the procedure can be applied recursively, as in stacked autoencoders.  2  d, ρ(zj1, zj2) < ρE}, where ρE is a hyperparam- eter of the algorithm. We will denote elements of the set of edges by ((cid:96)j,1, (cid:96)j,2), and the size of the set by |L| = L.  4. We construct edge features by subtracting the responses to correlated neighborhoods of each i = z(cid:96)j,1 other, that is, sj for all j = 1, . . . , L and ((cid:96)j,1, (cid:96)j,2) ∈ L.  i − z(cid:96)j,2  i  5. We concatenate neighborhood and edge fea- tures into a new representation of xi, that is, x(cid:48)  (cid:1) for all i = 1, . . . , n.  i =(cid:0)z1  i , . . . , sL i  i , . . . , zd  i , s1  2.1. Setting ρN and ρE  Both hyperparameters ρN and ρE threshold correla- tions, nevertheless, they have quite diﬀerent roles: ρN controls the neighborhood size whereas ρE con- trols the distance of neighborhoods under which an edge (that is, a signiﬁcantly diﬀerent response or a “surprise”) is an interesting feature. In practice, we found that the results were rather insensitive to the value of these parameters in the [0.3, 0.9] inter- val. For now we manually set ρN and ρE in order to control the number of features. In our experiments we found that it was rarely detrimental to increase the number of features in terms of the asymptotic test error (w.r.t. the number of boosting iterations T ) but the convergence of AdaBoost.MH slowed down if the number of features were larger than some thousands (either because each boosting iteration took too much time, or because we had to seriously subsample the features in each iteration, essentially generating random trees, and so the number of iter- ations exploded).  On images, where the dimensionality of the input space (number of pixels) is large, we subsample the pixels [xj]j=1,...,d before constructing the neighbor- hoods J(xj) to control the number of neighborhood features, again, for computational rather than statis- tical reasons. We simply run AdaBoost.MH with decision stumps in an autoassociative setup. A deci- sion stump uses a single pixel as input and outputs a prediction on all the pixels. We take the ﬁrst d(cid:48) stumps that AdaBoost picks, and use the corre- sponding pixels (a subset of the full set of pixels) to construct the neighborhoods.  On small-dimensional (non-image) sets we face the opposite problem: the small number of input fea- tures limit the number of neighborhoods. This ac- tually highlights a limitation of the algorithm: when the number of input features is small and they are not very correlated, the number of generated neigh-  borhood and edge features is small, and they essen- tially contain the same information as the original features. Nevertheless, we were curious whether we can see any improvement by blowing up the num- ber of features (similarly, in spirit, to what sup- port vector machines do). We obtain a larger num- ber of neighborhoods by deﬁning a set of thresh- olds {ρ1 N }, and constructing M “concentric” neighborhoods for each feature xj. On data sets with heterogeneous feature types it is also important to normalize the features by the usual transforma- tion xj i denote the mean and the standard deviation of the elements of x, respectively, before proceeding with the feature construction (Step 2).  i − µ(xj)(cid:1)/σ(xj), where µ(x) and σ(x) =(cid:0)xj  N, . . . , ρM  (cid:48)  Optimally, of course, automatic hyperparameter op- timization (Bergstra and Bengio, 2012; Bergstra et al., 2011; Snoek et al., 2012) is the way to go, es- pecially since AdaBoost.MH also has two or three hyperparameters, and manual grid search in a four- to-ﬁve dimensional hyperparameter space is not fea- sible. For now, we set aside this issue for future work.  2.2. AdaBoost.MH with Hamming trees  The constructed features can be input to any “shal- low” classiﬁer. Since we use AdaBoost.MH with Hamming trees, we brieﬂy describe them here. The full formal description with the pseudocode is in the documentation of MultiBoost (Benbouzid et al., 2012). It is available at the multiboost.org website along with the code itself.  i=1  (cid:80)K  The advantage of AdaBoost.MH over other multi- class boosting approaches is that it does not re- quire from the base learner to predict a single la-  bel (cid:98)y = h(x) for an input instance x, rather, it uses edge γ(h, W) =(cid:80)n  vector-valued base learners h(x). The requirement it suﬃces if the for these base learners is weaker: (cid:96)=1 wi,(cid:96)h(cid:96)(xi)yi,(cid:96) is slightly larger than zero, where W = [wi,(cid:96)](cid:96)=1,...,K i=1,...,n is the weight matrix (over instances and labels) in the cur- rent boosting iteration, and yi = (yi,1, . . . , yi,K) is a ±1-valued one-hot code of the label. This makes it easy to turn weak binary classiﬁers into multi-class base classiﬁers, without requiring that the multi- class zero-one base error be less than 1/2. In case h is a decision tree, there are two important conse- quences. First, the size (the number of leaves N ) of the tree can be arbitrary and can be tuned freely (whereas requiring a zero-one error to be less than 1/2 usually implies large trees). Second, one can de- sign trees with binary {±1}K-valued outputs, which  could not be used as standalone multi-class classi- ﬁers.  In a Hamming tree, at each node, the split is learned by a multi-class decision stump of the form h(x) = vϕ(x), where ϕ(x) is a (standard) scalar ±1-valued decision stump, and v is a {±1}K-valued vector. At leaf nodes, the full {±1}K-valued vector vϕ(x) is output for a given x, whereas at inner nodes, only the binary function ϕ(x) is used to decide whether the instance x goes left or right. The tree is con- structed top-down, and each node stump is opti- mized in a greedy manner, as usual. For a given x, unless v happens to be a one-hot vector, no single class can be output (because of ties). At the same time, the tree is perfectly boostable: the weighted sum of Hamming trees produces a real -valued vec- tor of length K, of which the predicted class can be easily derived using the arg max operator.  3. Experiments  We carried out experiments on four data sets: MNIST2 and CIFAR-103 are standard image clas- siﬁcation data sets, and the Pendigit and Letter sets are relatively large benchmarks from the UCI reposi- tory4. We boosted Hamming trees (Benbouzid et al., 2012) on each data sets. Boosted Hamming trees have three hyperparameters: the number of boosting iterations T , the number of leaves N , and the num- ber of (random) features d(cid:48) considered at each split (LazyBoost (Escudero et al., 2000) settings that give the ﬂavor of a random forest to the ﬁnal classi- ﬁer). Out of these three, we validated only the num- ber of leaves N in the “classical” way using 80/20 single validation on the training set. Since Ada- Boost.MH does not exhibit any overﬁtting even af- ter a very large number of iterations (see Figure 1),5 we run it for a large number of T = 105 iterations, and report the average test error of the last T /2 iter- ations. The number of (random) features d(cid:48) consid- ered at each split is another hyperparameter which does not have to be tuned in the traditional way. In our experience, the larger it is, the smaller the asymptotic test error is. On the other hand, the larger it is the slower the algorithm converges to this error. This means that d(cid:48) controls the trade-  2http://yann.lecun.com/exdb/mnist 3http://www.cs.toronto.edu/~kriz/cifar.html 4http://www.ics.uci.edu/~mlearn/MLRepository.  html  5This is certainly the case in these four experiments, but in our experience, making AdaBoost.MH overﬁt is really hard unless signiﬁcant label noise is present in the training set.  3  oﬀ between the accuracy of the ﬁnal classiﬁer and the training time. We tuned it to obtain the full learning curves in reasonable time.  The neighborhood and edge features were con- structed as described in Section 2. Estimating cor- relations can be done robustly on relatively small random samples, so we ran the algorithm on a ran- dom input matrix X with 1000 instances.  3.1. MNIST  MNIST consists of 60000 grey-scale training images of hand-written digits of size 28 × 28 = 784. In all experiments on MNIST, d(cid:48) was set to 100. The ﬁrst baseline run was AdaBoost.MH with Hamming trees of 8 leaves on the raw pixels (green curve in Figure 1(a)), achieving a test error of 1.25%. We also ran AdaBoost.MH with Hamming trees of 8 leaves in the roughly 300000-dimensional feature space gen- erated by ﬁve types of Haar ﬁlters (Viola and Jones 2004; red curve in Figure 1(a)). This setup pro- duced a test error of 0.85% which is the state of the art among boosting algorithms. For generating neighborhood and edge features, we ﬁrst ran autoas- sociative AdaBoost.MH with decision stumps for 800 iterations that picked the 326 pixels depicted by the white pixels in Figure 2(a). Then we constructed 326 neighborhood features using ρN = 0.5 and 1517 edge features using ρE = 0.7. The 100 most impor- tant features (picked by running AdaBoost.MH with decision stumps) is depicted in Figure 3. Fi- nally we ran AdaBoost.MH with Hamming trees of 8 leaves on the constructed features (blue curve in Figure 1(a)), achieving a test error of 0.94% which is one of the best results among methods that do not use explicit image priors (pixel order, speciﬁc distortions, etc.).  Note that AdaBoost.MH picked slightly more neighborhood than edge features relatively to their prior proportions. On the other hand, it was cru- cial to include both neighborhood and edge features: AdaBoost.MH was way suboptimal on either sub- set.  3.2. CIFAR  CIFAR-10 consists of 50000 color training images of 10 object categories of size 32 × 32 = 1024, giv- ing a total of 3 × 1024 = 3072 features. In all ex- periments on CIFAR, d(cid:48) was set to 10. The ﬁrst baseline run was AdaBoost.MH with Hamming trees of 20 leaves on the raw pixels (green curve in Figure 1(b)), achieving a test error of 39.1%. We also ran AdaBoost.MH with Hamming trees of  4  20 leaves in the roughly 350000-dimensional feature space generated by ﬁve types of Haar ﬁlters (Viola and Jones 2004; red curve in Figure 1(b)). This setup produced a test error of 36.1%. For generat- ing neighborhood and edge features, we ﬁrst ran au- toassociative AdaBoost.MH with decision stumps for 1000 iterations that picked the 922 color chan- nels depicted by the white and colored pixels in Fig- ure 2(b). Then we constructed 922 neighborhood features using ρN = 0.85 and 4552 edge features us- ing ρE = 0.85. Finally we ran AdaBoost.MH with Hamming trees of 20 leaves on the constructed fea- tures (blue curve in Figure 1(a)), achieving a test error of 33.6%.  None of these results are close to the sate of the art, but they are not completely oﬀ the map, either: they match the performance of one of the early techniques that reported error on CIFAR-10 (Ranzato et al., 2010). The main signiﬁcance of this experiment is that AdaBoost.MH with neighborhood and edge features can beat not only AdaBoost.MH on raw pixels but also AdaBoost.MH with Haar features.  3.3. UCI Pendigit and Letter  In principle, there is no reason why neighborhood and edge features could not work on non-image sets. To investigate, we ran some preliminary tests on the relatively large UCI data sets, Pendigit and Let- ter. Both of them contain 16 features and several thousand instances. The baseline results are 2.16% on Pendigit using AdaBoost.MH with Hamming trees of 4 leaves (green curve in Figure 1(d)) and 2.23% on Letter using AdaBoost.MH with Ham- ming trees of 20 leaves (green curve in Figure 1(c)). We constructed neighborhoods using a set of thresh- olds ρN = {0.1, 0.2, . . . , 0.9}, giving us 62 unique neighborhoods on Letter and 86 unique neighbor- hoods on Pendigit (out of the possible 9×16 = 126). We then proceeded by constructing edge features with ρE = 0.7, giving us 506 more features on Let- ter, and 1040 more features on Letter. We then ran AdaBoost.MH with Hamming trees of the same number of leaves as in the baseline experi- ments, using d(cid:48) = 100. On Pendigit, we obtained 2.05%, better than in the baseline (blue curve in Figure 1(d)), while on Letter we obtained 2.59%, signiﬁcantly worse than in the baseline (blue curve in Figure 1(c)). We see two reasons why a larger gain is diﬃcult on these sets. First, there is not much correlation between the features to exploit. In- deed, setting ρN and ρE to similar values to those we used on the image sets, neighborhoods would have been very small, and there would have been almost  (a) MNIST  (b) CIFAR-10  (c) UCI Letter  (d) UCI Pendigit  Figure 1. Test learning curves.  (a) MNIST  (b) CIFAR-10  Figure 2. Initial pixel selection using autoassociative AdaBoost.MH. (a) White means that the pixel is selected, black means it is not. (b) Colors are mixtures of the color channels selected. White means that all channels were selected, and black means that none of them were selected.  5  Neighborhoods&edgesRawpixelsHaarfilters1000200050001(cid:180)1042(cid:180)1045(cid:180)1041(cid:180)1050.0080.0100.0120.0140.0160.0180.020TerrorNeighborhoods&edgesRawpixelsHaarfilters10001041051060.300.350.400.450.50TerrorNeighborhoods&edgesRawfeatures10010001041050.0200.0250.0300.0350.040TerrorNeighborhoods&edgesRawfeatures10010001041050.0180.0200.0220.0240.0260.0280.030TerrorFigure 3. The 100 most important neighborhood and edge features picked by AdaBoost.MH. In neighborhood features we mark the middles of pixels in the neighborhood J(xj) by green dots. In edge features we mark the middles of pixels of the positive neighborhood by blue dots, and the pixels of the negative neighborhood by red dots. The black&white images in both cases are averages of the 20% of MNIST test images that respond the strongest to the given ﬁlter.  no edges. Second, AdaBoost.MH with Hamming trees is already a very good algorithm on these sets, so there is not much margin for improvement.  4. Future work  Besides running more experiments and tuning hy- perparameters automatically, the most interesting  6  question is whether stacking neighborhood and edge features would work. There is no technical prob- lem of re-running the feature construction on the features obtained in the ﬁrst round, but it is not clear whether there is any more structure this simple method can exploit. We did some preliminary trials on MNIST where it did not improve the results, but this may be because MNIST is a relatively simple  ing algorithms. In Advances in Neural Informa- tion Processing Systems, volume 25.  Viola, P. and Jones, M. (2004). Robust real-time face detection. International Journal of Computer Vision, 57, 137–154.  Zhu, J., Zou, H., Rosset, S., and Hastie, T. (2009). Multi-class AdaBoost. Statistics and its Interface, 2, 349–360.  set with not very complex features and rather ho- mogeneous classes. Experimenting with stacking on CIFAR is deﬁnitely the next step. Another inter- esting avenue is to launch a large scale exploration on more non-image benchmark sets to see whether there is a subclass of sets where the correlation-based feature construction may work, and then to try to characterize this subclass.  References  Benbouzid, D., Busa-Fekete, R., Casagrande, N., Collin, F.-D., and K´egl, B. (2012). MultiBoost: a multi-purpose boosting package. Journal of Ma- chine Learning Research, 13, 549–553.  Bergstra, J. and Bengio, Y. (2012). Random search for hyper-parameter optimization. Journal of Ma- chine Learning Research.  Bergstra, J., Bardenet, R., K´egl, B., and Bengio, Y. (2011). Algorithms for hyperparameter optimiza- tion. In Advances in Neural Information Process- ing Systems (NIPS), volume 24. The MIT Press.  Escudero, G., M`arquez, L., and Rigau, G. (2000). Boosting applied to word sense disambiguation. In Proceedings of the 11th European Conference on Machine Learning, pages 129–141.  K´egl, B. and Busa-Fekete, R. (2009). Boosting prod- ucts of base classiﬁers. In International Confer- ence on Machine Learning, volume 26, pages 497– 504, Montreal, Canada.  Le Roux, N., Bengio, Y., Lamblin, P., M., J., and K´egl, B. (2007). Learning the 2-D topology of images. In Advances in Neural Information Pro- cessing Systems, volume 20, pages 841–848. The MIT Press.  Li, P. (2009). ABC-Boost: Adaptive base class In Interna-  boost for multi-class classiﬁcation. tional Conference on Machine Learning.  Ranzato, M., Krizhevsky, A., and Hinton, G. E. (2010). Factored 3-way restricted Boltzmann ma- chines for modeling natural images. In Interna- tional Conference on Artiﬁcial Intelligence and Statistics.  Schapire, R. E. and Singer, Y. (1999).  Improved boosting algorithms using conﬁdence-rated pre- dictions. Machine Learning, 37(3), 297–336.  Snoek, J., Larochelle, H., and Adams, R. P. (2012). Practical Bayesian optimization of machine learn-  7  ","Motivated by an abstract notion of low-level edge detector filters, wepropose a simple method of unsupervised feature construction based on pairwisestatistics of features. In the first step, we construct neighborhoods offeatures by regrouping features that correlate. Then we use these subsets asfilters to produce new neighborhood features. Next, we connect neighborhoodfeatures that correlate, and construct edge features by subtracting thecorrelated neighborhood features of each other. To validate the usefulness ofthe constructed features, we ran this http URL on four multi-class classificationproblems. Our most significant result is a test error of 0.94% on MNIST with analgorithm which is essentially free of any image-specific priors. On CIFAR-10our method is suboptimal compared to today's best deep learning techniques,nevertheless, we show that the proposed method outperforms not only boosting onthe raw pixels, but also boosting on Haar filters."
1312.7381,2014,Rate-Distortion Auto-Encoders  ,"['Luis G. Sanchez Giraldo', 'Jose C. Principe']",https://arxiv.org/pdf/1312.7381.pdf,"Rate-Distortion Auto-Encoders  Dept. of Electrical and Computer Engineering  Dept. of Electrical and Computer Engineering  Luis G. Sanchez Giraldo  University of Florida  Gainesville, Florida, USA  Jose C. Principe  University of Florida  Gainesville, Florida, USA  4 1 0 2    r p A 7 1         ]  G L . s c [      2 v 1 8 3 7  .  2 1 3 1 : v i X r a  sanchez@cnel.ufl.edu  principe@cnel.ufl.edu  Abstract  A rekindled the interest in auto-encoder algorithms has been spurred by recent work on deep learning. Current efforts have been directed towards effective train- ing of auto-encoder architectures with a large number of coding units. Here, we propose a learning algorithm for auto-encoders based on a rate-distortion objec- tive that minimizes the mutual information between the inputs and the outputs of the auto-encoder subject to a ﬁdelity constraint. The goal is to learn a repre- sentation that is minimally committed to the input data, but that is rich enough to reconstruct the inputs up to certain level of distortion. Minimizing the mu- tual information acts as a regularization term whereas the ﬁdelity constraint can be understood as a risk functional in the conventional statistical learning setting. The proposed algorithm uses a recently introduced measure of entropy based on inﬁnitely divisible matrices that avoids the plug in estimation of densities. Exper- iments using over-complete bases show that the rate-distortion auto-encoders can learn a regularized input-output mapping in an implicit manner.  1 Introduction  Auto-encoders are unsupervised learning algorithms that capture the structure in data by ﬁnding an internal representation of the input patterns (encoding) from which they can be reconstructed at least approximately. By learning a transformation G (encoding) from the input x∈ X to z = G(x)∈ Z the auto-encoder tries to capture the structure of the input. To guarantee the transformation G preserves the information about x, a decoder ˜G−1 is also learned along way, such that a measure of ﬁdelity E[D(X, ˜X)] in the reconstruction is optimized. Perhaps, the most conventional class of auto-encoders are the bottle-neck networks that transform the input data x ∈ X ⊆ Rd to a lower dimensional space Z ⊆ R p, and then transform it back to the input space X . It is expected that the lower dimensional space forces the encoder to capture the relations between the variables in the input space. While this is clear when one is restricted to linear mappings, the problem becomes less well-understood if nonlinear mappings are allowed. Recent progress in deep learning has reignited the interest on developing procedures to train auto encoders. Nevertheless, research efforts are now focused on how to effectively train auto-encoder architectures with a large number of encoding units, possibly larger that the number of inputs. In this case, it is necessary to develop algorithms that can avoid trivial solutions. Approaches like sparse encoding, which can be applied to over-complete scenarios, dim(X ) < dim(Z ), use the concept of effective dimensionality. A sparsity constraint induces an active set of variables with an expected L0 norm smaller than the input space dimensionality. In general, sparse coding procedures require solving an optimization problem at inference time since the encoding mapping is not explicit. However, it has been shown that efﬁcient inference can be made possible by training a nonlinear feed-forward network to mimic the output of a sparse encoding algorithm [1]. Another recent approach focus on the idea of robustness. De-noising auto-encoders [2] learn a mapping by simply minimizing the error between the reconstruction of the auto-encoder when a  noisy version of the input is feed to the mapper. Let X be the random variable representing the input, and ˆX be the noisy version of it. The goal of the de-noising auto-encoder is to learn a map f (·) = ˜G−1(G(·)) such that E[D( f ( ˆX ),X)] is minimized. Contractive auto-encoders [3, 4] are another way of enforcing robustness on the learned encoder by penalizing the sensitivity of the representation G(x) to the input x. This is achieved by minimizing the expected Frobenius norm of the Jacobian E[JG(X)] of G while maximizing ﬁdelity of the reconstruction. Here, we adopt an information theoretic view of the problem. In all the above examples, implicitly or explicitly, the concept of compression plays a fundamental role. The idea of rate-distortion has been explored for nondeterministic mappings by [5]. In this case the structure of the data is represented  by a set of N points located along a manifold. The algorithm produces random map p(m |x) from  X to the embedding of a manifold denoted by M . The manifold is assumed to be a less faithful description of the data. The objective is to minimize a trade-off between ﬁdelity and information content. The information content correspond to the mutual information between the data and the manifold representation:  I(X;M ) = Zx∈X Zm ∈M  P(x, m )log  P(x, m )  p(x),PM (m )  dx  (1)  The locations of the points are adjusted according to the distortion measure. A Boltzmann distribu- tion containing the ﬁdelity measure arises as solution to the above objective. The update equations for an algorithm based on the above trade off correspond to an iterated projection between convex sets. Note however, that a solution of this form arises from the considerations of a random mapping from X to the embedded manifold.  Here, we propose an algorithm that incorporates the idea to learn a parametrized mapping, encoder and decoder function, that optimizes a trade off between compression and ﬁdelity in the reconstruc- tion. The intuition behind this procedure is that the auto-encoder should avoid trivial solutions if it is forced to map the input data to a compressed version of it.  2 The rate-distortion function  For the transmission of information through a channel, it was showed by Shannon that the noise and bandwidth inherent to the channel determine the rate at which information can be transfered, and that any attempt to transfer information above the limit imposed by the channel will incur in some error in the recovered message. This result known as the channel capacity problem is concerned with achieving zero error in the transmission. A complementary problem is to determine what is the minimum possible rate at which information can be transfered such that the error in the recovered message does not surpasses a certain level. Answering this question gives rise to the rate-distortion function. The information rate-distortion function R(D) is deﬁned as:  R(D) = min q( ˆx|x)  I(X; ˆX) subject to E[d(X, ˆX)] ≤ D  (2)  where d(·,·) is a distortion function that can be related to the loss incurred when the exact value of the input cannot be recovered from the output and E[d(X, ˆX)] = (cid:229) x, ˆx p(x)q( ˆx|x)d(x, ˆx) . The above problem (2) is a variational problem for which we search for q( ˆx|x) ≥ 0 subject to the regularity condition: (3)  q( ˆx|x) = 1 for all x ∈ X  ˆx∈ ˆX  This is a standard minimization problem of a convex function over a convex set [6] with a know numerical solution in the case of ﬁnite input/output alphabets. How could this function be utilized to ﬁnd a representation of the input variables?  2.1 Motivation Example: rate-distortion and PCA  The relation between rate-distortion and PCA arises from the assumption of a Gaussian source. Con- sider the Gaussian random vector X ∈ Rd with zero mean and covariance S . We can ﬁnd similarity transformation U to a Gaussian random vector Z ∈ Rd with zero mean and diagonal covariance ma- trix G . The transformation matrix U is unitary which implies det (U) = 1,  , such that UT S  S U = G  (cid:229) S S G G S G G and thus, h(X) = h(Z) and the mapping is a bijection 1. The following theorem from [7] is used on stating the result.  Theorem 2.1 (Rate distortion for a parallel Gaussian source): Let Xi ∼ N (0,g 2 be independent Gaussian random variables, and the distortion measure d(x, ˆx) = (cid:229) d Then the rate distortion function is given by  i ), i = 1, . . . ,d, i=1(xi − ˆxi)2.  R(D) =  d(cid:229)  i=1  1 2  log  g i Di  Di =(cid:26) l  g 2 i  if l < g 2 i if l ≥ g 2 i ,  where  and l  is chosen so that (cid:229) d  i=1 Di = D.  (4)  (5)  Applying the result to the random vector Z yields the conclusion that the rate for components with variance less than l will be zero, and thus, no effort in representing such components should be made. This is the way components are selected in PCA where a variance threshold is set and only those components above the threshold are retained. On the other hand, l plays the role of the observation noise in the generative model for PCA, but the main difference is that ˆZ is not assumed to be N (0,I). Note however that for the above conclusion, the similarity transformation was given in advance rather than derived from the rate-distortion objective. Our goal is to learn a map f : X 7→ X based on the principle of minimization of mutual information. The numerical procedure know as the Blahut-Arimoto algorithm provides a means to compute a stochastic map q( ˆx|x) that achieves the minimum rate under a given distortion constraint. Nevertheless, this method was developed for ﬁnite input/output alphabets. We are interested in an approximation that uses a deterministic map f (·) on continuous input/output spaces. 2.2 Rate-distortion as an objective for learning a map  Shannon’s deﬁnition of mutual information can be decomposed in terms of the marginal and condi- tional entropies as:  I(X;Y ) = H(X)− H(X|Y )  (6) where H(X) and H(X|Y ) are the marginal and conditional entropies of X and X given Y . Since the entropy of the input X is constant, we only need to consider H(X|Y ) when dealing with maps from X to Y . We can thus reformulate (2) in terms of the conditional entropy as: H(X| ˆX) subject to E[d(X, ˆX)] ≤ D.  maximize  (7)  f (X)= ˆX  Writing the Lagrangian of (7),  L ( f ,g ) = E[d(X, ˆX)]−  1 m H(X| ˆX),  (8)  where m determines the distortion level D, shows that the the objective can be understood as a regularized risk minimization problem. In (8), d(X, ˆX) plays the role of the loss functional and H(X| ˆX) acts as a regularization parameter. However, this form of regularization is not explicit on the parameters of the function f (·). Informally, we can see that maximizing the conditional entropy H(X| ˆX) = H(X; ˆX)− H( ˆX) can have the effect of lowering the entropy of the output variable ˆX and thus, we can think of the mapping f as a contraction.  3 Rate-Distortion Auto-Encoder Algorithm  On the previous section, we motivate the use of the rate-distortion objective to learn a map that is minimally committed in terms of retaining information about the inputs, yet able to provide a reconstruction with a desired level of distortion. However, solving the above objective function  1Note, we are not using mutual information to relate X and Z, cause in this case it is inﬁnite  would require the distribution of X to be known. Since we only have access to a set of i.i.d. samples {xi}N i=1 drawn from PX (x), a suitable estimator of both conditional entropy, and expected distortion is required. For the expected distortion we employ the empirical estimator  Demp( f ) =  1 N  N(cid:229)  i=1  d(xi, f (xi)).  (9)  For the conditional entropy term, we use an alternative deﬁnition of entropy introduced in [8].  i=1 ⊂ X and k  3.1 Matrix-Based Entropy Functional Let X = {xi}N : X × X 7→ R be a real valued positive deﬁnite kernel that is also inﬁnitely divisible. The Gram matrix K obtained from evaluating a positive deﬁnite kernel k (·,·) on all pairs of exemplars, that is (K)i j = k (xi,x j), can be employed to deﬁne a quantity with properties similar to those of an entropy, for which the probability distribution of X does not need to be esti- mated.  A matrix-based analogue to Renyi’s a -entropy for a positive deﬁnite matrix A of size N × N, such that tr(A) = 1, is given by the functional:  Sa (A) =  log2 [tr(A  a  )] =  1  1− a  1  1− a  log2"" N(cid:229)  i=1  a #, l i(A)  (10)  where l i(A) denotes the ith eigenvalue of the A matrix2.The matrix-based entropy estimate from the sample X can be obtained by evaluating (10) on following normalized version of K:  Furthermore, based on the the product kernel, we can deﬁne the notion of joint-entropy as:  Ki j  .  Ai j =  1 N  pKiiKj j tr(A◦ B)(cid:19) ,  Sa (cid:18) A◦ B  (11)  (12)  where A ◦ B denotes the Hadamard product between the matrices A and B. For a set of pairs i=1 such that xi ∈ X and yi ∈ Y , let k X and k Y be positive deﬁnite kernels deﬁned on {(xi,yi)}n X × X and Y × Y , respectively. If Ai j = k X (xi,x j) and Bi j = k Y (yi,y j), the Hadamard prod- uct A◦ B is nothing but the Gram matrix using the product kernel k ((xi,yi), (x j,y j)) = k X (xi,x j)· k Y (yi,y j) [9]. It can also be shown that if the Gram matrices A and B are constructed using nor- malized inﬁnitely divisible kernels (based on (11)), such that Aii = Bii = 1/n, (12) is never smaller than any of the individual entropies Sa (A) or Sa (B). This allows us to deﬁne a matrix notion of conditional entropy as follows:  Sa (A|B) = Sa (cid:18) A◦ B  tr(A◦ B)(cid:19)− Sa (B),  (13)  3.2 Maximum Matrix-Based Conditional Entropy Auto-encoder  Using the kernel matrix measure of entropy, we can formulate an objective function based on i=1, where xi ∈ Rdx we deﬁne a parametrized encoding mapping (13). For a set of points {xi}n ˆzi = GW,c(xi) = g(Wxi + c), as well as a decoding mapping ˜xi = ˜G−1 A,b(ˆzi) = Aˆzi + b. Notice that there is no imposed constraint on the mapping such as tied weights or explicit weight decay in our formulation. Let KX and K ˆX denote the normalized Gram matrices for {xi}n i=1, re- spectively. Our goal is to ﬁnd the set of pairs (W,c) and (A,b) that maximize the matrix-based conditional entropy of the inputs xi given the reconstructions ˜xi = ˜G−1 A (GW(xi)). This can be posed as the following optimization problem:  i=1 and {ˆxi}n  Sa (KX|K ˜X )  maximize (W,c,A,b)∈Q subject to 1 N  N(cid:229) i=1  d(xi, ˜xi) ≤ D.  (14)  2All eigenvalues are considered along with their multiplicities  Q Q ¶ Sa (K ˆX )  a  Let X = (x1, x2,··· ,xN)T denote the input data matrix, Z = XWT + 1NcT be the afﬁne transform of the input , ˆZ = g(Z) denote the encoder output after the nonlinearity g(·), G′ = g′(Z) be the derivatives of the encoder nonlinearity evaluated at Z, and ˆX = ˆZAT + 1NbT be the decoder output. The gradients of Sa (K ˆX ) and Sa (NKX ◦ K ˆX ) with respect to K ˆX are given by:  UL  a −1UT and  =  ¶ K ˆX ¶ Sa (NKX ◦ K ˆX )  (1− a )tr(Ka ˆX ) a (1− a )tr[(NKX ◦ K ˆX ) where UL our work, we use the normalized Gaussian kernel, k (xi,x j) = 1 output points. The partial derivatives of the conditional entropy with respect to the parameters of the auto-encoder are given by:  a −1VT(cid:1) , ¶ K ˆX G VT are the eigenvalue decompositions of K ˆX and NKX ◦ K ˆX , respectively. In x (cid:17), for both input and  NKX ◦(cid:0)VG n exp(cid:16)−kxi−x jk2  L UT and VG  (16)  2s 2  =  ]  a  (15)  = −4(A ˆZT) [D(P)− P] ˆZ, = 0,  ¶ A  ¶ Sa (K ˆX ) ¶ Sa (K ˆX ) ¶ Sa (K ˆX ) ¶ Sa (K ˆX )  ¶ W  ¶ b  = −4(cid:16)(ATA ˆZT [D(P)− P])◦ G′T(cid:17)X, = −4(cid:16)(ATA ˆZT [D(P)− P])◦ G′T(cid:17)1N,  ¶ K ˆX  ¶ Sa (K ˆX )  ◦ 1 2s 2 ˆx  where P =  ¶ c K ˆX and D(P) = diag(P1N). The partial derivatives of the joint entropy K ˆX . term have the same form of the above, we only need to replace P by Q = From the above set of derivatives, (17),(19), and (20), we can see that the term D(P)− P appears in all of them. Noticing that P is a positive deﬁnite and that indeed can be regarded as an afﬁnity matrix, we can think of D(P)− P as the graph Laplacian, where P is the weight matrix for the set of edges and D(P) the degree of the graph. Notice also that P corresponds to a data dependent kernel since its values depend on the spectrum of K ˆX . The derivatives with respect to the distortion measure Demp are given by:  ¶ Sa (NKX◦K ˆX )  ◦ 1 2s 2 ˆx  ¶ K ˆX  ¶ Demp ¶ A ¶ Demp ¶ b ¶ Demp ¶ W ¶ Demp ¶ c  = − = − = − = −  2 N 2 N 2  (X− ˆX)T ˆZ, (X− ˆX)T1N, N(cid:2)(cid:0)(X− ˆX)AT(cid:1)◦ G′(cid:3) N(cid:2)(cid:0)(X− ˆX)AT(cid:1)◦ G′(cid:3)  2  T X,  T 1N,  The above derivatives can be employed to search for the parameters for instance using gradient ascent. In our work we implemented gradient ascent for the following Lagrangian:  L (W,c,A,b) = Sa (NKX ◦ K ˆX )− Sa (K ˆX )− m 1  N  N(cid:229) i=1kxi − ˆxik2,  (25)  where m  is ﬁxed depending on the desired distortion level3.  4 Experiments  This section describe some of the experiments we have carried out with the rate-distortion auto- encoders. We qualitatively illustrate the regularization property of the rate-distortion auto-encoder 3Note that by ﬁxing the value of m we are implicitly enforcing an equality constraint on Demp rather than  an inequality constraint since the positive multiplier represents an active constraint.  (17)  (18)  (19)  (20)  (21)  (22)  (23)  (24)  L L G G L G 4  3  2  1  0  −1  −2  −3    −4 −4     Input Auto−Encoder Output  −3  −2  −1  0  1  2  3  4  4  3  2  1  0  −1  −2  −3    −4 −4     Input Auto−Encoder Output  −3  −2  −1  0  1  2  3  4  4  3  2  1  0  −1  −2  −3    −4 −4     Input Auto−Encoder Output  −3  −2  −1  0  1  2  3  4  (a) Linear g(x) = x  (c) logsig g(x) = 1/(1 + exp(−x)) Figure 1. Outputs for different activation functions of the rate-distortion auto- encoder with an over-complete representation when the inputs are Gaussian dis- tributed.  (b) ReLU g(x) = max{0, x}  with a simple set of examples involving an over-complete representations. First,we show how the rate-distortion objective regularizes different types of units implicitly. Algorithm such as contractive auto-encoder rely on explicit calculation of the Jacobian, which depends on the type of units selected. Following these experiments, we show some resulting basis after learning on the MNIST dataset.  4.1 Synthetic Data  Gaussian distributed data: The ﬁrst example corresponds to a set of data point drawn from a bivariate Gaussian distribution with zero mean and covariance matrix  S X =(cid:18) 1  0.95  0.95  1 (cid:19) .  (26)  We use a linear encoding, that is g(x) = x, that is also over-complete since the encoder project the 2-dimensional data points to 10 different directions. The conventional auto encoder would over ﬁt being able to achieve zero reconstruction error, but it won’t be able to implicitly retain what is thought to be the structure in the data. We also compare this output to the outputs of two nonlinear auto-encoders, one uses the logsig units g(x) = 1/(1 + exp(−x)), and the other a rectiﬁed linear Figure 1 shows the outputs of the three auto-encoders on the units (ReLU) g(x) = max{0,x}. Gaussian distributed data. It can be seen that the outputs approximately align with what corresponds roughly to the ﬁrst principal component of the data. Notice, that no bottleneck neither shrinkage was explicitly deﬁned. The parameters of our cost function are m = 0.5 for the distortion trade-off, and s x = 0.2√2 = s ˆx for the kernel size. Mixture of Gaussians: The second example, employs a mixture of three Gaussian distributions to show the output of the rate-distortion auto-encoder in a nonlinear scenario, where an over-complete representation followed by a nonlinearity can be advantageous. The means of and covariances of the mixture components are  m 1 =(cid:18) 2 −0.95  −2 (cid:19) , m 1 =(cid:18) 6 −2 (cid:19) , m 2 =(cid:18) −2 S 3 =(cid:18) 1 S 2 = S  −2 (cid:19); and 1 (cid:19) ,  −0.95  (cid:19) , S  0.95  0.95  1  1  S 1 =(cid:18)  (27)  respectively, and the mixing weights are p1 = 0.5, and p2 = p3 = 0.25. Figure 2 shows the outputs of the four auto-encoders on the mixture of Gaussian distributions. The auto-encoders employ: linear, rectiﬁed linear, sigmoidal, and saturated linear units. It can be seen that the outputs approximately align with what can be though as the principal curves of the data. Again, we want to stress that no bottleneck neither shrinkage was explicitly deﬁned. In this case each of the auto-encoder has 20 units for encoding, which would easily over ﬁt the data in the absence of any regularization or add-hoc constraints such as tied weights. The parameters of our cost function are m = 0.5 for the distortion trade-off, and s x = 0.2√2 = s ˆx for the kernel size. The linear units seem to ﬁt the data, but as we previously mentioned they favor the principal components. Lowering the value of m would collapse the reconstructed points into a line. This is not necessarily  S S S S S S Input Auto−Encoder Output  4  3  2  1  0  −1  −2  −3  −4     −6  −4  −2  0  2  4  6  (a) Linear g(x) = x  Input Auto−Encoder Output  4  3  2  1  0  −1  −2  −3              Input Auto−Encoder Output  4  3  2  1  0  −1  −2  −3  −4     −6  −4  −2  0  2  4  6  (b) ReLU g(x) = max{0, x}  Input Auto−Encoder Output  4  3  2  1  0  −1  −2  −3  −4     −6  −4  −2  0  2  4  6  −4     −6  −4  −2  0  2  4  6  (c) logsig g(x) = 1/(1 + exp(−x))  (d) SatLU g(x) = max{0, x}− max{0, x− 1}  Figure 2. Outputs for different activation functions of the rate-distortion auto- encoder with an over-complete representation when the inputs are a mixture of Gaussian distributions.  6  4  2  0  −2  −4  −6     −6  −4  −2  0  2  4  6  (a) Non regularized           6  4  2  0  −2  −4  −6     −6  −4  −2  0  2  4  6  6  4  2  0  −2  −4  −6     −6  −4  −2  0  2  4  6  (b) De-noising auto-encoder noise s = 0.5  (c) Rate-distortion auto-encoder  Figure 3. Energy kx− ˆxk2 landscapes for different auto-encoder algorithms  the case when nonlinear units are considered. Finally, in Figure 3 we show the resulting energy kx− ˆxk2 landscapes for the over-complete auto- encoder with soft rectiﬁed linear units after being trained with: Non-regularized, de-noising auto- encoder using isotropic Gaussian noise, and the proposed rate distortion function. The rate-distortion objective makes the auto-encoder carve well-deﬁned ravines in the energy landscape at the points were majority of the data lies.  4.2 Handwritten Digits  Here we use a subset of 20000 samples from MNIST to train a rate-distortion auto-encoder. Unlike conventional stochastic gradient, that uses mini-batches of randomly sample data to compute the  (a) Analysis  (b) Synthesis  Figure 4. Analysis and Synthesis basis encountered using the Rate-Distortion Auto- Encoder algorithm  estimate of the gradient at each step, we deﬁne mini-batches by pre-clustering the data. The reason behind this procedure is that the Gram matrix employed to compute entropy measure is approxi- mately block diagonal after reordering the samples by clustering. Therefore, the inﬂuence of the eigenvectors would be also local around the deﬁned clusters. At ﬁrst glance, this approach seems to add the computational overhead of the clustering. However, there are cases samples are natu- rally clustered, for example, sequences of images or processes that are assumed to be piecewise stationary. Figure 4 shows an example of the learned analysis and synthesis basis, after training an auto-encoder with 200 logsig units in the representation layer (hidden layer). The trade off parame- ter has been set to 0.5 and the kernel size for the entropy measure is 8.4. Pixels values were scaled from o to 1 and then data matrix was centered. In the ﬁgure, it can be seen how the auto-encoder not only learn localized blobs as would do normally using tied weights and weight decay, but also learn pen strokes without explicit sparsity constraint or predeﬁned corruption model as presented in the work on d-noising auto-encoders. Varying the trade off parameter gives different regimes for the obtained features. For smaller reconstruction errors features tend to be more localized, whereas for larger tolerance the features go from blobs, to pen strokes, to even full digits. Recall that No tied-weights were employed.  5 Conclusions  We presented an algorithm for auto-encoders based on a rate-distortion objective that tries to mini- mize the mutual information between the inputs and outputs subject to a ﬁdelity constraint. As a mo- tivation example, we showed that for multivariate Gaussian distributed data, PCA can be understood as an optimal mapping in the rate-distortion sense. Moreover, we described how the rate-distortion optimization problem can be understood as a learning objective where the ﬁdelity constraint plays the role of a risk functional and the mutual information acts as a regularization term. To provide a training algorithm, we employed a recently introduced measure of entropy based on inﬁnitely divis- ible matrices that avoids the plug in estimation of densities. Experiments using over-complete bases showed that the auto-encoder was able to learn useful encoding mapping (representation) can learn a regularized input-output in an implicit manner. As future work, we would like to investigate on the relation between the information theoretic objective and the graph Laplacian that arose from the parameter update rules.  References  [1] M. Ranzato, C. Poultney, S. Chopra, and Y. LeCun, “Efﬁcient learning of sparse representations with an  energy-based model,” in Neural Information Processing Systems, 2006.  [2] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol, “Extracting and composing robust features with denoising autoencoders,” in Proceedings of the 25th International Conference on Machine Learning, ser. ICML ’08. New York, NY, USA: ACM, 2008, pp. 1096–1103. [Online]. Available: http://doi.acm.org/10.1145/1390156.1390294  [3] S. Rifai, G. Mesnil, P. Vincent, X. Muller, Y. Bengio, Y. Dauphin, and X. Glorot, “Higher order contractive auto-encoder,” in European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD), 2011.  [4] S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y. Bengio, “Contractive auto-encoders: Explicit invariance  during feature extraction,” in 28th International Conference on Machine Learning, 2011.  [5] D. Chigirev and W. Bialek, “Optimal manifold representation of data: An information theoretic approach,”  in Neural Information Processing Systems, 2004.  [6] T. Berger, Rate Distortion Theory: Mathematical Basis for Data Compression. Prentice-Hall, 1971. [7] T. M. Cover and J. A. Thomas, Elements of Information Theory, 2nd ed. Wiley-Interscience, 2006. [8] L. G. Sanchez Giraldo and J. C. Principe, “Information theoretic learning with inﬁnitely divisible kernels,”  in International Conference on Learning Representations, 2013.  [9] C. Berg, J. P. R. Christensen, and P. Ressel, Harmonic Analysis on Semigroups: Theory of Positive Deﬁnite  and Related Functions, ser. Graduate Texts in Mathematics. Springer-Verlag, 1984, no. 100.  ","A rekindled the interest in auto-encoder algorithms has been spurred byrecent work on deep learning. Current efforts have been directed towardseffective training of auto-encoder architectures with a large number of codingunits. Here, we propose a learning algorithm for auto-encoders based on arate-distortion objective that minimizes the mutual information between theinputs and the outputs of the auto-encoder subject to a fidelity constraint.The goal is to learn a representation that is minimally committed to the inputdata, but that is rich enough to reconstruct the inputs up to certain level ofdistortion. Minimizing the mutual information acts as a regularization termwhereas the fidelity constraint can be understood as a risk functional in theconventional statistical learning setting. The proposed algorithm uses arecently introduced measure of entropy based on infinitely divisible matricesthat avoids the plug in estimation of densities. Experiments usingover-complete bases show that the rate-distortion auto-encoders can learn aregularized input-output mapping in an implicit manner."
1312.7302,2014,Learning Human Pose Estimation Features with Convolutional Networks  ,"['Ajrun Jain', 'Jonathan Tompson', 'Mykhaylo Andriluka', 'Graham Taylor', 'Christoph Bregler']",https://arxiv.org/pdf/1312.7302.pdf,"4 1 0 2    r p A 3 2         ]  V C . s c [      6 v 2 0 3 7  .  2 1 3 1 : v i X r a  Learning Human Pose Estimation Features with  Convolutional Networks  Arjun Jain  New York University  Jonathan Tompson New York University  Mykhaylo Andriluka MPI Saarbruecken  ajain@nyu.edu  tompson@cims.nyu.edu  andriluk@mpi-inf.mpg.de  Graham W. Taylor University of Guelph  Christoph Bregler  New York University  gwtaylor@uoguelph.ca  chris.bregler@nyu.edu  Abstract  This paper introduces a new architecture for human pose estimation using a multi- layer convolutional network architecture and a modiﬁed learning technique that learns low-level features and a higher-level weak spatial model. Unconstrained human pose estimation is one of the hardest problems in computer vision, and our new architecture and learning schema shows improvement over the current state- of-the-art. The main contribution of this paper is showing, for the ﬁrst time, that a speciﬁc variation of deep learning is able to meet the performance, and in many cases outperform, existing traditional architectures on this task. The paper also discusses several lessons learned while researching alternatives, most notably, that it is possible to learn strong low-level feature detectors on regions that might only cover a few pixels in the image. Higher-level spatial models improve somewhat the overall result, but to a much lesser extent than expected. Many researchers previously argued that the kinematic structure and top-down information are cru- cial for this domain, but with our purely bottom-up, and weak spatial model, we improve on other more complicated architectures that currently produce the best results. This echos what many other researchers, like those in the speech recogni- tion, object recognition, and other domains have experienced [26].  Figure 1: The green cross is our new technique’s wrist locator, the red cross is the state-of-the-art CVPR13 MODEC detector [38] on the FLIC database.  1  Introduction  One of the hardest tasks in computer vision is determining the high degree-of-freedom conﬁguration of a human body with all its limbs, complex self-occlusion, self-similar parts, and large variations due to clothing, body-type, lighting, and many other factors. The most challenging scenario for this problem is from a monocular RGB image and with no prior assumptions made using motion models, pose models, background models, or any other common heuristics that current state-of-the- art systems utilize. Finding a face in frontal or side view is relatively simple, but determining the  1  exact location of body parts such as hands, elbows, shoulders, hips, knees and feet, each of which sometimes only occupy a few pixels in the image in front of an arbitrary cluttered background, is signiﬁcantly harder. The best performing pose estimation methods, including those based on deformable part models, typically are based on body part detectors. Such body part detectors commonly consist of multiple stages of processing. The ﬁrst stage of processing in a typical pipeline consists of extracting sets of low-level features such as SIFT [25], HoG [11], or other ﬁlters that describe orientation statistics in local image patches. Next, these features are pooled over local spatial regions and sometimes across multiple scales to reduce the size of the representation and also develop local shift/scale invariance. Finally, the aggregate features are mapped to a vector, which is then either input to 1) a standard classiﬁer such as a support vector machine (SVM) or 2) the next stage of processing (e.g. assembling the parts into a whole). Much work is devoted to engineering the system to produce a vector representation that is sensitive to class (e.g. head, hands, torso) while remaining invariant to the various nuisance factors (lighting, viewpoint, scale, etc.) An alternative approach is representation learning: relying on the data instead of feature engineer- ing, to learn a good representation that is invariant to nuisance factors. For a recent review, see [6]. It is common to learn multiple layers of representation, which is referred to as deep learning. Several such techniques have used unsupervised or semi-supervised learning to extract multi-layer domain-speciﬁc invariant representations, however, it is purely supervised techniques that have won several recent challenges by large margins, including ImageNet LSVRC 2012 and 2013 [23, 51]. These end-to-end learning systems have capitalized on advances in computing hardware (notably GPUs), larger datasets like ImageNet, and algorithmic advances (speciﬁcally gradient-based train- ing methods and regularization). While these methods are now proven in generic object recognition, their use in pose estimation has been limited. Part of the challenge in making end-to-end learning work for human pose estimation is related to the nonrigid structure of the body, the necessity for precision (deep recognition systems of- ten throw away precise location information through pooling), and the complex, multi-modal nature of pose. In this paper, we present the ﬁrst end-to-end learning approach for full-body human pose estima- tion. While our approach is based on convolutional networks (convnets) [24], we want to stress that the na¨ıve implementation of applying this model “off-the-shelf” will not work. Therefore, the contribution of this work is in both a model that outperforms state of the art deformable part models (DPMs) on a modern, challenging dataset, and also an analysis of what is needed to make convnets work in human pose estimation. In particular, we present a two-stage ﬁltering approach whereby the response maps of convnet part detectors are denoised by a second process informed by the part hierarchy.  2 Related Work  Detecting people and their pose has been investigated for decades. Many early techniques rely on sliding-window part detectors based on hand-crafted or learned features or silhouette extraction techniques applied to controlled recording conditions. Examples include [14, 49, 5, 30]. We refer to [35] for a complete survey of this era. More recently, several new approaches have been proposed that are applied to unconstrained domains. In such domains, good performance has been achieved with so-called “bag of features” followed by regression-based, nearest neighbor or SVM-based ar- chitectures. Examples include “shape-context” edge-based histograms from the human body [28, 1] or just silhouette features [19]. Shakhnarovich et al. [39] learn a parameter sensitive hash function to perform example-based pose estimation. Many relevant techniques have also been applied to hand tracking such as [48]. A more general survey of the large ﬁeld of hand tracking can be found in [12]. Many techniques have been proposed that extract, learn, or reason over entire body features. Some use a combination of local detectors and structural reasoning (see [36] for coarse tracking and [10] for person-dependent tracking). In a similar spirit, more general techniques using pictorial structures [2, 3, 17, 37, 33, 34], “poselets” [9], and other part-models [16, 50] have received increased attention. We will focus on these techniques and their latest incarnations in the following sections.  2  Further examples come from the HumanEva dataset competitions [41], or approaches that use higher-resolution shape models such as SCAPE [4] and further extensions [20, 8]. These differ from our domain in that the images considered are of higher quality and less cluttered. Also many of these techniques work on images from a single camera, but need video sequence input (not single images) to achieve impressive results [42, 52]. As an example of a technique that works for single images against cluttered backgrounds, Shotton et al.’s Kinect based body part detector [40] uses a random forest of decision trees trained on synthetic depth data to create simple body part detectors. In the proposed work, we also adopt simple part- based detectors, however, we focus on a different learning strategy. There are a number of successful end-to-end representation learning techniques which perform pose estimation on a limited subset of body parts or body poses. One of the earliest examples of this type was Nowlan and Platt’s convolutional neural network hand tracker [30], which tracked a single hand. Osadchy et al. applied a convolutional network to simultaneously detect and estimate the pitch, yaw and roll of a face [31]. Taylor et al. [44] trained a convolutional neural network to learn an embedding in which images of people in similar pose lie nearby. They used a subset of body parts, namely, the head and hand locations to learn the “gist” of a pose, and resorted to nearest-neighbour matching rather than explicitly modeling pose. Perhaps most relevant to our work is Taylor et al.’s work on tracking people in video [45], augmenting a particle ﬁlter with a structured prior over human pose and dynamics based on learning representations. While they estimated a posterior over the whole body (60 joint angles), their experiments were limited to the HumanEva dataset [41], which was collected in a controlled laboratory setting. The datasets we consider in our experiments are truly poses “in the wild”, though we do not consider dynamics. A factor limiting earlier methods from tacking full pose-estimation with end-to-end learning meth- ods, in particular deep networks, was the limited amount of labeled data. Such techniques, with millions or more parameters, require more data than structured techniques that have more a priori knowledge, such as DPMs. We attack this issue on two fronts. First, directly, by using larger labeled training sets which have become available in the past year or two, such as FLIC [38]. Second, in- directly, by better exploiting the data we have. The annotations provided by typical pose estimation datasets contain much richer information compared to the class labels in object recognition datasets In particular, we show that the relationships among parts contained in these annotations can be used to build better detectors.  3 Model  To perform pose estimation with a convolutional network architecture [24] (convnet), the most ob- vious approach would be to map the image input directly to a vector coding the articulated pose: i.e. the type of labels found in pose datasets. The convnet output would represent the unbounded 2-D or 3-D positions of joints, or alternatively a hierarchy of joint angles. However, we found that this worked very poorly. One issue is that pooling, while useful for improving translation invariance dur- ing object recognition, destroys precise spatial information which is necessary to accurately predict pose. Convnets that produce segmentation maps, for example, avoid pooling completely [47, 13]. Another issue is that the direct mapping from input space to kinematic body pose coefﬁcients is highly non-linear and not one-to-one. However, even if we took this route, there is a deeper issue with attempting to map directly to a representation of full body pose. Valid poses represent a much lower-dimensional manifold in the high-dimensional space in which they are captured. It seems troublesome to make a discriminative network map to a space in which the majority of conﬁgura- tions do not represent valid poses. In other words, it makes sense to restrict the net’s output to a much smaller class of valid conﬁgurations. Rather than perform multiple-output regression using a single convnet to learn pose coefﬁcients directly, we found that training multiple convnets to perform independent binary body-part clas- siﬁcation, with one network per feature, resulted in improved performance on our dataset. These convnets are applied as sliding windows to overlapping regions of the input, and map a window of pixels to a single binary output: the presence or absence of that body part. The result of applying the convnet is a response-map indicating the conﬁdence of the body part at that location. This lets us use much smaller convnets, and retain the advantages of pooling, at the expense of having to maintain a separate set of parameters for each body part. Of course, a series of independent part  3  Figure 2: The convolutional network architecture used in our experiments.  detectors cannot enforce consistency in pose in the same way as a structured output model, which produces valid full-body conﬁgurations. In the following sections, we ﬁrst describe in detail the con- volutional network architecture and then a method of enforcing pose consistency using parent-child relationships.  3.1 Convolutional Network Architecture  The lowest level of our two-stage feature detection pipeline is based on a standard convnet architec- ture, an overview of which is shown in Figure 2. Convnets, like their fully-connected, deep neural network counterparts, perform end-to-end feature learning and are trained with the back-propagation algorithm. However, they differ in a number of respects, most notably local connectivity, weight sharing, and local pooling. The ﬁrst two properties signiﬁcantly reduce the number of free parame- ters, and reduce the need to learn repeated feature detectors at different locations of the input. The third property makes the learned representation invariant to small translations of the input. The convnet pipeline shown in Figure 2 starts with a 64×64 pixel RGB input patch which has been local contrast normalized (LCN) [22] to emphasize geometric discontinuities and improve generalization performance [32]. The LCN layer is comprised of a 9×9 pixel local subtractive normalization, followed by a 9×9 local divisive normalization. The input is then processed by three convolution and subsampling layers, which use rectiﬁed linear units (ReLUs) [18] and max-pooling. As expected, we found that internal pooling layers help to a) reduce computational complexity1 and b) improve classiﬁcation tolerance to small input image translations. Unfortunately, pooling also results in a loss of spatial precision. Since the target application for this convnet was ofﬂine (rather than real-time) body-pose detection, and since we found that with sufﬁcient training exemplars, invariance to input translations can be learned, we choose to use only 2 stages of 2 × 2 pooling (where the total image downsampling rate is 4 × 4). Following the three stages of convolution and subsampling, the top-level pooled map is ﬂattened to a vector and processed by three fully connected layers, analogous to those used in deep neural networks. Each of these output stages is composed of a linear matrix-vector multiplication with learned bias, followed by a point-wise non-linearity (ReLU). The output layer has a single logistic unit, representing the probability of the body part being present in that patch. To train the convnet, we performed standard batch stochastic gradient descent. From the training set images, we set aside a validation set to tune the network hyper-parameters, such as number and size of features, learning rate, momentum coefﬁcient, etc. We used Nesterov momentum [43] as well as RMSPROP [46] to accelerate learning and we used L2 regularization and dropout [21] on the input to each of the fully-connected linear stages to reduce over-ﬁtting the restricted-size training set.  1The number of operations required to calculate the output of the the three fully-connected layers is O(cid:0)n2(cid:1)  in the size of the Rn input vectors. Therefore, even small amounts of pooling in earlier stages can drastically reduce training time.  4  RGB (LCN) 64x64px 8192 Full +ReLU reshape 500 100 Full +ReLU Full +logistic 1 5x5 Conv +ReLU +MaxPool 16 feats 32x32px 5x5 Conv +ReLU +MaxPool 32 feats 16x16px 5x5 Conv +ReLU 32 feats 16x16px Figure 3: Spatial Model Connectivity with Spatial Priors  3.2 Enforcing Global Pose Consistency with a Spatial Model  When applied to the validation set, the raw output of the network presented in Section 3.1 produces many false-positives. We believe this is due to two factors: 1) the small image context as input to the convnet (64×64 pixels or approximately 5% of the input image area) does not give the model enough contextual information to perform anatomically consistent joint position inference and 2) the training set size is limited. We therefore use a higher-level spatial model with simple body-pose priors to remove strong outliers from the convnet output. We do not expect this model to improve the performance of poses that are close to the ground truth labels (within 10 pixels for instance), but rather it functions as a post processing step to de-emphasize anatomically impossible poses due to strong outliers. The inter-node connectivity of our simple spatial model is displayed in Figure 3. It consists of a lin- ear chain of kinematic 2D nodes for a single side of the human body. Throughout our experiments we used the left shoulder, elbow and wrist; however we could have used the right side joints without loss of generality (since detection of the right body parts simply requires a horizontal mirror of the input image). For each node in the chain, our convnet detector generates response-map unary dis- tributions pfac (x), psho (x), pelb (x), pwri (x) over the dense pixel positions x, for the face, shoulder, elbow and wrist joints respectively. For the remainder of this section, all distributions are assumed to be a function over the pixel position, and so the x notation will be dropped. The output of our spatial model will produce ﬁltered response maps: ˆpfac, ˆpsho, ˆpelb, and ˆpwri. The body part priors for a pair of joints (a, b), pa|b=(cid:126)0, are calculated by creating a histogram of joint a locations over the training set, given that the adjacent joint b is located at the image center (x = (cid:126)0). The histograms are then smoothed (using a gaussian ﬁlter) and normalized. The learned priors for psho|fac=(cid:126)0, pelb|sho=(cid:126)0, and pwri|elb=(cid:126)0 are shown in Figure 4. Note that due to symmetry, the prior for pelb|wri=(cid:126)0 is a 180° rotation of pwri|elb=(cid:126)0 (as is the case of other adjacent pairs). Rather than assume a simple Gaussian distribution for modeling pairwise interactions of adjacent nodes, as is standard in many parts-based detector implementations, we have found that the these non-parametric spatial priors lead to improved detection performance.  a) psho|fac=(cid:126)0  b) pelb|sho=(cid:126)0  c) pwri|elb=(cid:126)0  Figure 4: Part priors for left body parts  Given the full set of prior conditional distributions and the convnet unary distributions, we can now construct the ﬁltered distribution for each part by using an approach that is analogous to the sum- product belief propagation algorithm. For body part i, with a set of neighbouring nodes U, the ﬁnal distribution is deﬁned as:  5  face shoulder elbow psho|fac pelb|sho wrist pwri|elb psho|elb pelb|wri 501001502002503005010015020025030050100150200250300501001502002503005010015020025030050100150200250300501001502002503005010015020025030050100150200250300501001502002503005010015020025030050100150200250300Figure 5: Global prior for the face: hfac  (cid:16)  λ (cid:89)  u∈U  ˆpi ∝ pi  (cid:17)  pi|u=(cid:126)0∗ pu  (1)  where λ is a mixing parameter and controls the conﬁdence of each joint’s unary distribution towards its ﬁnal ﬁltered distribution (we used λ = 1 for our experiments). The ﬁnal joint distribution is therefore a product of the unary distribution for that joint, as well as the beliefs from neighbouring nodes (as with standard sum-product belief propagation). In log space, the above product for the shoulder joint becomes:  log (ˆpsho) ∝ λ log (psho) + log  psho|fac=(cid:126)0∗ pfac  + log  (cid:16)  (cid:17)  (cid:16) psho|elb=(cid:126)0∗ pelb  (cid:17)  (2)  We also perform an equivalent computation for the elbow and wrist joints. The face joint is treated as a special case. Empirically, we found that incorporating image evidence from the shoulder joint to the ﬁltered face distribution resulted in poor performance. This is likely due to the fact that the convnet does a very good job of localizing the face position, and so incorporating noisy evidence from the shoulder detector actually increases uncertainty. Instead, we use a global position prior for the face, hfac, which is obtained by learning a location histogram over the face positions in the training set images, as shown in Figure 5. In log space, the output distribution for the face is then given by:  log (ˆpfac) ∝ λ log (pfac) + log (hfac)  (3)  Lastly, since the learned neural network convolution features and the spatial priors are not explicitly invariant to scale, we must run the convnet and spatial model on images at multiple scales at test time, and then use the most likely joint location across those scales as the ﬁnal joint location. For datasets containing examples with multiple persons (known a priori), we use non-maximal suppression [29] to ﬁnd multiple local maxima across the ﬁltered response-maps from each scale, and we then take the top n most likely joint candidates from each person in the scene.  4 Results  We evaluated our architecture on the FLIC [38] dataset, which is comprised of 5003 still RGB images taken from an assortment of Hollywood movies. Each frame in the dataset contains at least one person in a frontal pose (facing the camera), and each frame was processed by Amazon Mechanical Turk to obtain ground truth labels for the joint positions of the upper body of a single person. The FLIC dataset is very challenging for state-of-the-art pose estimation methodologies because the poses are unconstrained, body parts are often occluded, and clothing and background are not consistent. We use 3987 training images from the dataset, which we also mirror horizontally to obtain a total of 3987 × 2 = 7974 examples. Since the training images are not at the same scale, we also manually annotate the bounding box for the head in these training set images, and bring them to canonical scale. Further, we crop them to 320×240 such that the center of the shoulder annotations lies at (160 px, 80 px). We do not perform this image normalization at test time. Following the methodology of Felzenszwalb et al. [15], at test time we run our model on images with only one person (351 images  6  50100150200250300501001502005010015020025030050100150200of the 1016 test examples). As stated in Section 3, the model is run on 6 different input image scales and we then use the joint location with highest conﬁdence across those scales as the ﬁnal location. For training the convnet we use Theano [7], which provides a Python-based framework for efﬁcient GPU processing and symbolic differentiation of complex compound functions. To reduce GPU memory usage while training, we cache only 100 mini-batches on the GPU; this allows us to use larger convnet models and keep all training data on a single GPU. As part of this framework, our system has two main threads of execution: 1) a training function which runs on the GPU evaluating the batched-SGD updates, and 2) a data dispatch function which preprocesses the data on the CPU and transfers it on the GPU when thread 1) is ﬁnished processing the 100 mini batches. Training each convnet on an NVIDIA TITAN GPU takes 1.9ms per patch (fprop + bprop) = 41min total. We test on a cpu cluster with 5000 nodes. Testing takes: 0.49sec per image (0.94x scale) = 2.8min total. NMS and spatial model take negligible time. For testing, because of the shared nature of weights for all windows in each image, we convolve the learned ﬁlters with the full image instead of individual windows. This dramatically reduces the time to perform forward propagation on the full test set.  4.1 Evaluation  To evaluate our model on the FLIC dataset we use a measure of accuracy suggested by Sapp et al. [38]: for a given joint precision radius we report the percentage of joints in the test set correct within the radius threshold (where distance is deﬁned as 2D Euclidean distance in pixels). In Fig- ure 4.1 we evaluate this performance measure on the the wrist, elbow and shoulder joints. We also compare our detector to the DPM [15] and MODEC [38] architectures. Note that we use the same subset of 351 images when testing all detectors.  a) Wrist  b) Elbow  c) Shoulder  Figure 6: Comparison of Detector Performance on the Test set  Figure 4.1 shows that our architecture out-performs or is equal to the MODEC and DPM detectors for all three body parts. For the wrist and elbow joints our simple spatial model improves joint localization for approximately 5% of the test set cases (at a 5 pixel threshold), which enables us to outperform all other detectors. However, for the shoulder joint our spatial model actual decreases the joint location accuracy for large thresholds. This is likely due to the poor performance of the convnet on the elbow. As expected, the spatial model cannot improve the joint accuracy of points that are already close to the correct value, however it is never-the-less successful in removing outliers for the wrist and elbow joints. Figure 4.1 is an example where a strong false positive results in an incorrect part location before the spatial model is applied, which is subsequently removed after applying our spatial model.  5 Conclusion  We have shown successfully how to improve the state-of-the-art on one of the most complex com- puter vision tasks: unconstrained human pose estimation. Convnets are impressive low-level feature detectors, which when combined with a global position prior is able to outperform much more com- plex and popular models. We explored many different higher level structural models with the aim to  7  0246810020406080100Precision Threshold (pixels)Percentage of Examples Within Threshold  MODECDPMOurs: No Spatial ModelOurs: With Spatial Model0246810020406080100Precision Threshold (pixels)Percentage of Examples Within Threshold  MODECDPMOurs: No Spatial ModelOurs: With Spatial Model0246810020406080100Precision Threshold (pixels)Percentage of Examples Within Threshold  MODECDPMOurs: No Spatial ModelOurs: With Spatial Modela) RGB and joints  b) distribution before  c) distribution after spatial model.  Figure 7: Impact of Our Spatial Model: Red cross is MODEC, Blue cross is before our Spatial Model, Green cross is after our Spatial Model  Figure 8: Failure cases: The green cross is our new technique’s wrist locator, the red cross is the state-of-the-art CVPR13 MODEC detector [38] on the FLIC database.  further improve the results, but the most generic higher level spatial model achieved the best results. As mentioned in the introduction, this is counter-intuitive to common belief for human kinematic structures, but it mirrors results in other domains. For instance in speech recognition, researchers observed, if the learned transition probabilities (higher level structure) are reset to equal probabili- ties, the recognition performance, now mainly driven by the emission probabilities does not reduce signiﬁcantly [27]. Other domains are discussed in more detail by [26]. We expect to obtain further improvement by enlarging the training set with a new pose-based warp- ing technique that we are currently investigating. Furthermore, we are also currently experimenting with multi-resolution input representations, that take a larger spatial context into account.  6 Acknowledgements  This research was funded in part by the Ofﬁce of Naval Research ONR Award N000141210327 and by a Google award.  References  [1] A. Agarwal, B. Triggs, I. Rhone-Alpes, and F. Montbonnot. Recovering 3D human pose from monocular  images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(1):44–58, 2006. 2  [2] M. Andriluka, S. Roth, and B. Schiele. Pictorial structures revisited: People detection and articulated  pose estimation. In CVPR, 2009. 2  [3] M. Andriluka, S. Roth, and B. Schiele. Monocular 3d pose estimation and tracking by detection.  In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 623–630. IEEE, 2010. 2  8  Figure 9: Success cases: The green cross is our new technique’s wrist locator, the red cross is the state-of-the-art CVPR13 MODEC detector [38] on the FLIC database.  [4] D. Anguelov, P. Srinivasan, D. Koller, S. Thrun, J. Rodgers, and J. Davis. Scape: shape completion and animation of people. In ACM Transactions on Graphics (TOG), volume 24, pages 408–416. ACM, 2005. 3  [5] V. Athitsos, J. Alon, S. Sclaroff, and G. Kollios. Boostmap: A method for efﬁcient approximate similarity  rankings. CVPR, 2004. 2  [6] Y. Bengio, A. C. Courville, and P. Vincent. Representation learning: A review and new perspectives.  Technical report, University of Montreal, 2012. 2  [7] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley, and Y. Bengio. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Conference (SciPy), June 2010. Oral Presentation. 7  [8] M. Black, D. Hirshberg, M. Loper, E. Rachlin, and A. Weiss. Co-registration – simultaneous alignment and modeling of articulated 3D shapes. European patent application EP12187467.1 and US Provisional Application, Oct. 2012. 3  [9] L. Bourdev and J. Malik. Poselets: Body part detectors trained using 3d human pose annotations. In  ICCV, sep 2009. 2  aligned subtitles). CVPR, 2009. 2  [10] P. Buehler, A. Zisserman, and M. Everingham. Learning sign language by watching TV (using weakly  [11] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection.  In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 886–893. IEEE, 2005. 2  [12] A. Erol, G. Bebis, M. Nicolescu, R. D. Boyle, and X. Twombly. Vision-based hand pose estimation: A  review. Computer Vision and Image Understanding, 108(1):52–73, 2007. 2  [13] C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Scene parsing with multiscale feature learning, purity  trees, and optimal covers. In ICML, 2012. 3  [14] A. Farhadi, D. Forsyth, and R. White. Transfer Learning in Sign language. In CVPR, 2007. 2 [15] P. Felzenszwalb, D. McAllester, and D. Ramanan. A discriminatively trained, multiscale, deformable part  model. In CVPR, 2008. 6, 7  [16] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discrimina-  tively trained part-based models. PAMI’10. 2  [17] V. Ferrari, M. Marin-Jimenez, and A. Zisserman. Pose search: Retrieving people using their pose. In  CVPR, 2009. 2  9  [18] X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectiﬁer networks.  In Proceedings of the 14th In- ternational Conference on Artiﬁcial Intelligence and Statistics. JMLR W&CP Volume, volume 15, pages 315–323, 2011. 4  [19] K. Grauman, G. Shakhnarovich, and T. Darrell. Inferring 3d structure with a statistical image-based shape  model. In ICCV, pages 641–648, 2003. 2  [20] N. Hasler, C. Stoll, M. Sunkel, B. Rosenhahn, and H.-P. Seidel. A statistical model of human pose and body shape. In P. Dutr’e and M. Stamminger, editors, Computer Graphics Forum (Proc. Eurographics 2008), volume 2, Munich, Germany, Mar. 2009. 3  [21] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov.  Improving neural  networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012. 4  [22] K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun. What is the best multi-stage architecture for object recognition? In Computer Vision, 2009 IEEE 12th International Conference on, pages 2146–2153, Sept 2009. 4  [23] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deep convolutional neural net-  works. In Advances in Neural Information Processing Systems 25, pages 1106–1114, 2012. 2  [24] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.  Proc. IEEE, 86(11):2278–2324, 1998. 2, 3  [25] D. G. Lowe. Object recognition from local scale-invariant features.  In Computer vision, 1999. The proceedings of the seventh IEEE international conference on, volume 2, pages 1150–1157. Ieee, 1999. 2 [26] A. Lucchi, Y. Li, X. Boix, K. Smith, and P. Fua. Are spatial and global constraints really necessary for segmentation? In Computer Vision (ICCV), 2011 IEEE International Conference on, pages 9–16. IEEE, 2011. 1, 8  [27] N. Morgan. personal communication. 8 [28] G. Mori and J. Malik. Estimating human body conﬁgurations using shape context matching. ECCV, 2002.  2  [29] A. Neubeck and L. Van Gool. Efﬁcient non-maximum suppression. In Proceedings of the 18th Interna- tional Conference on Pattern Recognition - Volume 03, ICPR ’06, pages 850–855, Washington, DC, USA, 2006. IEEE Computer Society. 6  [30] S. J. Nowlan and J. C. Platt. A convolutional neural network hand tracker. Advances in Neural Information  Processing Systems, pages 901–908, 1995. 2, 3  [31] M. Osadchy, Y. L. Cun, and M. L. Miller. Synergistic face detection and pose estimation with energy-  based models. The Journal of Machine Learning Research, 8:1197–1215, 2007. 3  [32] N. Pinto, D. D. Cox, and J. J. DiCarlo. Why is real-world visual object recognition hard? PLoS compu-  tational biology, 4(1):e27, 2008. 4  [33] L. Pishchulin, A. Jain, M. Andriluka, T. Thormaehlen, and B. Schiele. Articulated people detection and  pose estimation: Reshaping the future. In CVPR’12. 2  [34] L. Pishchulin, A. Jain, C. Wojek, T. Thormaehlen, and B. Schiele. In good shape: Robust people detection  based on appearance and shape. In BMVC’11. 2  [35] R. Poppe. Vision-based human motion analysis: An overview. Computer Vision and Image Understand-  ing, 108(1-2):4–18, 2007. 2  [36] D. Ramanan, D. Forsyth, and A. Zisserman. Strike a pose: Tracking people by ﬁnding stylized poses. In  CVPR, 2005. 2  [37] B. Sapp, C. Jordan, and B.Taskar. Adaptive pose priors for pictorial structures. In CVPR, 2010. 2 [38] B. Sapp and B. Taskar. Multimodal decomposable models for human pose estimation. In CVPR’13. 1, 3,  6, 7, 8, 9  [39] G. Shakhnarovich, P. Viola, and T. Darrell. Fast pose estimation with parameter-sensitive hashing. In  ICCV, pages 750–759, 2003. 2  [40] J. Shotton, T. Sharp, A. Kipman, A. Fitzgibbon, M. Finocchio, A. Blake, M. Cook, and R. Moore. Real- time human pose recognition in parts from single depth images. Communications of the ACM, 56(1):116– 124, 2013. 3  [41] L. Sigal, A. Balan, and B. M. J. HumanEva: Synchronized video and motion capture dataset and baseline  algorithm for evaluation of articulated human motion. IJCV, 87(1/2):4–27, 2010. 3  [42] C. Stoll, N. Hasler, J. Gall, H. Seidel, and C. Theobalt. Fast articulated motion tracking using a sums of gaussians body model. In Computer Vision (ICCV), 2011 IEEE International Conference on, pages 951–958. IEEE, 2011. 3  [43] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and momentum in  deep learning. 4  [44] G. Taylor, R. Fergus, I. Spiro, G. Williams, and C. Bregler. Pose-sensitive embedding by nonlinear NCA regression. In Advances in Neural Information Processing Systems 23 (NIPS), pages 2280–2288, 2010. 3 [45] G. Taylor, L. Sigal, D. Fleet, and G. Hinton. Dynamical binary latent variable models for 3d human pose tracking. In Proc. of the 23rd IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), 2010. 3  10  [46] T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent  magnitude. COURSERA: Neural Networks for Machine Learning, 2012. 4  [47] S. C. Turaga, J. F. Murray, V. Jain, F. Roth, M. Helmstaedter, K. Briggman, W. Denk, and H. S. Seung. Convolutional networks can learn to generate afﬁnity graphs for image segmentation. Neural Computa- tion, 22:511–538, 2010. 3  [48] R. Y. Wang and J. Popovi´c. Real-time hand-tracking with a color glove. In ACM Transactions on Graphics  (TOG), volume 28, page 63. ACM, 2009. 2  [49] C. Wren, A. Azarbayejani, T. Darrell, and A. Pentland. Pﬁnder: Real-time tracking of the human body.  IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(7):780–785, 1997. 2  [50] Y. Yang and D. Ramanan. Articulated pose estimation with ﬂexible mixtures-of-parts. In Computer Vision  and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1385–1392. IEEE, 2011. 2  [51] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional neural networks. arXiv preprint  arXiv:1311.2901, 2013. 2  [52] S. Zufﬁ, J. Romero, C. Schmid, and M. J. Black. Estimating human pose with ﬂowing puppets. 3  11  ","This paper introduces a new architecture for human pose estimation using amulti- layer convolutional network architecture and a modified learningtechnique that learns low-level features and higher-level weak spatial models.Unconstrained human pose estimation is one of the hardest problems in computervision, and our new architecture and learning schema shows significantimprovement over the current state-of-the-art results. The main contribution ofthis paper is showing, for the first time, that a specific variation of deeplearning is able to outperform all existing traditional architectures on thistask. The paper also discusses several lessons learned while researchingalternatives, most notably, that it is possible to learn strong low-levelfeature detectors on features that might even just cover a few pixels in theimage. Higher-level spatial models improve somewhat the overall result, but toa much lesser extent then expected. Many researchers previously argued that thekinematic structure and top-down information is crucial for this domain, butwith our purely bottom up, and weak spatial model, we could improve other morecomplicated architectures that currently produce the best results. This mirrorswhat many other researchers, like those in the speech recognition, objectrecognition, and other domains have experienced."
1312.6197,2014,An empirical analysis of dropout in piecewise linear networks  ,"['David Warde-Farley', 'Ian Goodfellow', 'Aaron Courville', 'Yoshua Bengio']",https://arxiv.org/pdf/1312.6197.pdf,"4 1 0 2     n a J    2      ] L M  . t a t s [      2 v 7 9 1 6  .  2 1 3 1 : v i X r a  An empirical analysis of dropout  in piecewise linear networks  David Warde-Farley, Ian J. Goodfellow, Aaron Courville, Yoshua Bengio  D´epartement d’informatique et de recherche op´erationnelle  Universit´e de Montr´eal Montr´eal, QC H3C 3J7  {wardefar,goodfeli}@iro.umontreal.ca,  {aaron.courville,yoshua.bengio}@umontreal.ca  Abstract  The recently introduced dropout training criterion for neural networks has been the subject of much attention due to its simplicity and remarkable effectiveness as a regularizer, as well as its interpretation as a training procedure for an exponen- tially large ensemble of networks that share parameters. In this work we empir- ically investigate several questions related to the efﬁcacy of dropout, speciﬁcally as it concerns networks employing the popular rectiﬁed linear activation function. We investigate the quality of the test time weight-scaling inference procedure by evaluating the geometric average exactly in small models, as well as compare the performance of the geometric mean to the arithmetic mean more commonly employed by ensemble techniques. We explore the effect of tied weights on the ensemble interpretation by training ensembles of masked networks without tied weights. Finally, we investigate an alternative criterion based on a biased estima- tor of the maximum likelihood ensemble gradient.  1  Introduction  Dropout (Hinton et al., 2012) has recently garnered much attention as a novel regularization strategy for neural networks involving the use of structured masking noise during stochastic gradient-based optimization. Dropout training can be viewed as a form of ensemble learning similar to bagging (Breiman, 1994) on an ensemble of size exponential in the number of hidden units and input features, where all members of the ensemble share subsets of their parameters. Combining the predictions of this enormous ensemble would ordinarily be prohibitively expensive, but a scaling of the weights admits an approximate computation of the geometric mean of the ensemble predictions. Dropout has been a crucial ingredient in the winning solution to several high-proﬁle competitions, notably in visual object recognition (Krizhevsky et al., 2012a) as well as the Merck Molecular Activity Challenge and the Adzuna Job Salary Prediction competition. It has also inspired work on activation function design (Goodfellow et al., 2013a) as well as extensions to the basic dropout tech- nique (Wan et al., 2013; Wang and Manning, 2013) and similar fast approximate model averaging methods (Zeiler and Fergus, 2013). Several authors have recently investigated the mechanism by which dropout achieves its regulariza- tion effect in linear models (Baldi and Sadowski, 2013; Wang and Manning, 2013; Wager et al., 2013), as well as linear and sigmoidal hidden units (Baldi and Sadowski, 2013). However, many of the recent empirical successes of dropout, and feed forward neural networks more generally, have utilised piecewise linear activation functions (Jarrett et al., 2009; Glorot et al., 2011; Goodfellow et al., 2013a; Zeiler et al., 2013). In this work, we empirically study dropout in rectiﬁed linear networks, employing the recently popular hidden unit activation function f (x) = max(0, x).  1  We begin by expanding upon previous work which investigated the quality of dropout’s approx- imate ensemble prediction by comparing against Monte Carlo estimates of the correct geometric average (Srivastava, 2013; Goodfellow et al., 2013a). Here, we compare against the true average, in networks of size small enough that the exact computation is tractable. We ﬁnd, by exhaustive enumeration of all sub-networks in these small cases, that the weight scaling approximation is a remarkably and somewhat surprisingly accurate surrogate for the true geometric mean. Next, we consider the importance of the geometric mean itself. Traditionally, bagged ensembles produce an averaged prediction via the arithmetic mean, but the weight scaling trick employed with dropout provides an efﬁcient approximation only for the geometric mean. While, as noted by (Baldi and Sadowski, 2013), the difference between the two can be bounded (Cartwright and Field, 1978), it is not immediately obvious what effect this source of error will have on classiﬁcation performance in practice. We therefore investigate this question empirically and conclude that the geometric mean is indeed a suitable replacement for the arithmetic mean in the context of a dropout-trained ensemble. The questions raised thus far pertain primarily to the approximate model averaging performed at test time, but dropout training also raises some important questions. At each update, the dropout learning rule follows the same gradient that true bagging training would follow. However, in the case of traditional bagging, all members of the ensemble would have independent parameters. In the case of dropout training, all of the models share subsets of their parameters. It is unclear how much this coordination serves to regularize the eventual ensemble. It is also not clear whether the most important effect is that dropout performs model averaging, or that dropout encourages each individual unit to work well in a variety of contexts. To investigate this question, we train a set of independent models on resamplings (with replace- ment) of the training data, as in traditional bagging. Each ensemble member is trained with a single randomly sampled dropout mask ﬁxed throughout all steps of training. We combine these indepen- dently trained networks into ensembles of varying size, and compare the ensembles’ performance with that of a single network of identical size, trained instead with dropout. We ﬁnd evidence to support the claim that the weight sharing taking place in the context of dropout (between members of the implicit ensemble) plays an important role in further regularizing the ensemble. Finally, we investigate an alternative criterion for training the exponentially large shared-parameter ensemble invoked by dropout. Rather than performing stochastic gradient descent on a randomly selected sub-network in a manner similar to bagging, we consider a biased estimator of the gra- dient of the geometrically averaged ensemble log likelihood (i.e. the gradient of the model being approximately evaluated at test-time), with the particular estimator bearing a resemblance to boost- ing (Schapire, 1990). We ﬁnd that this new criterion, employing masking noise with the exact same distribution as is employed by dropout, yields no discernible robustness gains over networks trained with ordinary stochastic gradient descent.  2 Review of dropout  Dropout is an ensemble learning and prediction technique that can be applied to deterministic feed- forward architectures that predict a target y given input vector v. These architectures contain a series of hidden layers h = {h(1), . . . , h(L)}. Dropout trains an ensemble of models consisting of the set of all models that contain a subset of the variables in both v and h. The same set of parameters θ is used to parameterize a family of distributions p(y | v; θ, µ) where µ ∈ M is a binary mask vector determining which variables to include in the model, e.g., for a given µ, each input unit and each hidden unit is set to zero if the corresponding element of µ is 0. On each presentation of a training example, we train a different sub-network by following the gradient of log p(y | v; θ, µ) for a differ- ent randomly sampled µ. For many parameterizations of p (such as most multilayer perceptrons) the instantiation of different sub-networks p(y | v; θ, µ) can be obtained by element-wise multiplication of v and h with the mask µ.  2.1 Dropout as bagging  Dropout training is similar to bagging (Breiman, 1994) and related ensemble methods (Opitz and Maclin, 1999). Bagging is an ensemble learning technique in which a set of models are trained on different subsets of the same dataset. At test time, the predictions of each of the models are averaged  2  together. The ensemble predictions formed by voting in this manner tend to generalize better than the predictions of the individual models. Dropout training differs from bagging in three ways:  1. All of the models share parameters. This means that they are no longer really trained on separate subsets of the dataset, and much of what we know about bagging may not apply. 2. Training stops when the ensemble starts to overﬁt. There is no guarantee that the individual models will be trained to convergence. In fact, typically, the vast majority of sub-networks are never trained for even one gradient step.  3. Because there are too many models to average together explicitly, dropout averages them together with a fast approximation. This approximation is to the geometric mean, rather than the arithmetic mean.  2.2 Approximate model averaging  The functional form of the model becomes important when it comes time for the ensemble to make a prediction by averaging together all the sub-networks’ predictions. When p(y | v; θ) = softmax(vT W + b), the predictive distribution deﬁned by renormalizing the geometric mean of p(y | v; θ, µ) over M is simply given by softmax(vT W/2 + b). This is also true for sigmoid output units, which are special cases of the softmax. This result holds exactly in the case of a single layer softmax model (Hinton et al., 2012) or an MLP with no non-linearity applied to each unit (Good- fellow et al., 2013a). Previous work on dropout applies the same scheme in deep architectures with hidden units that have nonlinearities, such as rectiﬁed linear units, where the W/2 method is only an approximation to the geometric mean. The approximation has been characterized mathemati- cally for linear and sigmoid networks (Baldi and Sadowski, 2013; Wager et al., 2013), but seems to perform especially well in practice for nonlinear networks with piecewise linear activation func- tions (Srivastava, 2013; Goodfellow et al., 2013a).  3 Experimental setup  Our initial investigations employed rectiﬁer networks with 2 hidden layers and 10 hidden units per layer, and a single logistic sigmoid output unit. We applied this class of networks to six binary classiﬁcation problems derived from popular multi-class benchmarks, simpliﬁed in this fashion in order to allow for much simpler architectures to effectively solve the task, as well as a synthetic task of our own design. Speciﬁcally, we chose four binary sub-tasks from the MNIST handwritten digit database (LeCun et al., 1998). Our training sets consisted of all occurrences of two digit classes (1 vs. 7, 1 vs. 8, 0 vs. 8, and 2 vs. 3) within the ﬁrst 50,000 examples of the MNIST training set, with the occurrences from the last 10,000 examples held back as a validation set. We used the corresponding occurrences from the ofﬁcial MNIST test set for evaluating test error. We also chose two binary sub-tasks from the CoverType dataset of the UCI Machine Learning Repository, speciﬁcally discriminating classes 1 and 2 (Spruce-Fir vs. Lodgepole Pine) and classes 3 and 4 (Ponderosa Pine vs. Cottonwood/Willow). This task represents a very different domain than the ﬁrst two datasets, but one where neural network approaches have nonetheless seen success (see e.g. Rifai et al. (2011)).1 The ﬁnal task is a synthetic task in two dimensions: inputs lie in (−1, 1) × (−1, 1) ⊂ R2, and the domain is divided into two regions of equal area: the diamond with corners (1, 0), (0, 1), (−1, 0), (0,−1) and the union of the outlying triangles. In order to keep the synthetic task moderately challenging, the training set size was restricted to 100 points sampled uniformly at random. An additional 500 points were sampled for a validation set and another 1000 as a test set. In order to keep the mask enumeration tractable in the case of the larger input dimension tasks, we chose to apply dropout in the hidden layers only. This has the added beneﬁt of simplifying the 1Unlike Rifai et al. (2011), we train and evaluate on the records of each class from the data split advertised in the original dataset description. This makes the task much more challenging and many methods prone to overﬁtting.  3  ensemble computation: though dropout is typically applied in the input layer, inclusion probabil- ities higher than 0.5 are employed (e.g. 0.8 in Hinton et al. (2012); Krizhevsky et al. (2012b)), making it necessary to unevenly weight the terms in the average. We chose hyperparameters by random search (Bergstra and Bengio, 2012) over learning rate and momentum (initial values and de- crease/increase schedules, respectively), as well as mini-batch size. We performed early stopping on the validation set, terminating when a lower validation error had not been observed for 100 epochs; when training with dropout, the ﬁgure of merit for early stopping was the validation error using the weight-scaled predictions.  4 Weight scaling versus Monte Carlo or exact model averaging  Srivastava (2013); Goodfellow et al. (2013a) previously investigated the ﬁdelity of the weight scaling approximation in the context of rectiﬁer networks and maxout networks, respectively, through the use of a Monte Carlo approximation to the true model average. By concerning ourselves with small networks where exhaustive enumeration is possible, we were able to avoid the effect of additional variance due to the Monte-Carlo average and compute the exact geometric mean over all possible dropout sub-networks. On each of the 7 tasks, we randomly sampled 50 sets of hyperparameters and trained 50 networks with dropout. We then computed, for each point in the test set for each task, the activities of the network corresponding to each of the 220 possible dropout masks. We then geometrically averaged their predictions (by arithmetically averaging all values of the input to the sigmoid output unit) and computed the geometric average prediction for each point in the test set. Finally, we compared the misclassiﬁcation rate using these predictions to that obtained using the approximate, weight-scaled predictions.  Figure 1: Comparison of test error obtained with an exhaustive computation of the geometric mean (on the x-axes) and the relative difference in the test error obtained with the weight-scaling approximation.  4  10-310-210-1100Test error (geometric mean)0.60.40.20.00.20.40.6Rel. diff. using weight-scaling approx.CoverType: 1 vs. 210-310-210-1100Test error (geometric mean)0.60.40.20.00.20.40.6Rel. diff. using weight-scaling approx.CoverType: 3 vs. 410-310-210-1100Test error (geometric mean)0.60.40.20.00.20.40.6Rel. diff. using weight-scaling approx.Diamond Task10-310-210-1100Test error (geometric mean)0.60.40.20.00.20.40.6Rel. diff. using weight-scaling approx.MNIST: 0 vs. 810-310-210-1100Test error (geometric mean)0.60.40.20.00.20.40.6Rel. diff. using weight-scaling approx.MNIST: 1 vs. 710-310-210-1100Test error (geometric mean)0.60.40.20.00.20.40.6Rel. diff. using weight-scaling approx.MNIST: 1 vs. 810-310-210-1100Test error (geometric mean)0.60.40.20.00.20.40.6Rel. diff. using weight-scaling approx.MNIST: 2 vs. 3Figure 2: Comparison of test error obtained with an exhaustive computation of the arithmetic mean (on the x-axes) and the relative difference in the test error obtained with the (exhaustively computed) geometric mean.  The results are shown in Figure 1, where each point represents a different hyperparameter conﬁgu- ration. The overall result is that the approximation yields a network that performs very similarly. In order to make differences visible, we plot on the y-axis the relative difference in test error between the true geometric average network and the weight-scaled approximation for different networks achieving different values of the test error. Additionally, we statistically tested the ﬁdelity of the approximation via the Wilcoxon signed-rank test, a nonparametric paired sample test similar to the paired t-test, applying a Bonferroni correction for multiple hypotheses. At α = 0.01, no signiﬁcant differences were observed for any of the seven tasks.  5 Geometric mean versus arithmetic mean  Though the inexpensive computation of an approximate geometric mean was noted in (Hinton et al., 2012), little has been said of the choice of the geometric mean. Ensemble methods in the literature often employ an arithmetic mean for model averaging. It is thus natural to pose the question as to whether the choice of the geometric mean has an impact on the generalization capabilities of the ensemble. Using the same networks trained in Section 4, we combined the forward-propagated predictions of all 220 models using the arithmetic mean. In Figure 2, we plot the relative difference in test error between the arithmetic mean predictions. We ﬁnd that across all seven tasks, the geometric mean is a reasonable proxy for the arithmetic mean, with relative error rarely exceeding 20% except for the synthetic task. In absolute terms, the discrepancy between the test error achieved by the geometric mean and the arithmetic mean never exceeded 0.75% for any of the tasks.  5  10-310-210-1100Test error (arithmetic mean)0.60.40.20.00.20.40.6Rel. diff. using geometric meanCoverType: 1 vs. 210-310-210-1100Test error (arithmetic mean)0.60.40.20.00.20.40.6Rel. diff. using geometric meanCoverType: 3 vs. 410-310-210-1100Test error (arithmetic mean)0.60.40.20.00.20.40.6Rel. diff. using geometric meanDiamond Task10-310-210-1100Test error (arithmetic mean)0.60.40.20.00.20.40.6Rel. diff. using geometric meanMNIST: 0 vs. 810-310-210-1100Test error (arithmetic mean)0.60.40.20.00.20.40.6Rel. diff. using geometric meanMNIST: 1 vs. 710-310-210-1100Test error (arithmetic mean)0.60.40.20.00.20.40.6Rel. diff. using geometric meanMNIST: 1 vs. 810-310-210-1100Test error (arithmetic mean)0.60.40.20.00.20.40.6Rel. diff. using geometric meanMNIST: 2 vs. 36 Dropout ensembles versus untied weights  We now turn from our investigation of the characteristics of inference in dropout-trained networks to an investigation of the training procedure. For the remainder of the experiments, we trained networks of a more realistic size and capacity on the full multiclass MNIST problem. Once again, we employed two layers of rectiﬁed linear units. In addition to dropout, we utilised norm constraint regularization on the incoming weights to each hidden unit. We again performed random search over hyperparameter values, now including in our search the initial ranges of weights, the number of hidden units in each of two layers, and the maximum weight vector norms of each layer. Dropout training can be viewed as performing bagging on an ensemble that is of size exponential in the number of hidden units, where each member of the ensemble shares parameters with other members of the ensemble. Because each gradient step is taken on a different mini-batch of training data, each sub-network can be seen to be trained on a different resampling of the training set, as in traditional bagging. Furthermore, while each step is taken with respect to the log likelihood of a single ensemble member, the effect of the weight update is applied to all members of the ensemble simultaneously2 We investigate the role of this complex weight-sharing scheme by training an ensemble of independent networks on resamplings of the training data, each with a single dropout mask ﬁxed in place throughout training. We ﬁrst performed a hyperparameter search by sampling 50 hyperparameter conﬁgurations and choosing the network with the lowest validation error. The best of these networks obtains a test error of 1.06%, matching results reported by Srivastava (2013). Using the same hyperparameters, we trained 360 models initialized with different random seeds, on different resamplings (with re- placement) of the training set, as in traditional bagging. Instead of applying dropout during training (and thus applying a different mask at each gradient step), we sampled one dropout mask per model and held it ﬁxed throughout training and at test time. The resulting networks thus have architectures sampled from the same distribution as the sub-networks trained during dropout training, but each network’s parameters are independent of all other networks. We then evaluate test error for ensembles of these networks, combining their predictions (with the dropout mask used during training still ﬁxed in place at test time) via the geometric mean, as is approximately done in the context of dropout. Our results for various sizes of ensemble are shown in Figure 3. Our results suggest that there indeed an effect; combining all 360 independently trained  Figure 3: Average test error on MNIST for varying sizes of untied-weight ensembles. 360 networks were trained to convergence, each with a single randomly sampled dropout mask ﬁxed in place throughout. These networks pre-softmax activations were then averaged to produce predictions for varying sizes of ensembles. For each size n, (cid:98)360/n(cid:99) disjoint subsets were combined in this fashion, and the test error mean and standard deviation over ensembles is shown here.  models yields a test error of 1.66%, far above the even the suboptimally tuned networks trained with dropout. Aside from the size of the independent ensemble being considerably smaller, one potential  2At least, all members of the ensemble that share any parameters with the sub-network just updated. There  certainly exist pairs of ensemble members whose parameter sets are disjoint.  6  020406080100120Number of ensemble members0.01600.01650.01700.01750.01800.01850.0190Test errorEnsembles of fixed-mask, untied networksconfounding factor is that the non-architectural hyperparameters were selected in the context of their performance when using dropout and used as-is to train the networks with untied weights; although each of these was early-stopped independently, it remains unclear how to efﬁciently optimize hy- perparameters for the individual members of a large ensemble so as to facilitate a fairer comparison (indeed, this highlights a general issue with the high cost of training ensembles of neural networks, that dropout conveniently sidesteps).  7 Dropout bagging versus dropout boosting  Other algorithms such as denoising autoencoders (Vincent et al., 2010) are motivated by the idea that models trained with noise are robust to slight transformations of their inputs. Previous work has drawn connections between noise and regularization penalties (Bishop, 1995); similar connections in the case of dropout have recently been noted (Baldi and Sadowski, 2013; Wager et al., 2013). It is natural to question whether dropout can be wholly characterized in terms of learned noise robustness, and whether the model-averaging perspective is necessary or fruitful. In order to investigate this question we propose an algorithm that injects exactly the same noise as dropout. For this test to be effective, we require an algorithm that can successfully minimize training error, and obtain acceptable generalization performance. It needs to perform at least as well as standard maximum likelihood; otherwise all we have done is designed a pathological algorithm that fails to train. We therefore introduce dropout boosting. The objective function for each (sub-network, example) pair in dropout boosting is the likelihood of the data according to the ensemble; however, only the parameters of the current sub-network may be updated for each example. Ordinary dropout performs bagging by maximizing the likelihood of the correct target for the current example under the current sub-network, whereas dropout boosting takes into account the contributions of other sub-networks, in a manner reminiscent of boosting. µ∈M log p(y | v; θ, µ). For dropout boosting, assume The objective function for dropout is each mask µ has a separate set of parameters θµ (though in reality these parameters are tied, as in conventional dropout). The dropout boosting objective function is then given by log pensemble(y | v; θ), where  2|M|(cid:80)  1  ˜p(y | v; θ)  1 Z  ˜p(y(cid:48) | v; θ)  pensemble(y | v; θ) =  (cid:88) ˜p(y | v; θ) = 2|M|(cid:113)  Z =  y(cid:48)  Πµ∈Mp(y | v; θµ).  The boosting learning rule is to select one model and update its parameters given all of the other models. In conventional boosting, these other models have already been trained to convergence. In dropout boosting, the other models actually share parameters with the network being trained at any given step, and initially the other models have not been trained at all. The learning rule is to select a sub-network indexed by µ and follow the ensemble gradient ∇θµ log pensemble(y | v; θ), i.e.  ∇θµ log p(y | v; θµ, µ) +  (cid:88)  y(cid:48)  ∆θµ ∝ 1 2|M|  pensemble(y(cid:48) | v)∇θµ log p(y(cid:48) | v; θµ, µ)   .  Rather than using the boosting-like algorithm, one could obtain a generic Monte-Carlo procedure for maximizing the log likelihood of the ensemble by averaging together the gradient for multiple values of µ, and optionally using a different µ for the term in the left and the term on the right. Empirically, we obtained the best results in the special case of boosting, where the term on the left uses the same µ as the term on the right – that is, both terms of the gradient apply updates only to one member of the ensemble, even though the criterion being optimized is global. Note that the intractable pensemble still appears in the learning rule. To implement the training algo- rithm efﬁciently, we can approximate the ensemble predictions using the weight scaling approxima- tion. This introduces further bias into the estimator, but our ﬁndings in Section 4 suggest that the approximation error is small.  7  Note that dropout boosting employs exactly the same noise as regular dropout uses to perform bag- ging, and thus should perform similarly to conventional dropout if learned noise robustness is the important ingredient. If we instead take the view that this is a large ensemble of complex learners whose likelihood is being jointly optimized, we would expect that employing a criterion more sim- ilar to boosting than bagging would perform more poorly. As boosting maximizes the likelihood of the ensemble, it would perhaps be prone to overﬁtting in this setting, as the ensemble is very large and the learners are not particularly weak.  Figure 4: Comparison of dropout (left) and dropout boosting (right) to stochastic gradient descent with matched hyperparameters.  Starting with the 50 models trained in Section 6, we employed the same hyperparameters to train a matched set of 50 networks with dropout boosting, and another with plain stochastic gradient de- scent. In Figure 4, we plot the relative performance of dropout and dropout boosting compared to a model with the same hyperparameters trained with SGD. While dropout unsurprisingly shows a very consistent edge, dropout boosting performs, on average, little better than stochastic gradient descent. The Wilcoxon signed-rank test similarly failed to ﬁnd a signiﬁcant difference between dropout boosting and SGD (p > 0.7). While several outliers approach very good performance (per- haps owing to the added stochasticity), dropout boosting is, on average, no better and often slightly worse than maximum likelihood training, in stark contrast with dropout’s systematic advantage in generalization performance.  8 Conclusion  We investigated several questions related to the efﬁcacy of dropout, focusing on the speciﬁc case of the popular rectiﬁed linear nonlinearity for hidden units. We showed that the weight-scaling approximation is a remarkably accurate proxy for the usually intractable geometric mean over all possible sub-networks, and that the geometric mean (and thus its weight-scaled surrogate) com- pares favourably to the traditionally popular arithmetic mean in terms of classiﬁcation performance. We demonstrated that weight-sharing between members of the implicit dropout ensemble appears to have a signiﬁcant regularization effect, by comparing to analogously trained ensembles of the same form that did not share parameters. Finally, we demonstrated that simply adding noise, even noise with identical characteristics to the noise applied during dropout training, is not sufﬁcient to obtain the beneﬁts of dropout, by introducing dropout boosting, a training procedure utilising the same masking noise as conventional dropout, which successfully trains networks but loses dropout’s beneﬁts, instead performing roughly as well as ordinary stochastic gradient descent. Our results suggest that dropout is an extremely effective ensemble learning method, paired with a clever approximate inference scheme that is remarkably accurate in the case of rectiﬁed linear networks. Further research is necessary to shed more light on the model averaging interpretation of dropout. Hinton et al. (2012) noted that dropout forces each hidden unit to perform computation that is useful in a wide variety of contexts. Our results with a sizeable ensemble of independent bagged models seem to lend support to this view, though our experiments were limited to ensembles of several hundred networks at most, tiny in comparison with the weight-sharing ensemble invoked by dropout. The relative importance of the astronomically large ensemble versus the learned “mixabil- ity” of hidden units remains an open question. Another interesting direction involves methods that are able to efﬁciently, approximately average over different classes of model that share parameters in some manner, rather than merely averaging over members of the same model class.  8  0.0120.0140.0160.0180.0200.022SGD test error0.20.00.20.40.60.8Relative improvement in test errorRelative improvement: dropout (MNIST)0.0120.0140.0160.0180.0200.022SGD test error0.20.00.20.40.60.8Relative improvement in test errorRelative improvement: dropout boosting (MNIST)Acknowledgments  The authors would like to acknowledge the efforts of the many developers of Theano (Bergstra et al., 2010; Bastien et al., 2012), pylearn2 (Goodfellow et al., 2013b) which were utilised in experiments. We would also like to thank NSERC, Compute Canada, and Calcul Qu´ebec for providing computa- tional resources. Ian Goodfellow is supported by the 2013 Google Fellowship in Deep Learning.  References Baldi, P. and Sadowski, P. J. (2013). Understanding dropout. In Advances in Neural Information Processing  Systems 26, pages 2814–2822.  Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A., Bouchard, N., and Bengio, Y. (2012). Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.  Bergstra, J. and Bengio, Y. (2012). Random search for hyper-parameter optimization. J. Machine Learning  Res., 13, 281–305.  Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-Farley, D., and Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Conference (SciPy). Oral Presentation.  Bishop, C. M. (1995). Training with noise is equivalent to Tikhonov regularization. Neural Computation, 7(1),  108–116.  Breiman, L. (1994). Bagging predictors. Machine Learning, 24(2), 123–140. Cartwright, D. I. and Field, M. J. (1978). A reﬁnement of the arithmetic mean-geometric mean inequality.  Proceedings of the American Mathematical Society, 71(1), pp. 36–38.  Glorot, X., Bordes, A., and Bengio, Y. (2011). Deep sparse rectiﬁer neural networks.  In JMLR W&CP: Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2011).  Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. (2013a). Maxout networks. In  ICML’2013.  Goodfellow, I. J., Warde-Farley, D., Lamblin, P., Dumoulin, V., Mirza, M., Pascanu, R., Bergstra, J., Bastien, F., and Bengio, Y. (2013b). Pylearn2: a machine learning research library. arXiv preprint arXiv:1308.4214. Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinv, R. (2012). Improving neural  networks by preventing co-adaptation of feature detectors. Technical report, arXiv:1207.0580.  Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2009). What is the best multi-stage architecture for object recognition? In Proc. International Conference on Computer Vision (ICCV’09), pages 2146–2153. IEEE.  Krizhevsky, A., Sutskever, I., and Hinton, G. (2012a). ImageNet classiﬁcation with deep convolutional neural  networks. In Advances in Neural Information Processing Systems 25 (NIPS’2012).  Krizhevsky, A., Sutskever, I., and Hinton, G. (2012b). ImageNet classiﬁcation with deep convolutional neural  networks. In NIPS’2012.  LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied to document recog-  nition. Proceedings of the IEEE, 86(11), 2278–2324.  Opitz, D. and Maclin, R. (1999). Popular ensemble methods: An empirical study. Journal of Artiﬁcial Intelli-  gence Research, 11, 169–198.  Rifai, S., Dauphin, Y., Vincent, P., Bengio, Y., and Muller, X. (2011). The manifold tangent classiﬁer.  NIPS’2011. Student paper award.  In  Schapire, R. E. (1990). The strength of weak learnability. Machine Learning, 5(2), 197–227. Srivastava, N. (2013). Improving Neural Networks With Dropout. Master’s thesis, U. Toronto. Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol, P.-A. (2010). Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11, 3371–3408.  Wager, S., Wang, S., and Liang, P. (2013). Dropout training as adaptive regularization. In Advances in Neural  Information Processing Systems 26, pages 351–359.  Wan, L., Zeiler, M., Zhang, S., LeCun, Y., and Fergus, R. (2013). Regularization of neural networks using  dropconnect. In ICML’2013.  Wang, S. and Manning, C. (2013). Fast dropout training. In ICML’2013.  9  Zeiler, M. D. and Fergus, R. (2013). Stochastic pooling for regularization of deep convolutional neural net-  works. Technical Report Arxiv 1301.3557.  Zeiler, M. D., Ranzato, M., Monga, R., Mao, M., Yang, K., Le, Q., Nguyen, P., Senior, A., Vanhoucke, V.,  Dean, J., and Hinton, G. E. (2013). On rectiﬁed linear units for speech processing. In ICASSP 2013.  10  ","The recently introduced dropout training criterion for neural networks hasbeen the subject of much attention due to its simplicity and remarkableeffectiveness as a regularizer, as well as its interpretation as a trainingprocedure for an exponentially large ensemble of networks that shareparameters. In this work we empirically investigate several questions relatedto the efficacy of dropout, specifically as it concerns networks employing thepopular rectified linear activation function. We investigate the quality of thetest time weight-scaling inference procedure by evaluating the geometricaverage exactly in small models, as well as compare the performance of thegeometric mean to the arithmetic mean more commonly employed by ensembletechniques. We explore the effect of tied weights on the ensembleinterpretation by training ensembles of masked networks without tied weights.Finally, we investigate an alternative criterion based on a biased estimator ofthe maximum likelihood ensemble gradient."
1312.6086,2014,The return of AdaBoost.MH: multi-class Hamming trees  ,['Balázs Kégl'],https://arxiv.org/pdf/1312.6086.pdf,"The return of ADABOOST.MH: multi-class Hamming trees  Bal´azs K´egl LAL/LRI, University of Paris-Sud, CNRS, 91898 Orsay, France  BALAZS.KEGL@GMAIL.COM  3 1 0 2    c e D 0 2         ]  G L . s c [      1 v 6 8 0 6  .  2 1 3 1 : v i X r a  Abstract  Within the framework of ADABOOST.MH, we propose to train vector-valued decision trees to optimize the multi-class edge without reducing the multi-class problem to K binary one-against- all classiﬁcations. The key element of the method is a vector-valued decision stump, factorized into an input-independent vector of length K and label-independent scalar classiﬁer. At inner tree nodes, the label-dependent vector is discarded and the binary classiﬁer can be used for partition- ing the input space into two regions. The algo- rithm retains the conceptual elegance, power, and computational efﬁciency of binary ADABOOST. In experiments it is on par with support vector machines and with the best existing multi-class boosting algorithm AOSOLOGITBOOST, and it is signiﬁcantly better than other known imple- mentations of ADABOOST.MH.  1. Introduction ADABOOST (Freund & Schapire, 1997) is one of the most inﬂuential supervised learning algorithms of the last twenty years. It has inspired learning theoretical developments and also provided a simple and easily interpretable mod- eling tool that proved to be successful in many applica- tions (Caruana & Niculescu-Mizil, 2006). It is especially the method of choice when any-time solutions are required on large data sets, so it has been one of the most successful techniques in recent large-scale classiﬁcation and ranking challenges (Dror et al., 2009; Chapelle et al., 2011). The and of Freund Schapire (Freund & Schapire, 1997), besides deﬁning binary ADABOOST, also described two multi-class ex- tensions, ADABOOST.M1 and ADABOOST.M2. Both required a quite strong performance from the base learners, partly defeating the purpose of boosting, and saw limited practical success. The breakthrough came with Schapire  original ADABOOST  paper  and Singer’s seminal paper (Schapire & Singer, 1999), which proposed, among other interesting extensions, ADABOOST.MH. The main idea of the this approach is to use vector-valued base classiﬁers to build a multi-class discriminant function of K outputs (for K-class classi- ﬁcation). The weight vector, which plays a crucial role is replaced by a weight matrix in binary ADABOOST, over instances and labels. The simplest implementation of the concept is to use K independent one-against-all classiﬁers in which base classiﬁers are only loosely connected through the common normalization of the weight matrix. This setup works well with single decision stumps, but in most of the practical problems, boosting stumps is suboptimal compared to boosting more complex base classiﬁers such as trees. Technically, it is possible to build K one-against-all binary decision trees in each iteration, but this approach, for one reason or another, has not produced state-of-the-art results. As a consequence, several recent papers concentrate on replacing the boosting objective and the engine that optimizes this objective (Li, 2009a;b; Zhu et al., 2009; Sun et al., 2012; Mukherjee & Schapire, 2013). The main misconception that comes back in several pa- pers is that ADABOOST.MH has to train K parallel one- against-all classiﬁers in each iteration. It turns out that the original setup is more general. For example, staying within the classical ADABOOST.MH framework, K´egl & Busa-Fekete (2009) trained products of simple classiﬁers and obtained state-of-the-art results on several data sets. In this paper, we describe multi-class Hamming trees, an- other base learner that optimizes the multi-class edge with- out reducing the problem to K binary classiﬁcations. The key idea is to factorize general vector-valued classiﬁers into an input-independent vector of length K, and label- independent scalar classiﬁer. It turns out that optimizing such base classiﬁers using decision stumps as the scalar component is almost as simple as optimizing simple binary stumps on binary data. The technique can be intuitively un- derstood as optimizing a binary cut and an output code at the same time. The main consequence of the setup is that now it is easy to build trees of these classiﬁers by simply discarding the label-dependent vector and using the binary classiﬁer for partitioning the input space into two regions.  Multi-class Hamming trees  The algorithm retains the conceptual elegance, power, and computational efﬁciency of binary ADABOOST. Algorith- mically it cannot fail (the edge is always positive) and in practice it almost never overﬁts. Inheriting the ﬂexibil- ity of ADABOOST.MH, it can be applied directly (with- out any modiﬁcation) to multi-label and multi-task classi- ﬁcation. In experiments (carried out using an open source package of Benbouzid et al. (2012) for reproducibility) we found that ADABOOST.MH with Hamming trees performs on par with the best existing multiclass boosting algorithm AOSOLOGITBOOST (Sun et al., 2012) and with support vector machines (SVMs; Boser et al. 1992). It is also sig- niﬁcantly better than other known implementations of AD- ABOOST.MH (Zhu et al., 2009; Mukherjee & Schapire, 2013). The paper is organized as follows. In Section 2 we give the formal multi-class setup used in the paper and AD- ABOOST.MH, and show how to train factorized base learn- ers in general. The algorithm to build Hamming trees is described in Section 3. Experiments are described in Sec- tion 4 before a brief conclusion in Section 5.  2. ADABOOST.MH In this section we ﬁrst introduce the general multi- class learning setup (Section 2.1), then we describe AD- ABOOST.MH in detail (Section 2.2). We proceed by ex- plaining the general requirements for base learning in AD- ABOOST.MH, and introduce the notion of the factorized vector-valued base learner (Section 2.3). Finally, we ex- plain the general objective for factorized base learners and the algorithmic setup to optimize that objective. (Sec- tion 2.4).  2.1. The multi-class setup: single-label and  multi-label/multi-task  training data be D = (cid:8)(x1, y1), . . . , (xn, yn)(cid:9), where  For the formal description of ADABOOST.MH, let the xi ∈ Rd are observation vectors, and yi ∈ {±1}K are la- bel vectors. Sometimes we will use the notion of an n × d observation matrix of X = (x1, . . . , xn) and an n × K label matrix Y = (y1, . . . , yn) instead of the set of pairs D.1 In multi-class classiﬁcation, the single label (cid:96)(x) of the observation x comes from a ﬁnite set. Without loss of gen- erality, we will suppose that (cid:96) ∈ L = {1, . . . , K}. The la- bel vector y is a one-hot representation of the correct class: the (cid:96)(x)th element of y will be 1 and all the other elements will be −1. Besides expressing faithfully the architecture of a multi-class neural network or multi-class ADABOOST,  1We will use bold capitals X for matrices, bold small letters xi and x,.j for its row and column vectors, respectively, and italic for its elements xi,j.  I{(cid:96)(xi) (cid:54)= (cid:96)f (xi)}.3  (1)  this representation has the advantage to be generalizable to multi-label or multi-task learning when an observation x can belong to several classes. To avoid confusion, from now on we will call y and (cid:96) the label and the label index of x, respectively. For emphasizing the distinction between multi-class and multi-label classiﬁcation, we will use the term single-label for the classical multi-class setup, and re- serve multi-class to situations when we talk about the three setups in general. The goal of learning is to infer a vector-valued multi-class discriminant function f : X → RK.2 The single-label output of the algorithm is then (cid:96)f (x) = arg max(cid:96) f(cid:96)(x). The classical measure of the performance of the multi- class discriminant function f is the single-label one-loss  LI(cid:0)f , (x, (cid:96))) = I{(cid:96) (cid:54)= (cid:96)f (xi)}, which deﬁnes the single-  label training error  n(cid:88)  1 n  (cid:98)RI(f ) = (cid:0)f , (x, y), w(cid:1) = (cid:80)K  i=1  1 n  vector over labels. The corresponding empirical risk (train- ing error) is  Another, perhaps more comprehensive, way to measure the performance of f is by computing the weighted Hamming loss LH  (cid:9) (cid:96)=1 w(cid:96)I(cid:8)sign(cid:0)f(cid:96)(x)(cid:1) (cid:54)= y(cid:96) (cid:3) is an RK-valued “user-deﬁned” weight where w = (cid:2)w(cid:96) K(cid:88) n(cid:88) (cid:9), (2) (cid:98)RH(f , W) = (cid:3) is an n × k weight matrix over data where W = (cid:2)wi,(cid:96)  wi,(cid:96)I(cid:8)sign(cid:0)f(cid:96)(xi)(cid:1) (cid:54)= yi,(cid:96)  points and labels. In the multi-label/multi-task setup, when, for example, it is equally important to predict that a song is “folk” as pre- dicting that it is sung by a woman, the Hamming loss with uniform weights w(cid:96) = 1/K, (cid:96) = 1, . . . , K is a natural measure of performance: it represents the uniform error rate of missing any class sign y(cid:96) of a given observation x. In single-label classiﬁcation, w is usually set asymmetri- cally to  (cid:96)=1  i=1  w(cid:96) =  2 2(K−1)  1  if (cid:96) = (cid:96)(x) (i.e., if y(cid:96) = 1), otherwise (i.e., if y(cid:96) = −1).  (3)  The idea behind this scheme is that it will create K well- balanced one-against-all binary classiﬁcation problems: if  2Instead of the original notation of (Schapire & Singer, 1999) where both x and (cid:96) are inputs of a function f (x, (cid:96)) out- putting a single real-valued score, we use the notation f (x) =  (f1(x), . . . , fK (x)(cid:1) since we feel it expresses better that x is (in  general) continuous and (cid:96) is a discrete index.  3The indicator function I{A} is 1 if its argument A is true and  0 otherwise.  (cid:40) 1  Multi-class Hamming trees  we start with a balanced single-label multi-class problem, that is, if each of the K classes have n/K examples in D, then for each class (cid:96), the sum of the weights of the positive examples in the column w·,(cid:96) of the weight matrix W will be equal to the sum of the weights of the negative examples. Note that both schemes boil down to the classical uniform weighting in binary classiﬁcation.  2.2. ADABOOST.MH  exponential margin-based error  (cid:0)f (T ), W(cid:1) =  The goal of the ADABOOST.MH algorithm (Schapire & Singer 1999; Figure 1) is to return a vector-valued discrim- inant function f (T ) : Rd → RK with a small Hamming  loss (cid:98)RH(f , W) (2) by minimizing the weighted multi-class wi,(cid:96) exp(cid:0)−f (T ) (cid:1). (cid:98)REXP (cid:0)f (T ), W(cid:1) (2). ADABOOST.MH builds the ming loss (cid:98)RH ﬁnal discriminant function f (T )(x) = (cid:80)T base learner algorithm BASE(cid:0)X, Y, W(t)(cid:1) in each itera-  (4) Since exp(−ρ) ≥ I{ρ < 0}, (4) upper bounds the Ham-  t=1 h(t)(x) as a sum of T base classiﬁers h(t) : X → RK returned by a  n(cid:88)  K(cid:88)  (xi)yi,(cid:96)  1 n  (cid:96)=1  i=1  (cid:96)  tion t.  ADABOOST.MH(X, Y, W, BASE(·,·,·), T )  1 W(1) ← 1 2 3 4 5  n W for t ← 1 to T  (cid:0)α(t), v(t), ϕ(t)(·)(cid:1) ← BASE(cid:0)X, Y, W(t)(cid:1)  h(t)(·) ← α(t)v(t)ϕ(t)(·) for i ← 1 to n for (cid:96) ← 1 to K e−h(t)  i,(cid:96) ← w(t) w(t+1)  i,(cid:96)  (cid:96) (xi)yi,(cid:96)  K(cid:88) Z(cid:0)h(t), W(t)(cid:1)  i(cid:48),(cid:96)(cid:48)e−h(t) (cid:123)(cid:122) w(t)  n(cid:88) (cid:124) t=1 h(t)(·)  (cid:96)(cid:48)=1  i(cid:48)=1  (cid:96)(cid:48) (xi(cid:48) )yi(cid:48) ,(cid:96)(cid:48)  (cid:125)  return f (T )(·) =(cid:80)T  6  7  Figure 1. The pseudocode of the ADABOOST.MH algorithm with factorized base classiﬁers (6). X is the n × d observation matrix, Y is the n× K label matrix, W is the user-deﬁned weight matrix used in the deﬁnition of the weighted Hamming error (2) and the weighted exponential margin-based error (4), BASE(·,·,·) is the base learner algorithm, and T is the number of iterations. α(t) is the base coefﬁcient, v(t) is the vote vector, ϕ(t)(·) is the scalar base (weak) classiﬁer, h(t)(·) is the vector-valued base classiﬁer, and f (T )(·) is the ﬁnal (strong) discriminant function.  2.3. Base learning for ADABOOST.MH  The goal of multi-class base learning is to minimize the base objective  Z(cid:0)h, W(t)(cid:1) =  n(cid:88)  K(cid:88)  i=1  (cid:96)=1  Z (t) = min h  i,(cid:96) e−h(cid:96)(xi)yi,(cid:96). (5) w(t)  error (cid:98)RI(f (T )) (1) is upper bounded by(cid:81)T holds, (cid:98)RI(f ) becomes zero in T ∼ O(log n) iterations.  It is easy to show (Schapire & Singer, 1999) that i) the one- t=1 Z (t), and so ii) if the standard weak-learning condition Z (t) ≤ 1 − δ  In general, any vector-valued multi-class learning algo- rithm can be used to minimize (5). Although this goal is clearly deﬁned in (Schapire & Singer, 1999), efﬁcient base learning algorithms have never been described in de- tail. In most recent papers (Zhu et al., 2009; Mukherjee & Schapire, 2013) where ADABOOST.MH is used as base- line, the base learner is a classical single-label decision tree which has to be grown rather large to satisfy the weak- learning condition, and, when boosted, yields suboptimal results (Section 4). The reason why methods for learning multi-class {±1}K-valued base classiﬁers had not been de- veloped before is because they have to be boosted: since they do not select a single label, they cannot be used as stand-alone multi-class classiﬁers. Although it is not described in detail, it seems that the base classiﬁer used in the original paper of Schapire & Singer (1999) is a vector of K independent decision stumps  h(x) = (cid:0)h1(x), . . . , hK(x)(cid:1). These stumps cannot be  used as node classiﬁers to grow decision trees since they do not deﬁne a single cut that depends only on the input (see Section 3 for a more detailed discussion). To over- come this problem, we propose base learning algorithms that factorize h(x) into  h(x) = αvϕ(x),  (6)  where α ∈ R+ is a positive real valued base coefﬁcient, v is an input-independent vote vector of length K, and ϕ(x) is a label-independent scalar classiﬁer. In discrete ADABOOST.MH, both components are binary, that is, v ∈ {±1}K and ϕ(x) : Rd → {±1}. The setup can be ex- tended to real-valued classiﬁers ϕ(x) : Rd → R, also known as conﬁdence-rated classiﬁers, and it is also easy to make the vote vector v real-valued (in which case, with- out the loss of generality, α would be set to 1). Both vari- ants are known under the name of real ADABOOST.MH. Although there might be slight differences in the practical performance of real and discrete ADABOOST.MH, here we decided to stick to the discrete case for the sake of simplic- ity.  Multi-class Hamming trees  2.4. Casting the votes  To start, we show how to set α and v in general if the scalar base classiﬁer ϕ is given. The intuitive semantics of (6) is the following. The binary classiﬁer ϕ(x) cuts the in- put space into a positive and a negative region. In binary classiﬁcation this is the end of the story: we need ϕ(x) to be well-correlated with the binary class labels y. In multi- class classiﬁcation it is possible that ϕ(x) correlates with some of the class labels y(cid:96) and anti-correlates with oth- ers. This free choice is expressed by the binary “votes” v(cid:96) ∈ {±1}. We say that ϕ(x) votes for class (cid:96) if v(cid:96) = +1 and it votes against class (cid:96) if v(cid:96) = −1. As in binary clas- siﬁcation, α expresses the overall quality of the classiﬁer vϕ(x): α is monotonically decreasing with respect to the weighted error of vϕ(x). The advantage of the setup is that, given the binary classi- ﬁer ϕ(x), the optimal vote vector v and the coefﬁcient α can be set in an efﬁcient way. To see this, ﬁrst let us deﬁne the weighted per-class error rate  µ(cid:96)− =  wi,(cid:96)I{ϕ(xi) (cid:54)= yi,(cid:96)},  (7)  and the weighted per-class correct classiﬁcation rate  wi,(cid:96)I{ϕ(xi) = yi,(cid:96)}  i=1  µ(cid:96)+ =  for each class (cid:96) = 1, . . . , K. With this notation, Z(cid:0)h, W(cid:1) (cid:0)µ(cid:96)+ − µ(cid:96)−(cid:1).  simpliﬁes to (see Appendix A)  − eα − e−α  eα + e−α  K(cid:88)  Z(h, W) =  v(cid:96)  (8)  2  2  (cid:96)=1  (9)  n(cid:88)  i=1  n(cid:88)  The quantity  γ(cid:96) = v(cid:96)  (cid:0)µ(cid:96)+ − µ(cid:96)−(cid:1) =  n(cid:88)  i=1  is called the classwise edge of h(x). The full multi-class edge of the classiﬁer is then  K(cid:88)  K(cid:88)  (cid:0)µ(cid:96)+ − µ(cid:96)−(cid:1)  γ = γ(v, ϕ, W) =  γ(cid:96) =  v(cid:96)  n(cid:88)  K(cid:88)  =  (cid:96)=1  (cid:96)=1  (11)  wi,(cid:96)v(cid:96)ϕ(xi)yi,(cid:96).  i=1  (cid:96)=1  With this notation, the classical (Freund & Schapire, 1997) binary coefﬁcient α is recovered: it is easy to see that (9) is minimized when  α =  1 2  log  1 + γ 1 − γ  .  (12)  With this optimal coefﬁcient, (9) becomes Z(h, W) =  (cid:112)1 − γ2, so Z(h, W) is minimized when γ is maximized. agrees with the sign of(cid:0)µ(cid:96)+ − µ(cid:96)−(cid:1), that is,  From (11) it then follows that Z(h, W) is minimized if v(cid:96)  (cid:40)  v(cid:96) =  1 −1  if µ(cid:96)+ > µ(cid:96)− otherwise  (13)  for all classes (cid:96) = 1, . . . , K. The setup of factorized base classiﬁcation (6) has an- other important consequence: the preservation of the weak- learning condition. Indeed, if ϕ(x) is slightly better then a coin toss, γ will be positive. Another way to look at it is to say that if a (ϕ, v) combination has a negative edge γ < 0, then the edge of its complement (either (−ϕ, v) or (ϕ,−v)) will be −γ > 0. To understand the signif- icance of this, consider a classical single-label base clas- siﬁer h : X → L = {1, . . . , K}, required by AD- ABOOST.M1. Now if h(x) is slightly better than a coin toss, all one can hope for is an error rate slightly lower than K−1 K (which is equivalent to an edge slightly higher than 2−K K ). To achieve the error of 1 2 (zero edge), required for continuing boosting, one has to come up with a base learner which is signiﬁcantly better than a coin toss. There is a long line of research on output codes similar in spirit to our setup. The boosting engine in these works is usually slightly different from ADABOOST.MH since it attempts to optimize the multi-class hinge loss, but the fac- torization of the multi-class base classiﬁer is similar to (6). Formally, the vote vector v in this framework is one column in an output code matrix. In the simplest setup this matrix is ﬁxed beforehand by maximizing the error correcting ca- pacity of the matrix (Dietterich & Bakiri, 1995; Allwein et al., 2001). A slightly better solution (Schapire, 1997; Guruswami & Sahai, 1999; Sun et al., 2005) is to wait until the given iteration to pick v by maximizing  n(cid:88)  K(cid:88)  wi,(cid:96)I(cid:8)v(cid:96) (cid:54)= v(cid:96)(xi)  (cid:9),  v  i=1  (cid:96)=1  and then to choose the optimal binary classiﬁer ϕ with this ﬁxed vote (or code) vector v∗ (although in practice it seems to be better to ﬁx v to a random binary vector; Sun et al. 2005). The state of the art in this line of research is to iterate between optimizing ϕ with a ﬁxed v and then picking the best v with a ﬁxed ϕ (Li, 2006; K´egl & Busa-Fekete, 2009; Gao & Koller, 2011). It turns out that if ϕ is a decision stump, exhaustive search for both the best binary cut (threshold) and the best vote vector can be carried out using one single sweep in Θ(nK) time. The algorithm is a simple extension of the classi- cal binary decision stump learner; for the sake of com- pleteness, we provide the pseudocode in Appendix B. The  wi,(cid:96)v(cid:96)ϕ(xi)yi,(cid:96)  (10)  v∗ = arg max  Multi-class Hamming trees  recursively as  computational efﬁciency of this learning algorithm com- bined with the factorized form (6) of the classiﬁer allows us to build multiclass Hamming trees in an efﬁcient man- ner, circumventing the problem of global maximization of the edge with respect to ϕ and v.  3. Hamming trees Classiﬁcation trees (Quinlan, 1986) have been widely used for multivariate classiﬁcation since the 80s. They are especially efﬁcient when used as base learners in AD- ABOOST (Caruana & Niculescu-Mizil, 2006; Quinlan, 1996). Their main disadvantage is their variance with re- spect to the training data, but when averaged over T dif- ferent runs, this problem largely disappears. The most commonly used tree learner is C4.5 of Quinlan (1993). Whereas this tree implementation is a perfect choice for binary ADABOOST, it is suboptimal for ADABOOST.MH since it outputs a single-label classiﬁer with no guarantee of a positive multi-class edge (11). Although this problem can be solved in practice by building large trees, it seems that using these large single-class trees is suboptimal (Sec- tion 4). The main technical difﬁculty of building trees out of generic {±1}K-valued multi-class classiﬁers h(x) is that they do not necessarily implement a binary cut x (cid:55)→ {±1}, and partitioning the data into all the possibly 2K children at a tree node leads to rapid overﬁtting. Factorizing the multi- class classiﬁer h(x) into an input-independent vote vector v and a label-independent binary classiﬁer ϕ(x) as in (6) solves this problem. Base classiﬁers are trained as usual at each new tree leaf. In case this leaf remains a leaf, the full classiﬁer h(x) is used for instances x that arrive to this leaf. If it becomes an inner node, the vote vector v is discarded, and the partitioning of the data set is based on solely the bi- nary classiﬁer ϕ(x). An advantage of this formalization is that we can use any multi-class base classiﬁer of the form (6) for the tree cuts, so the Hamming tree algorithm can be considered as a “meta learner” which can be used on the top of any factorized base learner. Formally, a binary classiﬁcation tree with N inner nodes (N + 1 leaves) consists of a list of N base classiﬁers H = (h1, . . . , hN ) of the form hj(x) = αjvjϕj(x) and two index lists l = (l1, . . . , lN ) and r = (r1, . . . , rN ) with l, r ∈ (N ∪ {NULL})N . lj and rj represent the indices of the left and right children of the jth node of the tree, re- spectively. The node classiﬁer in the jth node is deﬁned    −vj  vj  hlj (x)  hrj (x)  hj(x) =  if ϕj(x) = −1 ∧ lj = NULL (left leaf), if ϕj(x) = +1 ∧ rj = NULL (right leaf), if ϕj(x) = −1 ∧ lj (cid:54)= NULL (left inner node), if ϕj(x) = +1 ∧ rj (cid:54)= NULL (right inner node).  (14)  The ﬁnal tree classiﬁer hH,l,r(x) = αh1(x) itself is not a factorized classiﬁer (6).4 In particular, hH,l,r(x) uses the local vote vectors vj determined by each leaf instead of a global vote vector. On the other hand, the coefﬁcient α is unique, and it is determined in the standard way  α =  1 2  log  1 + γ(h1, W) 1 − γ(h1, W)  based on the edge of the tree classiﬁer h1. The local coefﬁ- cients αj returned by the base learners are discarded (along with the vote vectors in the inner nodes). Finding the optimal N-inner-node tree is a difﬁcult combi- natorial problem. Most tree-building algorithms are there- fore sub-optimal by construction. For ADABOOST this is not a problem: we can continue boosting as long as the edge is positive. Classiﬁcation trees are usually built in a greedy manner: at each stage we try to cut all the current leaves j by calling the base learner of the data points reach- ing the jth leaf, then select the best node to cut, convert the old leaf into an inner node, and add two new leaves. The difference between the different algorithms is in the way the best node is selected. Usually, we select the node that improves a gain function the most. In ADABOOST.MH the natural gain is the edge (11) of the base classiﬁer. Since the data set (X, Y) is different at each node, we include it explicitly in the argument of the full multi-class edge  γ(v, ϕ, X, Y, W) =  I{xi ∈ X}wi,(cid:96)v(cid:96)ϕ(xi)yi,(cid:96).  i=1  (cid:96)=1  Note that in this deﬁnition we do not require that the weights of the selected points add up to 1. Also note that this gain function is additive on subsets of the original data set, so the local edges in the leaves add up to the edge of the full tree. This means that any improvement in the lo- cal edge directly translates to an improvement of the tree edge. This is a crucial property: it assures that the edge of the tree is always positive as long as the local edges in the  4Which is not a problem: we will not want to build trees of  trees.  n(cid:88)  K(cid:88)  Multi-class Hamming trees  inner nodes are positive, so any weak binary classiﬁer φ(x) can be used to deﬁne the inner cuts and the leaves. The basic operation when adding a tree node with a scalar binary classiﬁer (cut) ϕ is to separate the data matrices X, Y, and W according to the sign of classiﬁcation ϕ(xi) for all xi ∈ X. The pseudocode is straightforward, but for the sake of completeness, we include it in the supplementary (Appendix C, Figure 5). Building a tree is usually described in a recursive way but we ﬁnd the iterative procedure easier to explain, so our pseudocode in Figure 2 contains this version. The main idea is to maintain a priority queue, a data structure that allows inserting objects with numerical keys into a set, and extracting the object with the maximum key (Cormen et al., 2009). The key will represent the improvement of the edge when cutting a leaf. We ﬁrst call the base learner on the full data set (line 1) and insert it into the priority queue with its edge γ(v, ϕ, X, Y, W) (line 3) as the key. Then in each iteration, we extract the leaf that would provide the best edge improvement among all the leaves in the priority queue (line 7), we partition the data set (line 11), call the base learners on the two new leaves (line 12), and insert them into the priority queue using the difference between the old edge on the partitioned data sets and the new edges of the base classiﬁers in the two new leaves (line 13). When inserting a leaf into the queue, we also save the sign of the cut (left or right child) and the index of the parent, so the index vectors l and r can be set properly in line 8. When the priority queue is implemented as a heap, both the insertion and the extraction of the maximum takes O(log N ) time (Cormen et al., 2009), so the total running  time of the procedure is O(cid:0)N (TBASE + n + log N )(cid:1), where not be more than n, the running time is O(cid:0)N (TBASE + n)(cid:1).  TBASE is the running time of the base learner. Since N can-  If the base learners cutting the leaves are decision stumps, the total running time is O(nKdN ). In the procedure we have no explicit control over the shape of the tree, but if it happens to be balanced, the running time can further be improved to O(nKd log N ).  4. Experiments Full reproducibility was one of the key motivations when we designed our experimental setup. All experiments were done using the open source multiboost software of Ben- bouzid et al. (2012), version 1.2. In addition, we will make public all the conﬁguration ﬁles, train/test/validation cuts, and the scripts that we used to set up the hyperparameter validation. We carried out experiments on ﬁve mid-sized (isolet, letter, optdigits, pendigits, and USPS) and nine small (balance, blood, wdbc, breast, ecoli, iris, pima, sonar,  and wine) data sets from the UCI repository. The ﬁve sets were chosen to overlap with the selections of most of the recent multi-class boosting papers (K´egl & Busa-Fekete, 2009; Li, 2009a;b; Zhu et al., 2009; Sun et al., 2012; Mukherjee & Schapire, 2013), The small data sets were selected for comparing ADABOOST.MH with SVMs using Gaussian kernels, taking the results of a recent paper (Duch et al., 2012) whose experimen- tal setup we adopted. All numerical results (multi-class  test errors (cid:98)RI(f ) (1) and test learning curves) are avail-  able at https://www.lri.fr/˜kegl/research/ multiboostResults.pdf, one experiment per page for clarity. Tables 1 and 2 contain summaries of the results.  balance blood wdbc breast ecoli iris pima sonar wine  AB.MH 6.0 ± 4.0 22.0 ± 4.0 3.0 ± 2.0 34.0 ± 13.0 15.0 ± 6.0 7.0 ± 6.0 24.0 ± 5.0 13.0 ± 10.0 2.0 ± 3.0  SVM  10.0 ± 2.0 21.0 ± 5.0 2.0 ± 3.0 37.0 ± 8.0 16.0 ± 6.0 5.0 ± 6.0 24.0 ± 4.0 14.0 ± 8.0 3.0 ± 4.0  Table 2. Test error percentages on small benchmark data sets.  Hyperparameter optimization is largely swept under the rug in papers describing alternative multi-class boosting meth- ods. Some report results with ﬁxed hyperparameters (Zhu et al., 2009; Sun et al., 2012) and others give the full table of test errors for a grid of hyperparameters (K´egl & Busa- Fekete, 2009; Li, 2009a;b; Mukherjee & Schapire, 2013). Although the following procedure is rather old, we feel the need to detail it for promoting a more scrupulous compari- son across papers. For the small data sets we ran 10×10 cross-validation (CV) to optimize the hyperparameters and the estimate the gen- eralization error. For the number of inner nodes we do a grid search (we also considered using the “one sigma” rule for biasing the selection towards smaller trees, but the sim- ple minimization proved to be better). For robustly esti- mating the optimal stopping time we use a smoothed test  error. For the formal description, let (cid:98)R(t) be the average  test error (1) of the ten validation runs after t iterations. We run ADABOOST.MH for Tmax iterations, and compute the optimal stopping time using the minimum of the smoothed test error using a linearly growing sliding window, that is,  T ∗ =  arg min  T :Tmin<T≤Tmax  1  T − (cid:98)0.8T(cid:99)  T(cid:88)  (cid:98)R(t),  t=(cid:98)0.8T(cid:99)  (15)  where Tmin was set to a constant 50 to avoid stopping too early due to ﬂuctuations. For selecting the best number of inner nodes N, we simply minimized the smoothed test  TREEBASE(X, Y, W, BASE(·,·,·), N )  Multi-class Hamming trees  (cid:46) key = edge γ  (cid:46) initialize classiﬁer list  lj ← rj ← NULL  S ← PRIORITYQUEUE  H ← () for j ← 1 to N  (cid:46) O(log N ) insertion and extraction of maximum key  (cid:0)α, v, ϕ(·)(cid:1) ← BASE(X, Y, W) INSERT(cid:0)S,(cid:0)v, ϕ(·), X, Y, NULL, 0(cid:1), γ(v, ϕ, X, Y, W)(cid:1) (cid:0)vj, ϕj(·), Xj, Yj,•, jP (X−, Y−, W−, X+, Y+, W+) ← CUTDATASET(cid:0)Xj, Yj, W, ϕj(·)(cid:1) (cid:0)α•, v•, ϕ•(·)(cid:1) ← BASE(X•, Y•, W•) INSERT(cid:0)S,(cid:0)v•, ϕ•(·), X•, Y•,•, j(cid:1), γ(v•, ϕ•,X•,Y•,W•) − γ(vj, ϕj,X•,Y•,W•)(cid:1)  if • = − then ljP ← j else if • = + then rjP ← j H ← APPEND(H, vjϕj(·))  (cid:1) ← EXTRACTMAX(S)  (cid:46) best node in the priority queue (cid:46) child index of parent  (cid:46) adding hj(·) = vjϕj(·) to H  (cid:46) insert children into priority queue  (cid:46) initialize child indices  for • ∈ {−, +}  (cid:46) key = edge improvement over parent edge 1 + γ(h1, W) 1 − γ(h1, W)  1 2  log  α =  return(cid:0)α, H, l, r(cid:1)  (cid:46) standard coefﬁcient of the full tree classiﬁer h1 (14)  1 2 3 4 5 6 7 8 9 10 11 12 13  14  15  Figure 2. The pseudocode of the Hamming tree base learner. N is the number of inner nodes. The algorithm returns a list of base classiﬁers H, two index lists l and r, and the base coefﬁcient α. The tree classiﬁer is then deﬁned by (14).  Method ADABOOST.MH w Hamming trees ADABOOST.MH w Hamming prod. (K´egl & Busa-Fekete, 2009) AOSOLOGITBOOST J = 20, ν = 0.1 (Sun et al., 2012) ABCLOGITBOOST J = 20, ν = 0.1 (Li, 2009b) ABCMART J = 20, ν = 0.1 (Li, 2009a) LOGITBOOST J = 20, ν = 0.1 (Li, 2009b) SAMME w single-label trees (Zhu et al., 2009) ADABOOST.MH w single-label trees (Zhu et al., 2009) ADABOOST.MM (Mukherjee & Schapire, 2013) ADABOOST.MH w single-label trees (Mukherjee & Schapire, 2013)  isolet 3.5 ± 0.5 4.2 ± 0.5 3.5 ± 0.5 4.2 ± 0.5 5.0 ± 0.6 4.7 ± 0.5  letter 2.1 ± 0.2 2.5 ± 0.2 2.3 ± 0.2 2.2 ± 0.2 2.5 ± 0.2 2.8 ± 0.3 2.3 ± 0.2 2.6 ± 0.3 2.5 ± 0.2 9.0 ± 0.5  optdigits 2.0 ± 0.3 2.1 ± 0.4 2.1 ± 0.3 3.1 ± 0.4 2.6 ± 0.4 3.6 ± 0.4  pendigits 2.1 ± 0.3 2.1 ± 0.2 2.4 ± 0.3 2.9 ± 0.3 3.0 ± 0.3 3.1 ± 0.3 2.5 ± 0.3 2.8 ± 0.3 2.7 ± 0.3 7.0 ± 0.4  USPS 4.5 ± 0.5 4.4 ± 0.5 4.9 ± 0.5 4.9 ± 0.5 5.2 ± 0.5 5.8 ± 0.5  Table 1. Test error percentages on mid-sized benchmark data sets.  error over a predeﬁned grid N∗ = min  N∈N (cid:98)R(T ∗  N )(N )  N and (cid:98)R(t)(N ) are the optimal stopping time (15)  N∗. The error (cid:98)Ri in the ith training/test fold is  where T ∗ and the test error, respectively, in the run with N inner nodes, and N is the set of inner nodes participating in the grid search. Then we re-run ADABOOST.MH on the joined training/validation set using the selected hyperparameters N∗ and T ∗ then computed on the held-out test set. In the tables we report the mean error and the standard deviation. On the medium-size data sets we ran 1 × 5 CV (using the des- ignated test sets where available) following the same pro- cedure. In this case the report the binomial standard devia- tion  (cid:113)(cid:98)R(1 − (cid:98)R)/n. Further details and the description and  explanation of some slight variations of this experimental setup are available at https://www.lri.fr/˜kegl/ research/multiboostResults.pdf. On the small data sets, Duch et al. (2012) used the exact same protocol, so, although the folds are not the same, the results are directly comparable. The error bars represent the standard deviation of the test errors over the ten test folds not divided by 10, contrary to common practice, since the training set of the folds are highly correlated. The large error bars are the consequence of the small size and the noisiness of these sets. They make it difﬁcult to es- tablish any signiﬁcant trends. We can safely state that AD- ABOOST.MH is on par with SVM (it is certainly not worse, “winning” on six of the nine sets), widely considered one of the the best classiﬁcation methods for small data sets.  √  Multi-class Hamming trees  Even though on the mid-sized data sets there are dedicated test sets used by most of the experimenters, comparing ADABOOST.MH to alternative multi-class boosting tech- niques is somewhat more difﬁcult since none of the papers do proper hyperparameter tuning. Most of the papers re- port results with a table of errors given for a set of hy- perparameter choices, without specifying which hyperpa- rameter choice would be picked by proper validation. For methods that are non-competitive with ADABOOST.MH (SAMME of Zhu et al. (2009) and ADABOOST.MM of Mukherjee & Schapire (2013)) we report the post-validated best error which may be signiﬁcantly lower than the er- ror corresponding to the hyperparameter choice selected by proper validation. For methods where this choice would unfairly bias the comparison (AOSOLOGITBOOST (Sun et al., 2012), ABCLOGITBOOST, LOGITBOOST, and ABCMART (Li, 2009a;b)), we chose the best overall hyperparameter J = 20 and ν = 0.1, suggested by the Li (2009a;b). At https://www.lri.fr/˜kegl/ research/multiboostResults.pdf (but not in Table 1) we give both errors for some of the methods. Proper hyperparameter-validation should put the correct test error estimates between those two limits. Since ADABOOST.MH with decision products (K´egl & Busa- Fekete, 2009) is also implemented in multiboost (Ben- bouzid et al., 2012), for this method we re-ran experiments with the protocol described above. The overall conclusion is that AOSOLOGITBOOST (Sun et al., 2012) and ADABOOST.MH with Hamming trees are the best algorithms (ADABOOST.MH winning on all the ﬁve data sets but within one standard deviation). ADABOOST.MH with decision products (K´egl & Busa- Fekete, 2009) and ABCLOGITBOOST are slightly weaker, as also noted by (Sun et al., 2012). SAMME (Zhu et al., 2009) and ADABOOST.MM (Mukherjee & Schapire, 2013) perform below the rest of the methods on the two data sets shared among all the papers (even though we give post-validated results). Another important con- clusion is that ADABOOST.MH with Hamming trees is signiﬁcantly better then other implementations of AD- ABOOST.MH in (Zhu et al., 2009; Mukherjee & Schapire, 2013), assumably implemented using single-label trees (the errors reported by Mukherjee & Schapire (2013) are espe- cially conspicuous). ADABOOST.MH with Hamming trees also achieves good results on image recognition problems. On MNIST, boost- ing trees of stumps over pixels with eight inner nodes and about 50000 iterations has a test error of 1.25%, making it one of the best no-domain-knowledge “shallow” classi- ﬁers. Using stumps over Haar ﬁlters (Viola & Jones, 2004), boosted trees with four inner nodes and 10000 iterations achieves a test error of 0.85%, comparable to classical con- volutional nets (LeCun et al., 1998).  ADABOOST.MH with Hamming trees, usually combined with calibration (Platt, 2000; Niculescu-Mizil & Caruana, 2005) and model averaging, has been also successful in re- cent data challenges. On the Kaggle emotions data chal- lenge, although not competitive with deep learning tech- niques, out-of-the-box ADABOOST.MH with Hamming trees over Haar ﬁlters ﬁnished 17th place with a test error of 57%. In the Yahoo! Learning-to-Rank Challenge (Chapelle et al., 2011) it achieved top ten performances with results not signiﬁcantly different from the winning scores. Finally, in the recent INTERSPEECH Challenge it won the Emo- tion sub-challenge and it was runner up in the Social Sig- nals sub-challenge.  5. Conclusion In this paper we introduced Hamming trees that optimize the multi-class edge prescribed by ADABOOST.MH with- out reducing the multi-class problem to K binary one- against-all classiﬁcations. We showed that without this restriction, often considered mandatory, ADABOOST.MH is one of the best off-the-shelf multi-class classiﬁcation algorithms. The algorithm retains the conceptual ele- gance, power, and computational efﬁciency of binary AD- ABOOST. Using decision stumps at the inner nodes and at the leaves of the tree is a natural choice due to the efﬁciency of the learning algorithm, nevertheless, the general setup de- scribed in this paper allows for using any binary classiﬁer. One of the avenues investigated for future work is to try stronger classiﬁers, such as SVMs, as binary cuts. The for- mal setup described in Section 2.1 does not restrict the al- gorithm to single-label problems; another direction for fu- ture work is to benchmark it on standard multi-label and sequence-to-sequence classiﬁcation problems (Dietterich et al., 2008).  References Allwein, E. L., Schapire, R. E., and Singer, Y. Reduc- ing multiclass to binary: a unifying approach for margin classiﬁers. Journal of Machine Learning Research, 1: 113–141, 2001.  Benbouzid, D., Busa-Fekete, R., Casagrande, N., Collin, F.-D., and K´egl, B. MultiBoost: a multi-purpose boost- ing package. Journal of Machine Learning Research, 13: 549–553, 2012.  Boser, B., Guyon, I., and Vapnik, V. A training algorithm for optimal margin classiﬁers. In Fifth Annual Workshop on Computational Learning Theory, pp. 144–152, 1992.  Caruana, R. and Niculescu-Mizil, A. An empirical compar- ison of supervised learning algorithms. In Proceedings  Multi-class Hamming trees  Mukherjee, I. and Schapire, R. E. A theory of multiclass boosting. Journal of Machine Learning Research, 14: 437–497, 2013.  Niculescu-Mizil, A. and Caruana, R. Obtaining calibrated probabilities from boosting. In Proceedings of the 21st International Conference on Uncertainty in Artiﬁcial In- telligence, pp. 413–420, 2005.  Platt, J. Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. In Smola, A.J., Bartlett, P., Schoelkopf, B., and Schuur- mans, D. (eds.), Advances in Large Margin Classiﬁers, pp. 61–74. MIT Press, 2000.  Quinlan, J. Induction of decision trees. Machine Learning,  1(1):81–106, 1986.  Quinlan, J. C4.5: Programs for Machine Learning. Mor-  gan Kaufmann, 1993.  Quinlan, J. Bagging, boosting and C4.5. In Proceedings of the 13th National Conference on Artiﬁcial Intelligence, pp. 725–730, 1996.  Schapire, R. E. Using output codes to boost multiclass lear- ing problems. In International Conference on Machine Learning, 1997.  Schapire, R. E. and Singer, Y.  Improved boosting al- gorithms using conﬁdence-rated predictions. Machine Learning, 37(3):297–336, 1999.  Sun, P., Reid, M. D., and Zhou, J. AOSO-LogitBoost: Adaptive one-vs-one LogitBoost for multi-class prob- lem. In International Conference on Machine Learning (ICML), 2012.  Sun, Y., Todorovic, S., Li, J., and Wu, D. Unifying the error-correcting and output-code AdaBoost within the margin framework. In International Conference on Ma- chine Learning, 2005.  Viola, P. and Jones, M. Robust real-time face detection. International Journal of Computer Vision, 57:137–154, 2004.  Zhu, J., Zou, H., Rosset, S., and Hastie, T. Multi-class AdaBoost. Statistics and its Interface, 2:349–360, 2009.  of the 23rd International Conference on Machine Learn- ing, pp. 161–168, 2006.  Chapelle, O., Chang, Y., and Liu, T.Y. (eds.).  Ya- hoo! Learning-to-Rank Challenge, volume 14 of JMLR W&CP, 2011.  Cormen, T., Leiserson, C., and Rivest, R. Introduction to  Algorithms. MIT Press, 2009.  Dietterich, T. G. and Bakiri, G. Solving multiclass learning problems via error-correcting output codes. Journal of Artiﬁcial Intelligence Research, 2:263–286, 1995.  Dietterich, T. G., Hao, Guohua, and Ashenfelter, A. Gradi- ent tree boosting for training conditional random ﬁelds. Journal of Machine Learning Research, 9:2113–2139, 2008.  Dror, G., Boull´e, M., Guyon, I., Lemaire, V., and Vogel, D. (eds.). Proceedings of KDD-Cup 2009 competition, vol- ume 7 of JMLR Workshop and Conference Proceedings, 2009.  Duch, W., Jankowski, N., and Maszczyk, T. Make it cheap: Learning with O(nd) complexity. In International Joint Conference on Neural Networks (IJCNN), pp. 1–4, 2012.  Freund, Y. and Schapire, R. E. A decision-theoretic gener- alization of on-line learning and an application to boost- ing. Journal of Computer and System Sciences, 55:119– 139, 1997.  Gao, T. and Koller, D. Multiclass boosting with hinge loss based on output coding. In International Conference on Machine Learning, 2011.  Guruswami, V. and Sahai, A. Multiclass learning, boosting, and error-correcting codes. In Conference on Computa- tional Learning Theory, 1999.  K´egl, B. and Busa-Fekete, R. Boosting products of base In International Conference on Machine classiﬁers. Learning, volume 26, pp. 497–504, Montreal, Canada, 2009.  LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient- based learning applied to document recognition. Pro- ceedings of the IEEE, 86(11):2278–2324, 1998.  Li, Ling. Multiclass boosting with repartitioning. In Inter-  national Conference on Machine Learning, 2006.  Li, P. ABC-Boost: Adaptive base class boost for multi- class classiﬁcation. In International Conference on Ma- chine Learning, 2009a.  Li, P. ABC-LogitBoost for multi-class classiﬁcation. Tech-  nical Report arXiv:0908.4144, Arxiv preprint, 2009b.  A. Showing (9)  Z(h, W) =  =  =  =  =  =  =  Multi-class Hamming trees  +  i=1  i=1  i=1  (cid:96)=1  (cid:96)=1  (cid:96)=1  (cid:96)=1  K(cid:88)  n(cid:88)  (cid:1) =  n(cid:88) K(cid:88) wi,(cid:96) exp(cid:0)−h(cid:96)(xi)yi,(cid:96) wi,(cid:96) exp(cid:0)−αv(cid:96)ϕ(xi)yi,(cid:96) (cid:1) wi,(cid:96)I{v(cid:96)ϕ(xi)yi,(cid:96) = 1}e−α + wi,(cid:96)I{v(cid:96)ϕ(xi)yi,(cid:96) = −1}eα(cid:17) (cid:16) n(cid:88) K(cid:88) K(cid:88) (cid:0)µ(cid:96)+I{v(cid:96) = +1} + µ(cid:96)−I{v(cid:96) = −1}(cid:1)e−α K(cid:88) (cid:0)µ(cid:96)−I{v(cid:96) = +1} + µ(cid:96)+I{v(cid:96) = −1}(cid:1)eα (cid:16)I{v(cid:96) = +1}(cid:0)e−αµ(cid:96)+ + eαµ(cid:96)−(cid:1) + I{v(cid:96) = −1}(cid:0)e−αµ(cid:96)− + eαµ(cid:96)+ K(cid:88) (cid:18) 1 + v(cid:96) K(cid:88) (cid:0)e−αµ(cid:96)+ + eαµ(cid:96)−(cid:1) + (cid:16)(cid:0)eα + e−α(cid:1)(cid:0)µ(cid:96)+ + µ(cid:96)−(cid:1) − v(cid:96) K(cid:88) − eα − e−α  (cid:1)(cid:19) (cid:0)e−αµ(cid:96)− + eαµ(cid:96)+ (cid:0)eα − e−α(cid:1)(cid:0)µ(cid:96)+ − µ(cid:96)−(cid:1)(cid:17)  1 2 eα + e−α  K(cid:88)  1 − v(cid:96)  (cid:96)=1  (cid:96)=1  (cid:96)=1  (cid:96)=1  2  2  v(cid:96)  2  2  (cid:96)=1  (cid:1)(cid:17)  (16)  (17)  (18)  (cid:0)µ(cid:96)+ − µ(cid:96)−(cid:1). K(cid:88) n(cid:88)  wi,(cid:96) = 1.  K(cid:88)  (cid:0)µ(cid:96)+ + µ(cid:96)−(cid:1) =  (cid:96)=1  i=1  (cid:96)=1  (16) comes from the deﬁnition (6) of h and (17) follows from the deﬁnitions (7) and (8) of µ(cid:96)− and µ(cid:96)+. In the ﬁnal step (18) we used the fact that  (cid:40)  1 −1  if x(j) ≥ b, otherwise,  B. Multi-class decision stumps The simplest scalar base learner used in practice on numerical features is the decision stump, a one-decision two-leaf decision tree of the form  where j is the index of the selected feature and b is the decision threshold. If the feature values(cid:0)x(j)  ϕj,b(x) =  (cid:1) are pre-  1 , . . . , x(j)  n  ordered before the ﬁrst boosting iteration, a decision stump maximizing the edge (11) (or minimizing the energy (16)5) can be found very efﬁciently in Θ(ndK) time. The pseudocode of the algorithm is given in Figure 3. STUMPBASE ﬁrst calculates the edge vector γ(0) of the constant classiﬁer h(0)(x) ≡ 1 which will serve as the initial edge vector for each featurewise edge-maximizer. Then it loops over the features, calls BESTSTUMP to return the best featurewise stump, and then selects the best of the best by minimizing the energy (16). BESTSTUMP loops over all (sorted) feature values s1, . . . , sn−1. It considers all thresholds b halfway between two non-identical feature values si (cid:54)= si+1. The main trick (and, at the same time, the bottleneck of the algorithm) is the update of the classwise edges in lines 4-5: when the threshold moves from b = si−1+si , the classwise edge γ(cid:96) of 1ϕ(x) (that is, vϕ(x) with v = 1) can only change by ±wi,(cid:96), depending on the sign yi,(cid:96) (Figure 4). The total edge of vϕ(x) with optimal votes (13) is then the sum of the absolute values of the classwise edges of 1ϕ(x) (line 7).  to b = si+si+1  2  2  5Note the distinction: for full binary v the two are equivalent, but for ternary or real valued v and/or real valued φ(x) they are not. In Figure 3 we are maximizing the edge within each feature (line 7 in BESTSTUMP) but across features we are minimizing the energy (line 7 in STUMPBASE). Updating the energy inside the inner loop (line 4) could not be done in Θ(K) time.  Multi-class Hamming trees  wi,(cid:96)yi,(cid:96)  (cid:46) classwise edges (10) of constant classiﬁer h(0)(x) ≡ 1  STUMPBASE(X, Y, W)  for (cid:96) ← 1 to K (cid:46) for all classes  γ(0)  (cid:96) ← n(cid:88) s ← SORT(cid:0)x(j)  n  i=1  (cid:46) all (numerical) features  1 , . . . , x(j)  for j ← 1 to d  (cid:1) Z(cid:0)αjvjϕj,bj , W(cid:1) return(cid:0)αj∗ , vj∗ , ϕj∗,bj∗ (·)(cid:1)  log j∗ ← arg min  1 + γj 1 − γj  (vj, bj, γj) ← BESTSTUMP(s, Y, W, γ(0)) αj ← 1 (cid:46) base coefﬁcient (12) 2  j  BESTSTUMP(s, Y, W, γ(0))  (cid:46) sort the jth column of X  (cid:46) best stump per feature  (cid:46) best stump across features  γ∗ ← γ(0) γ ← γ(0) for i ← 1 to n − 1  (cid:46) best edge vector (cid:46) initial edge vector  for (cid:96) ← 1 to K (cid:46) for all classes  (cid:46) for all points in order s1 ≤ . . . ≤ sn−1  if si (cid:54)= si+1 then  γ(cid:96) ← γ(cid:96) − 2wi,(cid:96)yi,(cid:96)  if(cid:80)K (cid:96)=1 |γ(cid:96)| >(cid:80)K  (cid:46) update classwise edges of stump with v = 1  (cid:46) no threshold if identical coordinates si = si+1 (cid:96)=1 |γ∗  (cid:46) found better stump  (cid:96) | then  γ∗ ← γ b∗ ← si+si+1  (cid:46) update best edge vector  (cid:46) update best threshold  for (cid:96) ← 1 to K (cid:46) for all classes  2  if γ∗ = γ(0)  (cid:96) ← sign(γ(cid:96)) v∗ return (v∗,−∞,(cid:107)γ∗(cid:107)1)  (cid:46) set vote vector according to (13) (cid:46) did not beat the constant classiﬁer  else  return (v∗, b∗,(cid:107)γ∗(cid:107)1)  (cid:46) best stump  (cid:46) constant classiﬁer with optimal votes  1  2  3 4 5  6  7  8  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  Figure 3. Exhaustive search for the best decision stump. BESTSTUMP receives a sorted column (feature) s of the observation matrix X. The sorting in line 4 can be done once for all features outside of the boosting loop. BESTSTUMP examines all thresholds b halfway between two non-identical coordinates si (cid:54)= si+1 and returns the threshold b∗ and vote vector v∗ that maximizes the edge γ(v, ϕj,b, W). STUMPBASE then sets the coefﬁcient αj according to (12) and chooses the stump across features that minimizes the energy (5).  C. Cutting the data set The basic operation when adding a tree node with a scalar binary classiﬁer (cut) ϕ is to separate the data matrices X, Y, and W according to the sign of the classiﬁcation ϕ(xi) for all xi ∈ X. Figure 5 contains the pseudocode of this simple operation.  Multi-class Hamming trees  Figure 4. Updating the edge γ(cid:96) in line 5 of BESTSTUMP. If yi,(cid:96) = 1, then γ(cid:96) decreases by 2wi,(cid:96), and if yi = −1, then γ(cid:96) increases by 2wi,(cid:96).  CUTDATASET(cid:0)X, Y, W, ϕ(·)(cid:1)  1 2 3 4 5 6 7 8 9 10 11 12  X− ← Y− ← W− ← X+ ← Y+ ← W+ ← () for i ← 1 to n  (cid:46) empty vectors  if xi ∈ X then  if ϕ(xi) = −1 then  X− ← APPEND(X−, xi) Y− ← APPEND(Y−, yi) W− ← APPEND(W−, wi)  else  X+ ← APPEND(X+, xi) Y+ ← APPEND(Y+, yi) W+ ← APPEND(W+, wi)  return (X−, Y−, W−, X+, Y+, W+)  Figure 5. The basic operation when adding a tree node is to separate the data matrices X, Y, and W according to the sign of classiﬁcation ϕ(xi) for all xi ∈ X.  si(cid:45)1sisi(cid:43)1si(cid:45)1(cid:43)si2si(cid:43)si(cid:43)12x(cid:72)j(cid:76)(cid:45)11(cid:106)j,(cid:215)(cid:72)x(cid:72)j(cid:76)(cid:76)","Within the framework of this http URL, we propose to train vector-valueddecision trees to optimize the multi-class edge without reducing themulti-class problem to $K$ binary one-against-all classifications. The keyelement of the method is a vector-valued decision stump, factorized into aninput-independent vector of length $K$ and label-independent scalar classifier.At inner tree nodes, the label-dependent vector is discarded and the binaryclassifier can be used for partitioning the input space into two regions. Thealgorithm retains the conceptual elegance, power, and computational efficiencyof binary AdaBoost. In experiments it is on par with support vector machinesand with the best existing multi-class boosting algorithm AOSOLogitBoost, andit is significantly better than other known implementations of this http URL."
1312.6077,2014,Efficient Visual Coding: From Retina To V2  ,"['Honghao Shan', 'Garrison Cottrell']",https://arxiv.org/pdf/1312.6077.pdf,"Efficient Visual Coding: From Retina To V2      UCSD   Honghao Shan     Garrison Cottrell  Computer Science and Engineering   La Jolla, CA 92093-0404   shanhonghao@gmail.com, gary@ucsd.edu        Abstract        The  human  visual  system  has  a  hierarchical  structure  consisting  of  layers  of  processing, such as the retina, V1, V2, etc. Understanding the functional roles of  these visual processing layers would help to integrate the psychophysiological  and  neurophysiological  models  into  a  consistent  theory  of  human  vision,  and  would also provide insights to computer vision research. One classical theory of  the  early  visual  pathway  hypothesizes  that  it  serves  to  capture  the  statistical  structure of the visual inputs by efficiently coding the visual information in its  outputs. Until recently, most computational models following this theory have  focused  upon  explaining  the  receptive  field  properties  of  one  or  two  visual  layers.  Recent  work  in  deep  networks  has  eliminated  this  concern,  however,  there  is  till  the  retinal  layer  to  consider.  Here  we  improve  on  a  previously- described hierarchical model Recursive ICA (RICA) [1] which starts with PCA,  followed  by  a  layer  of  sparse  coding  or  ICA,  followed  by  a  component-wise  nonlinearity  derived  from  considerations  of  the  variable  distributions  expected  by ICA. This process is then repeated. In this work, we improve on this model  by  using  a  new  version  of  sparse  PCA  (sPCA),  which  results  in  biologically- plausible  receptive  fields  for  both  the  sPCA  and  ICA/sparse  coding.  When  applied to natural image patches, our model learns visual features exhibiting the  receptive  field  properties  of  retinal  ganglion  cells/lateral  geniculate  nucleus  (LGN)  cells,  V1  simple  cells,  V1  complex  cells,  and  V2  cells.  Our  work  provides  predictions  for  experimental  neuroscience  studies.  For  example,  our  result  suggests  that  a  previous  neurophysiological  study  improperly  discarded  some of their recorded neurons; we predict that their discarded neurons capture  the shape contour of objects.  Introduction   1  The visual layers that appear early in the human visual pathway, such as retina, V1, and V2, have  been  of  particular  interest  to  theoretical  neuroscientists  and  computer  vision  researchers.  First,  these early visual layers capture simpler visual structures and receive less top-down influence than  the  later  visual  layers.  As  a  result,  experimental  neuroscientists  have  collected  detailed  descriptions of the early visual layers. Second, these early visual layers are shared components in  the visual pathway; they develop and mature in our early life, and then subserve all kinds of visual  tasks  during  the  rest  of  life.  Hence  they  appear  to  encode  certain  universal  visual  features  that  work  as  building  blocks  of  visual  scenes.  It  is  unlikely  that  these  features  are  learned  in  a  supervised manner. Hence, it is of particular interest to understand what kinds of visual features  they encode, and how these features are learned automatically from natural scenes.   Figure  1.  The  model  framework.  Image  patches  are  preprocessed  and  passed  to  the  first  layer.  Within the first layer, dimensions of the data are first reduced by the sparse autoencoder (sPCA);  then the dimensions are expanded using the sparse coding algorithm (ICA). The layer-1 outputs  are transformed to the layer-2 inputs by coordinate-wise nonlinear activation functions.   The efficient coding theory has been popular in explaining the functional roles of the early visual  layers.  It  follows  the  long-standing  hypothesis  that  the  visual  perception  serves  to  capture  the  statistical structure of the visual inputs, so that actions can be taken accordingly to maximize the  chances  of  survival.  Attneave  suggested  that  the  visual  structure  could  be  measured  by  the  redundancy of the visual inputs: how much visual structure we perceive depends on how well we  can  predict  a  removed  piece  of  the  image  by  observing  its  remaining  parts.  Barlow  further  hypothesized  that  the  sensory  system  could  capture  the  statistical  structure  of  its  inputs  by  removing  the  redundancy  in  its  outputs,  because  to  do  so  the  system  must  have  a  complete  knowledge of the statistical structure. Linear implementations of the efficient coding theory, such  as independent components analysis and sparse coding, have been used to explain the receptive  field properties of V1 simple cells. When these algorithms are applied to natural image patches,  they learn visual features resembling the edge/bar shaped receptive fields of V1 simple cells.  Here  we  propose  a  hierarchical  model  following  the  efficient  coding  principle,  as  illustrated  in  Figure 1. This model is a variant on  the Recursive ICA model (RICA) [1]. The model has two  characteristics.  First,  this  model  adopts  a  layered  framework:  we  apply  the  efficient  coding  algorithm to capture the statistical structure of the inputs, and then apply another layer of efficient  coding algorithm to capture higher-order statistical structure not captured by the previous layer.  Such a layered framework was motivated by the observation that different parts of the brain share  similar anatomical structure and hence are likely to work under similar computational principles.  Second,  within  each  layer  we  first  apply  sparse  Principal  Components  Analysis  (SPCA)  (see  Methods) to reduce the dimensionality of the data, and then use the sparse coding algorithm to  expand the dimensionality. This scheme is widely used in computer vision algorithms: we usually  apply PCA to reduce dimensionality before applying ICA to capture the statistical structure of the  inputs, which helps to reduce noise as well as computational complexity. Moreover, a previous  study  has  shown  that  SPCA  learns  the  centre-surround  shaped  filters  resembling  the  receptive  fields  of  retinal  ganglion  cells  and  lateral  geniculate  nucleus  cells;  applying  the  sparse  coding  algorithm afterwards produces the edge/bar-shaped basis functions resembling the receptive fields  of V1 simple cells. In this work, we show that applying SPCA and then overcomplete ICA to the  result of these first two layers of processing gives V1 complex cell receptive fields followed by  receptive fields that explain data recently reported concerning V2, respectively.  One  difference  between  previous  work  in  multi-layer  ICA  and  ours  is  the  application  of  a  component-wise nonlinearity to the outputs of the ICA layer. To allow the second layer to more  efficiently  capture  the  statistical  structure  in  the  first  layer  outputs,  we  derived  coordinate-wise  nonlinear activation functions to transform the first layer's outputs to the second layer's inputs. The  activation  functions  contain  two  steps:  (1)  take  the  absolute  values;  (2)  make  the  marginal  distribution a Gaussian distribution. The idea here is that, as a generative model, ICA generates the  input pixels as a linear combination of many independent variables, which results in a Gaussian  distribution. Hence, our transformation “formats” the outputs of ICA for the next layer up.  2  Image dataset. We use the sixty-two 1000 by 1280 colour images of the natural scenes taken near  Kyoto. We transform them to grey-scale images, down-scale them by half on both sides. We then  normalize each image to have zero-mean and unit-variance pixel values.  Sparse PCA. We began with consideration of the constraints on retinal ganglion cells. The retina  compresses the approximately 100 million photoreceptor responses into a million ganglion cell re-   Methods   sponses.  Hence  the  first  consideration  is  that  we  would  like  the  ganglion  cells  to  retain  the  maximum amount of information about the photoreceptor responses. If we make the simplifying  assumption that ganglion cells respond linearly, then the optimal linear compression technique in  terms of reconstruction error is principal components analysis (PCA). One can map PCA into a  neural network as in Figure 1(a) (18,19). The weight vectors of each hidden unit in this network  each correspond to one eigenvector of the covariance matrix of the data. In standard PCA, there is  an ordering to the hidden units, such that the first hidden unit has very high response variance and  the last hidden unit has practically no variance, which means the first hidden unit is doing orders  of magnitude more work than the last one. The second consideration, then, is that we would like to  spread  the  work  evenly  among  the  hidden  units.  Hence  we  impose  a  threshold  on  the  average  squared output of the hidden units. As we will see from the simulations, in order to preserve the  maximum  information,  the  units  all  hit  this  threshold,  which  equalizes  the  work.  The  third  consideration is that PCA is profligate with connections - every ganglion cell would have non-zero  connections to every photoreceptor. Hence we also impose a constraint on the connectivity in the  network.  In  this  latter  constraint  we  were  inspired  by  the  earlier  work  of  (7).  They  proposed  a  model of retinal and early cortical processing based on energy minimization and showed that it  could create center-surround shaped receptive fields for grayscale images. However, their system  sometimes led to cells with two center-surround fields, and the optimization itself was unstable.  The three principles described above leads to our objective function. Given an input vector x ∈ RL,  the autoencoder seeks the optimal s* ∈ RM and A∈ RL×M(cid:1)such that the following objective function  is minimized:   E =  2 x − As 2  2  +λ A 1       subject to:  si  2 ≤1   ∀i    where  the  angle  brackets  denote  taking  average  over  all  the  input  samples.  The  first  term  minimizes the reconstruction error, maximizing the information retained by the encoding. Whenλ is small,  and L > M (i.e., the encoding compresses the information), the reconstruction error is  well approximated by a term that only involves the correlation matrix:  C = x ′x , which leads to a  fast  approximation  algorithm.  Given  that  the  second  term  minimizes  the  connections,  and  the  system  is  sensitive  to  correlations,  this  leads  to  local  receptive  fields.  The  final  constraint  equalizes the work, and the system pushes up against this limit, so that the average variance is  equal to 1.  We set lambda = 0.01 for the first layer, and lambda=0.03 for the second layer. We use gradient  descent to find the optimal s. We start with s=A'x, and update s for 100 steps with an update rate  of 0.01.  Sparse coding. Given an input vector x ∈ RL, the sparse coding algorithm seeks the optimal s* ∈  RM that minimizes the reconstruction error as well as a sparsity penalty on the outputs:   E =  2 x − As 2  2  + λf (s)    Once the optimal s is inferred, the connection weights A are updated by gradient descent in E. At  this point, as in previous work [1], a coordinate-wise nonlinear activation function is applied to  each filter. Our goal is to find the coordinate-wise activation function gi for each dimension of s  such that xi=gi(|si|) follows a generalized Gaussian distribution (in this work, we simply assume a  Gaussian).  Note  here,  as  in  previous  work,  we  discard  the  sign  on  si.  We  use  a  non-parametric  approach for efficiency. In this approach, all the samples |si| are sorted in ascending order. For N  samples of each coordinate si, cdf(|si|) is approximated by the ratio of its ranking in the list with N.  Then xi = g( si ) = F −1(cdˆf ( si )) ,  where  F  is  the  cumulative  density  function  of  a  normal  distribution, will approximately follow the standard normal distribution. Note that since xi depends  only on the rank order of |si|, the results would be the same if the signs are discarded by squaring  the si’s.   Model training. The model is trained layer by layer. For the first layer sparse autoencoder, we  randomly initialize A with Gaussian random variables, and update A for 40000 epochs. During  each epoch, we randomly select one image and sample 64 16x16 image patches from it. Then we  fix the layer-1 PCA filters, and learn the layer-1 ICA features with the sparse coding algorithm.  We expand the dimensions to 512, and update the basis functions for 40000 epochs with a learning  rate of 0.1.  We then expanded the receptive fields on the second layer. In each epoch, we randomly sample 64  32x32 image patches, infer the layer-1 ICA feature responses from the four adjacent 16x16 image  patches within the 32x32 image patch. All the 512*4=2048 filter responses are fed to the second  layer sparse PCA, and the dimensions are reduced to 128. The dimensions are then expanded to  256 by the layer-2 sparse coding algorithm.  3  We  apply  the  algorithms  to  the  Kyoto  natural  scene  dataset.  The  first  layer  of  sparse  PCA,  as  described above, results in center-surround receptive fields (see Figure 1). We have also applied  the algorithm to 1) color images, 2) video, and 3) sound. In these cases the results are 1) red-green,  the  blue-yellow,  and  grayscale  opponency,  2)  parasol/magnocellular  the  midget/parvocellular system), and 3) gammatone filters. This suggests that our three constraints  are somehow implemented by the perceptual system for efficient encoding at the peripheral level.   small,  persistent   fields  fields   Results   receptive   receptive   system),   (as  (as   large   in  in   fast,   and   Figure 2. Left: The receptive fields of sparse PCA on natural images. Right: We get the  standard result on the output of the sPCA algorithm: oriented local edge detectors.        Applying  the  sparse  coding  algorithm  to  the  output  of  the  sPCA,  we  get  the  standard  result:  oriented, local, edge detectors. Now, the interesting thing is what happens after applying sPCA to  the output of the first ICA layer. The receptive fields group together oriented edges of the same  orientation  in  a  local  receptive  field.  That  is,  they  automatically  do  a  pooling  operation,  like  complex cells in V1 (see Figure 3). Note that a positive connection means that this layer-2 PCA  feature  prefers  strong  responses,  either  positive  or  negative,  from  that  layer-1  ICA  feature.  A  negative connection means that it prefers weak or no responses from that layer-1 ICA feature. The  latter result shows that these model cells also show OFF-pooling, represented by the cold colors.  In  order  to  qualitatively  compare  our  sPCA  features  to  complex  cells,  we  performed  a  spike- triggered covariance analysis (STC). Neuroscientists have used this to characterize the receptive  fields of complex cells in V1. They present animals with white noise stimulation x and record the  neuron’s response. Then they calculate the average weighted covariance matrix:C = E[sx ′x ]. The  eigenvectors  of  C  with  the  biggest  eigenvalues  are  those  stimuli  that  would  best  activate  this  neuron; those eigenvectors with smallest eigenvalues are those stimuli that would most suppress  this neuron. An example from [Chen et al, 2007] is shown in Figure 4 (left). We plot the STC  analysis of a layer-2 sPCA model neuron in the same manner, in Figure 4 (right).     Figure 3. The receptive fields of four randomly-selected units in the second sPCA layer. Within  each patch, each bar represents one layer-1 ICA feature. Each layer-1 ICA feature is fitted to a  Gabor function, and the location, orientation, and length of the bar represent these features of the  Gabor fit. The colors of the bars represent the connection strength from the layer-1 ICA feature to  the  layer-2  sPCA  feature:  warm  colors,  positive  connections,  cold  colors,  negative,  and  gray  represents connections near 0.         Figure 4. Left: The top row displays the stimuli that would best activate the recorded neuron; the  bottom row displays the stimuli that would most suppress this neuron (Chen, Han, Poo, & Dan,  PNAS, 2007). Right: The same analysis performed on a layer-2 sPCA model neuron.  There  is  not  a  general  consensus  about  what  V2  cells  represent.  We  choose  a  recent  paper  to  compare our model to [6], because they plot the responses of their cells in a similar manner to the  technique we use here. They recorded 136 V2 cells from 16 macaque monkeys, but only reported  the results on 118 of them. The 18 cells they discarded showed no orientation preference. For each  V2 cell, they first identified its classical receptive field. Then they displayed 19 bars arranged in  hexagonal  arrays  within  the  receptive  field,  whose  sizes  were  much  smaller  than  the  receptive  field size. They varied the orientations of the bars, and measured the V2 neurons’ responses to  those settings. Using this approach, they were able to obtain a space-orientation RF map for each  V2 neuron. The result was that about 70% of the 118 neurons showed uniform orientation tuning  across their visual field. The remaining cells showed non-uniform tuning (e.g., like the “corner”  cell shown in Figure 5, right).         Figure 5 Two plots of the RF maps from the Van Essen experiment. The left plot represents about  70% of the 118 V2 neurons analyzed. These neurons show uniform orientation tuning across their  visual field. The right plot represents an example of the remainder of the cells, which show non- uniform orientation tuning.  Six examples of our layer-2 ICA features are plotted in Figure 6, in the same manner as the layer-2  sPCA features in Figure 3. The left-most column displays two model neurons that show uniform  orientation  preference  to  layer-1  ICA  features.  The  middle  column  displays  model  neurons  that  have  non-uniform/varying  orientation  preference  to  layer-1  ICA  features.  The  right  column  displays two model neurons that have location preference, but no orientation preference, to layer-1   ICA features. Van Essen’s group threw out about 13% of the neurons they recorded, because they  showed no orientation preference. We suggest that the cells in the right column are a prediction for  the  receptive  fields  of  the  discarded  neurons.  This  preference  for  location  but  not  shape  could  support recognition of contours.      Figure  6.  The  layer-2  ICA  features.  The  left-most  column  shows  two  cells  with  uniform  orientation preference. The middle column shows two cells with non-uniform preference. These  two types of cells are roughly in the same proportion as those found by van Essen. About 24% of  our  cells  resemble  the  ones  in  the  right  hand  column,  which  show  location  preference  but  no  orientation preference.  4  Conclusion  We have presented a hierarchical ICA model that is able to capture the characteristics of retinal  ganglion cells, V1 simple cells, V1 complex cells, and V2 cells. The model automatically  generates representations consistent with the pooling operation of deep networks. It also makes  predictions that are consistent with V2 cells that were rejected because of a lack of orientation  specificity.     Acknowledgments  This work was supported by NSF grants # #IIS-1219252, and SMA-041755 to the Temporal  Dynamics of Learning Center, an NSF Science of Learning Center. We would like to thank the  members of Gary’s Unbelievable Research Unit for helpful comments.  R eferences  [1] Shan, Honghao, Zhang, Lingyun and Garrison W. Cottrell (2007) Recursive ICA. In Advances  in Neural Information Processing Systems 19. MIT Press, Cambridge, MA.  [2] Attneave, F. (1954). Some informational aspects of visual perception. Psychological Review,  61(3):183-193.  [3] Barlow, H. B. (1961). Possible principles underlying the transformation of sensory messages.  In W. A. Rosenblith (Ed.), Sensory communication. Cambridge, MA, USA: MIT Press.  [4]  Barlow,  H.  B.  (2001).  The  exploitation  of  regularities  in  the  environment  by  the  brain.  Behavioral and Brain Sciences, 24, 602-607.  [5] Chen, X., Han, F., Poo, M.-m., and Dan, Y. (2007). Excitatory and suppressive receptive field  subunits in awake monkey V1. PNAS 104:19120-19125.    [6] Anzai, A., Peng, X., and Van Essen, D.C. (2007) Neurons in monkey visual area V2 encode  conjunctions of orientations. Nature Neuroscience 10(10):1313-1321.   ","The human visual system has a hierarchical structure consisting of layers ofprocessing, such as the retina, V1, V2, etc. Understanding the functional rolesof these visual processing layers would help to integrate thepsychophysiological and neurophysiological models into a consistent theory ofhuman vision, and would also provide insights to computer vision research. Oneclassical theory of the early visual pathway hypothesizes that it serves tocapture the statistical structure of the visual inputs by efficiently codingthe visual information in its outputs. Until recently, most computationalmodels following this theory have focused upon explaining the receptive fieldproperties of one or two visual layers. Recent work in deep networks haseliminated this concern, however, there is till the retinal layer to consider.Here we improve on a previously-described hierarchical model Recursive ICA(RICA) [1] which starts with PCA, followed by a layer of sparse coding or ICA,followed by a component-wise nonlinearity derived from considerations of thevariable distributions expected by ICA. This process is then repeated. In thiswork, we improve on this model by using a new version of sparse PCA (sPCA),which results in biologically-plausible receptive fields for both the sPCA andICA/sparse coding. When applied to natural image patches, our model learnsvisual features exhibiting the receptive field properties of retinal ganglioncells/lateral geniculate nucleus (LGN) cells, V1 simple cells, V1 complexcells, and V2 cells. Our work provides predictions for experimentalneuroscience studies. For example, our result suggests that a previousneurophysiological study improperly discarded some of their recorded neurons;we predict that their discarded neurons capture the shape contour of objects."
1312.6158,2014,Deep Belief Networks for Image Denoising  ,"['Mohammad Ali Keyvanrad', 'mohammad pezeshki', 'Mohammad Mehdi Homayounpour']",https://arxiv.org/pdf/1312.6158.pdf,"Deep Belief Networks for Image Denoising  Mohammad Ali Keyvanrad  Dept. of Computer Engineering and IT, Amirkabir University of Technology,  Mohammad Pezeshki  Dept. of Computer Engineering and IT, Amirkabir University of Technology,  Tehran, Iran  Tehran, Iran  keyvanrad@aut.ac.ir  m.pezeshki@aut.ac.ir  4 1 0 2     n a J    2      ]  G L . s c [      2 v 8 5 1 6  .  2 1 3 1 : v i X r a  Mohammad Mehdi Homayounpour Dept. of Computer Engineering and IT, Amirkabir University of Technology,  Tehran, Iran  homayoun@aut.ac.ir  Abstract  Deep Belief Networks which are hierarchical generative models are effective tools for feature representation and extraction. Furthermore, DBNs can be used in nu- merous aspects of Machine Learning such as image denoising. In this paper, we propose a novel method for image denoising which relies on the DBNs’ ability in feature representation. This work is based upon learning of the noise behavior. Generally, features which are extracted using DBNs are presented as the values of the last layer nodes. We train a DBN a way that the network totally distin- guishes between nodes presenting noise and nodes presenting image content in the last later of DBN, i.e. the nodes in the last layer of trained DBN are divided into two distinct groups of nodes. After detecting the nodes which are presenting the noise, we are able to make the noise nodes inactive and reconstruct a noise- less image. In section 4 we explore the results of applying this method on the MNIST dataset of handwritten digits which is corrupted with additive white Gaus- sian noise (AWGN). A reduction of 65.9% in average mean square error (MSE) was achieved when the proposed method was used for the reconstruction of the noisy images.  1  Introduction  Image signals are often corrupted due to noise. Removing noise from image is an important issue in computer vision, because this step could be the preprocessing step of many other applications. Up to now various methods have been proposed to remove noise (denoise) from visual data. Focus of many of these methods is on Fourier Analysis [1], Spatial Filtering [2], and Wavelet Transform [3]. Also there are some other methods based on Spare Coding and Dictionary Learning [4]. On the other hand, Machine Learning tools such as Convolutional Neural Networks (CNN) or Deep Neural Networks (DNN) have been used in several papers to tackle this issue[5][6]. One of the biggest difﬁculties in training such deep networks is that the cost function of such deep architectures gets stuck in poor local optima due to random initialization of weights. Hinton et al. [7] proposed a new greedy layer-wise algorithm to tackle this issue and introduced Deep Belief Networks (DBNs). DBNs are able to present a good ”feature” representation of data. These features which are deﬁned as the properties of input data are presented as nodes of the last layer of DBN. As a result, in this paper we train a DBN such that it learns to extract image features. The trained DBN distinguishes between ”noise features” and ”clean image features” in the last layer and presents them into two distinct groups of nodes. Furthermore, DBNs are capable of reconstructing the input data based  1  on the values of last layer nodes. Subsequently, if we eliminate the effects of nodes presenting the noise, the reconstructed image will be noiseless. From now on,we called the nodes presenting noise and the nodes presenting image content “noise nodes” and “image nodes”, respectively. The rest of the paper is organized as follows: In Section 2 we brieﬂy describe Deep Learning. In Section 3 we describe the learning process. Section 4 is our experimental results and ﬁnally, we conclude the paper in Section 5.  2 Deep Learning  Restricted Boltzmann Machines (RBMs) are the building blocks of DBNs. Hence, in this section ﬁrst we brieﬂy describe RBMs and then will explore DBNs.  2.1 Restricted Boltzmann Machines  Boltzmann Machines (BMs) and Restricted Boltzmann Machines (RBMs) were introduced in 1980s. But they attracted more attention since 2006 after Hinton et al. paper [8]. He showed that a very powerful neural network can be made by stacking RBMs. RBMs are a kind of Markov Random Fields (MRF) which have a two-layer structure. One layer is called visible and another is called hidden layer. They are restricted to have no visible-visible or hidden-hidden connection and con- nections are inter layer. A graphical depiction of RBM is shown in Figure 1.  Figure 1: Structure of a Restricted Boltzmann Machine.  A joint conﬁguration, (v, h) of visible and hidden units has an energy given by [9]:  E(v, h) = − (cid:88)  aivi − (cid:88)  i ∈visible  j ∈hidden  bjhj −(cid:88)  i,j  vihjwi,j  where vi, hj are the binary states of visible unit i and hidden unit j and ai, bj are their biases and wij is the weight between them. The network assigns a probability to every possible pair of a visible and a hidden vector via this energy function [10]:  e−E(v,h) v,h e−E(v,h)  (cid:80) (cid:80) (cid:80) h e−E(v,h) v,h e−E(v,h)  p(v, h) =  p(v) =  The probability that the network assigns to a visible vector, v , is given by summing over all possible hidden vectors:  ∂ log p(v))  ∂ωi,j  =< vihj >data − < vihj >model  where the angle brackets are used to denote expectations under the distribution species by the sub- script that follows. This leads to a very simple learning rule for performing stochastic steepest ascent in the log probability of the training data:  ∆ωij = (cid:15)(< vihj >data − < vihj >model)  (5)  2  (1)  (2)  (3)  (4)  where (cid:15) is a learning rate. But, exact maximum likelihood learning in this model is intractable because exact computation of the expectation model is very expensive. Hence, in practice, learning is done by following an approximation to the gradient of a different objective function, called the Contrastive Divergence (CD) [11].  2.2 Deep Belief Networks  One of the main problems in training deep networks is how to initialize weights. It is difﬁcult to optimize the weights in nonlinear Deep Networks with multiple hidden layers. With good initial weights, gradient descent works well, but ﬁnding such initial weights requires a very different type of algorithm that learns one layer of features at a time. Hinton et al. [1] introduced a new algorithm to solve the above problem based on the training of a sequence of RBMs. To construct a DBN we train sequentially as many RBMs as the number of hidden layers in the DBN, i.e. for a DBN with h hidden layers we have to train h RBMs. These RBMs are placed one on top of the other. Figure 2 gives an overview of the basic concept. For the ﬁrst RBM, which consists of the DBNs input layer and the ﬁrst hidden layer, the input of RBM is the training set. For the second RBM, which consists of the DBNs ﬁrst and second hidden layers, the input is the output of the previous RBM and so for other RBMs.  Figure 2: Left: Greedy learning a stack of RBMs in which the samples from the lower-level RBM are used as the data for training the next RBM. Right: The corresponding Deep Belief Network [13].  After performing this layer-wise algorithm we have obtained a good initialization for the hidden weights and biases of the DBN and then the DBN is ﬁne-tuned with respect to a typical supervised criterion (such as mean square error or cross-entropy) .  3 Learning  To train a DBN for image denoising, the normalized values of an image pixels are used. Using min-max normalization, grayscale value of a pixel (an integer between 0 and 255) is transformed to a ﬂoating point number between 0 and 1. Unlike ﬁrst and last layer of DBN, other layers have binary nodes. The main idea is to train a DBN such that it learns to map noisy images to images with lower noise or even without noise. The idea can be implemented by learning the behavior of noise and image contents and presenting these behaviors in some nodes at the last layer of the network. The network is trained with a collection consisted of both noisy and noiseless images. We used a criterion called relative activity to detect noise nodes. Relative activity of each node is deﬁned as the difference between two values of a particular node resulted from feeding the network using a noiseless image and its corresponding noisy image (Two images with same contents but one of them  3  is corrupted by noise). As a result, if a particular node is a noise node, it should have higher relative activity. On the other hand, if it is an image node, it should have lower relative activity. This theory is justiﬁed by the fact that the activation of image nodes should be same for both noiseless and its corresponding noisy images. This process is illustrated in Figure 3.  Figure 3: Relative activity of the last layer nodes can be com- puted by subtracting the last nodes values constructed by a noiseless image and its corre- sponding noisy image.  By performing above operation for all images and averaging on the values of each node in the last layer, average relative activity of the last layer nodes is computed. The nodes that still have high average relative activity are considered noise nodes. Now that the noise nodes are discovered, the next step is to lower their activity. Since noise nodes do not change much when clean images are fed to the network, we choose their average value for all clean images as their neutral values (the values which make nodes inactive). Finally, the noise nodes are inactive and consequently, a noiseless image can be reconstructed as it is shown in Figure 4.  Figure 4: Nodes in the last layer of the DBN are present- ing the noise and the contents of the input image. By reduc- ing the noisy nodes (gray nodes in the ﬁgure above) activity, a noiseless image will be created. Right side image is the noisy image and the left side image is noiseless reconstruction.  4 Experimental Results  MNIST dataset is a dataset of handwritten digits consisted of 60,000 images for training and 10,000 images for test. To model natural noise, we added additive white Gaussian noise (AWGN) to images with a variance of 0.20. Therefore, our new dataset was consisted of 120,000 noisy and clean images  4  along with 10,000 noisy images for test (the whole test set is noisy). We used a subset of training set with 20,000 elements for the training phase and the whole test set for the test phase. According to empirical results we created a DBN with 4 hidden layers: 784-1000-500-250-100. We trained this network by 200 batches of data each including 100 images. According to previous discussions we used relative activity to ﬁnd noise nodes in the last layer of the DBN: For all images in the dataset, we put a clean image and then its corresponding noisy image as the input of the network. Afterward, we computed the difference between the last nodes’ values. The average of difference in each node considering all images showed average relative activity of nodes (noisy vs. clean). Based on experimental results, nodes with an average relative activity higher than 0.9 were considered noise nodes. Now for all 10000 clean images in our training set, we compute the values of nodes in the last layer of our trained DBN. The average of these values for each node considered neutral value of node. Finally, to reconstruct a noiseless image from a noisy image, we change the values noise nodes to their neutral values. As a result, noise nodes are inactive and reconstruction would be noiseless. Figure 4 shows how the reconstructed results have lower noise. Also Table 5 shows that a reduction of 65.9% in average Mean Square Error (MSE) was achieved when the proposed method was used for the reconstruction of the noisy images.  Figure 5: From left to right: Noiseless images, Noisy images (AWGN with 0.20 variance), Reconstruction without elimi- nating any noisy node, Recon- struction with eliminating noise nodes. As it is clear, the re- constructed results have much lower noise after eliminating noise nodes.  5  5 Conclusion and Future Works  In this paper, a novel method for image denoising was proposed. The proposed method makes a model to learn noise behavior using a Deep Belief Network (DBN) and tries to present noise and image contents behavior into two distinct groups of nodes. Then by omitting noisy nodes, the network will be able to produce a noiseless (clean) image by reconstruction of the input image. In our work, thresholds for detection of noise nodes were determined manually. Future works will include: 1. Using an automatic technique to determine thresholds for detecting the noisy nodes in denoising technique presented in this paper. 2. Employing our denoising approach to tackle some other issues in Computer Vision, Speech Recognition, etc. These areas will be addressed in future phases of this project.  Data Noisy Image Reconstruction without eliminating any node Reconstruction with eliminating noise node  Mean Square Error (MSE) 0.0966 0.0416 0.0329  Table 1: The table above shows the average MSE for different kinds of reconstructions. A reduction of 65.9% (0.0966 to 0.0329) in average MSE was achieved by reconstruction after eliminating noisy nodes.  6 References  [1] Brigham, E. O., & Morrow, R. E. (1967). The fast Fourier transform. Spectrum, IEEE, 4(12), 63-70. [2] Kervrann, C., & Boulanger, J. (2006). Optimal spatial adaptation for patch-based image denoising. Image Processing, IEEE Transactions on, 15(10), 2866-2878. [3] Pan, Q., Zhang, L., Dai, G., & Zhang, H. (1999). Two denoising methods by wavelet transform. Signal Processing, IEEE Transactions on, 47(12), 3401-3406. [4] Elad, M., & Aharon, M. (2006). Image denoising via sparse and redundant representations over learned dictionaries. Image Processing, IEEE Transactions on, 15(12), 3736-3745. [5] LeCun, Y., Kavukcuoglu, K., & Farabet, C. (2010, May). Convolutional networks and applications in vision. In Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on (pp. 253- 256). IEEE. [6] Xie, Junyuan, Linli Xu, and Enhong Chen. ”Image denoising and inpainting with deep neural networks.” Advances in Neural Information Processing Systems. 2012. [7] Hinton, G. E., & Salakhutdinov, R. R. (2006). Reducing the dimensionality of data with neural networks. Science, 313(5786), 504-507. [8] Hinton, G. E., Osindero, S., & Teh, Y. W. (2006). A fast learning algorithm for deep belief nets. Neural computation, 18(7), 1527-1554. [9] Hinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. Neural computa- tion, 14(8), 1771-1800. [9] Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., & Manzagol, P. A. (2010). Stacked denoising autoen- coders: Learning useful representations in a deep network with a local denoising criterion. The Journal of Machine Learning Research, 9999, 3371-3408. [10] Hinton, G. (2010). A practical guide to training restricted Boltzmann machines. Momentum, 9(1). [11] Salakhutdinov, R. (2009). Learning deep generative models (Doctoral dissertation, University of Toronto).  6  ","Deep Belief Networks which are hierarchical generative models are effectivetools for feature representation and extraction. Furthermore, DBNs can be usedin numerous aspects of Machine Learning such as image denoising. In this paper,we propose a novel method for image denoising which relies on the DBNs' abilityin feature representation. This work is based upon learning of the noisebehavior. Generally, features which are extracted using DBNs are presented asthe values of the last layer nodes. We train a DBN a way that the networktotally distinguishes between nodes presenting noise and nodes presenting imagecontent in the last later of DBN, i.e. the nodes in the last layer of trainedDBN are divided into two distinct groups of nodes. After detecting the nodeswhich are presenting the noise, we are able to make the noise nodes inactiveand reconstruct a noiseless image. In section 4 we explore the results ofapplying this method on the MNIST dataset of handwritten digits which iscorrupted with additive white Gaussian noise (AWGN). A reduction of 65.9% inaverage mean square error (MSE) was achieved when the proposed method was usedfor the reconstruction of the noisy images."
1312.6157,2014,Distinction between features extracted using deep belief networks  ,"['mohammad pezeshki', 'Sajjad Gholami', 'Ahmad Nickabadi']",https://arxiv.org/pdf/1312.6157.pdf,"4 1 0 2     n a J    2      ]  G L . s c [      2 v 7 5 1 6  .  2 1 3 1 : v i X r a  Distinction between features extracted using  Deep Belief Networks  Mohammad Pezeshki  Department of Computer Eng. and IT Amirkabir University of Technology  Tehran, Iran  Sajjad Gholami  Department of Computer Eng. and IT Amirkabir University of Technology  Tehran, Iran  m.pezeshki@aut.ac.ir  s.gholami@aut.ac.ir  Ahmad Nickabadi  Department of Computer Eng. and IT Amirkabir University of Technology  Tehran, Iran  nickabadi@aut.ac.ir  Abstract  Data representation is an important pre-processing step in many machine learning algorithms. There are a number of methods used for this task such as Deep Belief Networks (DBNs) and Discrete Fourier Transforms (DFTs). Since some of the features extracted using automated feature extraction methods may not always be related to a speciﬁc machine learning task, in this paper we propose two methods in order to make a distinction between extracted features based on their relevancy to the task. We applied these two methods to a Deep Belief Network trained for a face recognition task.  1  Introduction  Efﬁciency of many machine learning algorithms depends on the quality of features used for training [1]. There are some automated feature extraction methods such as Principle Component Analysis and Deep Belief Networks. The result of these methods is potentially useful, but there is one issue with these features. It is not always transparent which features will be relevant for a given machine learning task. As a result, it would be a great job to separate extracted features based on their relevant to the task. One of the state-of-the-art tools for feature extraction is Deep Belief Network (DBN). It would be useful if we were able to distinguish between nodes which present different features. For example, in a face recognition task, the main subject of the task is objects of face and side information such are considered as noise. If we use a DBN for feature extraction, it is expected that some nodes in the last layer of the DBN present the face and others present side information. Therefore, if we ﬁnd nodes presenting face singly, obviously the efﬁciency of the face recognition task would be increased signiﬁcantly. In this paper, we propose two methods in order to make a distinction between last layer nodes of a DBN and in particular, examine the ability of a DBN to separate different features and represent them in distinct groups of nodes.  2 Deep Belief Networks  Deep Belief Networks (DBNs) are probabilistic graphical models which have multiple hidden lay- ers. DBN is a mixed directed-undirected model such that all layer are connected with directed  1  links except the top layer which forms an undirected bipartite graph. [Figure 1 (a)]. Hinton et al. introduced a fast greedy layer-wise algorithm which can be used for learning DBNs. [2] DBNs can be constructed by staking multiple bipartite undirected graphical models called Restricted Boltzmann Machines (RBMs). RBM is a Boltzmann Machine which is restricted to have only one hidden layer and one visible layer and also have no visible-visible and hidden-hidden connections [3]. A graphical depiction of an RBM is shown in Figure 2 (b).  Figure 1: (a) Restricted Boltzmann Machine. (b) A stack of RBMs. (c) The corresponding DBN. [4]  3 Proposed Methods  In this section, we will discuss proposed methods to make a distinction between last layer nodes in a DBN.  3.1 Method of Variances  This method based upon the fact that inputs with different aspects (set of features) activate different nodes. Trying this process on some same-aspect inputs should force some nodes to have a signiﬁcant variation against others. If we feed the network with a group of inputs consisting of just one aspect, the values of some particular nodes in the last layer would change signiﬁcantly. Consequently, these nodes would have higher variations. Hence, a statistical criterion such as Variance could be a good tool to distinguish between different kinds of nodes.  3.2 Method of Relative Activities  The second method relies on the concept of relative activity. Relative activity is an indicator for revealing the dependency of last layer nodes of a network to the features of the given input. In this technique, relative activity of nodes can be computed by subtracting the values of top layer nodes for two kinds of inputs. First input consists of only one feature, and second input consists of previous feature alongside another feature.  4 Experimental results  To evaluate the above-mentioned methods, we train a DBN with 4 hidden layers: 2000-1000-500- 100. Training and testing done using the following dataset which consisted three parts:  1. Face images from CMU PIE face database [5], size: 10,000 2. Handwritten digits from MNIST dataset [6], size: 5000 3. Face images corrupted by digit images, size: 5000  2  Some sample inputs are shown in Figure 2.  Figure 2: Sample inputs.  To discover the nodes presenting the face images we applied our two proposed methods in the following ways:  4.1 Using method of Variance  According to method 1, a group of inputs consisting of faces images singly are fed to the network. Now the Variance of nodes is computed and nodes with a Variance higher than 0.1 are considered as nodes which present the face images. Again the DBN is fed with another input consisting of digit images. In the same way, nodes with a Variance upper than 0.1 have a higher activity in comparison with other nodes as shown in Figure 2-a.  4.2 Using method of Relative activities  According to method 2, each mixed image and its corresponding clear digit image are given to the network respectively. Node-from-node difference between last layer nodes for these two images show the relative activity. Finally, the average relative activity for all images are computed and nodes with an average relative activity higher than 0.7 are considered as nodes presenting the face features. This process is illustrated in Figure 2-b.  Figure 3: (a) Different images activates different nodes. (Method of Variances) (b) Relative activity can be computed by subtracting the values of nodes. (Method of Relative activities)  By applying methods mentioned in the preceding paragraphs, we discovered face nodes (the nodes which present faces images). Now when a mixed image is fed to the DBN, all nodes are active. To reconstruct the whole face image which was previously corrupted by a digit, it is necessary to make digit nodes (the nodes which present digit images) inactive. Digit nodes would be inactive, when a neutral value is put instead of their current value. These neutral values can be computed by averaging on the values of these nodes when only face images are fed to the network. Now only the face nodes are used in reconstruction process in practice. The Figure 3 shows how the results of reconstruction process is improved when digit nodes are inactivated.  5 Conclusion  In this paper we focused on the properties of the features extracted using Deep Belief Networks. Obviously, it would be quite useful if we are able to make a distinction between the features extracted  3  Figure 4: (a) Corrupted images which are fed to the network. (b) Reconstructed images without changing the values of digit nodes. (c) Reconstructed images when digit nodes are inactivated.  using a DBN. We proposed two novel methods in order to understand which nodes are presenting which features. In our methods Variance and Relative activity are two criteria to make a distinction between nodes. We evaluated these methods on a data set consisting of MNIST handwritten digits and CMU PIE faces databases.  References  [1] Bengio, Y., Courville, A. C., & Vincent, P. Unsupervised feature learning and deep learning: A review and new perspectives. CoRR abs/1206.5538 (2012) [2] Hinton, G. E., Osindero, S., & Teh, Y. W. A fast learning algorithm for deep belief nets. Neural computation, 18(7), 1527-1554 (2006) [3] Hinton, G. A practical guide to training restricted Boltzmann machines. Momentum, 9(1) (2010) [4] Salakhutdinov, R. Learning deep generative models (Doctoral dissertation, University of Toronto) (2009) [5] Sim, T., Baker, S., & Bsat, M. The CMU pose, illumination, and expression database. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 25(12), 1615-1618 (2003) [6] LeCun, Y., & Cortes, C. The MNIST database of handwritten digits (1998)  4  ","Data representation is an important pre-processing step in many machinelearning algorithms. There are a number of methods used for this task such asDeep Belief Networks (DBNs) and Discrete Fourier Transforms (DFTs). Since someof the features extracted using automated feature extraction methods may notalways be related to a specific machine learning task, in this paper we proposetwo methods in order to make a distinction between extracted features based ontheir relevancy to the task. We applied these two methods to a Deep BeliefNetwork trained for a face recognition task."
1312.6171,2014,Learning Paired-associate Images with An Unsupervised Deep Learning Architecture  ,"['Ti Wang', 'Daniel L. Silver']",https://arxiv.org/pdf/1312.6171.pdf,"4 1 0 2     n a J    0 1      ] E N . s c [      2 v 1 7 1 6  .  2 1 3 1 : v i X r a  Learning Paired-associate Images with An Unsupervised Deep Learning Architecture  Ti Wang and Daniel L. Silver  Jodrey School of Computer Science  Acadia University  Wolfville, NS, Canada B4P 2R6 danny.silver@acadiau.ca  Abstract  This paper presents an unsupervised multi-modal learning system that learns as- sociative representation from two input modalities, or channels, such that input on one channel will correctly generate the associated response at the other and vice versa. In this way, the system develops a kind of supervised classiﬁcation model meant to simulate aspects of human associative memory. The system uses a deep learning architecture (DLA) composed of two input/output channels formed from stacked Restricted Boltzmann Machines (RBM) and an associative memory net- work that combines the two channels. The DLA is trained on pairs of MNIST handwritten digit images to develop hierarchical features and associative repre- sentations that are able to reconstruct one image given its paired-associate. Ex- periments show that the multi-modal learning system generates models that are as accurate as back-propagation networks but with the advantage of a bi-directional network and unsupervised learning from either paired or non-paired training ex- amples.  1  Introduction  Humans learn knowledge from the environment by data that is provided in several forms, or modal- ities, such as audio and visual signals. Psychologists deﬁne multi-modal learning as learning new knowledge from multiple sensory modalities [11]. Researchers have shown that people’s under- standing of new concepts is enhanced with mixed-modality knowledge representations [10]. The human brain has adapted to fuse associated sensory signals so as to learn more effectively and ef- ﬁciently. The long-term goal of this research is to develop a learning system that simulates aspects of the multi-modal learning ability of humans. In particular, we investigate unsupervised learning methods that can create a model capable of generalization and classiﬁcation from one input or output modality to another (eg. from visual to verbal). We are interested in how this can be done without resorting to any form of supervised learning that suffers from the need for labeled examples. Deep learning is a sub-area of machine learning, which typically uses Restricted Boltzmann Ma- chines (RBM), a type of stochastic associative artiﬁcial neural network (ANN), to develop a multi- layer generative models [6]. Deep learning architectures, or DLA, provide an exciting new substrate upon which to explore new computational and representational models of how knowledge can be acquired, consolidated and used [1]. Prior work has investigated the use of DLAs and unsupervised learning methods to develop models for a variety of purposes including auto-associative memory, pattern completion, and clustering as well as generalization and classiﬁcation [8]. This paper takes a ﬁrst step toward developing a multi-modal learning system by examining a DLA that is capable of learning paired-associate images at two input modalities (channels). The DLA must reconstruct the matching image at channel A when it observes it’s paired image at channel  1  B, and vice versa. By doing so the system uses unsupervised learning to develop an associative memory model that performs a form of classiﬁcation from one channel to another. Additionally, this DLA can learn not only paired-associate examples, but also non-paired independent examples at each sensory modality. Experimentation shows quantitatively and qualitatively that the system generates models that accurately generates associated images as compared to models developed using traditional supervised back-propagation networks.  2 Background  Artiﬁcial neural networks (ANN) are widely used to solve classiﬁcation problems such as image and speech recognition, however many do not work in the same fashion as the human nervous system. For example, back-propagation ANNs are good for modeling complex mapping relations between input and output data, but are not as good for reconstructing, or recalling a pattern. Humans have the ability to recover complete information from partial information; this is referred to as associative memory [4]. When a child watches a tennis game, he or she learns the appearance of the tennis ball and the racket. Next time when the child sees a picture of a tennis ball, the child may recall an image of a racket and of the game. Associations are clearly a major part of learning about the world. Associative ANNs are inspired by cognitive psychology and are designed to mimic the way that collections of biological neurons may store and recall associative memories [12]. Geoffrey Hinton, University of Toronto, advocates using Boltzmann Machine associative networks to simulating hu- man brain structure. After a Boltzmann Machine has been trained on a set of patterns, it has the ability to reconstruct any one of those patterns from a partial or noisy pattern. However, learning is slow in large Boltzmann Machines because of the many weights in a fully connected network and the iterative sampling of node activities required for each weight update.  2.1 Restricted Boltzmann Machine  i  j  i  1  1  wij vi)  wij hj )  (cid:80)  j bjhj −(cid:80)  1+exp(−bi−(cid:80)  1+exp(−bj−(cid:80)  −(cid:80) i bivi −(cid:80)  A Restricted Boltzmann Machine (RBM) is a variant of a BM that is meant to overcome long training times by limiting the number of connections in its network and using a modiﬁed learning algorithm. RBMs have both visible and hidden layers of neurons just like BMs, however there are no intra- layer connections, so they can be characterized as a bipartite graph (see Figure 1) [8]. When settling to equilibrium, neuron hj turns on with the probability pj = , and neuron . The states vi, hj of neuron i and vi turns on with the probability pi = j keep changing with probabilities pi and pj. The system computes the activation energy E = j vihjwij where bi and bj are the bias terms for their respective nodes [9]. The global energy E will be reduced more quickly in an RBM compared to a BM because of the reduced number of connections. The goal of training is to modify the weights of the network to establish low energy states that correspond with training patterns at the visible nodes. Similar input patterns will have energy states closer to each other, whereas two orthogonal patterns (e.g. patterns that share few common pixels) will have energy states more distant from each other. The method of weight update we use for this research is called Contrastive Divergence, or CD [8]. The weights of the network are initialized to small random values. When training data xi is given to the visible neuron vi, the RBM clamps the states of visible neurons and frees the states of hidden binary neuron hj (see Figure 1). Each weight wij of the RBM is updated as per the following formula ∆wij = η(< vihj >0 − < vihj >1), where η is the learning rate, < vihj > is the expectation over all possible pairs of visible and hidden node values, and the 0 and 1 superscripts indicate the expectation based on the training example and its reconstruction, respectively. This equation approximates the gradient of the log probability of a training example with respect to a weight. Weight wij is updated until the global energy E reduces below a threshold. With probability pi, neuron i will then reconstruct the input data xi. After training, the hidden layer weights of the RBM will have learned the feature distribution of the input space, that is wij is equal to the probability of feature hj given input vi. To test its ability to recall a pattern, the RBM is presented with all or some of the inputs xi of a test example at its visible units vi. These cause activations at each of the hidden units hj as described  2  Figure 1: RBM Training Process  Figure 2: Stacking Multi-level RBMs  above, and then the visible units are freed to generate new activations. If training has been successful, the reconstructed outputs at vi are close to the complete pattern of the original test example.  2.2 Deep Learning Architectures  Humans tend to organize ideas and concepts hierarchically [5]. Abstract concepts are learned and re- called through the composition of simpler concepts [1]. This approach makes sense in a world where most objects are made from parts which are in turn composed of smaller features. For instance, a car is a combination of smaller parts like wheels and a frame. And a wheel is made up of smaller features like a tire and a rim. Neuroscience studies have conﬁrmed that this compositional structure can be seen in the human nervous system. The mammalian brain uses a deep learning architecture with multiple levels of abstraction corresponding to different areas of the neocortex [14]. Deep learning architectures, or DLA, is a sub-area of machine learning that places heavy emphasis on hierarchical composition and unsupervised learning methods. DLAs can be developed by stack- ing layers of RBMs one on top of another [8]. They have been successfully used to develop models for recognizing hand-writing images of digits in a manner that simulates the human visual cortex [6] RBM-based DLA systems are capable of doing unsupervised clustering of unlabeled data based on a hierarchy of features. As shown in Figure 2, the hidden layer of one RBM can be used as the input layer for a higher level RBM [1]. The highest level features can be used to achieve classiﬁcation, if so desired. Subsequently, researchers feel that DLAs develop a hierarchy of features in a fashion similar to the mammalian brain. DLAs present a new way at looking at systems that learn. Deep architectures can be used as an auto- encoder to model high-dimensional data, such as images and audio [3]. Bengio reports that deep architectures are more expressive than shallow ones by analyzing the depth-breadth trade-off of ar- chitecture representation [2]. Perhaps most importantly, deep learning methods learn representative hierarchies directly from the data [1]. This is in contrast to approaches such as convolutional net- works that use receptive ﬁelds and modiﬁed back-propagation methods that rely heavily on known topological characteristics of the input space [13].  3 Multi-modal Learning Using an Unsupervised DLA  The objective of this research is to develop a learning system that can memorize and recall multi- channel data using an associative memory network. The learning system should be able to recall the pattern from the associative network on one sensory modality given data on another sensory modality. The long-term goal of our research is to create a system that can learn concepts using two or more sensory/motor modalities, such as audio, optical, and vocal (see Figure 3).  3.1 Learning Paired-Associate Images  Consider the problem of learning paired-associate images at two input modalities (channels). We propose to use a DLA network that, after training, will be able to generate a paired image on one channel when prompted with an image on another channel. The process is meant to simulate human sensory modalities and associative memory, and to provide insights into how classiﬁcation can be  3  Figure 3: Multi-modal data learning system  Figure 4: Two channels DLA  done using an unsupervised learning approach. The learning system is composed of two major parts, a associative memory network and two associative sensory channel networks (see Figure 4). The sensory channel networks are designed for the recognition and reconstruction of sensory data. The associative memory network ties the sensory channel networks together and simulates the human associative memory. Both parts can be built using RBMs. Because of its reduced representation, the recall capacity of an RBM is not as high as a fully- connected BM. We have determined that an RBM is unable to recall patterns when only half of the visible neurons are given correct pattern values [16]. Thus when an RBM is used as the top associative memory network, additional steps are required after the CD algorithm has completed training. As per Hinton, the weights of the network require ﬁne tuning [6]. To produce appropriate features at the top layer, the weights of the RBM model need to be ﬁne- tuned. However, ﬁne-tuning the bi-directional weights of the RBM may destroy their ability to generate lower level features. To protect the accuracy of the generative model, it is necessary to untie the weights between the top layer of each channel and the associative memory network layer and create two sets of weights - recognition weights and generative weights (see Figure 5) [7, 8]. The recognition weights are used in the bottom-up pass which receives an input pattern and the generative weights are used in the top-down pass to reconstruct an output pattern. The generative weights are left as trained by the RBM. The recognition weights are ﬁne-tuned using a back-ﬁtting algorithm, such that the associative memory network can generate a relatively accurate full set of associative memory features with only input from one channel. To ﬁne-tune channel 1, the recognition weights wij, where i is a neuron in hidden layer 2 and j is a neuron in hidden layer 3, are used as the initial weight values for a gradient descent regression over all paired patterns. For each training pattern, the posterior probabilities {pi} of hidden layer 2 are used as the input attribute, and the posterior probabilities {pj} of hidden layer 3 are used as the target output. A new set of posterior probabilities {p(cid:48) j} for hidden layer 3 are computed using p(cid:48) , and the weights are updated using gradient descent to minimize the error between {pj} and {p(cid:48) j}. In this way the recognition weights which pass the input signal from sensory channel 1 to the associative memory network are ﬁne-tuned to generate a full set of associative memory features which channel 2 can use to generate the appropriate output. With back-ﬁtting, the multi-modal DLA should be able to achieve the learning goal that was previ- ously done with supervised learning by Srivastava [15]. Without supervised learning between the two channels, the performance of the DLA is unlikely to exceed that of a traditional BP ANN ap- proach; however, we do expect it to do as well. The hierarchical feature learning of the sensory channels and the back-ﬁtting of the recognition weights are expected to make up for the shortcom- ings of purely unsupervised learning approach that we are taking.  1+exp(−(cid:80)  j =  wij pi)  1  i  4  Figure 5: Untieing the weights  Figure 6: BP ANN in Experiment 1  3.2  Impact of Learning Non-paired Patterns  Sensory data does not always come in pairs in real life. For example, one can see a cat meowing, see an image of a cat, or hear meowing without seeing a cat. In this case, the sound “meow” is the audio signal and the image of the cat is the visual signal. These two sensory channels can come together to allow paired-associate learning, but their individual channel representations can be learned and improved upon separately. We propose that learning each sensory modality with non-paired examples will help to improve the associative memories ability to generate the correct image on one channel when given its paired-associate on the other. It would be informative to have an experiment to test the impact on the multi-channel learning system by separately training the sensory channels with non-paired examples.  4 Empirical Studies  Three empirical studies were carried out using two different data sets. The ﬁrst and third experiments used paired images from the MNIST dataset of handwritten numeric digits. The second experiment used paired images from a synthetic dataset of numeric digits. In all experiments, ﬁve pairs of odd and even digits were associated with each: 1-2, 3-4, 5-6, 7-8, 9-0.  4.1 Experiment 1  Objective: The objective of this experiment is to compare the unsupervised DLA with a supervised BP ANN approach to learning paired-associate images. Each learning system is trained such that when a handwritten digit image is provided, the system will generate its paired digit image. Material and Methods: This experiment uses a dataset of paired MNIST handwritten digits as the learning domain. The experiment is repeated four times with different training sets, validation sets and test sets. Each of these datasets contains 1,000 paired-associate examples that are randomly selected from the MNIST dataset. A deep learning architecture of RBMs is used to develop an unsupervised learning model for the problem. The architecture is in accord with Figure 4. Each channel network is composed of two RBM layers, each of which contains 500 hidden neurons. Hidden layers 1 and 1’ and then layers 2 and 2’ will develop more abstract features of the original images [8]. The associative top layer contains 1,000 neurons. The unsupervised DLA uses back-ﬁtting to ﬁne-tune the weights of the associative top layer after the CD algorithm training is ﬁnished. When training the DLAs, the training process of each sensory channel stops when the maximum iteration of 60 is reached, and the associative memory network is trained to 100 iterations. Validation sets are used to monitor the back-ﬁtting to avoid over-ﬁtting. The odd digit part of a test example is used to test the reconstruction of its corresponding even digit image, and vice versa.  5  DLA  BP ANNs  1→2 95.25 98.0  2→1 95.88 72.5  3→4 82.63 83.75  4→3 94.63 95.13  5→6 92.38 90.38  6→5 88.75 82.88  7→8 90.5 91.13  8→7 79.75 82.88  9→0 91.63 89.0  0→9 93 92.5  Avg 90.74 88.82  Table 1: Accuracy of test set reconstruction (%)  Figure 7: Examples of reconstruction results with the DLA and BP ANNs  We developed two BP networks to learn the same paired-associate mapping. One network is trained to map odd digit images to even digits, the other vice versa. Both BP networks use the architecture shown in Figure 6. The BP networks use the same training set, validation set and testing set as the DLA. The validation set is used to prevent the BP algorithm from over-ﬁtting to the training set. The accuracy of reconstruction is measured by testing the output images using Hinton’s DLA hand- written digits classiﬁcation software. This software is known to classify MNIST dataset of handwrit- ten digits with only 1.15% errors [8]. One can pass the input images and the reconstructed images through Hinton’s classiﬁer to determine their digit category. The accuracy of the models is then based on the number of correctly paired images. Results and Discussion: Using Hinton’s software, the reconstruction accuracy was checked on the testing set. The average results of four replications of the experiments are shown in Table 1. On average, the unsupervised DLA (model 1) generated images that were 90.74% accurate, and the BP ANNs (model 2) generated images that were 88.82% accurate. One can see that the two models did equally well. This suggests that the unsupervised DLA models are able to achieve the same level of accuracy as the supervised BP approach. Figure 7 shows examples of reconstructed images produced by the DLAs and the BP ANNs. One can see that the images generated by the DLAs are clearer than those generated by the BP ANNs. We suspect this because the DLA models are able to better differentiate features from noise. This will be investigated further in the next experiment.  4.2 Experiment 2  Objective: The objective of this experiment is to develop auto-associative models that can over- come noise injected into synthetic training examples. An unsupervised DLA with back-ﬁtting and supervised BP ANNs will be developed from a noisy dataset, and the quality of their regenerated images will be compared. Material and Methods: This experiment uses a synthetic dataset that contains ﬁve different sets of 10 x 5 paired images from Figure 8. 10% random noise was added to each template image to produce 60 instances of each category, or 300 in total. The ﬁrst 100 of these images are used as a training set, the next 100 are used as a validation set, while the remaining 100 are used as a test set.  Figure 8: Templates of the synthetic dataset  A DLA architecture, in accord with the previous experiment, is used to develop an unsupervised learning model. Each of the sensory channel layers contains 50 hidden neurons, and the associative  6  DLA  BP ANNs  1→2 0.012 0.162  2→1 0.071 0.216  3→4 0.046 0.209  4→3 0.029 0.081  5→6 0.004 0.06  6→5 0.01 0.115  7→8 0.008 0.135  8→7 0.0 0.165  9→0 0.04 0.11  0→9 0.015 0.106  Avg 0.032 0.144  Table 2: RMSE of test set reconstruction (out of 1)  Figure 9: Examples of reconstruction results with DLA and BP ANNs  top layer contains 100 neurons. The training process of the sensory channel networks stops when the maximum iteration of 60 is reached; the associative memory network trains for 100 iterations. As in Experiment 1, two BP networks were developed to learn the same paired-associate mapping. Both BP networks used an architecture similar to that shown in Figure 6 with 50 neurons in layers 1 and 3 and 100 neurons in layer 2. The BP networks uses the same training set, validation set and test set as the DLA. The accuracy of reconstruction was measured by comparing the similarity between the generated images and their corresponding template images for a set of test examples. The RMSE between the pixels of each reconstructed image and its corresponding template (without noise) was computed to give an average error over all examples (image pixels are normalized to the range [0,1]). Results and Discussion: The RMSE of the reconstructed images for the test set is shown in Table 2. The DLA with back-ﬁtting out-performs the BP networks in generating the images in the presence of noise. Figure 9 shows examples of reconstructed images from the DLA and the BP ANNs. The generated images from the DLA are quite similar to the template images of Figure 8, while there is signiﬁcant noise on the generated images from the BP network. DLAs attempt to probabilistically differentiate features from noises, whereas BP ANNs attempt to map input pixels to output pixels. Features are formed in BP networks, but they are for the purpose of mapping and not reconstruction of the original images. Hence a DLA is a better choice if the objective is to construct a noiseless category example as a form of classiﬁcation.  4.3 Experiment 3  Objective: The preceeding experiments used paired-associate examples to develop neural network models, however, sensory data does not always come in pairs in real life. The objective of this experiment, in accord with Section 3.2, is to develop an associative learning system with both paired associative examples and independent non-paired examples. The experiment is designed to test if the performance of an associative learning system can be improved by separately training the sensory channels with non-paired examples. Material and Methods: This experiment uses the database of MNIST examples as in Experiment 1. The experiment is repeated four times with different training sets, validation sets and test sets. For each repetition, four models are built using the same architecture but with different amounts of training examples. The ﬁrst model is built with 100 paired-associate examples. The second model is built with 100 paired- associate examples, and 100 non-paired examples of even digit images. The third model is built with 100 paired-associate examples, 100 non-paired examples of even digit images, and 100 non-paired examples of odd digit images. The last model is built with 200 paired-associate examples. Figure 10  7  Figure 10: The number of examples used to train four models  Figure 11: Accuracy comparison between four models  shows the number of paired and non-paired examples in each training set. All the odd digits images are used to train the odd channel and all the even digits are used to train the even channel, but only the paired-associate examples are used to develop the associative memory. The four models use the same 3-layered architecture, parameters, validation sets and test sets as in Experiment 1. While doing back-ﬁtting, validation sets are used to monitor overﬁtting. Test sets are used to examine the associative learning performance of the learning system. The odd digits are used to test the recall of even digits, and vice versa. The recalled images are classiﬁed by Hinton’s classiﬁer to examine the accuracy of the models. Results and Discussion: The performance (averaged over four repetitions) of the four models at recalling even digits from odd digits, odd digits from even digits, and the average of them are shown in Figure 11; the error bars represent the 95% conﬁdence over the repeated studies. The mean accuracy increases marginally (the error bars show that the improvements are not signiﬁcant) from model 1 to model 3, which means that using non-paired examples to better develop one of the channels representation may im- prove the overall performance of an associative learning system. We conjecture that this is because both the recognition weights and the generative weights of this channel are optimized. Improving the recognition weight performance of the odd digits channel will provide better features to the asso- ciative memory network to generate the corresponding even digits. Better generative weights for the odd digits channel will generate more accurate odd digits when even digits are provided. In general, this result suggests that improving one of the sensory channel networks of a multi-channel learning system which contains more than two channels will improve any recall that involves that channel. It is also important to note that the reconstruction accuracy clearly increases from model 3 to model 4. This demonstrates that using more paired-associate examples to develop the associative memory network can improve the performance of the system over the equivalent number of non-paired ex- amples. In a system with three or more channels we conjecture that paired-associate examples for any two channels will be of beneﬁt to the entire associative memory network.  5 Conclusion  This paper presents recent work on an unsupervised multi-modal learning system that can develop an associative memory structure that combines two input/output channels. Our long-term goal is to develop learning systems that are able to learn conceptual representations from multiple sensory input and/or motor output modalities in a manner similar to humans. We have demonstrated an unsupervised deep learning architecture (DLA) that can reconstruct an image of a MNIST handwritten digit from another paired handwritten digit. The system develops a kind of supervised classiﬁcation model meant to simulate aspects of human associative mem-  8  ory. The DLA is formed with stacked Restricted Boltzmann Machines (RBM) and trained with the Contrastive Divergence (CD) algorithm. The RBM associative memory network that ties the input/output channels together requires reﬁnement using a back-ﬁtting technique to increase the recall accuracy when only 50% of its visible neurons are available from one channel. Experimenta- tion shows quantitatively (using an independent classiﬁcation method) and qualitatively (by viewing the generated images) that the system develops models that are able to reconstruct accurate paired images as compared to supervised back-propagation network models and have the advantage of unsupervised learning from either paired or non-paired training examples. In future work, different types of sensory data will be used to train the multi-modal learning sys- tem, such as audio signals. Furthermore, we are interested in knowledge transfer in DLAs using unsupervised methods for learning new tasks and new modalities.  References [1] Yoshua Bengio. Learning deep architectures for ai. Found. Trends Mach. Learn., 2(1):1–127, January  2009.  [2] Yoshua Bengio and Yann Lecun. Scaling learning algorithms towards AI. MIT Press, 2007. [3] Li Deng, Michael L. Seltzer, Dong Yu, Alex Acero, Abdel rahman Mohamed, and Geoffrey E. Hinton. Binary coding of speech spectrograms using a deep auto-encoder. In Takao Kobayashi, Keikichi Hirose, and Satoshi Nakamura, editors, Interspeech, pages 1692–1695. ISCA, 2010.  [4] Richard J. Gerrig and Philip G. Zimbardo. Psychology and Life. MyPsychLab Series. Pearson/Allen and  Bacon, 2007.  [5] Stephan Gouws. Deep unsupervised feature learning for natural language processing. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop, NAACL HLT ’12, pages 48–53, Strouds- burg, PA, USA, 2012. Association for Computational Linguistics.  [6] Geoffrey E. Hinton. Learning multiple layers of representation. Trends in Cognitive Sciences, 11:428–  434, 2007.  [7] Geoffrey E. Hinton, Peter Dayan, Brendan J. Frey, and Radford M. Neal. The wake-sleep algorithm for  unsupervised neural networks. Science, 268(5214):1158–1161, 1995.  [8] Geoffrey E. Hinton and Simon Osindero. A fast learning algorithm for deep belief nets. Neural Compu-  tation, 18:2006, 2006.  [9] Geoffrey E. Hinton and Terrence J. Sejnowski. Parallel distributed processing: explorations in the mi- crostructure of cognition, vol. 1. chapter Learning and relearning in Boltzmann machines, pages 282–317. MIT Press, Cambridge, MA, USA, 1986.  [10] Richard.E. Mayer. Multimedia Learning. Cambridge University Press, 2009. [11] Allan. Paivio. Mental representations. Oxford University Press, Incorporated, 1990. [12] G. Nther Palm. Neural associative memories and sparse coding. Neural Netw., 37:165–171, January 2013. [13] Marc’Aurelio Ranzato, Y lan Boureau, and Yann Lecun. Sparse feature learning for deep belief networks.  In NIPS-2007, 2007.  [14] Thomas Serre, Gabriel Kreiman, Minjoon Kouh, Charles Cadieu, Ulf Knoblich, and Tomaso Poggio. A  quantitative theory of immediate visual recognition. Prog Brain Res, pages 33–56, 2007.  [15] Nitish Srivastava and Ruslan Salakhutdinov. Multimodal learning with deep boltzmann machines.  In P. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 2231–2239. 2012.  [16] Ti Wang. Classiﬁcation Via Reconstruction Using A Multi-Channel Deep Learning Architecture. Masters  Thesis, Acadia University, Wolfvillle, NS, Canada, 2013.  9  ","This paper presents an unsupervised multi-modal learning system that learnsassociative representation from two input modalities, or channels, such thatinput on one channel will correctly generate the associated response at theother and vice versa. In this way, the system develops a kind of supervisedclassification model meant to simulate aspects of human associative memory. Thesystem uses a deep learning architecture (DLA) composed of two input/outputchannels formed from stacked Restricted Boltzmann Machines (RBM) and anassociative memory network that combines the two channels. The DLA is trainedon pairs of MNIST handwritten digit images to develop hierarchical features andassociative representations that are able to reconstruct one image given itspaired-associate. Experiments show that the multi-modal learning systemgenerates models that are as accurate as back-propagation networks but with theadvantage of a bi-directional network and unsupervised learning from eitherpaired or non-paired training examples."
1312.6173,2014,A Simple Model for Learning Multilingual Compositional Semantics  ,"['Karl Moritz Hermann', 'Phil Blunsom']",https://arxiv.org/pdf/1312.6173.pdf,"4 1 0 2    r a     M 0 2      ] L C . s c [      4 v 3 7 1 6  .  2 1 3 1 : v i X r a  Multilingual Distributed Representations without  Word Alignment  Karl Moritz Hermann and Phil Blunsom  Department of Computer Science  University of Oxford Oxford, OX1 3QD, UK  {karl.moritz.hermann,phil.blunsom}@cs.ox.ac.uk  Abstract  Distributed representations of meaning are a natural way to encode covariance relationships between words and phrases in NLP. By overcoming data sparsity problems, as well as providing information about semantic relatedness which is not available in discrete representations, distributed representations have proven useful in many NLP tasks. Recent work has shown how compositional semantic representations can successfully be applied to a number of monolingual applica- tions such as sentiment analysis. At the same time, there has been some initial success in work on learning shared word-level representations across languages. We combine these two approaches by proposing a method for learning distributed representations in a multilingual setup. Our model learns to assign similar embed- dings to aligned sentences and dissimilar ones to sentence which are not aligned while not requiring word alignments. We show that our representations are seman- tically informative and apply them to a cross-lingual document classiﬁcation task where we outperform the previous state of the art. Further, by employing parallel corpora of multiple language pairs we ﬁnd that our model learns representations that capture semantic relationships across languages for which no parallel data was used.  1  Introduction  Distributed representations of words are increasingly being used to achieve high levels of generalisa- tion within language modelling tasks. Successful applications of this approach include word-sense disambiguation, word similarity and synonym detection (e.g. [10, 27]). Subsequent work has also attempted to learn distributed semantics of larger structures, allowing us to apply distributed rep- resentation to tasks such as sentiment analysis or paraphrase detection (i.a. [1, 3, 12, 14, 21, 25]). At the same time a second strand of work has focused on transferring linguistic knowledge across languages, and particularly from English into low-resource languages, by means of distributed rep- resentations at the word level [13, 16]. Currently, work on compositional semantic representations focuses on monolingual data while the cross-lingual work focuses on word level representations only. However, it appears logical that these two strands of work should be combined as there exists a plethora of parallel corpora with aligned data at the sentence level or beyond which could be exploited in such work. Further, sentence aligned data provides a plausible concept of semantic similarity, which can be harder to deﬁne at the word level. Consider the case of alignment between a German compound noun (e.g. “Schwerlastverkehr”) and its English equivalent (“heavy goods vehicle trafﬁc”). Semantic alignment at the phrase level here appears far more plausible than aligning individual tokens for semantic transfer.  1  Using this rationale, and building on both work related to learning cross-lingual embeddings as well as to compositional semantic representations, we introduce a model that learns cross-lingual em- beddings at the sentence level. In the following section we will brieﬂy discuss prior work in these two ﬁelds before going on to describe the bilingual training signal that we developed for learning multilingual compositional embeddings. Subsequently, we will describe our model in greater detail as well as its training procedure and experimental setup. Finally, we perform a number of evalua- tions and demonstrate that our training signal allows a very simple compositional vector model to outperform the state of the art on a task designed to evaluate its ability to transfer semantic informa- tion across languages. Unlike other work in this area, our model does not require word aligned data. In fact, while we evaluate our model on sentence aligned data in this paper, there is no theoretical requirement for this and technically our algorithm could also be applied to document-level parallel data or even comparable data only.  2 Models of Compositional Distributed Semantics  In the case of representing individual words as vectors, the distributional account of semantics pro- vides a plausible explanation of what is encoded in a word vector. This follows the idea that the meaning of a word can be determined by “the company it keeps” [11], that is by the context it ap- pears in. Such context can easily be encoded in vectors using collocational methods, and is also underlying other methods of learning word embeddings [7, 20]. For a number of important problems, semantic representations of individual words do not sufﬁce, but instead a semantic representation of a larger structure—e.g. a phrase or a sentence—is required. This was highlighted in [10], who proposed a mechanism for modifying a word’s representation based on its individual context. The distributional account of semantics can, due to sparsity, not be applied to such larger linguistic units. A notable exception perhaps is Baroni and Zamparelli [1], who learned distributional representations for adjective noun pairs using a collocational approach on a corpus of unprecedented size. The bigram representations learned from that corpus were subsequently used to learn lexicalised composition functions for the constituent words. Most alternative attempts to extract such higher-level semantic representations have focused on learning composition functions that represent the semantics of a larger structure as a function of the representations of its parts. [21] provides an evaluation of a number of simple composition func- tions applied to bigrams. Applied recursively, such approaches can then easily be reconciled with the co-occurrence based word level representations. There are a number of proposals motivating such recursive or deep composition models. Notably, [3] propose a tensor-based model for semantic composition and, similarly, [4] develop a framework for semantic composition by combining dis- tributional theory with pregroup grammars. The latter framework was empirically evaluated and supported by the results in [12]. More recently, various forms of recursive neural networks have successfully been used for semantic composition and related tasks such as sentiment analysis. Such models include recursive autoencoders [24], matrix-vector recursive neural networks [25], untied recursive neural networks [14] or convolutional networks [15].  2.1 Multilingual Embeddings  Much research has been devoted to the task of inducing distributed semantic representations for single languages. In particular English, with its large number of annotated resources, has enjoyed most attention. Recently, progress has been made at representation learning for languages with fewer available resources. Klementiev et al. [16] described a form of multitask learning on word-aligned parallel data to transfer embeddings from one language to another. Earlier work, Haghighi et al. [13], proposed a method for inducing cross-lingual lexica using monolingual feature representations and a small initial lexicon to bootstrap with. This approach has recently been extended by [18, 19], who developed a method for learning transformation matrices to convert semantic vectors of one language into those of another. Is was demonstrated that this approach can be applied to improve tasks related to machine translation. Their CBOW model is also worth noting for its similarities to the composition function used in this paper. Using a slightly different approach, [29], also learned bilingual embeddings for machine translation. It is important to note that, unlike our proposed system, all of these methods require word aligned parallel data for training.  2  Two recent workshop papers deserve mention in this respect. Both Lauly et al. [17] and Sarath Chan- dar et al. [23] propose methods for learning word embeddings by exploiting bilingual data, not unlike the method proposed in this paper. Instead of the noise-contrastive method developed in this paper, both groups of authors make use of autoencoders to encode monolingual representations and to support the bilingual transfer. So far almost all of this work has been focused on learning multilingual representations at the word level. As distributed representations of larger expressions have been shown to be highly useful for a number of tasks, it seems to be a natural next step to also attempt to induce these using cross-lingual data. This paper provides a ﬁrst step in that direction.  3 Model Description  Language acquisition in humans is widely seen as grounded in sensory-motor experience [22, 2]. Based on this idea, there have been some attempts at using multi-modal data for learning better vector representations of words (e.g. [26]). Such methods, however, are not easily scalable across languages or to large amounts of data for which no secondary or tertiary representation might exist. We abstract the underlying principle one step further and attempt to learn semantics from multi- lingual data. The idea is that, given enough parallel data, a shared representation would be forced to capture the common elements between sentences from different languages. What two parallel sentences have in common, of course, is the semantics of those two sentences. Using this data, we propose a novel method for learning vector representations at the word level and beyond.  3.1 Bilingual Signal  Exploiting the semantic similarity of parallel sentences across languages, we can deﬁne a simple bilingual (and trivially multilingual) error function as follows: Given a compositional sentence model (CVM) MA, which maps a sentence to a vector, we can train a second CVM MB using a corpus CA,B of parallel data from the language pair A, B. For each pair of parallel sentences (a, b) ∈ CA,B, we attempt to minimize  Edist(a, b) = (cid:107)aroot − broot(cid:107)2  (1)  where aroot is the vector representing sentence a and broot the vector representing sentence b.  3.2 The BICVM Model  A CVM learns semantic representations of larger syntactic units given the semantic representations of their constituents. We assume individual words to be represented by vectors (x ∈ Rd). Previous methods employ binary parse trees on the data (e.g. [14, 25]) and use weighted or multi- plicative composition functions. Under such a setup, where each node in the tree is terminal or has two children (p → c0, c1), a binary composition function could take the following form:  p = g (W e[c0; c1] + be)  (2) where [c0; c1] is the concatenation of the two child vectors, W e ∈ Rd×2d and be ∈ Rd the encod- ing matrix and bias, respectively, and g an element-wise activation function such as the hyperbolic tangent. For the purposes of evaluation the bilingual signal proposed above, we simplify this com- position function by setting all weight matrices to the identity and all biases to zero. Thereby the CVM reduces to a simple additive composition function:  aroot =  ai  (3)  Of course, this is a very simpliﬁed CVM, as such a bag-of-words approach no longer accounts for word ordering and other effects which a more complex CVM might capture. However, for the purposes of this evaluation (and with the experimental evaluation in mind), such a simplistic composition function should be sufﬁcient to evaluate the novel objective function proposed here.  i=0  3  |a|(cid:88)  Figure 1: Description of a bilingual model with parallel input sentences a and b. The objective function of this model is to minimize the distance between the sentence level encoding of the bi- text. Principally any composition function can be used to generate the compositional sentence level representations. The composition function is represented by the CVM boxes in the diagram above.  Using this additive CVM we want to optimize the bilingual error signal deﬁned above (Eq. 1). For the moment, assume that MA is a perfectly trained CVM such that aroot represents the semantics of the sentence a. Further, due to the use of parallel data, we know that a and b are semantically equivalent. Hence we transfer the semantic knowledge contained in MA onto MB, by learning θMB to minimize:  Ebi(CA,B) =  Edist(a, b)  (4)  (cid:88)  (a,b)∈CA,B  Of course, this objective function assumes a fully trained model which we do not have at this stage. While this can be a useful objective for transferring linguistic knowledge into low-resource lan- guages [16], this precondition is not helpful when there is no model to learn from in ﬁrst place. We resolve this issue by jointly training both models MA and MB. Applying Ebi to parallel data ensures that both models learn a shared representation at the sentence level. As the parallel input sentences share the same meaning, it is reasonable to assume that mini- mizing Ebi will force the model to learn their semantic representation. Let θbi = θMA ∪ θMB . The joint objective function J(θbi) thus becomes:  J(θbi) = Ebi(CA,B) +  (cid:107)θbi(cid:107)2  λ 2  (5)  where λ(cid:107)θbi(cid:107)1 is the L2 regularization term. It is apparent that this joint objective J(θbi) is degenerate. The models could learn to reduce all embeddings and composition weights to zero and thereby minimize the objective function. We ad- dress this issue by employing a form of contrastive estimation penalizing small distances between non-parallel sentence pairs. For every pair of parallel sentences (a, b) we sample a number of ad- ditional sentences n ∈ CB, which—with high probability—are not exact translations of a. This is comparable to the second term of the loss function of a large margin nearest neighbour classiﬁer (see Eq. 12 in [28]):  Enoise(a, b, n) = [1 + Edist(a, b) − Edist(a, n)]+  (6)  4  where [x]+ = max(x, 0) denotes the standard hinge loss. Thus, the ﬁnal objective function to minimize for the BICVM model is:  (cid:88)  (cid:32) k(cid:88)  (a,b)∈CA,B  i=1  (cid:33)  Enoise(a, b, ni)  +  (cid:107)θbi(cid:107)2  λ 2  (7)  J(θbi) =  3.3 Model Learning  Given the objective function as deﬁned above, model learning can employ the same techniques as any monolingual CVM. In particular, as the objective function is differentiable, we can use standard gradient descent techniques such as stochastic gradient descent, L-BFGS or the adaptive gradient algorithm AdaGrad [8]. Within each monolingual CVM, we use backpropagation through structure after applying the joint error to each sentence level node.  4 Experiments  4.1 Data and Parameters  All model weights were randomly initialised using a Gaussian distribution. There are a number of parameters that can inﬂuence model training. We selected the following values for simplicity and comparability with prior work. In future work we will investigate the effect of these parameters in greater detail. L2 regularization (1), step-size (0.1), number of noise elements (50), margin size (50), embedding dimensionality (d=40). The noise elements samples were randomly drawn from the corpus at training time, individually for each training sample and epoch. We use the Europarl corpus (v7)1 for training the bilingual model. The corpus was pre-processed using the set of tools provided by cdec2 [9] for tokenizing and lowercasing the data. Further, all empty sentences as well as their translations were removed from the corpus. We present results from two experiments. The BICVM model was trained on 500k sentence pairs of the English-German parallel section of the Europarl corpus. The BICVM+ model used this dataset in combination with another 500k parallel sentences from the English-French section of the corpus, resulting in 1 million English sentences, each paired up with either a German or a French sentence. Each language’s vocabulary used distinct encodings to avoid potential overlap. The motivation behind BICVM+ is to investigate whether we can learn better embeddings by intro- ducing additional data in a different language. This is similar to prior work in machine translation where English was used as a pivot for translation between low-resource languages [5]. We use the adaptive gradient method, AdaGrad [8], for updating the weights of our models, and ter- minate training after 50 iterations. Earlier experiments indicated that the BICVM model converges faster than the BICVM+ model, but we report results on the same number of iterations for better comparability3.  4.2 Cross-Lingual Document Classiﬁcation  We evaluate our model using the cross-lingual document classiﬁcation (CLDC) task of Klementiev et al. [16]. This task involves learning language independent embeddings which are then used for document classiﬁcation across the English-German language pair. For this, CLDC employs a par- ticular kind of supervision, namely using supervised training data in one language and evaluating without supervision in another. Thus, CLDC is a good task for establishing whether our learned representations are semantically useful across multiple languages. We follow the experimental setup described in [16], with the exception that we learn our embeddings using solely the Europarl data and only use the Reuters RCV1/RCV2 corpora during the classiﬁer training and testing stages. Each document in the classiﬁcation task is represented by the average  1http://www.statmt.org/europarl/ 2https://github.com/redpony/cdec 3These numbers were updated following comments in the ICLR open review process. Results for other  dimensionalities and our source code for our model are available at http://www.karlmoritz.com.  5  Model Majority Class Glossed MT I-Matrix BICVM BICVM+  en → de 46.8 65.1 68.1 77.6 83.7 86.2  de → en 46.8 68.6 67.4 71.1 71.4 76.9  Table 1: Classiﬁcation accuracy for training on English and German with 1000 labeled examples. Cross-lingual compositional representations (BICVM and BICVM+), cross-lingual representations using learned embeddings and an interaction matrix (I-Matrix) [16] translated (MT) and glossed (Glossed) words, and the majority class baseline. The MT and Glossed results are also taken from Klementiev et al. [16].  )  %  (  y c a r u c c A n o i t a c ﬁ  i s s a l C  80  70  60  50  100  80  70  60  50  100  500  200 Training Documents (en)  1000 5000 10000  500  200 Training Documents (de)  1000 5000 10000  BICVM  BICVM+  I-Matrix  MT  Glossed  Majority Class  Figure 2: Classiﬁcation accuracy for a number of models (see Table 1 for model descriptions). The left chart shows results for these models when trained on English data and evaluated on German data, the right chart vice versa.  of the d-dimensional representations of all its sentences. We train the multiclass classiﬁer using the same settings and implementation of the averaged perceptron classiﬁer [6] as used in [16]. We ran the CLDC experiments both by training on English and testing on German documents and vice versa. Using the data splits provided by [16], we used varying training data sizes from 100 to 10,000 documents for training the multiclass classiﬁer. The results of this task across training sizes are shown in Figure 2. Table 1 shows the results for training on 1,000 documents. Both models, BICVM and BICVM+ outperform all prior work on this task. Further, the BICVM+ model outperforms the BICVM model, indicating the usefulness of adding training data even from a separate language pair.  4.3 Visualization  While the CLDC experiment focused on establishing the semantic content of the sentence level representations, we also want to brieﬂy investigate the induced word embeddings. In particular the BICVM+ model is interesting for that purpose, as it allows us to evaluate our approach of using English as a pivot language in a multilingual setup. In Figure 3 we show the t-SNE projections for a number of English, French and German words. Of particular interest should be the right chart, which highlights bilingual embeddings between French and German words. Even though the model did not use any parallel French-German data during training, it still managed to learn semantic word-word similarity across these two languages.  6  Figure 3: The left scatter plot shows t-SNE projections for a weekdays in all three languages using the representations learned in the BICVM+ model. Even though the model did not use any parallel French-German data during training, it still learns semantic similarity between these two languages using English as a pivot. To highlight this, the right plot shows another set of words (months of the year) using only the German and French words.  5 Conclusions  With this paper we have proposed a novel method for inducing cross-lingual distributed represen- tations for compositional semantics. Using a very simple method for semantic composition, we nevertheless managed to obtain state of the art results on the CLDC task, speciﬁcally designed to evaluate semantic transfer across languages. After extending our approach to include multilingual training data in the BICVM+ model, we were able to demonstrate that adding additional languages further improves the model. Furthermore, using some qualitative experiments and visualizations, we showed that our approach also allows us to learn semantically related embeddings across languages without any direct training data. Our approach provides great ﬂexibility in training data and requires little to no annotation. Hav- ing demonstrated the successful training of semantic representations using sentence aligned data, a plausible next step is to attempt training using document-aligned data or even corpora of comparable documents. This may provide even greater possibilities for working with low-resource languages. In the same vein, the success of our pivoting experiments suggest further work. Unlike other pivot approaches, it is easy to extend our model to have multiple pivot languages. Thus some pivots could preserve different aspects such as case, gender etc., and overcome other issues related to having a single pivot language. As we have achieved the results in this paper with a relatively simple CVM, it would also be inter- esting to establish whether our objective function can be used in combination with more complex compositional vector models such as MV-RNN [25] or tensor-based approaches, to see whether these can further improve results on both mono- and multilingual tasks when used in conjunction with our cross-lingual objective function. Related to this, we will also apply our model to a wider variety of tasks including machine translation and multilingual information extraction.  Acknowledgements  The authors would like to thank Alexandre Klementiev and his co-authors for making their datasets and averaged perceptron implementation available, as well as answering a number of questions related to their work on this task. This work was supported by EPSRC grant EP/K036580/1 and a Xerox Foundation Award.  7  References [1] Marco Baroni and Roberto Zamparelli. Nouns are vectors, adjectives are matrices: Represent-  ing adjective-noun constructions in semantic space. In Proceedings of EMNLP, 2010.  [2] Paul Bloom. Precis of how children learn the meanings of words. Behavioral and Brain  Sciences, 24:1095–1103, 2001.  [3] Stephen Clark and Stephen Pulman. Combining symbolic and distributional models of mean- ing. In Proceedings of AAAI Spring Symposium on Quantum Interaction. AAAI Press, 2007. [4] Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. Mathematical foundations for a com- positional distributional model of meaning. Lambek Festschrift. Linguistic Analysis, 36:345– 384, 2010.  [5] Trevor Cohn and Mirella Lapata. Machine translation by triangulation: Making effective use of multi-parallel corpora. In Proceedings of ACL, pages 728–735, Prague, Czech Republic, June 2007. Association for Computational Linguistics.  [6] Michael Collins. Discriminative training methods for hidden markov models: Theory and In Proceedings of ACL-EMNLP. Association for  experiments with perceptron algorithms. Computational Linguistics, 2002. doi: 10.3115/1118693.1118694.  [7] Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing:  Deep neural networks with multitask learning. In Proceedings of ICML, 2008.  [8] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121–2159, July 2011. ISSN 1532-4435.  [9] Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. cdec: A decoder, alignment, and In Proceedings of learning framework for ﬁnite-state and context-free translation models. ACL, 2010.  [10] K. Erk and S. Pad´o. A structured vector space model for word meaning in context. Proceedings  of EMNLP, 2008.  [11] J. R. Firth. A synopsis of linguistic theory 1930-55. 1952-59:1–32, 1957. [12] Edward Grefenstette and Mehrnoosh Sadrzadeh. Experimental support for a categorical com-  positional distributional model of meaning. In Proceedings of EMNLP, 2011.  [13] Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. Learning bilingual  lexicons from monolingual corpora. In Proceedings of ACL-HLT, 2008.  [14] Karl Moritz Hermann and Phil Blunsom. The Role of Syntax in Vector Space Models of  Compositional Semantics. In Proceedings of ACL, 2013.  [15] Nal Kalchbrenner and Phil Blunsom. Recurrent convolutional neural networks for discourse compositionality. In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, 2013.  [16] Alexandre Klementiev, Ivan Titov, and Binod Bhattarai.  representations of words. In Proceedings of COLING, 2012.  Inducing crosslingual distributed  [17] Stanislas Lauly, Alex Boulanger, and Hugo Larochelle. Learning multilingual word represen-  tations using a bag-of-words autoencoder. In Deep Learning Workshop at NIPS, 2013.  [18] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word  representations in vector space. CoRR, 2013.  [19] Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. Exploiting similarities among languages for  machine translation. CoRR, 2013.  [20] Tom´aˇs Mikolov, Martin Karaﬁ´at, Luk´aˇs Burget, Jan ˇCernock´y, and Sanjeev Khudanpur. Re-  current neural network based language model. In Proceedings of INTERSPEECH, 2010.  [21] Jeff Mitchell and Mirella Lapata. Vector-based models of semantic composition. In In Pro-  ceedings of ACL, 2008.  [22] D. Roy. Grounded spoken language acquisition: Experiments in word learning. IEEE Trans- actions on Multimedia, 5(2):197–209, June 2003. ISSN 1520-9210. doi: 10.1109/TMM.2003. 811618.  8  [23] A P Sarath Chandar, M Khapra Mitesh, B Ravindran, Vikas Raykar, and Amrita Saha. Multi-  lingual deep learning. In Deep Learning Workshop at NIPS, 2013.  [24] Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Man- ning. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Pro- ceedings of EMNLP, 2011.  [25] Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. Semantic compo- sitionality through recursive matrix-vector spaces. In Proceedings of EMNLP-CoNLL, pages 1201–1211, 2012.  [26] Nitish Srivastava and Ruslan Salakhutdinov. Multimodal learning with deep boltzmann ma-  chines. In Proceedings of NIPS. 2012.  [27] P. D. Turney and P. Pantel. From frequency to meaning: Vector space models of semantics.  Journal of Artiﬁcial Intelligence Research, 37(1):141–188, 2010.  [28] Kilian Q. Weinberger and Lawrence K. Saul. Distance metric learning for large margin nearest neighbor classiﬁcation. Journal of Machine Learning Research, 10:207–244, June 2009. ISSN 1532-4435.  [29] Will Y. Zou, Richard Socher, Daniel Cer, and Christopher D. Manning. Bilingual word em-  beddings for phrase-based machine translation. In Proceedings of EMNLP, 2013.  9  ","Distributed representations of meaning are a natural way to encode covariancerelationships between words and phrases in NLP. By overcoming data sparsityproblems, as well as providing information about semantic relatedness which isnot available in discrete representations, distributed representations haveproven useful in many NLP tasks. Recent work has shown how compositionalsemantic representations can successfully be applied to a number of monolingualapplications such as sentiment analysis. At the same time, there has been someinitial success in work on learning shared word-level representations acrosslanguages. We combine these two approaches by proposing a method for learningdistributed representations in a multilingual setup. Our model learns to assignsimilar embeddings to aligned sentences and dissimilar ones to sentence whichare not aligned while not requiring word alignments. We show that ourrepresentations are semantically informative and apply them to a cross-lingualdocument classification task where we outperform the previous state of the art.Further, by employing parallel corpora of multiple language pairs we find thatour model learns representations that capture semantic relationships acrosslanguages for which no parallel data was used."
1312.6120,2014,Exact solutions to the nonlinear dynamics of learning in deep linear neural networks  ,"['Andrew Saxe', 'James L. McClelland', 'Surya Ganguli']",https://arxiv.org/pdf/1312.6120.pdf,"4 1 0 2     b e F 9 1         ] E N . s c [      3 v 0 2 1 6  .  2 1 3 1 : v i X r a  Exact solutions to the nonlinear dynamics of learning in  deep linear neural networks  Andrew M. Saxe (asaxe@stanford.edu)  Department of Electrical Engineering  James L. McClelland (mcclelland@stanford.edu)  Department of Psychology  Surya Ganguli (sganguli@stanford.edu)  Department of Applied Physics  Stanford University, Stanford, CA 94305 USA  Abstract  Despite the widespread practical success of deep learning methods, our theoretical under- standing of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systemati- cally analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient de- scent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining ini- tial conditions than from random initial conditions. We provide an analytical description of these phenomena by ﬁnding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising ﬁnding that as the depth of a network approaches inﬁnity, learning speed can nevertheless remain ﬁnite: for a special class of initial conditions on the weights, very deep networks incur only a ﬁnite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can ﬁnd this special class of initial conditions, while scaled random Gaussian initializations cannot. We further ex- hibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.  Deep learning methods have realized impressive performance in a range of applications, from visual object classiﬁcation [1, 2, 3] to speech recognition [4] and natural language processing [5, 6]. These successes have been achieved despite the noted difﬁculty of training such deep architectures [7, 8, 9, 10, 11]. Indeed, many explanations for the difﬁculty of deep learning have been advanced in the literature, including the presence of many local minima, low curvature regions due to saturating nonlinearities, and exponential growth or decay of back-propagated gradients [12, 13, 14, 15]. Furthermore, many neural network simulations have observed  1  strikingly nonlinear learning dynamics, including long plateaus of little apparent improvement followed by almost stage-like transitions to better performance. However, a quantitative, analytical understanding of the rich dynamics of deep learning remains elusive. For example, what determines the time scales over which deep learning unfolds? How does training speed retard with depth? Under what conditions will greedy unsupervised pretraining speed up learning? And how do the ﬁnal learned internal representations depend on the statistical regularities inherent in the training data? Here we provide an exact analytical theory of learning in deep linear neural networks that quantitatively answers these questions for this restricted setting. Because of its linearity, the input-output map of a deep linear network can always be rewritten as a shallow network. In this sense, a linear network does not gain ex- pressive power from depth, and hence will underﬁt and perform poorly on complex real world problems. But while it lacks this important aspect of practical deep learning systems, a deep linear network can nonetheless exhibit highly nonlinear learning dynamics, and these dynamics change with increasing depth. Indeed, the training error, as a function of the network weights, is non-convex, and gradient descent dynamics on this non-convex error surface exhibits a subtle interplay between different weights across multiple layers of the network. Hence deep linear networks provide an important starting point for understanding deep learning dynamics. To answer these questions, we derive and analyze a set of nonlinear coupled differential equations describing learning dynamics on weight space as a function of the statistical structure of the inputs and outputs. We ﬁnd exact time-dependent solutions to these nonlinear equations, as well as ﬁnd conserved quantities in the weight dynamics arising from symmetries in the error function. These solutions provide intuition into how a deep network successively builds up information about the statistical structure of the training data and embeds this information into its weights and internal representations. Moreover, we compare our analytical solutions of learning dynamics in deep linear networks to numerical simulations of learning dynamics in deep non-linear networks, and ﬁnd that our analytical solutions provide a reasonable approximation. Our solutions also reﬂect nonlinear phenomena seen in simulations, including alternating plateaus and sharp pe- riods of rapid improvement. Indeed, it has been shown previously [16] that this nonlinear learning dynamics in deep linear networks is sufﬁcient to qualitatively capture aspects of the progressive, hierarchical differ- entiation of conceptual structure seen in infant development. Next, we apply these solutions to investigate the commonly used greedy layer-wise pretraining strategy for training deep networks [17, 18], and recover conditions under which such pretraining speeds learning. We show that these conditions are approximately satisﬁed for the MNIST dataset, and that unsupervised pretraining therefore confers an optimization advan- tage for deep linear networks applied to MNIST. Finally, we exhibit a new class of random orthogonal initial conditions on weights that, in linear networks, provide depth independent learning times, and we show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos. In this regime, synaptic gains are tuned so that linear ampliﬁcation due to propagation of neural activity through weight matrices exactly balances dampening of activity due to saturating nonlinearities. In particular, we show that even in nonlinear networks, operating in this special regime, Jacobians that are involved in backpropagating error signals act like near isometries.  1 General learning dynamics of gradient descent  We begin by analyzing learning in a three layer network (in- put, hidden, and output) with linear activation functions (Fig 1). We let Ni be the number of neurons in layer i. The input- output map of the network is y = W 32W 21x. We wish to train the network to learn a particular input-output map from  2  Figure 1: The three layer network analyzed in this section.  W21W32x∈RN1h∈RN2y∈RN3squared error(cid:80)P P(cid:88)  a set of P training examples {xµ, yµ} , µ = 1, . . . , P . Training is accomplished via gradient descent on the  (cid:13)(cid:13)yµ − W 32W 21xµ(cid:13)(cid:13)2 between the desired feature output, and the network’s feature W 32T(cid:0)yµxµT − W 32W 21xµxµT(cid:1) , (cid:0)yµxµT − W 32W 21xµxµT(cid:1) W 21T  output. This gradient descent procedure yields the batch learning rule  ∆W 32 = λ  ∆W 21 = λ  P(cid:88)  µ=1  ,  µ=1  µ=1  τ  d dt  where Σ11 ≡(cid:80)P  (1) where λ is a small learning rate. As long as λ is sufﬁciently small, we can take a continuous time limit to obtain the dynamics,  W 21 = W 32T(cid:0)Σ31 − W 32W 21Σ11(cid:1) , µ=1 xµxµT is an N1 × N1 input correlation matrix, Σ31 ≡(cid:80)P  (2) µ=1 yµxµT is an N3 × N1 input-output correlation matrix, and τ ≡ 1 λ . Here t measures time in units of iterations; as t varies from 0 to 1, the network has seen P examples corresponding to one iteration. Despite the linearity of the network’s input-output map, the gradient descent learning dynamics given in Eqn (2) constitutes a complex set of coupled nonlinear differential equations with up to cubic interactions in the weights.  W 32 =(cid:0)Σ31 − W 32W 21Σ11(cid:1) W 21T  d dt  τ  ,  1.1 Learning dynamics with orthogonal inputs  =(cid:80)N1  Our fundamental goal is to understand the dynamics of learning in (2) as a function of the input statistics Σ11 and input-output statistics Σ31. In general, the outcome of learning will reﬂect an interplay between input correlations, described by Σ11, and the input-output correlations described by Σ31. To begin, though, we further simplify the analysis by focusing on the case of orthogonal input representations where Σ11 = I. This assumption will hold exactly for whitened input data, a widely used preprocessing step. Because we have assumed orthogonal input representations (Σ11 = I), the input-output correlation matrix contains all of the information about the dataset used in learning, and it plays a pivotal role in the learning dynamics. We consider its singular value decomposition (SVD)  Σ31 = U 33S31V 11T  α=1 sαuαvT α ,  (3) which will be central in our analysis. Here V 11 is an N1 × N1 orthogonal matrix whose columns contain input-analyzing singular vectors vα that reﬂect independent modes of variation in the input, U 33 is an N3 × N3 orthogonal matrix whose columns contain output-analyzing singular vectors uα that reﬂect independent modes of variation in the output, and S31 is an N3 × N1 matrix whose only nonzero elements are on the diagonal; these elements are the singular values sα, α = 1, . . . , N1 ordered so that s1 ≥ s2 ≥ ··· ≥ sN1. V 11T , W 32 = U 33W Now, performing the change of variables on synaptic weight space, W 21 = W the dynamics in (2) simplify to  32,  21  21  W  = W  32T  (S31 − W  32  21  ),  W  τ  d dt  τ  d dt  32  W  = (S31 − W  21  32  W  )W  21T  .  (4)  To gain intuition for these equations, note that while the matrix elements of W 21 and W 32 connected neurons iα as connecting input mode in one layer to neurons in the next layer, we can think of the matrix element W vα to hidden neuron i, and the matrix element W αi as connecting hidden neuron i to output mode uα. Let 32. Intuitively, aα is a column vector of N2 aα be the αth column of W synaptic weights presynaptic to the hidden layer coming from input mode α, and bα is a column vector of  21, and let bαT be the αth row of W  21  32  3  N2 synaptic weights postsynaptic to the hidden layer going to output mode α. In terms of these variables, or connectivity modes, the learning dynamics in (4) become  aα = (sα − aα · bα) bα −(cid:88)  τ  d dt  bγ (aα · bγ),  τ  d dt  γ(cid:54)=α  aγ (bα · aγ).  (5)  Note that sα = 0 for α > N1. These dynamics arise from gradient descent on the energy function  bα = (sα − aα · bα) aα −(cid:88) (cid:88)  γ(cid:54)=α  (aα · bβ)2,  1 2τ  α(cid:54)=β  (cid:88)  α  E =  1 2τ  (sα − aα · bα)2 +  (6)  and display an interesting combination of cooperative and competitive interactions. Consider the ﬁrst terms in each equation. In these terms, the connectivity modes from the two layers, aα and bα associated with the same input-output mode of strength sα, cooperate with each other to drive each other to larger magnitudes as well as point in similar directions in the space of hidden units; in this fashion these terms drive the product of connectivity modes aα · bα to reﬂect the input-output mode strength sα. The second terms describe competition between the connectivity modes in the ﬁrst (aα) and second (bβ) layers associated with different input modes α and β. This yields a symmetric, pairwise repulsive force between all distinct pairs of ﬁrst and second layer connectivity modes, driving the network to a decoupled regime in which the different connectivity modes become orthogonal.  1.2 The ﬁnal outcome of learning  (cid:19)  (cid:18) r  The ﬁxed point structure of gradient descent learning in linear networks was worked out in [19]. In the language of the connectivity modes, a necessary condition for a ﬁxed point is aα · bβ = sαδαβ, while aα and bα are zero whenever sα = 0. To satisfy these relations for undercomplete hidden layers (N2 < N1, N2 < N3), aα and bα can be nonzero for at most N2 values of α. Since there are rank(Σ31) ≡ r nonzero values of sα, there are families of ﬁxed points. However, all of these ﬁxed points are unstable, except for the one in which only the ﬁrst N2 strongest modes, i.e. aα and bα for α = 1, . . . , N2 are active. Thus remarkably, the dynamics in (5) has only saddle points and no non-global local minima [19]. In terms of the original synaptic variables W 21 and W 32, all globally stable ﬁxed points satisfy  N2  W 32W 21 =(cid:80)N2  α=1 sαuαvT α .  Hence when learning has converged, the network will represent the closest rank N2 approximation to the true input-output correlation matrix. In this work, we are interested in understanding the dynami- cal weight trajectories and learning time scales that lead to this ﬁnal ﬁxed point.  1.3 The time course of learning  It is difﬁcult though to exactly solve (5) starting from arbitrary initial conditions because of the competitive interactions between different input-output modes. Therefore, to gain intuition for the general dy- namics, we restrict our attention to a special class of initial conditions of the form aα and bα ∝ rα for α = 1, . . . , N2, where rα · rβ = δαβ, with all other connectivity modes aα and bα set to zero (see [20] for  4  (7)  Figure 2: Vector ﬁeld (blue), stable manifold (red) and two solution tra- jectories (green) for the two dimen- sional dynamics of a and b in (8), with τ = 1, s = 1.  −2−1012−2−1.5−1−0.500.511.52abFigure 3: Left: Dynamics of learning in a three layer neural network. Curves show the strength of the network’s representation of seven modes of the input-output correlation matrix over the course of learning. Red traces show analytical curves from Eqn. 12. Blue traces show simulation of full dynamics of a linear network (Eqn. (2)) from small random initial conditions. Green traces show simulation of a nonlinear three layer network with tanh activation functions. To generate mode strengths for the nonlinear network, we computed the nonlinear network’s evolving input-output correlation matrix, and plotted the diagonal elements of U 33T Σ31 tanhV 11 over time. The training set consists of 32 orthogonal input patterns, each associated with a 1000-dimensional feature vector generated by a hierarchical diffusion process described in [16] with a ﬁve level binary tree and ﬂip probability of 0.1. Modes 1, 2, 3, 5, 12, 18, and 31 are plotted with the rest excluded for clarity. Network training parameters were λ = 0.5e−3, N2 = 32, u0 = 1e−6. Right: Delay in learning due to competitive dynamics and sigmoidal nonlinearities. Vertical axis shows the difference between simulated time of half learning and the analytical time of half learning, as a fraction of the analytical time of half learning. Error bars show standard deviation from 100 simulations with random initializations. solutions to a partially overlapping but distinct set of initial condi- tions, further discussed in Supplementary Appendix A). Here rα is a ﬁxed collection of N2 vectors that form an orthonormal basis for synaptic connections from an input or output mode onto the set of hidden units. Thus for this set of initial conditions, aα and bα point in the same direction for each alpha and differ only in their scalar magnitudes, and are orthogonal to all other connectivity modes. Such an initialization can be obtained by computing the SVD of Σ31 and taking W 32 = U 33DaRT , W 21 = RDbV 11T where Da, Db are diagonal, and R is an arbitrary orthogonal matrix; however, as we show in subsequent experiments, the solutions we ﬁnd are also excellent approximations to trajectories from small random initial conditions. It is straightforward to verify that starting from these initial conditions, aα and bα will remain parallel to rα for all future time. Furthermore, because the different active modes are orthogonal to each other, they do not compete, or even interact with each other (all dot products in the second terms of (5)-(6) are 0). Thus this class of conditions deﬁnes an invariant manifold in weight space where the modes evolve indepen- dently of each other. If we let a = aα · rα, b = bα · rα, and s = sα, then the dynamics of the scalar projections (a, b) obeys,  a = b (s − ab),  τ  d dt  b = a (s − ab).  τ  d dt  (8)  Thus our ability to decouple the connectivity modes yields a dramatically simpliﬁed two dimensional non- linear system. These equations can by solved by noting that they arise from gradient descent on the error,  (9) This implies that the product ab monotonically approaches the ﬁxed point s from its initial value. Moreover, E(a, b) satisﬁes a symmetry under the one parameter family of scaling transformations a → λa, b → b λ. This symmetry implies, through Noether’s theorem, the existence of a conserved quantity, namely a2 − b2,  E(a, b) = 1  2τ (s − ab)2.  5  05001000020406080t (Epochs)mode strength  051015202530−0.100.10.20.30.40.50.6Input−output mode(thalf−tanaly)/tanaly  LinearTanhwhich is a constant of motion. Thus the dynamics simply follows hyperbolas of constant a2− b2 in the (a, b) plane until it approaches the hyperbolic manifold of ﬁxed points, ab = s. The origin a = 0, b = 0 is also a ﬁxed point, but is unstable. Fig. 2 shows a typical phase portrait for these dynamics. As a measure of the timescale of learning, we are interested in how long it takes for ab to approach s from any given initial condition. The case of unequal a and b is treated in the Supplementary Appendix A due to space constraints. Here we pursue an explicit solution with the assumption that a = b, a reasonable limit when starting with small random initial conditions. We can then track the dynamics of u ≡ ab, which from (8) obeys  This equation is separable and can be integrated to yield  (cid:90) uf  u0  t = τ  u = 2u(s − u).  τ  d dt  du  2u(s − u)  =  τ 2s  ln  uf (s − u0) u0(s − uf )  .  (10)  (11)  Here t is the time it takes for u to travel from u0 to uf . If we assume a small initial condition u0 = (cid:15), and ask when uf is within (cid:15) of the ﬁxed point s, i.e. uf = s − (cid:15), then the learning timescale in the limit (cid:15) → 0 is t = τ /s ln (s/(cid:15)) = O(τ /s) (with a weak logarithmic dependence on the cutoff). This yields a key result: the timescale of learning of each input-output mode α of the correlation matrix Σ31 is inversely proportional to the correlation strength sα of the mode. Thus the stronger an input-output relationship, the quicker it is learned. We can also ﬁnd the entire time course of learning by inverting (11) to obtain  uf (t) =  se2st/τ  e2st/τ − 1 + s/u0  .  (12)  This time course describes the temporal evolution of the product of the magnitudes of all weights from an input mode (with correlation strength s) into the hidden layers, and from the hidden layers to the same output mode. If this product starts at a small value u0 < s, then it displays a sigmoidal rise which asymptotes to s as t → ∞. This sigmoid can exhibit sharp transitions from a state of no learning to full learning. This analytical sigmoid learning curve is shown in Fig. 3 to yield a reasonable approximation to learning curves in linear networks that start from random initial conditions that are not on the orthogonal, decoupled invariant manifold–and that therefore exhibit competitive dynamics between connectivity modes–as well as in nonlinear networks solving the same task. We note that though the nonlinear networks behaved similarly to the linear case for this particular task, this is likely to be problem dependent.  2 Deeper multilayer dynamics  The network analyzed in Section 1 is the minimal example of a multilayer net, with just a single layer of hidden units. How does gradient descent act in much deeper networks? We make an initial attempt in this direction based on initial conditions that yield particularly simple gradient descent dynamics. In a linear neural network with Nl layers and hence Nl−1 weight matrices indexed by W l, l = 1,··· , Nl−1, the gradient descent dynamics can be written as  (cid:33)T(cid:34)  (cid:32) Nl−1(cid:89)  (cid:35)(cid:32)l−1(cid:89) i=a W i = W bW (b−1) ··· W (a−1)W a with the special case that(cid:81)b  (cid:32)Nl−1(cid:89)  Σ31 −  (cid:33)  W l =  d dt  Σ11  W i  W i  i=l+1  i=1  i=1  τ  (cid:33)T  i=a W i = I, the identity, if  W i  ,  (13)  where(cid:81)b  a > b.  6  To describe the initial conditions, we suppose that there are Nl orthogonal matrices Rl that diagonalize the starting weight matrices, that is, RT l+1Wl(0)Rl = Dl for all l, with the special case that R1 = V 11 and RNl = U 33. This requirement essentially demands that the output singular vectors of layer l be the input singular vectors of the next layer l + 1, so that a change in mode strength at any layer propagates to the output without mixing into other modes. We note that this formulation does not restrict hidden layer size; each hidden layer can be of a different size, and may be undercomplete or overcomplete. Making the change of variables Wl = Rl+1W lRT l along with the assumption that Σ11 = I leads to a set of decoupled connectivity modes that evolve independently of each other. In analogy to the simpliﬁcation occurring in the three layer network from (2) to (8), each connectivity mode in the Nl layered network can be described by Nl − 1 scalars a1, . . . , aNl−1, whose dynamics obeys gradient descent on the energy function (the analog of (9)),  E(a1,··· , aNl−1) =  1 2τ i − a2 This dynamics also has a set of conserved quantities a2 transformation ai → λai, aj → aj  in which ai(t = 0) = a0 for all i, and track the dynamics of u = (cid:81)Nl−1  j arising from the energetic symmetry w.r.t. the λ , and hence can be solved exactly. We focus on the invariant submanifold i=1 ai, the overall strength of this  ai  .  (14)  i=1  mode, which obeys (i.e. the generalization of (10)),  (cid:32) s − Nl−1(cid:89)  (cid:33)2  u = (Nl − 1)u2−2/(Nl−1)(s − u).  (15)  τ  d dt  This can be integrated for any positive integer Nl, though the expression is complicated. Once the overall strength increases sufﬁciently, learning explodes rapidly. Eqn. (15) lets us study the dynamics of learning as depth limits to inﬁnity. In particular, as Nl → ∞ we have the dynamics  (cid:20) 1  τ  d dt  u = Nlu2(s − u)  (cid:18) uf (u0 − s)  (cid:19)  (cid:21)  which can be integrated to obtain  τ Nl  1 su0  − 1 suf  (17)  .  +  t =  (cid:17)  s2 log  (cid:16) 1  An estimate of  u0(uf − s) the learning time as measured by the num- Remarkably this implies that, for a ﬁxed learning rate, ber of iterations required tends to zero as Nl goes to inﬁnity. This result depends on the con- tinuous time formulation, however. Any implementation will operate in discrete time and must choose a ﬁnite learning rate that yields stable dynamics. learn- ing rate can be derived from the maximum eigenvalue of the Hessian over the region of interest. For linear networks with ai = aj = a, this optimal learn- ing rate αopt decays with for large depth as O Nl (see Supplementary Ap- pendix B). Incorporating this dependence of the learning rate on depth, the learning time as depth approaches in- ﬁnity still surprisingly re- mains ﬁnite: with the opti- mal learning rate, the differ- ence between learning times  Figure 4: Left: Learning time as a function of depth on MNIST. Right: Empirically optimal learning rates as a function of depth.  the optimal  Nls2  (16)  7  050100050100150200250Nl (Number of layers)Learning time (Epochs)05010000.20.40.60.811.2x 10−4Optimal learning rateNl (Number of layers)for an Nl = 3 network and an Nl = ∞ network is t∞ − t3 ∼ O (s/(cid:15)) for small (cid:15) (see Supplementary Appendix B.1). We emphasize that our analysis of learning speed is based on the number of iterations re- quired, not the amount of computation–computing one iteration of a deep network will require more time than doing so in a shallow network. To verify these predictions, we trained deep linear networks on the MNIST classiﬁcation task with depths ranging from Nl = 3 to Nl = 100. We used hidden layers of size 1000, and calculated the iteration at which training error fell below a ﬁxed threshold corresponding to nearly complete learning. We optimized the learning rate separately for each depth by training each network with twenty rates logarithmically spaced between 10−4 and 10−7 and picking the fastest. See Supplementary Appendix C for full experimental details. Networks were initialized with decoupled initial conditions and starting initial mode strength u0 = 0.001. Fig. 4 shows the resulting learning times, which saturate, and the empirically optimal learning rates, which scale like O(1/Nl) as predicted. Thus learning times in deep linear networks that start with decoupled initial conditions are only a ﬁnite amount slower than a shallow network regardless of depth. Moreover, the delay incurred by depth scales inversely with the size of the initial strength of the association. Hence ﬁnding a way to initialize the mode strengths to large values is crucial for fast deep learning.  3 Finding good weight initializations: on greediness and randomness  The previous subsection revealed the existence of a decoupled submanifold in weight space in which con- nectivity modes evolve independently of each other during learning, and learning times can be independent of depth, even for arbitrarily deep networks, as long as the initial composite, end to end mode strength, denoted by u above, of every connectivity mode is O(1). What numerical weight initilization procedures can get us close to this weight manifold, so that we can exploit its rapid learning properties? A breakthrough in training deep neural networks started with the discovery that greedy layer-wise unsu- pervised pretraining could substantially speed up and improve the generalization performance of standard gradient descent [17, 18]. Unsupervised pretraining has been shown to speed the optimization of deep networks, and also to act as a special regularizer towards solutions with better generalization performance [18, 12, 13, 14]. At the same time, recent results have obtained excellent performance starting from carefully- scaled random initializations, though interestingly, pretrained initializations still exhibit faster convergence [21, 13, 22, 3, 4, 1, 23] (see Supplementary Appendix D for discussion). Here we examine analytically how unsupervised pretraining achieves an optimization advantage, at least in deep linear networks, by ﬁnding the special class of orthogonalized, decoupled initial conditions in the previous section that allow for rapid supervised deep learning, for input-output tasks with a certain precise structure. Subsequently, we analyze the properties of random initilizations. We consider the following pretraining and ﬁnetuning procedure: First, using autoencoders as the unsuper- vised pretraining module [18, 12], the network is trained to produce its input as its output (yµ pre = xµ). Subsequently, the network is ﬁnetuned on the ultimate input-output task of interest (e.g., a classiﬁcation task). In the following we consider the case N2 = N1 for simplicity. pre is simply the input correlation matrix During the pretraining phase, the input-output correlation matrix Σ31 Σ11. Hence the SVD of Σ31 pre = Σ11 = QΛQT , where Q are eigenvectors of Σ11 and Λ is a diagonal matrix of variances. Our analysis of the learning dynamics in Section 1.1 does not directly apply, because here the input correlation matrix is not white. In Supplementary Appendix E we generalize our results to handle this case. During pretraining, the weights approach W 32W 21 = Σ31(Σ31)−1, but since they do not reach the ﬁxed point in ﬁnite time, they will end at W 32W 21 = QM QT where M is a diagonal matrix that is approaching the identity matrix during  pre is PCA on the input correlation matrix, since Σ31  8  Figure 5: MNIST satisﬁes the consistency condition for greedy pretraining. Left: Submatrix from the raw MNIST input correlation matrix Σ11. Center: Submatrix of V 11Σ11V 11T which is approximately diagonal as required. Right: Learning curves on MNIST for a ﬁve layer linear network starting from random (black) and pretrained (red) initial conditions. Pretrained curve starts with a delay due to pretraining time. The small random initial conditions correspond to all weights chosen i.i.d. from a zero mean Gaussian with standard deviation 0.01. learning. Hence in general, W 32 = QM 1/2C−1 and W 21 = CM 1/2QT where C is any invertible matrix. When starting from small random weights, though, each weight matrix will end up with a roughly balanced contribution to the overall map. This corresponds to having C ≈ R2 where R2 is orthogonal. Hence at the end of the pretraining phase, the input-to-hidden mapping will be W 21 = R2M 1/2QT where R2 is an arbitrary orthogonal matrix. Now consider the ﬁne-tuning phase. Here the weights are trained on the ultimate task of interest with input-output correlations Σ31 = U 33S31V 11. The matrix W 21 begins from the pretrained initial condition W 21 = R2M 1/2QT . For the ﬁne-tuning task, a decoupled initial condition for W 21 is one that can be written as W 21 = R2D1V 11T (see Section 2). Clearly, this will be possible only if  Q = V 11.  (18)  Then the initial condition obtained from pretraining will also be a decoupled initial condition for the ﬁnetun- ing phase, with initial mode strengths D1 = M 1/2 near one. Hence we can state the underlying condition required for successful greedy pretraining in deep linear networks: the right singular vectors of the ultimate input-ouput task of interest V 11 must be similar to the principal components of the input data Q. This is a quantitatively precise instantiation of the intuitive idea that unsupervised pretraining can help in a subsequent supervised learning task if (and only if) the statistical structure of the input is consistent with the structure of input-output map to be learned. Moreover, this quantitative instantiation of this intuitive idea gives a simple empirical criterion that can be evaluated on any new dataset: given the input-output correlation Σ31 and input correlation Σ11, compute the right singular vectors V 11 of Σ31 and check that V 11Σ11V 11T is approximately diagonal. If the condition in Eqn. (18) holds, autoencoder pretraining will have properly set up decoupled initial conditions for W 21, with an appreciable initial association strength near 1. This argu- ment also goes through straightforwardly for layer-wise pretraining of deeper networks. Fig. 5 shows that this consistency condition empirically holds on MNIST, and that a pretrained deep linear neural network learns faster than one started from small random initial conditions, even accounting for pretraining time (see Supplementary Appendix F for experimental details). We note that this analysis is unlikely to carry over completely to nonlinear networks. Some nonlinear networks are approximately linear (e.g., tanh nonlin- earities) after initialization with small random initializations, and hence our solutions may describe these dynamics well early in learning. However as the network enters its nonlinear regime, our solutions should not be expected to remain accurate. As an alternative to greedy layerwise pre-training, [13] proposed choosing appropriately scaled initial condi- tions on weights that would preserve the norm of typical error vectors as they were backpropagated through the deep network. In our context, the appropriate norm-preserving scaling for the initial condition of an N by N connectivity matrix W between any two layers corresponds to choosing each weight i.i.d. from a  9  010020030040050011.522.53x 104EpochError  PretrainRandom  2000400060008000  051015x 105Figure 6: A Left: Learning time (on MNIST using the same architecture and parameters as in Fig. 4) as a function of depth for different initial conditions on weights (scaled i.i.d. uniform weights chosen to preserve the norm of propagated gradients as proposed in [13] (blue), greedy unsupervised pre-training (green) and random orthogonal matrices (red). The red curve lies on top of the green curve. Middle: Optimal learning rates as a function of depth for different weight initilizations. Right: The eigenvalue spectrum, in the complex plane, of a random 100 by 100 orthogonal matrix. B Histograms of the singular values of products of Nl − 1 independent random Gaussian N by N matrices whose elements themselves are chosen i.i.d. from √ N. In all cases, N = 1000, and histograms are taken over a zero mean Gaussian with standard deviation 1/ 500 realizations of such random product matrices, yielding a total 5 · 105 singular values in each histogram. C Histograms of the eigenvalue distributions on the complex plane of the same product matrices in B. The bin width is 0.1, and, for visualization purposes, the bin containing the origin has been removed in each case; this bin would otherwise dominate the histogram in the middle and right plots, as it contains 32% and 94% of the eigenvalues respectively.  √ N. With this choice, (cid:104)vT W T W v(cid:105)W = vT v, where (cid:104)·(cid:105)W zero mean Gaussian with standard deviation 1/ denotes an average over distribution of the random matrix W . Moreover, the distribution of vT W T W v con- centrates about its mean for large N. Thus with this scaling, in linear networks, both the forward propagation of activity, and backpropagation of gradients is typically norm-preserving. However, with this initialization, the learning time with depth on linear networks trained on MNIST grows with depth (Fig. 6A, left, blue). This growth is in distinct contradiction with the theoretical prediction, made above, of depth independent learning times starting from the decoupled submanifold of weights with composite mode strength O(1). This suggests that the scaled random initialization scheme, despite its norm-preserving nature, does not ﬁnd this submanifold in weight space. In contrast, learning times with greedy layerwise pre-training do not grow with depth (Fig. 6A, left, green curve hiding under red curve), consistent with the predictions of our theory (as a technical point: note that learning times under greedy pre-training initialization in Fig. 6A are faster than those obtained in Fig. 4 by explicitly choosing a point on the decoupled submanifold, because there the initial mode strength was chosen to be small (u = 0.001) whereas greedy pre-training ﬁnds a composite mode strength closer to 1).  10  Nl−1(cid:89)  Is there a simple random initialization scheme that does enjoy the rapid learning properties of greedy- layerwise pre-training? We empirically show (Fig. 6A, left, red curve) that if we choose the initial weights in each layer to be a random orthogonal matrix (satisifying W T W = I), instead of a scaled random Gaussian matrix, then this orthogonal random initialization condition yields depth independent learning times just like greedy layerwise pre-training (indeed the red and green curves are indistinguishable). Theoretically, why do random orthogonal initializations yield depth independent learning times, but not scaled random Gaussian initializations, despite their norm preserving nature? The answer lies in the eigenvalue and singular value spectra of products of Gaussian versus orthgonal random matrices. While a single random orthogonal matrix has eigenvalue spectra lying exactly on the unit circle in the complex plane (Fig. 6A right), the eigenvalue spectra of random Gaussian matrices, whose elements have variance 1/N, form a uniform distribution on a solid disk of radius 1 the complex plane (Fig. 6C left). Moreover the singular values of an orthogonal matrix are all exactly 1, while the squared singular values of a scaled Gaussian random matrix have the well known Marcenko-Pasteur distribution, with a nontrivial spread even as N → ∞, (Fig. 6B left shows the distribution of singular values themselves). Now consider a product of these matrices across all Nl layers, representing the total end to end propagation of activity across a deep linear network:  WTot =  W (i+1,i).  (19)  i=1  Due to the random choice of weights in each layer, WTot is itself a random matrix. On average, it preserves the norm of a typical vector v no matter whether the matrices in each layer are Gaussian or orthogonal. However, the singular value spectra of WTot differ markedly in the two cases. Under random orthogonal initilization in each layer, WTot is itself an orthogonal matrix and therefore has all singular values equal to 1. However, under random Gaussian initialization in each layer, there is as of yet no complete theoretical characterization of the singular value distribution of WTot. We have computed it numerically as a function of different depths in Fig. 6B, and we ﬁnd that it develops a highly kurtotic nature as the depth increases. Most of the singular values become vanishingly small, while a long tail of very large singular values remain. Thus WTot preserves the norm of a typical, randomly chosen vector v, but in a highly anisotropic manner, by strongly amplifying the projection of v onto a very small subset of singular vectors and attenuating v in all other directions. Intuitively WTot, as well as the linear operator W T Tot that would be closely related to backpropagation of gradients to early layers, act as amplifying projection operators at large depth Nl. In contrast, all of the eigenvalues of WTot in the scaled Gaussian case concentrate closer to the origin as depth increases. This discrepancy between the behavior of the eigenvalues and singular values of WTot, a phenomenon that could occur only if the eigenvectors of WTot are highly non-orthogonal, reﬂects the highly non-normal nature of products of random Gaussian matrices (a non-normal matrix is by deﬁnition a matrix whose eigenvectors are non-orthogonal). While the combination of ampliﬁcation and projection in WTot can preserve norm, it is clear that it is not a good way to backpropagate errors; the projection of error vectors onto a high dimensional subspace corre- sponding to small singular values would be strongly attenuated, yielding vanishingly small gradient signals corresponding to these directions in the early layers. This effect, which is not present for random orthogonal initializations or greedy pretraining, would naturally explain the long learning times starting from scaled random Gaussian initial conditions relative to the other initilizations in Fig. 6A left. For both linear and nonlinear networks, a more likely appropriate condition on weights for generating fast learning times would be that of dynamical isometry. By this we mean that the product of Jacobians associated with error signal backpropagation should act as a near isometry, up to some overall global O(1) scaling, on a subspace of as high a dimension as possible. This is equivalent to having as many singular values of the product of Jacobians as possible within a small range around an O(1) constant, and is closely related to the notion of restricted isometry in compressed sensing and random projections. Preserving norms is a necessary but not sufﬁcient condition for achieving dynamical isometry at large depths, as demonstrated in Fig. 6B, and we  11  have shown that for linear networks, orthogonal initializations achieve exact dynamical isometry with all singular values at 1, while greedy pre-training achieves it approximately. We note that the discrepancy in learning times between the scaled Gaussian initialization and the orthogonal or pre-training initializations is modest for the depths of around 6 used in large scale applications, but is magniﬁed at larger depths (Fig. 6A left). This may explain the modest improvement in learning times with greedy pre-training versus random scaled Gaussian initializations observed in applications (see discussion in Supplementary Appendix D). We predict that this modest improvement will be magniﬁed at higher depths, even in nonlinear networks. Finally, we note that in recurrent networks, which can be thought of as inﬁnitely deep feed-forward networks with tied weights, a very promising approach is a modiﬁcation to the training objective that partially promotes dynamical isometry for the set of gradients currently being back-propagated [24].  4 Achieving approximate dynamical isometry in nonlinear networks  We have shown above that deep random orthogonal linear networks achieve perfect dynamical isometry. Here we show that nonlinear versions of these networks can also achieve good dynamical isometry proper- ties. Consider the nonlinear feedforward dynamics  xl+1 i =  g W (l+1,l)  ij  φ(xl  j),  (20)  (cid:88)  j  i denotes the activity of neuron i in layer l, W (l+1,l)  where xl is a random orthogonal connectivity matrix from layer l to l + 1, g is a scalar gain factor, and φ(x) is any nonlinearity that saturates as x → ±∞. We show in Supplementary appendix G that there exists a critical value gc of the gain g such that if g < gc, activity will decay away to zero as it propagates through the layers, while if g > gc, the strong linear positive gain will combat the damping due to the saturating nonlinearity, and activity will propagate indeﬁnitely without decay, no matter how deep the network is. When the nonlinearity is odd (φ(x) = −φ(−x)), so that the mean activity in each layer is approximately 0, these dynamical properties can be quantitatively captured by the neural population variance in layer l,  ij  (21)  N(cid:88)  i=1  ql ≡ 1 N  (xl  i)2.  Thus liml→∞ ql → 0 for g < gc and liml→∞ ql → q∞(g) > 0 for g > gc. When φ(x) = tanh(x), we compute gc = 1 and numerically compute q∞(g) in Fig. 8 in Supplementary appendix G. Thus these non- linear feedforward networks exhibit a phase-transition at the critical gain; above the critical gain, inﬁnitely deep networks exhibit chaotic percolating activity propagation, so we call the critical gain gc the edge of chaos, in analogy with terminology for recurrent networks. Now we are interested in how errors at the ﬁnal layer Nl backpropagate back to earlier layers, and whether or not these gradients explode or decay with depth. To quantify this, for simplicity we consider the end to end Jacobian  ,  (22)  (cid:12)(cid:12)(cid:12)(cid:12)xNl  J Nl,1 ij  (xNl ) ≡ ∂xNl i ∂x1 j  which captures how input perturbations propagate to the output. If the singular value distribution of this Jacobian is well-behaved, with few extremely large or small singular values, then the backpropagation of gradients will also be well-behaved, and exhibit little explosion or decay. The Jacobian is evaluated at a particular point xNl in the space of output layer activations, and this point is in turn obtained by iterating (20) starting from an initial input layer activation vector x1. Thus the singular value distribution of the  12  Figure 7: Singular value distribution of the end to end Jacobian, deﬁned in (22), for various values of the gain g in (20) and the input layer population variance q = q1 in (21). The network architecture consists of Nl = 100 layers with N = 1000 neurons per layer, as in the linear case in Fig. 6B.  Jacobian will depend not only on the gain g, but also on the initial condition x1. By rotational symmetry, we expect this distribution to depend on x1, only through its population variance q1. Thus for large N, the singular value distribution of the end-to-end Jacobian in (22) (the analog of WTot in (19) in the linear case), depends on only two parameters: gain g and input population variance q1. We have numerically computed this singular value distribution as a function of these two parameters in Fig. 7, for a single random orthogonal nonlinear network with N = 1000 and Nl = 100. These results are typical; replotting the results for different random networks and different initial conditions (with the same input variance) yield very similar results. We see that below the edge of chaos, when g < 1, the linear dampening over many layers yields extremely small singular values. Above the edge of chaos, when g > 1, the combination of positive linear ampliﬁcation, and saturating nonlinear dampening yields an anisotropic distribution of singular values. At the edge of chaos, g = 1, an O(1) fraction of the singular value distribu-  13  0123x 10−5050100q = 0.2g = 0.90246x 10−30204060g = 0.9500.10.20.30.4010203040g = 100.511.52050100g = 1.0502460100200300400g = 1.10123x 10−5010203040q = 101234x 10−301020304000.10.20.30.401020304000.511.50501000123401002003004000123x 10−5010203040q = 401234x 10−3010203000.10.20.30.401020304000.511.505010015001230200400600tion is concentrated in a range that remains O(1) despite 100 layers of propagation, reﬂecting appoximate dynamical isometry. Moreover, this nice property at g = 1 remains valid even as the input variance q1 is increased far beyond 1, where the tanh function enters its nonlinear regime. Thus the right column of Fig. 7 at g near 1 indicates that the useful dynamical isometry properties of random orthogonal linear networks described above survives in nonlinear networks, even when activity patterns enter deeply into the nonlinear regime in the input layers. Interestingly, the singular value spectrum is more robust to perturbations that increase g from 1 relative to those that decrease g. Indeed, the anisotropy in the singular value distribution at g = 1.1 is relatively mild compared to that of random linear networks with scaled Gaussian initial conditions (compare the bottom row of Fig. 7 with the right column of panel B in Fig. 6). Thus overall, these numerical results suggest that being just beyond the edge of orthogonal chaos may be a good regime for learning in deep nonlinear networks. 5 Discussion  In summary, despite the simplicity of their input-output map, the dynamics of learning in deep linear net- works reveals a surprising amount of rich mathematical structure, including nonlinear hyperbolic dynamics, plateaus and sudden performance transitions, a proliferation of saddle points, symmetries and conserved quantities, invariant submanifolds of independently evolving connectivity modes subserving rapid learning, and most importantly, a sensitive but computable dependence of learning time scales on input statistics, ini- tial weight conditions, and network depth. With the right initial conditions, deep linear networks can be only a ﬁnite amount slower than shallow networks, and unsupervised pretraining can ﬁnd these initial conditions for tasks with the right structure. Moreover, we introduce a mathematical condition for faithful backprop- agation of error signals, namely dynamical isometry, and show, surprisingly that random scaled Gaussian initializations cannot achieve this condition despite their norm-preserving nature, while greedy pre-training and random orthogonal initialization can, thereby achieving depth independent learning times. Finally, we show that the property of dynamical isometry survives to good approximation even in extremely deep non- linear random orthogonal networks operating just beyond the edge of chaos. At the cost of expressivity, deep linear networks gain theoretical tractability and may prove fertile for addressing other phenomena in deep learning, such as the impact of carefully-scaled initializations [13, 23], momentum [23], dropout regulariza- tion [1], and sparsity constraints [2]. While a full analytical treatment of learning in deep nonlinear networks currently remains open, one cannot reasonably hope to move towards such a theory without ﬁrst completely understanding the linear case. In this sense, our work fulﬁlls an essential pre-requisite for progress towards a general, quantitative theory of deep learning.  References  [1] A. Krizhevsky, I. Sutskever, and G.E. Hinton. ImageNet Classiﬁcation with Deep Convolutional Neural  Networks. In Advances in Neural Information Processing Systems 25, 2012.  [2] Q.V. Le, M.A. Ranzato, R. Monga, M. Devin, K. Chen, G.S. Corrado, J. Dean, and A.Y. Ng. Build- ing high-level features using large scale unsupervised learning. In 29th International Conference on Machine Learning, 2012.  [3] D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column Deep Neural Networks for Image Classiﬁca-  tion. In IEEE Conf. on Computer Vision and Pattern Recognition, pages 3642–3649, 2012.  [4] A. Mohamed, G.E. Dahl, and G. Hinton. Acoustic Modeling Using Deep Belief Networks.  Transactions on Audio, Speech, and Language Processing, 20(1):14–22, January 2012.  IEEE  [5] R. Collobert and J. Weston. A Uniﬁed Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. In Proceedings of the 25th International Conference on Machine Learning, 2008.  14  [6] R. Socher, J. Bauer, C.D. Manning, and A.Y. Ng. Parsing with Compositional Vector Grammars. In  Association for Computational Linguistics Conference, 2013.  [7] S. Hochreiter. Untersuchungen zu dynamischen neuronalen Netzen. PhD thesis, TU Munich, 1991. [8] Y. Bengio, P. Simard, and P. Frasconi. Learning Long-Term Dependencies with Gradient Descent is  Difﬁcult. IEEE Transactions on Neural Networks, 5(2):157–166, 1994.  [9] Y. LeCun, L. Bottou, G.B. Orr, and K.R. M¨uller. Efﬁcient BackProp. Neural networks: Tricks of the  trade, 1998.  [10] Y. Bengio and Y. LeCun. Scaling learning algorithms towards AI. In L. Bottou, O. Chapelle, D. De- Coste, and J. Weston, editors, Large-Scale Kernel Machines, number 1, pages 1–41. MIT Press, 2007. [11] D. Erhan, P.A. Manzagol, Y. Bengio, S. Bengio, and P. Vincent. The Difﬁculty of Training Deep Ar- chitectures and the Effect of Unsupervised Pre-Training. In 12th International Conference on Artiﬁcial Intelligence and Statistics, volume 5, 2009.  [12] Y. Bengio. Learning Deep Architectures for AI. 2009. [13] X. Glorot and Y. Bengio. Understanding the difﬁculty of training deep feedforward neural networks.  13th International Conference on Artiﬁcial Intelligence and Statistics, 2010.  [14] D. Erhan, Y. Bengio, A. Courville, P.A. Manzagol, and P. Vincent. Why does unsupervised pre-training  help deep learning? Journal of Machine Learning Research, 11:625–660, 2010.  [15] Y.N. Dauphin and Y. Bengio. Big Neural Networks Waste Capacity. In International Conference on  Learning Representations, 2013.  [16] A.M. Saxe, J.L. McClelland, and S. Ganguli. Learning hierarchical category structure in deep neural  networks. In Proceedings of the 35th Annual Conference of the Cognitive Science Society, 2013.  [17] G.E. Hinton and R.R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Sci-  ence, 313(5786):504–7, July 2006.  [18] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy Layer-Wise Training of Deep Net-  works. Advances in Neural Information Processing Systems 20, 2007.  [19] P. Baldi and K. Hornik. Neural networks and principal component analysis: Learning from examples  without local minima. Neural Networks, 2(1):53–58, January 1989.  [20] K. Fukumizu. Effect of Batch Learning In Multilayer Neural Networks. In Proceedings of the 5th  International Conference on Neural Information Processing, pages 67–70, 1998.  [21] J. Martens. Deep learning via Hessian-free optimization.  Conference on Machine Learning, 2010.  In Proceedings of the 27th International  [22] O. Chapelle and D. Erhan. Improved Preconditioner for Hessian Free Optimization. In NIPS Workshop  on Deep Learning and Unsupervised Feature Learning, 2011.  [23] I. Sutskever, J. Martens, G. Dahl, and G.E. Hinton. On the importance of initialization and momentum  in deep learning. In 30th International Conference on Machine Learning, 2013.  [24] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training recurrent neural  networks. Technical report, Universite de Montreal, 2012.  [25] P. Lamblin and Y. Bengio. Important gains from supervised ﬁne-tuning of deep architectures on large  labeled sets. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2010.  15  Supplementary Material  A Hyperbolic dynamics of learning  In Section 1.3 of the main text we treat the dynamics of learning in three layer networks where mode strengths in each layer are equal, i.e, a = b, a reasonable limit when starting with small random initial conditions. More generally, though, we are interested in how long it takes for ab to approach s from any given initial condition. To access this, given the hyperbolic nature of the dynamics, it is useful to make the hyperbolic change of coordinates,  √ √  √ b = √  θ 2 θ 2  θ 2 θ 2  a =  c0 cosh  c0 sinh  for a2 > b2  (23)  b =  a =  c0 sinh  (24) Thus θ parametrizes the dynamically invariant manifolds a2 − b2 = ±c0. For any c0 and θ, this coordinate system covers the region a + b > 0, which is the basin of attraction of the upper right component of the hyperbola ab = s. A symmetric situation exists for a + b < 0, which is attracted to the lower left component of ab = s. We use θ as a coordinate to follow the dynamics of the product ab, and using the relations ab = c0 sinh θ and a2 + b2 = c0 cosh θ, we obtain  for a2 < b2.  c0 cosh  This differential equation is separable in θ and t and can be integrated to yield  (cid:90) θf  θ0  t = τ  dθ  s − c0 sinh θ  =  τ  dθ dt  = s − c0 sinh θ.  (cid:34)  ln  (cid:112)c2 (cid:112)c2  τ(cid:112)c2  0 + s2  (cid:35)θf  .  (25)  (26)  0 + s2 + c0 + s tanh θ 2 0 + s2 − c0 − s tanh θ  2  θ0  Here t is the amount of time it takes to travel from θ0 to θf along the hyperbola a2 − b2 = ±c0. The ﬁxed −1 s/c0, but the dynamics cannot reach the ﬁxed point in ﬁnite time. Therefore we point lies at θ = sinh introduce a cutoff (cid:15) to mark the endpoint of learning, so that θf obeys sinh θf = (1 − (cid:15))s/c0 (i.e. ab is close to s by a factor 1 − (cid:15)). We can then average over the initial conditions c0 and θ0 to obtain the expected learning time of an input-output relation that has a correlation strength s. Rather than doing this, it is easier to obtain a rough estimate of the timescale of learning under the assumption that the initial weights are small, so that c0 and θ0 are close to 0. In this case t = O(τ /s) (with a weak logarithmic dependence on the cutoff (i.e. ln(1/(cid:15))). This modestly generalizes the result given in the main text: the timescale of learning of each input-output mode α of the correlation matrix Σ31 is inversely proportional to the correlation strength sα of the mode even when a and b differ slightly, i.e., c0 small. This is not an unreasonable limit for random initial conditions because |c0| = |a · a − b · b| where a and b are random vectors of N2 synaptic weights into and out of the hidden units. Thus we expect the lengths of the two random vectors to be approximately equal and therefore c0 will be small relative to the length of each vector. These solutions are distinctly different from solutions for learning dynamics in three layer networks found in [20]. In our notation, in [20], it was shown that if the initial vectors aα and bα satisfy the matrix identity α bαbαT then the dynamics of learning becomes equivalent to a matrix Riccatti equation. However, the hyperbolic dynamics derived here arises from a set of initial conditions that do not satisfy the restrictions of [20] and therefore do not arise through a solution to a matrix Ricatti equation. Moreover, in going beyond a statement of the matrix Riccatti solution, our analysis provides intuition about the time- scales over which the learning dynamics unfolds, and crucially, our methods extend beyond the three layer case to the arbitrary Nl layer case, not studied in [20].  =(cid:80)  α aαaαT  (cid:80)  16  B Optimal discrete time learning rates  In Section 2 we state results on the optimal learning rate as a function of depth in a deep linear network, which we derive here. Starting from the decoupled initial conditions given in the main text, the dynamics arise from gradient descent on  Hence for each ai we have  The elements of the Hessian are thus  ∂2E ∂aiaj  for i (cid:54)= j, and  E(a1,··· , aNl−1) =  .  k=1  ak  ak  ak  1 2τ  (cid:33) (cid:32) s − Nl−1(cid:89)  ≡ f (ai) (cid:33)Nl−1(cid:89) (cid:32) s − Nl−1(cid:89)  − 1 Nl−1(cid:89) (cid:32) s − Nl−1(cid:89) 2 Nl−1(cid:89)  ≡ h(ai)  k(cid:54)=i  k(cid:54)=i  ak  ak  ak  k=1  k=1  =  τ  ∂2E ∂a2 i  1 τ  k(cid:54)=i  ∂E ∂ai  = − 1 τ  Nl−1(cid:89)  ak  =  1 τ  k(cid:54)=j ≡ g(ai, aj)  (cid:33)Nl−1(cid:89)  k(cid:54)=i,j    ak  (27)  (28)  (29)  (30)  (31)  for i = j. We now assume that we start on the symmetric manifold, such that ai = aj = a for all i, j. Thus we have  (cid:0)s − aNl−1(cid:1) , (cid:0)s − aNl−1(cid:1) aNl−2,  1 E(a) = 2τ f (a) = − 1 τ a2Nl−4 − 1 2 τ τ a2Nl−4 1 τ  h(a) =  g(a) =  saNl−3   .  g g  g g  ...  h g g h  1 τ    ··· h g g h ··· ... ... ··· ···  g g  g g  17  The Hessian is  H(a) =  One eigenvector is v1 = [11··· 1]T with eigenvalue λ1 = h + (Nl − 2)g, or saNl−3.  a2Nl−4 − (Nl − 2)  λ1 = (2Nl − 3)  1 τ  (32)  (33)  (34)  (35)  (36)  (37)  Now consider the second order update (Newton-Raphson) (here we use 1 to denote a vector of ones)  at+11 = at1 − H−1f (at)1 = at1 − f (at)H−11 at+1 = at − f (at)/λ1(at)  (38) (39) (40) Note that the basin of attraction does not include small initial conditions, because for small a the Hessian is not positive deﬁnite. To determine the optimal learning rate for ﬁrst order gradient descent, we compute the maximum of λ1 over the range of mode strengths that can be visited during learning, i.e., a ∈ [0, s1/(Nl−1)]. This occurs at the optimum, aopt = s1/(Nl−1). Hence substituting this into (37) we have 2Nl−4 Nl−1 .  λ1(aopt) = (Nl − 1)  (41)  s  The optimal learning rate α is proportional to 1/λ1(aopt), and hence scales as  1 τ  (cid:19)  (cid:18) 1  Nls2  α ∼ O  (42)  for large Nl.  B.1 Learning speeds with optimized learning rate  How does the optimal learning rate impact learning speeds? We compare the three layer learning time to the inﬁnite depth limit learning time, with learning rate set inversely proportional to Eqn. (41) with proportion- ality constant c. This yields a three layer learning time t3 of  uf (s − u0) u0(s − uf )  t3 = c ln  (cid:20)  log  (cid:18) uf (u0 − s)  u0(uf − s)  (cid:19)  +  s u0  − s uf  (cid:21)  ,  (43)  (44)  and an inﬁnite layer learning time t∞ of  t∞ = c  Hence the difference is  (45) where the ﬁnal approximation is for u0 = (cid:15), uf = s − (cid:15), and (cid:15) small. Thus very deep networks incur only a ﬁnite delay relative to shallow networks.  t∞ − t3 =  cs u0  − cs uf  ≈ cs (cid:15)  C Experimental setup for MNIST depth experiment  We trained deep linear networks on the MNIST dataset with ﬁfteen different depths Nl = {3, 5, 8, 10, 14, 20, 28, 36, 44, 54, 64, 74, 84, 94, 100}. Given a 784-dimensional input example, the network tried to predict a 10-dimensional output vector containing a 1 in the index for the correct class, and zeros elsewhere. The network was trained using batch gradient descent via Eqn. (13) on the 50,000 sample MNIST training dataset. We note that Eqn. (13) makes use of the linearity of the network to speed training and re- duce memory requirements. Instead of forward propagating all 50,000 training examples, we precompute  18  Σ31 and forward propagate only it. This enables experiments on very deep networks that otherwise would be computationally infeasible. Experiments were accelerated on GPU hardware using the GPUmat package. We used overcomplete hidden layers of size 1000. Here the overcompleteness is simply to demonstrate the applicability of the theory to this case; overcompleteness does not improve the representational power of the network. Networks were initialized with decoupled initial conditions and starting initial mode strength u0 = 0.001, as described in the text. The random orthogonal matrices Rl were selected by generating ran- dom Gaussian matrices and computing a QR decomposition to obtain an orthogonal matrix. Learning times were calculated as the iteration at which training error fell below a ﬁxed threshold of 1.3×104 corresponding to nearly complete learning. Note that this level of performance is grossly inferior to what can be obtained using nonlinear networks, which reﬂects the limited capacity of a linear network. We optimized the learning rate λ separately for each depth by training each network with twenty rates logarithmically spaced between 10−4 and 10−7 and picking the one that yielded the minimum learning time according to our threshold crite- rion. The range 10−4 and 10−7 was selected via preliminary experiments to ensure that the optimal learning rate always lay in the interior of the range for all depths.  D Efﬁcacy of unsupervised pretraining  Recently high performance has been demonstrated in deep networks trained from random initial conditions [21, 13, 22, 3, 4, 1, 23], suggesting that deep networks may not be as hard to train as previously thought. These results show that pretraining is not necessary to obtain state-of-the-art performance, and to achieve this they make use of a variety of techniques including carefully-scaled random initializations, more sophis- ticated second order or momentum-based optimization methods, and specialized convolutional architectures. It is therefore important to evaluate whether unsupervised pretraining is still useful, even if it is no longer necessary, for training deep networks. In particular, does pretraining still confer an optimization advantage and generalization advantage when used in conjunction with these new techniques? Here we review results from a variety of papers, which collectively show that unsupervised pretraining still confers an optimization advantage and a generalization advantage.  D.1 Optimization advantage  The optimization advatage of pretraining refers to faster convergence to the local optimum (i.e., faster learn- ing speeds) when starting from pretrained initializations as compared to random initializations. Faster learn- ing speeds starting from pretrained initial conditions have been consistently found with Hessian free opti- mization [21, 22]. This ﬁnding holds for two carefully-chosen random initialization schemes, the sparse connectivity scheme of [21], and the dense scaled scheme of [13] (as used by [22]). Hence pretraining still confers a convergence speed advantage with second order methods. Pretrained initial conditions also result in faster convergence than carefully-chosen random initializations when optimizing with stochastic gradient descent [22, 13]. In light of this, it appears that pretrained initial conditions confer an optimization advantage beyond what can be obtained currently with carefully-scaled random initializations, regardless of optimization technique. If run to convergence, second order methods and well-chosen scalings can erase the discrepancy between the ﬁnal objective value obtained on the training set for pretrained relative to random initializations [21, 22]. The optimization advantage is thus purely one of convergence speed, not of ﬁnding a better local minimum. This coincides with the situation in linear networks, where all methods will even- tually attain the same global minimum, but the rate of convergence can vary. Our analysis shows why this optimization advantage due to pretraining persists over well-chosen random initializations. Finally, we note that Sutskever et al. show that careful random initialization paired with carefully-tuned momentum can achieve excellent performance [23], but these experiments did not try pretrained initial conditions. Krizhevsky et al. used convolutional architectures and did not attempt pretraining [1]. Thus  19  the possible utility of pretraining in combination with momentum, and in combination with convolutional architectures, dropout, and large supervised datasets, remains unclear.  D.2 Generalization advantage  Pretraining can also act as a special regularizer, improving generalization error in certain instances. This generalization advantage appears to persist with new second order methods [21, 22], and in comparison to gradient descent with careful random initializations [13, 22, 25, 4]. An analysis of this effect in deep linear networks is out of the scope of this work, though promising tools have been developed for the three layer linear case [20].  E Learning dynamics with task-aligned input correlations  In the main text we focused on orthogonal input correlations (Σ11 = I) for simplicity, and to draw out the main intuitions. However our analysis can be extended to input correlations with a very particular structure. Recall that we decompose the input output correlations using the SVD as Σ31 = U 33S31V 11T . We can generalize our solutions to allow input correlations of the form Σ11 = V 11DV 11T . Intuitively, this condition requires the axes of variation in the input to coincide with the axes of variation in the input-output task, though the variances may differ. If we take D = I then we recover the whitened case Σ11 = I, and if we take D = Λ, then we can treat the autoencoding case. The ﬁnal ﬁxed points of the weights are given by the best rank N2 approximation to Σ31(Σ11)−1. Making the same change of variables as in Eqn. (4) we now obtain  21  W  = W  32T  (S31 − W  21  32  W  D),  τ  d dt  τ  d dt  32  W  = (S31 − W  21  32  W  D)W  21T  .  (46)  which, again, is decoupled if W our results for the learning dynamics.  32 and W  21 begin diagonal. Based on this it is straightforward to generalize  F MNIST pretraining experiment  We trained networks of depth 5 on the MNIST classiﬁcation task with 200 hidden units per layer, starting either from small random initial conditions with each weight drawn independently from a Gaussian distribu- tion with standard deviation 0.01, or from greedy layerwise pretrained initial conditions. For the pretrained network, each layer was trained to reconstruct the output of the next lower layer. In the ﬁnetuning stage, the network tried to predict a 10-dimensional output vector containing a 1 in the index for the correct class, and zeros elsewhere. The network was trained using batch gradient descent via Eqn. (13) on the 50,000 sam- ple MNIST training dataset. Since the network is linear, pretraining initializes the network with principal components of the input data, and, to the extent that the consistency condition of Eqn. (18) holds, decouples these modes throughout the deep network, as described in the main text.  G Analysis of Neural Dynamics in Nonlinear Orthogonal Networks  We can derive a simple, analytical recursion relation for the propagation of neural population variance ql, deﬁned in (21), across layers l under the nonlinear dynamics (20). We have  N(cid:88)  i=1  ql+1 =  1 N  N(cid:88)  i=1  (xl+1  i  )2 = g2 1 N  20  φ(xl  i)2,  (47)  (cid:90)  Dz φ(cid:0)(cid:112)  qlz(cid:1)2  due to the dynamics in (20) and the orthogonality of W (l+1,l). Now we know that by deﬁnition, the layer l population xl i has normalized variance ql. If we further assume that the distribution of activity across neurons in layer l is well approximated by a Gaussian distribution, we can replace the sum over neurons i with an integral over a zero mean unit variance Gaussian variable z:  ql+1 = g2  ,  (48)  where Dz ≡ 1√  2π  e− 1  2 z2  dz is the standard Gaussian measure. This map from input to output variance  Figure 8: Left: The map from variance in the input layer qin = ql to variance in the output layer qout = ql+1 in (48) for g = 1 and φ(x) = tanh(x). Right: The stable ﬁxed points of this map, q∞(g), as a function of the gain g. The red curve is the analytic theory obtained by numerically solving (49). The blue points are obtained via numerical simulations of the dynamics in (20) for networks of depth Nl = 30 with N = 1000 neurons per layer. The asymptotic population variance q∞ is obtained by averaging the population variance in the last 5 layers. is numerically computed for g = 1 and φ(x) = tanh(x) in Fig. 8, left (other values of g yield a simple  21  00.511.5200.20.40.60.8qinqoutInput to Output Variance Map at g=100.511.500.511.52gq∞(g)Stable population variance versus gainmultiplicative scaling of this map). This recursion relation has a stable ﬁxed point q∞(g) obtained by solving the nonlinear ﬁxed point equation  (cid:90)  Dz φ(cid:0)√  q∞z(cid:1)2  q∞ = g2  .  (49)  Graphically, solving this equation corresponds to scaling the curve in Fig. 8 left by g2 and looking for intersections with the line of unity. For g < 1, the only solution is q∞ = 0. For g > 1, this solution remains, but it is unstable under the recurrence (48). Instead, for g > 1, a new stable solution appears for some nonzero value of q∞. The entire set of stable solutions as a function of g is shown as the red curve in Fig. 8 right. It constitutes a theoretical prediction of the population variance at the deepest layers of a nonlinear network as the depth goes to inﬁnity. It matches well for example, the empirical population variance obtained from numerical simulations of nonlinear networks of depth 30 (blue points in Fig. 8 right). Overall, these results indicate a dynamical phase transition in neural activity propagation through the non- linear network as g crosses the critical value gc = 1. When g > 1, activity propagates in a chaotic manner, and so g = 1 constitutes the edge of chaos.  22  ","Despite the widespread practical success of deep learning methods, ourtheoretical understanding of the dynamics of learning in deep neural networksremains quite sparse. We attempt to bridge the gap between the theory andpractice of deep learning by systematically analyzing learning dynamics for therestricted case of deep linear neural networks. Despite the linearity of theirinput-output map, such networks have nonlinear gradient descent dynamics onweights that change with the addition of each new hidden layer. We show thatdeep linear networks exhibit nonlinear learning phenomena similar to those seenin simulations of nonlinear networks, including long plateaus followed by rapidtransitions to lower error solutions, and faster convergence from greedyunsupervised pretraining initial conditions than from random initialconditions. We provide an analytical description of these phenomena by findingnew exact solutions to the nonlinear dynamics of deep learning. Our theoreticalanalysis also reveals the surprising finding that as the depth of a networkapproaches infinity, learning speed can nevertheless remain finite: for aspecial class of initial conditions on the weights, very deep networks incuronly a finite, depth independent, delay in learning speed relative to shallownetworks. We show that, under certain conditions on the training data,unsupervised pretraining can find this special class of initial conditions,while scaled random Gaussian initializations cannot. We further exhibit a newclass of random orthogonal initial conditions on weights that, likeunsupervised pre-training, enjoys depth independent learning times. We furthershow that these initial conditions also lead to faithful propagation ofgradients even in deep nonlinear networks, as long as they operate in a specialregime known as the edge of chaos."
1312.6203,2014,Spectral Networks and Locally Connected Networks on Graphs  ,"['Joan Bruna', 'Wojciech Zaremba', 'Arthur Szlam', 'Yann LeCun']",https://arxiv.org/pdf/1312.6203.pdf,"4 1 0 2     y a M 1 2         ]  G L . s c [      3 v 3 0 2 6  .  2 1 3 1 : v i X r a  Spectral Networks and Deep Locally Connected  Networks on Graphs  Joan Bruna  New York University  bruna@cims.nyu.edu  Wojciech Zaremba New York University  woj.zaremba@gmail.com  Arthur Szlam  The City College of New York aszlam@ccny.cuny.edu  Yann LeCun  New York University yann@cs.nyu.edu  Abstract  Convolutional Neural Networks are extremely efﬁcient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possi- ble generalizations of CNNs to signals deﬁned on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low- dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efﬁcient deep architectures.  1  Introduction  Convolutional Neural Networks (CNNs) have been extremely succesful in machine learning prob- lems where the coordinates of the underlying data representation have a grid structure (in 1, 2 and 3 dimensions), and the data to be studied in those coordinates has translational equivariance/invariance with respect to this grid. Speech [11], images [14, 20, 22] or video [23, 18] are prominent examples that fall into this category. On a regular grid, a CNN is able to exploit several structures that play nicely together to greatly reduce the number of parameters in the system:  1. The translation structure, allowing the use of ﬁlters instead of generic linear maps and  hence weight sharing.  2. The metric on the grid, allowing compactly supported ﬁlters, whose support is typically  much smaller than the size of the input signals.  3. The multiscale dyadic clustering of the grid, allowing subsampling, implemented through  stride convolutions and pooling.  If there are n input coordinates on a grid in d dimensions, a fully connected layer with m outputs requires n · m parameters, which in typical operating regimes amounts to a complexity of O(n2) parameters. Using arbitrary ﬁlters instead of generic fully connected layers reduces the complexity to O(n) parameters per feature map, as does using the metric structure by building a “locally con- nected” net [8, 17]. Using the two together gives O(k · S) parameters, where k is the number of feature maps and S is the support of the ﬁlters, and as a result the learning complexity is independent of n. Finally, using the multiscale dyadic clustering allows each succesive layer to use a factor of 2d less (spatial) coordinates per ﬁlter.  1  In many contexts, however, one may be faced with data deﬁned over coordinates which lack some, or all, of the above geometrical properties. For instance, data deﬁned on 3-D meshes, such as surface tension or temperature, measurements from a network of meteorological stations, or data coming from social networks or collaborative ﬁltering, are all examples of structured inputs on which one cannot apply standard convolutional networks. Another relevant example is the intermediate representation arising from deep neural networks. Although the spatial convolutional structure can be exploited at several layers, typical CNN architectures do not assume any geometry in the “feature” dimension, resulting in 4-D tensors which are only convolutional along their spatial coordinates. Graphs offer a natural framework to generalize the low-dimensional grid structure, and by extension the notion of convolution. In this work, we will discuss constructions of deep neural networks on graphs other than regular grids. We propose two different constructions. In the ﬁrst one, we show that one can extend properties (2) and (3) to general graphs, and use them to deﬁne “locally” con- nected and pooling layers, which require O(n) parameters instead of O(n2). We term this the spatial construction. The other construction, which we call spectral construction, draws on the properties of convolutions in the Fourier domain. In Rd, convolutions are linear operators diagonalised by the Fourier basis exp(iω·t), ω, t ∈ Rd. One may then extend convolutions to general graphs by ﬁnding the corresponding “Fourier” basis. This equivalence is given through the graph Laplacian, an opera- tor which provides an harmonic analysis on the graphs [1]. The spectral construction needs at most O(n) paramters per feature map, and also enables a construction where the number of parameters is independent of the input dimension n. These constructions allow efﬁcient forward propagation and can be applied to datasets with very large number of coordinates.  1.1 Contributions  Our main contributions are summarized as follows:  • We show that from a weak geometric structure in the input domain it is possible to obtain efﬁcient architectures using O(n) parameters, that we validate on low-dimensional graph datasets. • We introduce a construction using O(1) parameters which we empirically verify, and we  discuss its connections with an harmonic analysis problem on graphs.  2 Spatial Construction  The most immediate generalisation of CNN to general graphs is to consider multiscale, hierarchical, local receptive ﬁelds, as suggested in [3]. For that purpose, the grid will be replaced by a weighted graph G = (Ω, W ), where Ω is a discrete set of size m and W is a m×m symmetric and nonnegative matrix.  2.1 Locality via W  The notion of locality can be generalized easily in the context of a graph. Indeed, the weights in a graph determine a notion of locality. For example, a straightforward way to deﬁne neighborhoods on W is to set a threshold δ > 0 and take neighborhoods  Nδ(j) = {i ∈ Ω : Wij > δ} .  We can restrict attention to sparse “ﬁlters” with receptive ﬁelds given by these neighborhoods to get locally connected networks, thus reducing the number of parameters in a ﬁlter layer to O(S · n), where S is the average neighborhood size.  2.2 Multiresolution Analysis on Graphs  CNNs reduce the size of the grid via pooling and subsampling layers. These layers are possible because of the natural multiscale clustering of the grid: they input all the feature maps over a cluster, and output a single feature for that cluster. On the grid, the dyadic clustering behaves nicely with respect to the metric and the Laplacian (and so with the translation structure). There is a large literature on forming multiscale clusterings on graphs, see for example [16, 25, 6, 13]. Finding  2  multiscale clusterings that are provably guaranteed to behave well w.r.t. Laplacian on the graph is still an open area of research. In this work we will use a naive agglomerative method. Figure 1 illustrates a multiresolution clustering of a graph with the corresponding neighborhoods.  Figure 1: Undirected Graph G = (Ω0, W ) with two levels of clustering. The original points are drawn in gray.  2.3 Deep Locally Connected Networks  The spatial construction starts with a multiscale clustering of the graph, similarly as in [3] We consider K scales. We set Ω0 = Ω, and for each k = 1 . . . K, we deﬁne Ωk, a partition of Ωk−1 into dk clusters; and a collection of neighborhoods around each element of Ωk−1:  Nk = {Nk,i ; i = 1 . . . dk−1} .  With these in hand, we can now deﬁne the k-th layer of the network. We assume without loss of generality that the input signal is a real signal deﬁned in Ω0, and we denote by fk the number of “ﬁlters” created at each layer k. Each layer of the network will transform a fk−1-dimensional signal indexed by Ωk−1 into a fk-dimensional signal indexed by Ωk, thus trading-off spatial resolution with newly created feature coordinates. More formally, if xk = (xk,i ; i = 1 . . . fk−1) is the dk−1 × fk−1 is the input to layer k, its the output xk+1 is deﬁned as  fk−1(cid:88)  i=1   (j = 1 . . . fk) ,  xk+1,j = Lkh  Fk,i,jxk,i  (2.1)  where Fk,i,j is a dk−1 × dk−1 sparse matrix with nonzero entries in the locations given by Nk, and Lk outputs the result of a pooling operation over each cluster in Ωk. This construcion is illustrated in Figure 2. In the current code, to build Ωk and Nk we use the following construction:  (cid:88)  (cid:88)  W0 = W  Ak(i, j) =  Wk−1(s, t) , (k ≤ K)  s∈Ωk(i)  t∈Ωk(j)  Wk = rownormalize(Ak) , (k ≤ K) Nk = supp(Wk) . (k ≤ K)  3  Figure 2: Spatial Construction as described by (2.1), with K = 2. For illustration purposes, the pooling operation is assimilated with the ﬁltering stage. Each layer of the transformation loses spatial resolution but increases the number of ﬁlters.  1. This is just one amongst many strategies to perform and Ωk is found as an (cid:15) covering for Wk hierarchicial agglomerative clustering. For a larger account of the problem, we refer the reader to [10]. If Sk is the average support of the neighborhoods Nk, we verify from (2.1) that the number of parameters to learn at layer k is  O(Sk · |Ωk| · fk · fk−1) = O(n) .  In practice, we have Sk ·|Ωk| ≈ α·|Ωk−1|, where α is the oversampling factor, typically α ∈ (1, 4). The spatial construction might appear na¨ıve, but it has the advantage that it requires relatively weak regularity assumptions on the graph. Graphs having low intrinsic dimension have localized neigh- borhoods, even if no nice global embedding exists. However, under this construction there is no easy way to induce weight sharing across different locations of the graph. One possible option is to consider a global embedding of the graph into a low dimensional space, which is rare in practice for high-dimensional data.  3 Spectral Construction  The global structure of the graph can be exploited with the spectrum of its graph-Laplacian to gen- eralize the convolution operator.  3.1 Harmonic Analysis on Weighted Graphs The combinatorial Laplacian L = D − W or graph Laplacian L = I − D−1/2W D−1/2 are gener- alizations of the Laplacian on the grid; and frequency and smoothness relative to W are interrelated through these operators [2, 25]. For simplicity, here we use the combinatorial Laplacian. If x is an m-dimensional vector, a natural deﬁnition of the smoothness functional ||∇x||2  W at a node i is  (cid:107)∇x(cid:107)2  W (i) =  (cid:88) (cid:88)  j  (cid:88)  Wij[x(i) − x(j)]2,  Wij[x(i) − x(j)]2 ,  and  (cid:107)∇x(cid:107)2  W =  (3.1)  With this deﬁnition, the smoothest vector is a constant: (cid:107)∇x(cid:107)2  v0 = arg min  √ W = (1/  x∈Rm (cid:107)x(cid:107)=1  m)1m.  i  j  1An (cid:15)-covering of a set Ω using a similarity kernel K is a partition P = {P1, . . . ,Pn} such that  supn supx,x(cid:48)∈Pn K(x, x(cid:48)) ≥ (cid:15).  4  Each succesive  vi =  arg min  x∈Rm (cid:107)x(cid:107)=1 x⊥{v0,...,vi−1}  (cid:107)∇x(cid:107)2  W  is an eigenvector of L, and the eigenvalues λi allow the smoothness of a vector x to be read off from the coefﬁcients of x in [v0, ...vm−1], equivalently as the Fourier coefﬁcients of a signal deﬁned in a grid. Thus, just an in the case of the grid, where the eigenvectors of the Laplacian are the Fourier vectors, diagonal operators on the spectrum of the Laplacian modulate the smoothness of their operands. Moreover, using these diagonal operators reduces the number of parameters of a ﬁlter from m2 to m. These three structures above are all tied together through the Laplacian operator on the d-  dimensional grid ∆x =(cid:80)d  ∂2x ∂u2 i  :  i=1  1. Filters are multipliers on the eigenvalues of the Laplacian ∆. 2. Functions that are smooth relative to the grid metric have coefﬁcients with quick decay in  the basis of eigenvectors of ∆.  3. The eigenvectors of the subsampled Laplacian are the low frequency eigenvectors of ∆.  3.2 Extending Convolutions via the Laplacian Spectrum  As in section 2.3, let W be a weighted graph with index set denoted by Ω, and let V be the eigen- vectors of the graph Laplacian L, ordered by eigenvalue. Given a weighted graph, we can try to generalize a convolutional net by operating on the spectrum of the weights, given by the eigenvec- tors of its graph Laplacian. For simplicity, let us ﬁrst describe a construction where each layer k = 1 . . . K transforms an input vector xk of size |Ω| × fk−1 into an output xk+1 of dimensions |Ω| × fk, that is, without spatial subsampling:  V  fk−1(cid:88)   (j = 1 . . . fk) ,  xk+1,j = h  Fk,i,jV T xk,i  (3.2)  i=1  where Fk,i,j is a diagonal matrix and, as before, h is a real valued nonlinearity. Often, only the ﬁrst d eigenvectors of the Laplacian are useful in practice, which carry the smooth geometry of the graph. The cutoff frequency d depends upon the intrinsic regularity of the graph and also the sample size. In that case, we can replace in (3.2) V by Vd, obtained by keeping the ﬁrst d columns of V . If the graph has an underlying group invariance this construction can discover it; the best example being the standard CNN; see 3.3. However, in many cases the graph does not have a group structure, or the group structure does not commute with the Laplacian, and so we cannot think of each ﬁlter as passing a template across Ω and recording the correlation of the template with that location. Ω may not be homogenous in a way that allows this to make sense, as we shall see in the example from Section 5.1. Assuming only d eigenvectors of the Laplacian are kept, equation (3.2) shows that each layer re- quires fk−1 · fk · d = O(|Ω|) paramters to train. We shall see in section 3.4 how the global and local regularity of the graph can be combined to produce layers with O(1) parameters, i.e. such that the number of learnable parameters does not depend upon the size of the input. This construction can suffer from the fact that most graphs have meaningful eigenvectors only for the very top of the spectrum. Even when the individual high frequency eigenvectors are not mean- ingful, a cohort of high frequency eigenvectors may contain meaningful information. However this construction may not be able to access this information because it is nearly diagonal at the highest frequencies. Finally, it is not obvious how to do either the forwardprop or the backprop efﬁciently while applying the nonlinearity on the space side, as we have to make the expensive multiplications by V and V T ; and it is not obvious how to do standard nonlinearities on the spectral side. However, see 4.1.  5  3.3 Rediscovering standard CNN’s  A simple, and in some sense universal, choice of weight matrix in this construction is the covariance of the data. Let X = (xk)k be the input data distribution, with xk ∈ Rn. If each coordinate j = 1 . . . n has the same variance,  j = E(cid:0)|x(j) − E(x(j))|2(cid:1) ,  σ2  then diagonal operators on the Laplacian simply scale the principal components of X. While this may seem trivial, it is well known that the principal components of the set of images of a ﬁxed size are (experimentally) correspond to the Discrete Cosine Transform basis, organized by frequency. This can be explained by noticing that images are translation invariant, and hence the covariance operator  Σ(j, j) = E ((x(j) − E(x(j)))(x(j(cid:48)) − E(x(j(cid:48)))))  satisﬁes Σ(j, j(cid:48)) = Σ(j − j(cid:48)), hence it is diagonalized by the Fourier basis. Moreover, it is well  known that natural images exhibit a power spectrum E(|(cid:98)x(ξ)|2) ∼ ξ−2, since nearby pixels are more  correlated than far away pixels. It results that principal components of the covariance are essentially ordered from low to high frequencies, which is consistent with the standard group structure of the Fourier basis. The upshot is that, when applied to natural images, the construction in 3.2 using the covariance as the similarity kernel recovers a standard convolutional network, without any prior knowledge. Indeed, the linear operators V Fi,jV T from Eq (3.2) are by the previous argument diagonal in the Fourier basis, hence translation invariant, hence “classic” convolutions. Moreover, Section 4.1 explains how spatial subsampling can also be obtained via dropping the last part of the spectrum of the Laplacian, leading to max-pooling, and ultimately to deep convolutonal networks.  3.4 O(1) construction with smooth spectral multipliers  In the standard grid, we do not need a parameter for each Fourier function because the ﬁlters are compactly supported in space, but in (3.2), each ﬁlter requires one parameter for each eigenvector on which it acts. Even if the ﬁlters were compactly supported in space in this construction, we still would not get less than O(n) parameters per ﬁlter because the spatial response would be different at each location. One possibility for getting around this is to generalize the duality of the grid. On the Euclidian grid, the decay of a function in the spatial domain is translated into smoothness in the Fourier domain, and viceversa. It results that a funtion x which is spatially localized has a smooth frequency response ˆx = V T x. In that case, the eigenvectors of the Laplacian can be thought of as being arranged on a grid isomorphic to the original spatial grid. This suggests that, in order to learn a layer in which features will be not only shared across locations but also well localized in the original domain, one can learn spectral multipliers which are smooth. Smoothness can be prescribed by learning only a subsampled set of frequency multipliers and using an interpolation kernel to obtain the rest, such as cubic splines. However, the notion of smoothness requires a geometry in the domain of spectral coordinates, which can be obtained by deﬁning a dual  graph(cid:102)W as shown by (3.1). As previously discussed, on regular grids this geometry is given by the  notion of frequency, but this cannot be directly generalized to other graphs. A particularly simple and navie choice consists in choosing a 1-dimensional arrangement, obtained by ordering the eigenvectors according to their eigenvalues. In this setting, the diagonal of each ﬁlter Fk,i,j (of size at most |Ω|) is parametrized by  diag(Fk,i,j) = K αk,i,j ,  where K is a d × qk ﬁxed cubic spline kernel and αk,i,j are the qk spline coefﬁcients. If one seeks to have ﬁlters with constant spatial support (ie, whose support is independent of the input size |Ω|), it follows that one can choose a sampling step α ∼ |Ω| in the spectral domain, which results in a constant number qk ∼ |Ω| · α−1 = O(1) of coefﬁcients αk,i,j per ﬁlter. Although results from section 5 seem to indicate that the 1-D arrangement given by the spectrum of the Laplacian is efﬁcient at creating spatially localized ﬁlters, a fundamental question is how to  6  a dual graph(cid:99)W by measuring the similarity of in the spectral domain: (cid:98)X = V T X. The similarity  deﬁne a dual graph capturing the geometry of spectral coordinates. A possible algorithmic stategy is to consider an input distribution X = (xk)k consisting on spatially localized signals and to construct could be measured for instance with E((|ˆx| − E(|ˆx)|))T (|ˆx| − E(|ˆx|)).  4 Relationship with previous work  There is a large literature on building wavelets on graphs, see for example [21, 7, 4, 5, 9]. A wavelet basis on a grid, in the language of neural networks, is a linear autoencoder with certain provable regularity properties (in particular, when encoding various classes of smooth functions, sparsity is guaranteed). The forward propagation in a classical wavelet transform strongly resembles the forward propagation in a neural network, except that there is only one ﬁlter map at each layer (and it is usually the same ﬁlter at each layer), and the output of each layer is kept, rather than just the output of the ﬁnal layer. Classically, the ﬁlter is not learned, but constructed to facilitate the regularity proofs. In the graph case, the goal is the same; except that the smoothness on the grid is replaced by smooth- ness on the graph. As in the classical case, most works have tried to construct the wavelets explicitly (that is, without learning), based on the graph, so that the corresponding autencoder has the correct sparsity properties. In this work, and the recent work [21], the “ﬁlters” are constrained by con- struction to have some of the regularity properties of wavelets, but are also trained so that they are appropriate for a task separate from (but perhaps related to) the smoothness on the graph. Whereas [21] still builds a (sparse) linear autoencoder that keeps the basic wavelet transform setup, this work focuses on nonlinear constructions; and in particular, tries to build analogues of CNN’s. Another line of work which is rellevant to the present work is that of discovering grid topologies from data. In [19], the authors empirically conﬁrm the statements of Section 3.3, by showing that one can recover the 2-D grid structure via second order statistics. In [3, 12] the authors estimate similarities between features to construct locally connected networks.  4.1 Multigrid  We could improve both constructions, and to some extent unify them, with a multiscale clustering of the graph that plays nicely with the Laplacian. As mentioned before, in the case of the grid, the standard dyadic cubes have the property that subsampling the Fourier functions on the grid to a coarser grid is the same as ﬁnding the Fourier functions on the coarser grid. This property would eliminate the annoying necessity of mapping the spectral construction to the ﬁnest grid at each layer to do the nonlinearity; and would allow us to interpret (via interpolation) the local ﬁlters at deeper layers in the spatial construction to be low frequency. This kind of clustering is the underpinning of the multigrid method for solving discretized PDE’s (and linear systems in general) [24]. There have been several papers extending the multigrid method, and in particular, the multiscale clustering(s) associated to the multigrid method, in settings more general than regular grids, see for example [16, 15] for situations as in this paper, and see [24] for the algebraic multigrid method in general. In this work, for simplicity, we use a naive multiscale clus- tering on the space side construction that is not guaranteed to respect the original graph’s Laplacian, and no explicit spatial clustering in the spectral construction.  5 Numerical Experiments  The previous constructions are tested on two variations of the MNIST data set. In the ﬁrst, we subsample the normal 28 × 28 grid to get 400 coordinates. These coordinates still have a 2-D structure, but it is not possible to use standard convolutions. We then make a dataset by placing d = 4096 points on the 3-D unit sphere and project random MNIST images onto this set of points, as described in Section 5.2. In all the experiments, we use Rectiﬁed Linear Units as nonlinearities and max-pooling. We train the models with cross-entropy loss, using a ﬁxed learning rate of 0.1 with momentum 0.9.  7  (a)  (b)  Figure 3: Subsampled MNIST examples.  5.1 Subsampled MNIST  We ﬁrst apply the constructions from sections 3.2 and 2.3 to the subsampled MNIST dataset. Figure 3 shows examples of the resulting input signals, and Figures 4, 5 show the hierarchical clustering constructed from the graph and some eigenfunctions of the graph Laplacian, respectively. The per- formance of various graph architectures is reported in Table 1. To serve as a baseline, we compute the standard Nearest Neighbor classiﬁer, which performs slightly worse than in the full MNIST dataset (2.8%). A two-layer Fully Connected neural network reduces the error to 1.8%. The geo- metrical structure of the data can be exploited with the CNN graph architectures. Local Receptive Fields adapted to the graph structure outperform the fully connected network. In particular, two layers of ﬁltering and max-pooling deﬁne a network which efﬁciently aggregates information to the ﬁnal classiﬁer. The spectral construction performs slightly worse on this dataset. We consid- ered a frequency cutoff of N/2 = 200. However, the frequency smoothing architecture described in section 3.4, which contains the smallest number of parameters, outperforms the regular spectral construction. These results can be interpreted as follows. MNIST digits are characterized by localized oriented strokes, which require measurements with good spatial localization. Locally receptive ﬁelds are constructed to explicitly satisfy this constraint, whereas in the spectral construction the measure- ments are not enforced to become spatially localized. Adding the smoothness constraint on the spectrum of the ﬁlters improves classiﬁcation results, since the ﬁlters are enforced to have better spatial localization. This fact is illustrated in Figure 6. We verify that Locally Receptive ﬁelds encode different templates across different spatial neighborhoods, since there is no global strucutre tying them together. On the other hand, spectral constructions have the capacity to generate local measurements that generalize across the graph. When the spectral multipliers are not constrained, the resulting ﬁlters tend to be spatially delocalized, as shown in panels (c)-(d). This corresponds to the fundamental limitation of Fourier analysis to encode local phenomena. However, we observe in panels (e)-(f) that a simple smoothing across the spectrum of the graph Laplacian restores some form of spatial localization and creates ﬁlters which generalize across different spatial positions, as should be expected for convolution operators.  5.2 MNIST on the sphere  We test in this section the graph CNN constructions on another low-dimensional graph. In this case, we lift the MNIST digits to the sphere. The dataset is constructed as follows. We ﬁrst sample 4096 random points S = {sj}j≤4096 from the unit sphere S2 ⊂ R3. We then consider an orthogonal basis E = (e1, e2, e3) of R3 with (cid:107)e1(cid:107) = 1 , (cid:107)e2(cid:107) = 2 , (cid:107)e3(cid:107) = 3 and a random covariance operator Σ = (E+W )T (E+W ), where W is a Gaussian iid matrix with variance σ2 < 1. For each signal xi from the original MNIST dataset, we sample a covariance operator Σi from the former distribution and consider its PCA basis Ui. This basis deﬁnes a point of view and in-plane rotation which we use  8  (a)  (b)  Figure 4: Clusters obtained with the agglomerative clustering. (a) Clusters corresponding to the ﬁnest scale k = 1, (b) clusters for k = 3 .  (a)  (b)  Figure 5: Examples of Eigenfunctions of the Graph Laplacian v2, v20.  Table 1: Classiﬁcation results on MNIST subsampled on 400 random locations, for different ar- chitectures. FCN stands for a fully connected layer with N outputs, LRFN denotes the locally connected construction from Section 2.3 with N outputs, MPN is a max-pooling layer with N outputs, and SPN stands for the spectral layer from Section 3.2.  method  Nearest Neighbors 400-FC800-FC50-10  400-LRF1600-MP800-10  400-LRF3200-MP800-LRF800-MP400-10  400-SP1600-10 (d1 = 300, q = n) 400-SP1600-10 (d1 = 300, q = 32) 400-SP4800-10 (d1 = 300, q = 20)  Parameters Error N/A 4.11 3.6 · 105 1.8 7.2 · 104 1.8 1.6 · 105 1.3 3.2 · 103 2.6 1.6 · 103 2.3 5 · 103 1.8  9  (a)  (c)  (e)  (b)  (d)  (f)  Figure 6: Subsampled MNIST learnt ﬁlters using spatial and spectral construction. (a)-(b) Two dif- ferent receptive ﬁelds encoding the same feature in two different clusters. (c)-(d) Example of a ﬁlter obtained with the spectral construction. (e)-(f) Filters obtained with smooth spectral construction.  to project xi onto S using bicubic interpolation. Figure 7 shows examples of the resulting projected digits. Since the digits ‘6’ and ‘9’ are equivalent modulo rotations, we remove the ‘9’ from the dataset. Figure 8 shows two eigenvectors of the graph Laplacian. We ﬁrst consider “mild” rotations with σ2 = 0.2. The effect of such rotations is however not negligible. Indeed, table 2 shows that the Nearest Neighbor classifer performs considerably worse than in the previous example. All the neural network architectures we considered signiﬁcatively improve over this basic classiﬁer. Furthermore, we observe that both convolutional constructions match the fully connected constructions with far less parameters (but in this case, do not improve its performance). Figure 9 displays the ﬁlters learnt using different constructions. Again, we verify  10  Table 2: Classiﬁcation results on the MNIST-sphere dataset generated using partial rotations, for different architectures  method  Nearest Neighbors  4096-FC2048-FC512-9  4096-LRF4620-MP2000-FC300-9  4096-LRF4620-MP2000-LRF500-MP250-9  4096-SP32K-MP3000-FC300-9 (d1 = 2048, q = n) 4096-SP32K-MP3000-FC300-9 (d1 = 2048, q = 64)  Parameters Error 19 5.6 6 6.5 7 6  N/A 107 8 · 105 2 · 105 9 · 105 9 · 105  that the smooth spectral construction consistently improves the performance, and learns spatially localized ﬁlters, even using the naive 1-D organization of eigenvectors, which detect similar features across different locations of the graph (panels (e)-(f)). Finally, we consider the uniform rotation case, where now the basis Ui is a random basis of R3. In that case, the intra-class variability is much more severe, as seen by inspecting the performance of the Nearest neighbor classiﬁer. All the previously described neural network architectures signiﬁcantly improve over this classiﬁer, although the performance is notably worse than in the mild rotation scenario. In this case, an efﬁcient representation needs to be fully roto-translation invariant. Since this is a non-commutative group, it is likely that deeper architectures perform better than the models considered here.  (a)  (b)  Figure 7: Examples of some MNIST digits on the sphere.  (a)  (b)  Figure 8: Examples of Eigenfunctions of the Graph Laplacian v20, v100  11  (a)  (c)  (e)  (b)  (d)  (f)  Figure 9: Filters learnt on the MNIST-sphere dataset, using spatial and spectral construction. (a)-(b) Two different receptive ﬁelds encoding the same feature in two different clusters. (c)-(d) Example of a ﬁlter obtained with the spectral construction. (e)-(f) Filters obtained with smooth spectral construction.  6 Conclusion  Using graph-based analogues of convolutional architectures can greatly reduce the number of pa- rameters in a neural network without worsening (and often improving) the test error, while simul- taneously giving a faster forward propagation. These methods can be scaled to data with a large number of coordinates that have a notion of locality. There is much to be done here. We suspect with more careful training and deeper networks we can consistently improve on fully connected networks on “manifold like” graphs like the sampled sphere.  12  Table 3: Classiﬁcation results on the MNIST-sphere dataset generated using uniformly random ro- tations, for different architectures  method  Nearest Neighbors  4096-FC2048-FC512-9  4096-LRF4620-MP2000-FC300-9  4096-LRF4620-MP2000-LRF500-MP250-9  4096-SP32K-MP3000-FC300-9 (d1 = 2048, q = n) 4096-SP32K-MP3000-FC300-9 (d1 = 2048, q = 64)  Parameters Error 80 52 61 63 56 50  NA 107 8 · 105 2 · 105 9 · 105 9 · 105  Furthermore, we intend to apply these techniques to less artiﬁcal problems, for example, on netﬂix like recommendation problems where there is a biclustering of the data and coordinates. Finally, the fact that smoothness on the naive ordering of the eigenvectors leads to improved results and localized ﬁlters suggests that it may be possible to make “dual” constructions with O(1) parameters per ﬁlter in much more generality than the grid.  References [1] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embed-  ding and clustering. In NIPS, volume 14, pages 585–591, 2001.  [2] F. R. K. Chung. Spectral Graph Theory. American Mathematical Society. [3] Adam Coates and Andrew Y Ng. Selecting receptive ﬁelds in deep networks. In Advances in  Neural Information Processing Systems, 2011.  [4] R.R. Coifman and M. Maggioni. Diffusion wavelets. Appl. Comp. Harm. Anal., 21(1):53–94,  July 2006.  [5] Mark Crovella and Eric D. Kolaczyk. Graph wavelets for spatial trafﬁc analysis. In INFOCOM,  2003.  [6] Inderjit S. Dhillon, Yuqiang Guan, and Brian Kulis. Weighted graph cuts without eigenvectors a multilevel approach. IEEE Trans. Pattern Anal. Mach. Intell., 29(11):1944–1957, November 2007.  [7] Matan Gavish, Boaz Nadler, and Ronald R. Coifman. Multiscale wavelets on trees, graphs and high dimensional data: Theory and applications to semi supervised learning. In Johannes Frankranz and Thorsten Joachims, editors, ICML, pages 367–374, 2010.  [8] Karol Gregor and Yann LeCun. Emergence of complex-like cells in a temporal product net-  work with local receptive ﬁelds. CoRR, abs/1006.0448, 2010.  [9] I. Guskov, W. Sweldens, and P. Schr¨oder. Multiresolution signal processing for meshes. Com-  puter Graphics Proceedings (SIGGRAPH 99), pages 325–334, 1999.  [10] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning:  data mining, inference and prediction. Springer, 2 edition, 2009.  [11] Geoffrey E. Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, and Brian Kings- bury. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Process. Mag., 29(6):82–97, 2012.  [12] Yangqing Jia, Chang Huang, and Trevor Darrell. Beyond spatial pyramids: Receptive ﬁeld In Computer Vision and Pattern Recognition (CVPR),  learning for pooled image features. 2012 IEEE Conference on, pages 3370–3377. IEEE, 2012.  [13] George Karypis and Vipin Kumar. Metis - unstructured graph partitioning and sparse matrix  ordering system, version 2.0. Technical report, 1995.  [14] Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classiﬁcation with deep con- volutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1106–1114, 2012.  13  [15] D. Kushnir, M. Galun, and A. Brandt. Efﬁcient multilevel eigensolvers with applications to data analysis tasks. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 32(8):1377–1391, 2010.  [16] Dan Kushnir, Meirav Galun, and Achi Brandt. Fast multiscale clustering and manifold iden- tiﬁcation. Pattern Recognition, 39(10):1876 – 1891, 2006. ¡ce:title¿Similarity-based Pattern Recognition¡/ce:title¿.  [17] Quoc V. Le, Jiquan Ngiam, Zhenghao Chen, Daniel Chia, Pang Wei Koh, and Andrew Y. Ng.  Tiled convolutional neural networks. In In NIPS, 2010.  [18] Quoc V Le, Will Y Zou, Serena Y Yeung, and Andrew Y Ng. Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis. In Com- puter Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 3361–3368. IEEE, 2011.  [19] Nicolas Le Roux, Yoshua Bengio, Pascal Lamblin, Marc Joliveau, Bal´azs K´egl, et al. Learning  the 2-d topology of images. In NIPS, 2007.  [20] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document  recognition. In Intelligent Signal Processing, pages 306–351. IEEE Press, 2001.  [21] Raif M. Rustamov and Leonidas Guibas. Wavelets on graphs via deep learning. In NIPS, 2013. [22] Pierre Sermanet, Soumith Chintala, and Yann LeCun. Convolutional neural networks applied In International Conference on Pattern Recognition  to house numbers digit classiﬁcation. (ICPR 2012), 2012.  [23] Graham, W. Taylor, Rob Fergus, Yann LeCun, and Christoph Bregler. Convolutional learning of spatio-temporal features. In Proc. European Conference on Computer Vision (ECCV’10), 2010.  [24] Ulrich Trottenberg and Anton Schuller. Multigrid. Academic Press, Inc., Orlando, FL, USA,  2001.  [25] U. von Luxburg. A tutorial on spectral clustering. Technical Report 149, 08 2006.  14  ","Convolutional Neural Networks are extremely efficient architectures in imageand audio recognition tasks, thanks to their ability to exploit the localtranslational invariance of signal classes over their domain. In this paper weconsider possible generalizations of CNNs to signals defined on more generaldomains without the action of a translation group. In particular, we proposetwo constructions, one based upon a hierarchical clustering of the domain, andanother based on the spectrum of the graph Laplacian. We show throughexperiments that for low-dimensional graphs it is possible to learnconvolutional layers with a number of parameters independent of the input size,resulting in efficient deep architectures."
1312.5921,2014,Group-sparse Embeddings in Collective Matrix Factorization  ,"['Arto Klami', 'Guillaume Bouchard', 'Abhishek Tripathi']",https://arxiv.org/pdf/1312.5921.pdf,"Group-sparse Embeddings in Collective Matrix Factorization  Arto Klami Helsinki Institute for Information Technology HIIT, Department of Information and Computer Science, Univer- sity of Helsinki  arto.klami@cs.helsinki.fi  4 1 0 2     b e F 8 1         ] L M  . t a t s [      2 v 1 2 9 5  .  2 1 3 1 : v i X r a  Guillaume Bouchard Xerox Research Centre Europe  Abhishek Tripathi Xerox Research Centre India  Abstract  CMF is a technique for simultaneously learn- ing low-rank representations based on a col- lection of matrices with shared entities. A typical example is the joint modeling of user- item, item-property, and user-feature matri- ces in a recommender system. The key idea in CMF is that the embeddings are shared across the matrices, which enables transfer- ring information between them. The existing solutions, however, break down when the in- dividual matrices have low-rank structure not shared with others. In this work we present a novel CMF solution that allows each of the matrices to have a separate low-rank struc- ture that is independent of the other matri- ces, as well as structures that are shared only by a subset of them. We compare MAP and variational Bayesian solutions based on al- ternating optimization algorithms and show that the model automatically infers the na- ture of each factor using group-wise sparsity. Our approach supports in a principled way continuous, binary and count observations and is eﬃcient for sparse matrices involving missing data. We illustrate the solution on a number of examples, focusing in particu- lar on an interesting use-case of augmented multi-view learning.  1. INTRODUCTION  Matrix factorization techniques provide low-rank vec- torial representations by approximating a matrix X ∈ Rn×d as the outer product of two rank-k matrices U1 ∈ Rn×k and U2 ∈ Rd×k (Fig. 1-I). This formu- lation encompasses a multitude of standard data anal- ysis models from PCA and factor analysis to more re-  guillaume.bouchard@xrce.xerox.com  abishek.tripathi3@xerox.com  Figure 1. Examples of matrix factorization setups.  cent models such as NMF (Paatero and Tapper, 1994; Lee and Seung, 2001) and various sophisticated fac- torization models proposed for recommender system applications (Mnih and Salakhutdinov, 2007; Koren et al., 2009; Sarwar et al., 2000).  Many data analysis tasks call for more complex se- tups. Multi-view learning (Fig. 1-II) considers scenar- ios with multiple matrices Xm that share the same row entities but diﬀer in the column entities; for example, X1 might contain ratings given for d1 diﬀerent movies by n diﬀerent users, whereas X2 represents the same n users with d2 proﬁle features. For such setups the ap- propriate approach is to factorize the set of matrices {Xm} simultaneously so that (at least some of) the factors in U1 are shared across the matrices. Models that share all of the factors are fundamentally equiva- lent to simple factorizations of a concatenated matrix X = [X1, ..., Xm]. To reach a richer class of models one needs to allow each matrix to have also private factors, i.e. factors independent of the other matrices (Jia et al., 2010; Virtanen et al., 2012). For the case  e1 e2 e2 e3 1X2Xen … nXe1 I.Single-view – PCA, NMF etc II.Multi-view – CCA, IBFA etc. III.Augmented multi-view – CMF  I. II. 1X2X3XIII. e1 e3 e2 Example of Augmented multi-view set-up      - Gene expression (Patients e1, Genes e2)      - Copy number change (Patients e1, Genes e3)      - Augmented matrix (e2, e3) represents         chromosomal proximity of genes in two views   1X2X3X1U2U1XGroup-sparse Embeddings in Collective Matrix Factorization  of M = 2 the distinction is crystallized by the inter- battery factor analysis (IBFA) formulation of Klami et al. (2013).  Even more general setups with arbitrary collections of matrices that share some sets of entities have been pro- posed several times by diﬀerent authors, under names such as co-factorization or multi-relational matrix fac- torization, and most end up being either a variant of tensor factorization of knowledge bases (Nickel et al., 2011; Chen et al., 2013) or a special case of Collective Matrix Factorization (CMF; Singh and Gordon, 2008). In this paper, we concentrate on the CMF model, i.e. on bilinear forms, but the ideas can be easily extended to three-way interactions, i.e. tensors. A prototyp- ical example of CMF, illustrated by Bouchard et al. (2013), would be a recommender system setup where the target matrix X1 is complemented with two other matrices associating the users and items with their own features. If the users and items are described with the same features, for example by proximities to geograph- ical locations, the setup becomes circular. Another interesting use case for such circular setups is found in augmenting multi-view learning, in scenarios where additional information is provided on relationships be- tween the features of two (or more) views. Figure 1- III depicts an example where the two views X1 and X2 represent expression and copy number alteration of the same patients. Classical multi-view solutions to this problem would ignore the fact that the column features for both views correspond to genes. With CMF, however, we can encode this information as a third matrix X3 that provides chromosomal promixity of the probes used for measuring the two views. Even though this kind of setup is very common in practi- cal multi-view learning, the problem of handling such relationships has not attracted much attention.  Several solutions for the CMF problem have been pre- sented. Singh and Gordon (2008) provided a maxi- mum likelihood solution, Singh and Gordon (2010) and Yin et al. (2013) used Gibbs sampling to approximate the posterior, and Bouchard et al. (2013) presented a convex formulation of the problem. While all of these earlier solutions to the CMF problem provide mean- ingful factorizations, they share the same problem as the simplest solutions to the multi-view setup; they assume that all of the matrices are directly related to each other and that every factor describes variation in all matrices. Such strong assumptions are unlikely to hold in practical applications, and consequently the methods break down for scenarios where the individ- ual matrices have strong view-speciﬁc noise or, more generally, any subset of the matrices has structure in- dependent of the others. In this work we remove the  shortcoming by introducing a novel CMF solution that allows also factors private to arbitrary subsets of the matrices, by adding a group-wise sparsity constraint for the factors.  We use group-wise sparse regularization of factors, where the groups corresponds to all the entities with the same type. In the Bayesian setting, this group- regularization is obtained by using automatic rele- vance determination (ARD) for controlling factor ac- tity (Virtanen et al., 2012). This regularization en- ables us to automatically learn the nature of each fac- tor, resulting in a solution free of tuning parameters. The model supports arbitrary schemas for the collec- tion of matrices, as well as multiple likelihood poten- tials for various types of data (binary, count and con- tinous), using the quadratic lower bounds provided by Seeger and Bouchard (2012) for non-Gaussian likeli- hoods.  To illustrate the ﬂexibility of the CMF setup we dis- cuss interesting modeling tasks in Section 6. We pay particular attention to the augmented multi-view learning setup of Figure 1-III, showing that CMF pro- vides a natural way to improve on standard multi-view learning when the diﬀerent views lay in related obser- vation spaces. We also show experimentally the key advantage of ARD used for complexity control, com- pared to computationally intensive cross-validation of regularization parameters.  2. COLLECTIVE MATRIX  FACTORIZATION  ij  Given a set of M matrices Xm = [x(m) ] describing rela- tionships between E sets of entities (with cardinalities de), the goal of CMF is to jointly approximate the ma- trices with low-rank factorizations. We denote by rm and cm the entity sets corresponding to the rows and columns, respectively, of the m-th matrix. For a simple matrix factorization we have M = 1, E = 2, rm = 1, and cm = 2 (Fig. 1-I). Multi-view setups, in turn, have E = M + 1, rm = 1 ∀m, and cm ∈ {2, ..., M + 1} (Fig. 1-II). Some non-trivial CMF setups are depicted in Figures 1-III and 2.  2.1. Model  We approximate each matrix with a rank-K product plus additional row and column bias terms. For linear models, the element corresponding to the row i and column j of the m-th matrix is given by:  x(m) ij =  u(rm) ik u(cm)  jk + b(m,r)  i  + b(m,c)  j  + ε(m)  ij  ,  (1)  K(cid:88)  k=1  Group-sparse Embeddings in Collective Matrix Factorization  to solve the CMF problem; the fact that the blocks along the diagonal are unobserved will usually be cru- cial here, since it means that no quadratic terms will be involved in the optimization. In Section 4.2 a vari- ational Bayesian approximation is introduced to learn the model, but before we explain how the basic formu- lation needs to be extended to allow matrix-speciﬁc low-rank variations.  3. Group-wise sparse CMF  3.1. Private factors in CMF  Without further restrictions the solutions to (2) tie all matrices to each other; for each factor k the corre- sponding column of U has non-zero values for entities in every set e. This is undesirable for many practical CMF applications where the individual matrices are likely to have structured noise independent of other matrices. Since the structured noise cannot be cap- tured by the element-wise independent noise terms ε, the model will need to introduce new factors for mod- eling the variation speciﬁc to one matrix alone.  We use the following property of the basic CMF model: if the k-th columns of the factor matrices Ue are null for all but two entity types rm and cm, it implies that the k-th factor impacts only the matrix Xm, i.e. the factor k is a private factor for relation m. To allow the automatic creation of these private factors, we put group-sparse priors on the columns of the matrices Ue. Using the symmetric representation, this approach cre- ates group-sparse factorial representations similar to the one represented in Figure 2. Note that if more than two groups of variables are non-zero for a given factor k, it means that it is private for a group of matri- ces rather than a single matrix, and the standard CMF is obtained if no groups equal to zero. In Figure 2 the ﬁrst factor is a global factor as used in the standard CMF, since it is non-zero everywhere, and the rest are private to some matrices. Note that the last factor rep- resented in light-blue in (k = 6) is interesting because it is a private factor overlapping multiple matrices (X1 and X2) rather than a single one for the other private factors (matrix X1 for factors 2 and 3, matrix X3 for factors 4 and 5).  To emphasize the group-wise sparsity structure in im- plementing the private factors, we use the abbreviation gCMF for group-wise sparse CMF i.e. a CMF model with this ability to learn separate private factors.  3.2. Probabilistic model for gCMF  We instantiate the general model by specifying Gaus- sian likelihood and normal-gamma priors for the pro-  Figure 2. CMF setup encoded as a symmetric matrix fac- torization, with factors identiﬁed by colors. The zero pat- terns in the U matrix induce private factors in the resulting Y matrix. Contribution of factors are identiﬁed by small color patches next to the X matrices, and the question marks (?) represent missing data.  ik ] ∈ Rde×K is the low-rank matrix are the  where Ue = [u(e) related to the entity set e, b(m,r) j bias terms for the mth matrix, and ε(m) is element- wise independent noise. We immediately see that any two matrices sharing the same entity set use the same low-rank matrix as part of their approximation, which enables sharing information.  and b(m,c)  ij  i  The same model can also be expressed in a simpler form by crafting a single large symmetric observation matrix Y that contains all Xm, following the represen- tation introduced by Bouchard et al. (2013). We will use this representation because it allows implementing the private factors via group-wise sparsity. We create e=1 de entities and then arrange the observed matrices Xm into Y such that the blocks not corresponding to any Xm are left un- observed. The resulting Y is of size d× d but has only m=1 drmdcm unique observed elements. In particular, the blocks relating the entities of one type to themselves are not observed.  one large entity set with d =(cid:80)E (at most)(cid:80)M  The CMF model can then be formulated as a symmet- ric matrix factorization (see Figure 2)  Y = UUT + ε,  (2) where U ∈ Rd×K is a column-wise concatenation of all of the diﬀerent Ue matrices, and the bias terms are dropped for notational simplicity. The noise ε is now symmetric but still independent over the upper-diagonal elements, and the variance depends on the block the element belongs to. Given this re- formulation, any symmetric matrix factorization tech- nique capable of handling missing data can be used  0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3 4 5 6 TU? e1 e1 e2 e3 ? ? e4 e5 TX41X2Xe2 ? ? ? ? e3 TX1TX2? ? ? TX3? e4 ? 3X? 4X? ? ? ? e5 Y0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3 4 5 6 Uk = e1 e2 e3 e4 e5 1X2X3X4XGroup-sparse Embeddings in Collective Matrix Factorization  jections, so that in (1) we have  ij ∼ N (0, τ−1 ε(m) m ), ik ∼ N (0, α−1 u(e) ek ),  τm ∼ G(p0, q0), αek ∼ G(a0, b0).  where e is the entity set that contains the entity i. The crucial element here is the prior for U. Its purpose is to automatically select for each factor a set of matri- ces for which it is active, which it does by learning large precision αek for factors k that are not needed for modeling variation for entity set e. In particular, the prior takes care of matrix-speciﬁc low-rank struc- ture, by learning factors for which αek is small for only two entity sets corresponding to one particular matrix.  For the bias terms we use a hierarchical prior ∼ N (µcm, σ2  ∼ N (µrm, σ2  rm),  cm),  b(m,r) i µ·m ∼ N (0, 1),  b(m,c) j σ2·m ∼ U[0,∞].  The hierarchy helps especially in modeling rows (and equivalently columns) with lots of missing data, and in particular provides reasonable values also for rows with no observations (the cold-start problem of new users in recommender systems) through µrm.  4. LEARNING  4.1. MAP solution  Providing a MAP estimate for the model is straigh- forward, but results in a practical challenge of needing to choose the hyper-parameters {a0, b0, p0, q0}, usually through cross-validation. This is particularly diﬃcult for setups with several heterogeneous data matrices on arbitrary scales. Then large hyper-priors are needed for preventing overﬁtting, which in turn makes it dif- ﬁcult to push αek to suﬃciently large values to make the factors private to subsets of the matrices. Hence, we proceed to explain more reasonable variational ap- proximation that avoids these problems.  4.2. Variational Bayesian inference  It has been noticed that Bayesian approaches which take into account the uncertainty about the values of the latent variables lead to increased predictive perfor- mance (Singh and Gordon, 2010). Another important advantage of Bayesian learning is the ability to au- tomatically select regularization parameters by max- imizing the data evidence. While existing Bayesian approaches for CMF used MCMC techniques for learn- ing, we propose here to use variational Bayesian learn- ing (VB) by minimizing the KL divergence between a tractable approximation and the true observation  probability. We use a fully factorized approximation similar to what Ilin and Raiko (2010) presented for Bayesian PCA with missing data, and implement non- Gaussian likelihoods using the quadratic bounds by Seeger and Bouchard (2012). In the following we will summarize the main elements of the algorithm, leaving some of the technical details to these original sources.  Gaussian observations For Gaussian data we ap- proximate the posterior with  (cid:34) E(cid:89)  K(cid:89)  (cid:32)  e=1  k=1  Q(Θ) =   M(cid:89)  q(αek)  q(u(e) ik )  (cid:33)(cid:35)  de(cid:89) drm(cid:89)  i=1  q(τm)q(µrm)q(µcm)  q(b(m,r)  i  )  m=1  i=1  (3)   .  q(b(m,c)  j  )  dcm(cid:89)  j=1  Here q(α) and q(τ ) are Gamma distributions, whereas the others are normal distributions. For all other parameters we use closed-form updates, but ¯Ue, the mean parameters of q(Ue), are updated with Newton’s method for each factor at a time. The gradient-based updates are used because for observation matrices with missing entries closed-form updates would be available only for each element ¯u(e) ik separately, which would re- sult in very slow convergence (Ilin and Raiko, 2010). The update rules for Q(Θ) are in the supplementary material.  Non-Gaussian observations For non-Gaussian data we use the approximation schema presented by Seeger and Bouchard (2012), adaptively approximat- ing non-Gaussian likelihoods with spherical-variance Gaussians. This allows an optimization scheme that alternates between two steps: (i) updating Q(Θ) given pseudo-data Z (which is assumed Gaussian), and (ii) updating the pseudo-data Z by optimizing a quadratic term lower-bounding the desired likelihood potential. The full derivation of the approach is provided by Seeger and Bouchard (2012), but the resulting equa- tions as applied to gCMF are summarized below. We update the pseudodata with  ξm = E[Urm]E[Ucm]T , Zm = (ξm − f(cid:48)  m(ξm)/κm),  where the updates are element-wise and independent for each matrix. Here f(cid:48) m(ξm) is the derivative of the m-th link function − log p(Xm|Urm UT ) and κm is the maximum value of the second derivative of the same function. Given the pseudo-data Z, the approxima- tion Q(Θ) can be updated as in the Gaussian case, using τm = κm as the precision. Note that the link  cm  Group-sparse Embeddings in Collective Matrix Factorization  functions can be diﬀerent for diﬀerent observation ma- trices, which adds support for heterogeneous data; in Section 7 we illustrate binary and count data.  5. RELATED WORK  For M = 1 the model is equivalent to Bayesian (ex- ponential family) PCA. In particular, it reduces to gradient-based optimization for the model by Seeger and Bouchard (2012). For this special case it is typi- cally advisable to use their SVD-based algorithm, since it provides closed-form solution for the Gaussian case.  For multi-view setups where every matrix shares the same row-entities the model equals Bayesian inter- battery factor analysis (when M = 2) (Klami et al., 2013) and its extension group-factor analysis (when M > 2) (Virtanen et al., 2012). However, our infer- ence solution has a number of advantages. In partic- ular, our solution supports wider range of likelihood potentials and provides eﬃcient inference for missing data. These improvements suggests that the proposed algorithm should be preferred over the earlier solu- tions.  The most closely related methods are the earlier CMF solutions, in particular the ones presented in the prob- abilistic framework. The early solutions by Lippert et al. (2008) and Singh and Gordon (2008) provide only maximum-likelihood solutions, whereas Singh and Gordon (2010) provided fully Bayesian solution by formulating CMF as a hierarchical model. They use normal-Inverse-Wishart priors for the factors, with spherical hyper-prior for the Inverse-Wishart distribu- tion. This implies each factor is assumed to be roughly equally important in describing each of the matrices, and that their model will not provide matrix-speciﬁc factors as our model does. For inference they use com- putationally heavy Metropolis-Hastings. Their model also supports arbitrary likelihood potentials and ar- bitrary CMF schemas, though their experiments are limited to cases with M = 2.  6. USE CASES  Even though CMF is widely applicable to factoriza- tion of arbitrary matrix collections, it is worth describ- ing some typical setups to illustrate common use cases where data analysis practitioners might ﬁnd it useful.  Augmenting multi-view learning In multi-view learning (Fig. 1-II) the row entities are shared, but the column entities in diﬀerent views are arbitrary. In many practical applications, however, the column entities share some obvious relationships that are ig-  nored by the multi-view matrix factorization models. A common example considers computing CCA be- tween two diﬀerent high-throughput systems biology measurements of the same patients, so that both ma- trices are patients times genes (see, e.g., Witten and Tibshirani, 2009). In natural language processing, in turn, we have setups with diﬀerent languages as row entities and words as column entities (Tripathi et al., 2010). In both cases there are obvious relationships be- tween the column features. In the ﬁrst example it is an identity relation, whereas in the latter lexigographic or dictionary-based information provides proximity rela- tions for the column entities. Yet another example can be imagined in joint analysis of multiple brain imaging modalities; the column entities correspond to brain re- gions that have spatial relationships even though the level of representation might be very diﬀerent when, e.g., analyzing fMRI and EEG data jointly (Correa et al., 2010).  Such relationships between the column entities can easily be taken into account with CMF using the cycli- cal relational schema of Figure 1-III. We call this ap- proach augmented multi-view learning. We can en- code any kind of similarity between the features as long as the resulting matrix can reasonably be mod- eled as low-rank. In the experimental section we will demonstrate setups where the features live in a contin- uous space (genes along the chromosome, pixels in a two-dimensional space) and hence we can measure dis- tances between them. We then convert these distances into binary promixity relationships, to illustrate that already that is suﬃcient for augmenting the learning.  simplest  systems The  recom- Recommender mender systems seek to predict missing entries in a matrix of ratings or binary relevance indicators (Koren et al., 2009). The extensive literature on recommender systems indicates that incorporating additional infor- mation on the entities helps making such predictions (Stern et al., 2009; Fang and Si, 2011). CMF is a natural way of encoding such information, in form of additional matrices between the entities of interest and some features describing them.  While many other techniques can also be used for in- corporating additional information about the entities, the CMF formulation opens up two additional types of extra information not easily implemented by the al- ternative means. The ﬁrst is a circular setup where both the row and column entities of the matrix of in- terest are described by the same features (Bouchard et al., 2013). This is typically the case for example in social interaction recommenders where both rows and columns correspond to human individuals. The  Group-sparse Embeddings in Collective Matrix Factorization  Figure 3. Left: Relative error for a circular setup of M = 5 binary matrices (see text for details), scaled so that CMF with Gaussian likelihood has error of one. The correct likelihood helps for both gCMF and CMF and modeling the private factors helps for both likelihoods, the combined gain of both aspects being 30%. The results are similar for other values of M > 1. Right: Relative error of VB vs MAP, scaled so that zero corresponds to the ground truth and one to the error of the MAP solution. For small M MAP can still compete (though it is worse than VB already for M = 1), but for large M it becomes worthless; for M = 11 VB reduces the error to roughly half. Furthermore, VB requires no tuning parameters, whereas for the MAP solution we needed to perform cross-validation over two regularization parameters.  other interesting formulation uses higher-order auxil- iary data. For example, the movies in a classical rec- ommender system can be represented by presence of actors, whereas the actors themselves are then repre- sented by some set of features. This leads to a chain of matrices providing more indirect information on the relationships between the entities.  7. EXPERIMENTS  We start with technical validations showing the im- portance of choosing the correct likelihood potential and incorporating private factors in the model, as well as the advantages variational approximation provides over MAP estimation. We then proceed to show how CMF outperforms classical multi-view learning meth- ods in scenarios where we can augment the setup with between-feature relationships.  Since the main goal is to demonstrate the concep- tual importance of solving the CMF task with private factors, we use special cases of gCMF as comparison methods. This helps to show that the diﬀerence is re- ally due to the underlying idea instead of the inference procedure; for example, when comparing against Singh and Gordon (2010) the eﬀects could be masked by dif- ferences between Metropolis-Hastings and variational approximation that are here of secondary importance.  The closest comparison method, denoted by CMF, is obtained by forcing αek to be a constant αk for every entity type e. It corresponds to the VB solution of the earlier CMF models and hence does not support pri- vate factors. For the augmented multi-view setup we will also compare against the special cases of gCMF and CMF that use only two matrices over the three entity sets, denoting them by CCA and PCA, respec-  tively. Finally, in one experiment we will also com- pare against gCMF without the bias terms, to illus- trate their importance in recommender systems. For all methods we use suﬃciently large K, letting ARD prune out unnecessary components, and run the algo- rithms until the variational lower bound converges. We measure the error by root mean square error (RMSE), relative to one of the methods in each experiment.  7.1. Technical illustration  We start by demonstrating the diﬀerence between the proposed model and classical CMF approaches on an artiﬁcial data. We sample M binary matrices that form a cycle over M entity sets (of sizes 100− 150), so that the ﬁrst matrix is between the entity sets 1 and 2, the second between the entity sets 2 and 3, and ﬁnally the last one is between the M -th and ﬁrst entity set. We generate datasets that have 5 factors shared by all matrices plus two factors of low-rank noise speciﬁc to each matrix. This results in 5 + 2M true factors, and we learn the models with 10+2M factors, letting ARD prune out the extra ones.  Figure 3 (left) shows the accuracy in predicting the missing entries (40% of all) for gCMF as well as a stan- dard CMF model. For both models we show the re- sults for both (incorrect) Gaussian and Bernoulli like- lihoods. The experiment veriﬁes the expected results: Using the correct likelihood improves the accuracy, as does correctly modeling private noise factors.  We use the same setup to illustrate the importance of using variational approximation for inference, this time with Gaussian noise and entity set sizes between 40 − 80. For MAP we validate the strength of the Gamma hyper-priors for τ and α over a grid of 11× 11 values for a0 = b0 and p0 = q0, using two-fold cross-  gCMF−BernoulliCMF−BernoulligCMF−Gaussian0.70.80.91.0RMSECMF−GaussianM=1M=5M=110.20.40.60.81.0RMSEMAPGroup-sparse Embeddings in Collective Matrix Factorization  validation within the observed data. In total we hence need to run the MAP variant more than 200 times to get the result, in contrast to the single run of the VB algorithm with vague priors using 10−10 for every parameter. Figure 3 (right) shows that despite heavy cross-validation the MAP setup is always worse and the gap gets bigger for more complex setups. This illustrates how the VB solution with no tunable hy- perparameters is even more crucial for CMF than it would be for simpler matrix factorizations. For MAP using the same hyper-priors for all matrices neces- sarily becomes a compromise for matrices of diﬀerent scales, whereas validating separate scales for each ma- trix would be completely infeasible (requiring valida- tion over 2M parameters).  7.2. Augmented multi-view learning  We start with a multi-view setup in computational bi- ology, using data from Pollack et al. (2002) and the setup studied by Klami et al. (2013). The samples are 40 patients with breast cancer, and the two views correspond to high-throughput measurements of ex- pression and copy number alteration for 4287 genes. We compare the models in the task of predicting ran- dom missing entries in both views, as a function of the proportion of missing data.  The multi-view methods use the data as such, whereas the CMF variants also use a third d2 × d3 matrix that encodes the proximity of the genes in the two views. It is a binary matrix such that x(3) i,j is one with proba- bility exp(−|li− lj|), where li is the chromosomal loca- tion measured in 107 basepairs. This encodes the rea- sonable assumption that copy number alterations are more likely to inﬂuence the expression of nearby genes. Figure 4 shows how this information helps in making the predictions. For reasonable amounts of missing data, gCMF is consistently the best method, outper- forming both CMF as well as the standard multi-view methods. For extreme cases with at least 80% missing data the advantage is ﬁnally lost. The importance of the private factors is seen also in CCA outperforming PCA, whereas CMF and CCA are roughly as accurate; both include one of the strenghts of gCMF.  In another example we model images of faces taken in two alternative lighting conditions, but from the same viewing angle. We observe the raw grayscale pixels values of 50 × 50 images, and for the CMF methods we use a third matrix (size 2500 × 2500, of which ran- dom 10% is observed) to encode proximity of pixels in the two views, using Gaussian kernel to provide the probability of one for a binary relation. We train the model so that we have observed 6 images in both views  Figure 4. Relative prediction error for augmented multi- view gene experiment, scaled so that gCMF has error one and is represented by the horizontal black line. For reason- able amounts of missing data (x-axis) the methods with pri- vate factors (gCMF and CCA) outperform the ones with- out, and modeling the proximity relationship between the genes (gCMF and CMF) improves the accuracy. The con- ﬁdence intervals correspond to 10% and 90% quantiles over random choices of missing data.  Figure 5. Prediction error for a multi-view image recon- struction task as a function of the neighborhood width in constructing the proximity augmentation view. The aug- mentation helps for a wide range of promixity relationships, and the solution reverts back to the non-augmented accu- racy for very narrow and wide neighborhoods.  and then 7 images for each view alone, for a total of 20 images. The task is to predict the missing views for these images, without any observations.  Figure 5 plots the prediction errors as a function of the neighborhood σ used in constructing the promity rela- tionships. We see that for very narrow and very wide neighborhoods the CMF approach reverts back to the classical multi-view model, since the extra view con- sists almost completely of zeros or ones, respectively. For proper neighborhood relationships the accuracy in predicting the missing view is considerably improved.  7.3. Recommender systems  Next we consider classical recommender systems, us- ing MovieLens and Flickr data as used in earlier CMF experiments by Bouchard et al. (2013). We compare  0.991.011.031.05102030405060708090Percentage missingRMSEgCMFCMFCCAPCAlllllllllllllllllllllllllll010203040500.700.800.901.00Neighborhood widthRMSElllllllllllCMFPCAGroup-sparse Embeddings in Collective Matrix Factorization  Table 1. RMSE for two recommender system setups, with boldface indicating the best results. The results for convex CMF (CCMF) are taken from Bouchard et al. (2013) for the best regularization parameter values. Our model provides comparable result without the bias terms, without needing any tuning for the parameters, and the bias terms helps considerably with the cold-start problem especially in MovieLens. Without bias terms gCMF also outperforms CMF for all cases, but with the bias terms the methods are practically identical for these data sets. This suggests these data sets do not have strong private structure that could not be modeled with the bias terms alone. It is important to note that allowing for the private factors never hurts; gCMF is always at least as good as CMF.  Data Relation CCMF (reg=10) CMF without bias CMF with bias gCMF without bias gCMF with bias  Flickr  MovieLens X1-Count X1-Binary X2-Binary X3-Binary X4-Binary X5-Gaussian 1.0588 1.0569 0.9475 1.0418 0.9474  1.0033 1.0092 1.0033 1.0039 1.0033  0.7071 0.5120 0.5000 0.5003 0.5000  0.2473 0.2324 0.2369 0.2291 0.2369  0.3661 0.5093 0.2789 0.5014 0.2789  0.2384 0.2176 0.2109 0.2167 0.2109  gCMF with the convex CMF solution presented in that paper, showing that it ﬁnds the same solution when the bias terms are turned oﬀ (Table 1). We also illus- trate that modeling the bias terms explicitly is as use- ful for CMF as it has been shown to be for other types of recommender systems. To our knowledge gCMF is the ﬁrst CMF solution with such bias terms.  Both data sets have roughly 1 million observed entries, and our solutions were computed in a few minutes on a laptop. The total computation time is hence roughly comparable to the times Bouchard et al. (2013) re- ported for CCMF using one choice of regularization pa- rameters. Full CCMF solution is considerably slower since it has to validate over them.  8. DISCUSSION  Collective matrix factorization is a very general tech- nique for revealing low-rank representations for arbi- trary matrix collections. However, the practical ap- plicability of earlier solutions has been limited since they implicitly assume all factors to be relevant for all matrices. Here we presented a general technique for avoiding this problem, by learning the CMF solution as symmetric factorization of a large square matrix while enforcing group-wise sparse factors.  While any algorithm aiming at such sparsity structure will provide shared and private factors for a CMF, the variational Bayesian solution presented in this work has some notable advantages. It is more straighfor- ward than the sampling-based alternative by Singh and Gordon (2010) (which could be modiﬁed to in- corporate private factors) while being free of tunable regularization parameters required by the convex so- lution of Bouchard et al. (2013). The model also sub- sumes some earlier models and provides extensions  for them. In particular, it can be used to eﬃciently learn Bayesian CCA solution for missing data and non-conjugate likelihoods, providing the ﬁrst eﬃcient Bayesian CCA between binary observations.  One drawback of CMF is its inability to handle multi- ple relations accross two entity type. Tensor factoriza- tion methods alleviate this problem, as illustrated in the recent work on multi-relational data (Glorot et al., 2013; Chen et al., 2013).  Acknowledgments  We acknowledge support from the University Aﬀairs Committee of the Xerox Foundation. AK was also supported by Academy of Finland (grants 251170 and 266969) and Digile SHOK project D2I.  References  Guillaume Bouchard, Shengbo Guo, and Dawei Yin. Convex collective matrix factorization. In Proceed- ings of the 16th International Conference on Artiﬁ- cial Intelligence and Statistics, volume 31 of JMLR W&CP, pages 144–152. JMLR, 2013.  Danqi Chen, Richard Socher, Christopher D. Man- ning, and Andrew Y. Ng. Learning new facts from knowledge bases with neural tensor networks and semantic word vectors. In International Conference on Learning Representations, 2013.  Nicolle M. Correa, Tom Eichele, T¨ulay Adali, Yi-Ou Li, and Vince D. Calhoun. Multi-set canonical cor- relation analysis for the fusion of concurrent single trial ERP and functional MRI. Neuroimage, 50(4): 1438–1445, 2010.  Yi Fang and Luo Si. Matrix co-factorization for recom- mendation with rich side information and implicit  Group-sparse Embeddings in Collective Matrix Factorization  feedback. In Proceedings of the 2nd International Workshop on Information Heterogeneity and Fusion in Recommender Systems, pages 65–69. ACM, 2011.  Xavier Glorot, Antoine Bordes, Jason Weston, and Yoshua Bengio. A semantic matching energy func- tion for learning with multi-relational data. In Inter- national Conference on Learning Representations, volume abs/1301.3485, 2013.  Alexander Ilin and Tapani Raiko. Practical approaches to principal component analysis in the presence of missing data. Journal of Machine Learning Re- search, 11:1957–2000, 2010.  Yangqing Jia, Mathieu Salzmann, and Trevor Darrell. Factorized latent spaces with structured sparsity. In Advances in Neural Information Processing Systems 23, pages 982–990, 2010.  Arto Klami, Seppo Virtanen, and Samuel Kaski. Bayesian canonical correlation analysis. Journal of Machine Learning Research, 14:965–1003, 2013.  Yehuda Koren, Robert Bell, and Chris Volinsky. Ma- trix factorization techniques for recommender sys- tems. Computer, 42(8):30–37, 2009.  Daniel D. Lee and H. Sebastian Seung. Algorithms for non-negative matrix factorization. In Advances in neural information processing systems 13, pages 556–562, 2001.  Christoph Lippert, Stefan-Hagen Weber, Yi Huang, Volker Tresp, Matthias Schubert, and Hans-Peter Kriegel. Relation-prediction in multi-relational do- mains using matrix-factorization. In NIPS Work- shop: Structured Input - Structured Output. 2008.  Andriy Mnih and Ruslan Salakhutdinov. Probabilis- tic matrix factorization. In Advances in neural in- formation processing systems 20, pages 1257–1264, 2007.  Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective learning on multi-relational data. In ICML, page 809816, 2011.  Pentti Paatero and Unto Tapper. Positive matrix fac- torization: A non-negative factor model with op- timal utilization of error estimates of data values. Environmetrics, 5(2):111–126, 1994.  Jonathan R. Pollack, Therese Sorlie, and Charles M. Perou et al. Microarray analysis reveals a major direct role of DNA copy number alteration in the transcriptional program of human breast tumors. Proceedings of the National Academy of Sciences of  the United States of America, 99(20):12963–12968, 2002.  Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. Application of dimensionality reduction in recommender system-a case study. Technical re- port, DTIC Document, 2000.  Matthias Seeger and Guillaume Bouchard. Fast vari- ational Bayesian inference for non-conjugate matrix factorization models. In Proceedings of the 15th International Conference on Artiﬁcial Intelligence and Statistics, volume 22, pages 1012–1018. JMLR, 2012.  Ajit P. Singh and Geoﬀrey J. Gordon. Relational learning via collective matrix factorization. In Pro- ceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data min- ing (KDD), pages 650–658. ACM, New York, NY, USA, 2008.  Ajit P. Singh and Geoﬀrey J. Gordon. A Bayesian In  matrix factorization model for relational data. Uncertainty in Artiﬁcial Intelligence, 2010.  David H Stern, Ralf Herbrich, and Thore Graepel. Matchbox: large scale online Bayesian recommenda- tions. In Proceedings of the 18th international con- ference on World wide web, pages 111–120. ACM, 2009.  Abhishek Tripathi, Arto Klami, and Sami Virpioja. Bilingual sentence matching using kernel cca. In Ma- chine Learning for Signal Processing (MLSP), 2010 IEEE International Workshop on, pages 130–135. IEEE, 2010.  Seppo Virtanen, Arto Klami, Suleiman A. Khan, and Samuel Kaski. Bayesian group factor analysis. In Proceedings of the 15th International Conference on Artiﬁcial Intelligence and Statistics, volume 22 of JMLR W&CP, pages 1269–1277. JMLR, 2012.  Daniela M. Witten and Robert J. Tibshirani. Exten- sions of sparse canonical correlation analysis with applications to genomic data. Statistical applica- tions in genetics and molecular biology, 8(1):1–27, 2009.  Dawei Yin, Shengbo Guo, Boris Chidlovskii, Brian D and Guillaume Davison, Cedric Archambeau, Bouchard. Connecting comments and tags: im- proved modeling of social tagging systems. In Pro- ceedings of the sixth ACM international conference on Web search and data mining, pages 547–556. ACM, 2013.  Group-sparse Embeddings in Collective Matrix Factorization  Supplementary material  supplementary material  the manuscript This “Group-sparse Embeddings in Collective Matrix Fac- torization” provides more details on the variational ap- proximation described in the paper.  for  Notation  The factors in (3) are  ik ) = N (¯u(e)  ik , ˜u(e) ik ),  q(u(e) q(αek) = G(aek, bek), q(b(m,r) q(τm) = G(pm, qm), q(b(m,c)  i  j  ) = N (¯b(m,r) ) = N (¯b(m,c)  i  j  ),  i  , ˜b(m,r) , ˜b(m,c)  j  ).  and we denote by ¯α and ¯τ the expectations of α and τ . The observed entries in Xm are given by indicat- ij =  Om ∈ [0, 1]drm×dcm , with nm = (cid:80) (cid:16) ij −(cid:80)K  ing their total number. Finally, we denote ˆx(m) x(m)  jk − ¯b(m,r) ¯u(cm)  k=1 ¯u(rm)  − ¯b(m,c)  ij o(m)  (cid:17)  ik  ij  .  j  i  Algorithm  The full algorithm repeats the following steps until convergence.  1. For each entity set e, compute the gradient of ¯Ue using (4) and compute the variance parameter ˜Ue using (5).  2. Update ¯Ue with under-relaxed Newton’s step. ik + ik with 0 < λ < 1 as the regularization  The element-wise update is ¯u(e) ik )−1g(e) λ(˜u(e) parameter.  ik ← (1 − λ)¯u(e)  3. Update the approximations for the bias terms us-  ing (6).  4. Update the approximations for the automatic rel-  evance determination parameters using (7).  5. For all matrices Xm with Gaussian likelihood, up- date the approximations for the noise precision parameters using (8). For all matrices Xm with non-Gaussian likelihood, update the pseudo-data using (9).  Details  Updates for the factors: The gradient with re- spect to the mean parameters of the factors is com- puted as  (cid:104)−ˆx(m) (cid:104)−ˆx(m)  ij ¯u(cm)  jk + ¯u(e)  ik ˜u(cm)  jk  ij ¯u(rm)  ik + ¯u(e)  jk ˜u(rm)  ik  (cid:105) (cid:105)  (4)  .  g(e) ik = ¯αek ¯ue ¯τm  m;rm=e  m;cm=e  j  j  ¯τm  ik+(cid:88) (cid:88) (cid:88) (cid:88) ¯αek + (cid:88)  (cid:88) (cid:88)  ¯τm  m;rm=e  j  ˜u(e) ik =  +  For ˜Ue we have closed-form updates  (cid:88)  (cid:16)  (cid:17)  m;cm=e  j  ¯τm  (cid:16)  (¯u(rm)  jk  )2 + ˜u(rm)  jk  (cid:17)−1  (¯u(cm)  jk  )2 + ˜u(cm)  jk  .  (5)  Updates for the bias terms: The approximations for the row bias terms are updated as  ¯τm  −1 (cid:88) (cid:0)¯τmµi + µrm/σ2 (cid:1) ,  ij + σ−2 o(m)  ,  j  rm  ˜b(m,r) i  =  ˆb(m,r) i  = ˜b(m,r)  i  ij −(cid:80)  jk − ¯b(m,c)  is a shorthand notation for the mean of where µi x(m) k ¯u(m) ik ¯u(m) over the observed entries. We additionally update q(µrm) using standard varia- tional update for Gaussian likelihood and prior, and use point estimate for σ2 rm. The updates for the col- umn bias terms follow naturally.  j  (6)  Updates for the ARD terms: The approxima- tions for the ARD variance parameter terms are up- dated as  aek = aα  0 + de/2,  bek = b0 + 0.5  ds(cid:88)  K(cid:88)  (cid:16)  (7)  (cid:17)  .  (¯u(e)  ik )2 + ˜u(e)  ik  i=1  k=1  Group-sparse Embeddings in Collective Matrix Factorization  (8)  (cid:17)(cid:35)  ,  Updates for the precision terms: For each ma- trix with Gaussian likelihood the approximation for the precision term is updated as  pm = p0 + nm/2,  (cid:20)  (cid:88)  ij  1  2nm  qm = q0 +  K(cid:88)  (cid:16)  (ˆx(m)  ij )2 + ˜b(m,r)  i  + ˜b(m,c)  j  +  (¯u(rm)  ik  )2 ˜u(cm)  jk + (¯u(cm)  jk  )2 ˜u(rm)  ik + ˜u(cm)  jk ˜u(rm)  ik  k=1  where the sum for qm is over all observed entries.  Updated for the pseudo-data: For each matrix with non-Gaussian data we update the pseudo-data Zm using  ξm = E[Urm]E[Ucm ]T , Zm = (ξm − f(cid:48)  m(ξm)/κm),  (9)  where the updates are element-wise and independent for each matrix. Here f(cid:48) m(ξm) is the derivative of the m-th link function − log p(Xm|UrmUT ) and κm is the maximum value of the second derivative of the same function.  cm  MAP estimation  These update rules can be easily modiﬁed to provide the MAP estimate instead; the modiﬁcations mostly consist of dropping the variance terms and the re- sulting updates are not repeated here. Similarly, the updates are easy to modify for learning CMF models without private factors, by coercing αek into αk.  ","CMF is a technique for simultaneously learning low-rank representations basedon a collection of matrices with shared entities. A typical example is thejoint modeling of user-item, item-property, and user-feature matrices in arecommender system. The key idea in CMF is that the embeddings are sharedacross the matrices, which enables transferring information between them. Theexisting solutions, however, break down when the individual matrices havelow-rank structure not shared with others. In this work we present a novel CMFsolution that allows each of the matrices to have a separate low-rank structurethat is independent of the other matrices, as well as structures that areshared only by a subset of them. We compare MAP and variational Bayesiansolutions based on alternating optimization algorithms and show that the modelautomatically infers the nature of each factor using group-wise sparsity. Ourapproach supports in a principled way continuous, binary and count observationsand is efficient for sparse matrices involving missing data. We illustrate thesolution on a number of examples, focusing in particular on an interestinguse-case of augmented multi-view learning."
1312.6594,2014,Sequentially Generated Instance-Dependent Image Representations for Classification  ,"['Matthieu Cord', 'patrick gallinari', 'Nicolas Thome', 'Ludovic Denoyer', 'Gabriel Dulac-Arnold']",https://arxiv.org/pdf/1312.6594.pdf,"      000 001 002 003 004 005 006 007 008 009 010 011 012 4 013 1 014 0 2 015 016 b 017 e F 018 019 1 020 1   021   ] 022 V 023 C 024 . s 025 c 026 [   027   3 028 v 029 4 030 9 031 5 6 032 033 2 034 1 3 035 1 036 : 037 v i 038 X 039 r 040 a 041 042 043 044 045 046 047 048 049 050 051 052 053 054  .  Sequentially Generated Instance-Dependent Image Representations  for Classiﬁcation  Gabriel Dulac-Arnold  Ludovic Denoyer  Patrick Gallinari  Nicolas Thome  Matthieu Cord  LIP6, UPMC - Sorbonne University  Paris, France  {firstname.lastname}@lip6.fr  September 14, 2018  Abstract  In this paper, we investigate a new framework for image classiﬁcation that adaptively generates spatial representations. Our strategy is based on a sequential process that learns to explore the different regions of any image in order to in- fer its category. In particular, the choice of re- gions is speciﬁc to each image, directed by the actual content of previously selected regions.The capacity of the system to handle incomplete im- age information as well as its adaptive region se- lection allow the system to perform well in bud- geted classiﬁcation tasks by exploiting a dynam- icly generated representation of each image. We demonstrate the system’s abilities in a series of image-based exploration and classiﬁcation tasks that highlight its learned exploration and infer- ence abilities.  1. Introduction Many computer vision models are developped with a spe- ciﬁc image classiﬁcation task in mind, adapted to a partic- ular representation such as the bag-of-words (BoW) model or low-level local features (Sivic & Zisserman, 2003; van Gemert et al., 2010). In these representations, all the image information is used to take the decision, even in the spatial BoW extension of Lazebnik et al. (Lazebnik et al., 2006). However, as pointed out in recent work, humans do not need to consider the entire image to be able to interpret it (Sharma et al., 2012). On the contrary, humans are able to rapidly pick out the important regions of an image nec- essary to interpret it. This fact suggests that concentrating on speciﬁc subset of image regions in an intelligent manner should be sufﬁcient to properly classify an image. In addi- tion to simply selecting regions of an image, our system  1  Figure 1. Illustration of our classiﬁcation framework for two test images (ﬁrst and second line): According to the content of the center region, the next region to visit is selected (red arrow). Again, depending on the two ﬁrst regions’ contents, a third one is selected, and so on. After B iterations, the ﬁnal classiﬁcation is achieved. As the ﬁrst region is the same on both images, the second region explored is the same, but as these new regions’ contents differs, the next regions considered by the algorithm are different. can actively decide to consider certain regions of an image in more detail by increasing the BoW resolution for speciﬁc sub-regions of an image. This allows the system to adap- tively use more or less resources when classifying images of varying complexity. Similar performance-oriented goals have been recently put forward by Karayev et al. (Karayev et al., 2012). Importantly, this process is instance-speciﬁc, allowing the algorithm to adapt the choice of regions for each processed image. We are able to learn such a model by leveraging the datum-Wise classiﬁcation framework (Dulac-Arnold et al., 2012a), which is able to learn adaptive classiﬁca- tion policies using reinforcement learning (RL). We show that during inference, a signiﬁcant speed-up is obtained by only computing the local features on the selected regions, while preserving acceptable inference accuracy w.r.t. full- information models. The rest of the paper is organized as follows. Section 2 presents related works and highlights  055 056 057 058 059 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 100 101 102 103 104 105 106 107 108 109  110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164  our contributions. Section 3 gives the theoretical back- ground of our sequential model, and Section 4 details the training algorithm. Finally, Section 5 reports classiﬁcation scores on two challenging image datasets.  2. Background The standard image classiﬁcation pipeline follows three steps (Boureau et al., 2010) — (i) low-level local descrip- tor extraction, (ii) coding, and (iii) pooling — to get feature vectors that are then used for classiﬁcation. This strategy was deﬁnitively popularized in computer vision with the bag-of-words formalism using SIFT local features (Sivic & Zisserman, 2003). Alternatives to the standard coding scheme have been proposed, such as local soft coding (Liu et al., 2011) or sparse coding (Boureau et al., 2010). Af- ter the coding phase, most traditional BoW approaches use sum pooling or max pooling. The spatial pyramid matching (SPM) strategy (Lazebnik et al., 2006) extends the pooling by considering a ﬁxed predetermined spatial image pyra- mid. Many feature detectors have been proposed to get salient areas, afﬁne regions, and points of interest (Mikolajczyk & Schmid, 2005) on images. However, in contrast to the task of matching a speciﬁc target image or object, methods for category classiﬁcation show better performance when using a uniform feature sampling over a dense grid on the image (Chatﬁeld et al., 2011). Other approaches studying are motivated by human eye ﬁx- ation or salient object detection (Borji et al., 2012; Chang et al., 2011). Recently, several approaches combine dense sampling and saliency map or spatial weighting to obtain powerful image representations (Su & Jurie, 2012; Feng et al., 2011). Sharma et al. (Sharma et al., 2012) pro- poses a scheme to learn discriminative saliency maps at an image region level. They use the SPM scheme and ap- ply weights to each block of the pyramid to get a global saliency map. In the case of multiclass classiﬁcation, the weights are learned for each class in a discriminative way using one-against-all binary classiﬁcation, and the ﬁnal de- cision depends on the image content using a latent SVM representation. Other methods based on latent SVM formulation also at- tempt to jointly encode spatial and content information in image classiﬁcation. Parizi et al. (Parizi et al., 2012) intro- duce a reconﬁgurable model where each region is equiped with a latent variable representing a topic, such that only re- gions with similar topics are matched together in the ﬁnal representation. This model provides a ﬂexible framework, overcoming the shortcoming of the ﬁxed spatial grid used in SPM. In all these approaches, the whole image has to be pro-  cessed and all the information is used to classify, even if some regions of the image contain some misleading or ir- relevant contents. We propose a strategy to overcome these limitations: we avoid processing the whole image by focus- ing only on the most pertinent regions relative to the image classiﬁcation task. In a way more drastic than Sharma’s approach (Sharma et al., 2012), we constrain our system to take a decision by considering only a ﬁxed number of re- gions of the test image, thus allowing the computation of the local features to be signiﬁcantly reduced. The most important aspect of our method is the region se- lection model. In short, our model is effectively a learned sequential decision policy that sequentially chooses the best region to visit given a set of previously visited regions. Both the locality and actual contents of a region are used in a joint manner to represent the set of visited regions. In- spired by reinforcement learning algorithms, we propose a dedicated algorithm to learn the region selection policy used in our image classiﬁcation task. More recent work uses similar techniques to ﬁnd optimal orders for anytime object detection tasks (Karayev et al., 2012). Similar work has been presented that uses a foveal glimpse simulation, but the learning approach is quite dif- ferent (Larochelle & Hinton, 2010). Sequential learning techniques have been recently applied to different standard classiﬁcation tasks. In (Dulac-Arnold et al., 2012b), the authors propose to use leinforcement learning models for learning sparse classiﬁers on vectors, (Busa-Fekete et al., 2012) use sequential techniques for learning a cascade of classiﬁers depending on the content of the inputs, while (Dulac-Arnold et al., 2011) is an appli- cation of sequential learning models to text classiﬁcation. Finally, (R¨uckstieß et al.) propose a generic model able to minimize the data consumption with sequential online fea- ture selection. If our approach shares some common ideas with these re- cent works, we propose an original method that has been developed to handle the speciﬁc problem of classifying im- ages using a small set of regions and a new learning algo- rithm which is efﬁcient both in term of speed and perfor- mance. To summarize, the contributions presented in this paper are as follows:  • We propose a sequential model that, given an image, ﬁrst selects a subset of relevant regions in this image, and then classiﬁes it. The advantages of such a method are: (i) The classiﬁcation decision is based only on the features of the acquired regions, resulting in a speed- up of the classiﬁcation algorithm during inference. (ii) The algorithm is able to ignore misleading or irrele- vant regions. (iii) The way regions are selected de-  165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219  220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274  pends both on the position but also on the content of the regions, resulting in a model able to adapt its be- havior to the content of each image being classiﬁed. (iv) At last, the model is a multiclass model and the regions selection policy is learned globally for all the classes while other existing methods usually apply a category-speciﬁc region selection scheme.  • We propose a new learning algorithm inspired from reinforcement learning techniques adapted to the par- ticular problem faced here.  • We present an experimental evaluation of this method on three different classical datasets and propose a qualitative study explaining the behaviour of this model.  3. Classiﬁcation model 3.1. Notations Let us denote X the set of possible images and Y the dis- crete set of C categories. A classiﬁer is a parametrized function fθ such that fθ : X → Y where fθ(x) = y means that category1 y has been predicted for image x. To learn fθ, a set of (cid:96) labeled training images Strain = {(x1, y1), ..., (x(cid:96), y(cid:96))} is provided to the system. We also consider for x a ﬁxed grid N × M of regions {rx i }i ≤N×M where rx is the i-th region as illustrated in Fig. 2 (left). The set of all possible regions is denoted R, and R(x) corresponds to the set of regions over image x.2 i ) of size K. We i is represented by a feature vector φ(rx rx use a SIFT bag-of-words representation in our experiments.  i  3.2. Model formalization  1 , .., sx  The classiﬁer is modeled as a sequential decision process that, given an image, ﬁrst sequentially selects regions, and then classiﬁes the image using the information available in the visited regions. At each step, the classiﬁer has already t ) where selected a sequence of regions denoted (sx t is the index of the region of x selected at step t. The sx t ) is thus a representation tailored to the sequence (sx speciﬁc image and the current classiﬁcation task. S(x) de- notes the set of all possible trajectories over image x and S t(x) the trajectories composed of t selected regions. Given a ﬁxed budget B, new regions are acquired resulting in a trajectory of size B. Given this trajectory, the classiﬁer then decides which category to assign to the image. There are two important aspects of our approach: First, the way  1 , .., sx  where one input is associated to exactly one possible category.  1We consider in this paper the case of monolabel classiﬁcation 2Note that all the images have the same N × M number of  regions.  Figure 2. (left) Index of the regions of an image decomposed of 4 × 4 regions. (right) Example of possible trajectory (6, 10, 11, 2, 7).  these regions are acquired depends on the content of the previously acquired regions — c.f. Section 3.2.3 — result- ing in a classiﬁer that is able to adapt its representation to each image being classiﬁed, thus selecting the best regions for each image. Second, the ﬁnal decision is made given the features of the acquired regions only, without needing the computation of the features for the other regions, thus resulting in both a speed-up of the classiﬁcation process — not all features have to be computed — but also, for some cases as described in Section 5, in an improvement of the classiﬁcation rate due to the exclusion of noisy regions. We now give details concerning the features, the classi- ﬁcation phase — which classiﬁes the image given the B previously selected regions — and the exploration phase — which selects B − 1 additional regions3 over an image to classify.  3.2.1. FEATURE FUNCTION  As previously explained, the Φ function aims at aggregat- ing the content of already visited regions. Based on the K length vector φ(rx i ), we consider the following Γ mapping into a larger space of size K × (N × M ) as in (Parizi et al., 2012): Γ(φ(rx i ) 0 . . . 0)T where i ) is positioned at index i×K. The global feature func- φ(rx tion Φ is therefore deﬁned as:  i ), K) = (0 . . . 0 φ(rx  Φ(sx  1 , ...sx  t ) =  Γ(φ(rx sx i  ), K).  (1)  i=1  The goal of such a transformation is to conserve the in- formation concerning the index of the region, which corre- sponds to the actual position of the region within the source image.  3.2.2. CLASSIFICATION PHASE  The classiﬁcation phase consists in classifying an image given B acquired regions denoted (sx B). First, this 3Note that we consider that the ﬁrst region acquired by the  1 , ...sx  classiﬁer is a predeﬁned central region of the image.  t(cid:88)  275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329  330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384  set is transformed to a global feature vector that aggregates the individual features of each of its regions using Φ. The classiﬁcation is performed by using a classiﬁcation func- tion denoted fθ deﬁned as:  (cid:40)RK×N×M → Y  ,  (2)  fθ :  fθ(Φ(sx  1 , ...sx  B)) = y  where y is the predicted category. θ is the set of parameters that is learned using the training set as described in Section 4. Note that this function is computed by using as an in- put the vectorial representation of the sequence of regions Φ(sx  1 , ...sx  B).  3.2.3. EXPLORATION PHASE  ∀t, πt :  ,  (3)  In order to sequentially acquire the different regions of an image, the classiﬁcation process follows an exploration policy denoted πγ where γ is the set of parameters of this policy. A speciﬁc exploration policy is used at each timestep, and as such, π can be decomposed into a se- quence of sub-policies π = (π1, π2, ..., πB−1) such that πt computes the region to acquire, at time t, given a sequence of t previously acquired regions:  (cid:40)RK×N×M → N × M  πt(Φ(sx  1 , ..., sx  t )) = sx  t+1 t+1 is the index of the next region rx sx t+1  where sx to acquire. πt can be viewed as a multiclass classiﬁer which predicts the index of the next region to acquire, given the features of the previously acquired regions, and πt is restricted to predicting the index of a region that has not been previously acquired. In this paper, we consider t policies πγt parametrized by γt. Similarly to the classiﬁcation function fθ, πγt takes as an input the vectorial representation of the current sequence t ) and outputs a region in- of acquired regions Φ(sx dex to be considered. We deﬁne this policy as a multiclass one-against-all hinge loss perceptron in this paper, but any multiclass classiﬁer such as an SVM or a neural networks could be used as well.  1 , .., sx  3.2.4. FINAL INFERENCE POLICY  A complete classiﬁer policy is deﬁned by both an explo- ration policy (π1, ..., πB−1) plus a classiﬁcation policy fθ. The ﬁnal inference process is described in Algorithm 1 and consists in sequentially acquiring new regions (lines 1–4) and then computing the predicted category using the previ- ously acquired regions (line 5).  4. Learning Algorithm The idea of the learning algorithm is the following: the classiﬁcation policy is learned starting from the end.  Algorithm 1 Inference Algorithm: only B regions are ac- quired for classiﬁcation Require: B: budget Require: (π1, ..., πB−1): exploration policy Require: fθ: classiﬁcation policy Require: x: input image 1: Acquire region sx 2: for i = 1 to B − 1 do Acquire region sx 3: 4: end for 5: Compute category y = fθ(Φ(sx 6: return y  1 i.e. the central region of the image  i+1 using πγi(Φ(sx  1 , ..., sx  1 , ..., sx  B))  i ))  Algorithm 2 Complete Learning algorithm Require: (x1, ..., x(cid:96)): Training set of images Require: (y1, ..., y(cid:96)): Training labels 1: Learn fθ using algorithm 3 2: for k = B − 1 to 1 do previoulsy 3:  learned  , .., πB−1  γ  sub-policies , fθ to learn πγk using algorithm  Use πk+1 4  γ  4: end for 5: return Final policy: (πγ1, ...πγB−1, fθ)  We begin by ﬁrst learning fθ and then we sequen- tially learn πγB−1, πγB−2, up to πγ1. The underlying idea is to begin by learning a good fθ classiﬁca- tion policy able to obtain good performance given any subset of B regions. The learning of πγB−1, πγB−2, to πγ1 aims at acquiring relevant regions regions that will help fθ to take the right decision. i.e.  ...  The complete learning algorithm is given in Algorithm 2 and provides the general idea behind our method. At each iteration of the algorithm, a set of learning states is sam- pled from the training images using a uniform random dis- tribution. For each sampled state, the previously learned sub-policies are then used to simulate – using Monte Carlo techniques – the behavior of the algorithm and thus to pro- vide supervision to the sub-policy we are learning. The detailed process is given in Sections 4.1 and 4.2. Note that this learning algorithm is original and derived from both the rollout classiﬁcation policy iteration (RCPI) method and the Fitted-Q Learning model that are generic Rein- forcement Learning models proposed in (Dimitrakakis & Lagoudakis, 2008) and (Ernst et al., 2005). Our method is an adaptation of these two classical algorithms in the par- ticular case described here.  385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439  440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494  Algorithm 3 Classiﬁcation Policy Learning Algorithm Require: (x1, ..., x(cid:96)): Training set of images Require: (y1, ..., y(cid:96)): Training labels Require: B: Budget Require: n: 1: T = {} {Training set} 2: {For each training image} 3: for xi do 4: 5:  Sample B regions (sxi ploration policy (πrandom, ..., πrandom)  for k = 1 to n do  1 , ..., sxi  T ← T (cid:83)(Φ(sxi  end for  6: 7: 8: end for 9: Learn fθ on T using a classical learning algorithm 10: return fθ  1 , ..., sxi  B ), yi)  B ) using random ex-  4.1. Learning the Classiﬁcation Policy  Our approach for learning the classiﬁcation policy is described in Algorithm 3. The underlying idea is to auto- matically learn from the training set a classiﬁer which is optimal for classifying any subset of B regions for any in- put image. The classiﬁcation policy fθ is learned on a large training set of images which have had B regions uniformly sampled — lines 5 and 6 of Alg. 3 — over the training images. Each set of regions is transformed to a feature vector using the Φ function and provided to the learning algorithm using the label of the image as the supervision. This set of regions and labels is used to train fθ – line 9.  At the end of the process, fθ is a classiﬁer able to prop- erly predict the category of any image given any randomly sampled set of B regions. As explained in the next section, the goal of learning the exploration policy is to improve the quality of fθ by ﬁnding a good representation instead of us- ing a uniform sampling approach. This is done by ﬁnding a region selection policy that provides image-speciﬁc sub- sets of B image regions that are most likely to increase the classiﬁer’s classiﬁcation accuracy.  4.2. Learning the Optimal Exploration Policy  Consider now that fθ has been properly learned. Given a new image x, using only fθ applied to a uniformly sampled set of regions has one main drawback: for some sampled sets, fθ will certainly predict the right classiﬁcation label, but for other samples, it will make a classiﬁcation error, particularly for samples that contain irrelevant or mislead- ing regions. The goal of the exploration policy π is thus to provide fθ with a set of good regions i.e. a set of regions on which the classiﬁcation function will predict the correct category. In other words, π aims at reducing the error rate  495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549  1 , ...sx  Figure 3. Illustration of the learning algorithm for πγB−1. On a B−1) of B − 1 regions, all remaining regions sx sample (sx B are considered and classiﬁed by fθ (left) (simulation step, lines 6-8 of Algorithm 4). For some regions – the ﬁrst one here – fθ computes the right label, for other regions fθ makes a predic- tion error (line 9 of Alg. 4). The regions on which fθ provides the good label are considered as training examples for learning πγB−1 (line 10).  1 , ...sx  of fθ by changing the way regions are sampled. The com- plete learning method is given in Algorithm 4. Given this principle, the idea of how to learn π is as follows: consider the case where fθ has been learned and we are currently learning πγB−1 i.e. the sub-policy that aims at acquiring the B-th and ﬁnal region. Given any sample of B − 1 regions (sx B−1), πγB−1 can decide to acquire any of the remaining regions. If it acquires some of the classiﬁcation policy will predict the right category while, for some other regions, fθ will predict the incorrect category as illustrated in Fig. 3. The method we propose consists thus in simulating the decision of fθ over all the possible B-th regions that can be acquired by πγB−1, and to use the regions that correspond to a good ﬁnal classiﬁcation decision as supervised examples for learning πγB−1. The result of this learning is to obtain a sub-policy that tends to select regions for which fθ will be able to properly predict.  these regions,  The same type of reasoning can be also used for learning the other policies i.e. πγB−2 will be learned in order to improve the quality of the sub-policy (πγB−1, fθ), πγB−3 will be learned in order to improve the quality of the sub- policy (πγB−2 , πγB−1, fθ) and so on. When learning sub- policy πγt, we ﬁrst start by building a simulation set where each element is an image represented by t − 1 randomly sampled regions (line 5). Then, for each element, we test each possible remaining regions (lines 6-7) by simulating the previously learnt sub-policies (πγt+1 , ..., fθ) (line 8). We thus build a training set (lines 9-10) that is used to learn πγt (line 15).  550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604  Algorithm 4 Exploration Sub-policy πγk Learn. Algo- rithm Require: (πk+1  , fθ): previously learned sub-  , ..., πB−1  γ  γ  policies  Require: (x1, ..., x(cid:96)): Training set of images Require: (y1, ..., y(cid:96)): Training labels 1: T = {} {Training set} 2: {For each training image} 3: for xi do 4: 5:  for k = 1 to n do  6: 7: 8:  Sample k − 1 regions (sxi exploration policy {For each region that has not been acquired} for sxi  1 , ..., sxi  k ) using random  k do  1 , ..., sxi  k+1 /∈ sxi Use sub-policies (πk+1 k , sxi (sxi if ˆy == yi then  T ← T (cid:83)(Φ(sxi  1 , ..., sxi  γ  k+1) to compute ˆy  1 , ..., sxi  k ), sxi  k+1)  , ..., πB−1  γ  , fθ) from  end if end for  9: 10: 11: 12: 13: 14: end for 15: Learn πk 16: return πk γ  end for  γ on T using a classical learning algorithm  4.3. Complexity  The learning complexity of our method is the following: in order to simulate the behavior of the different sub-policies, we have to compute the features over all the regions of the training images. Moreover, for each training image, many samples of regions will be built (line 5 of Algorithms 3 and 4). Let us denote (cid:96) the number of training images and k the number of sample built for each image, at each step of the learning. The ﬁnal complexity4 is O(N × M × C + B × R((cid:96) × k)) where R(n) corresponds to the complexity of learning over n examples. R depends on the machine learning algorithm used for representing fθ and π. If we consider a classical BoW model, this complexity becomes O(N × M × C + R((cid:96))). Our method is thus slower to learn than standard models but still reasonable enough to allow the model to be learnt on large datasets.  5. Experiments We evaluate the proposed method on two challenging image databases corresponding to different tasks: ﬁne- grained image classiﬁcation (People Playing Musical Instruments dataset5) (Yao & Fei-Fei, 2010) and scene  4We do not consider the complexity of the simulation phase of the algorithm i.e. line 8 of Algorithm 4 which is usually negligible w.r.t the other factors of the learning method.  5http://ai.stanford.edu/ bangpeng/ppmi.html  recognition (15-scenes dataset) (Lazebnik et al., 2006). Let us detail the experimental setup by presenting the low-level image features and the chosen vectorial representation for each region. We densely extract gray SIFT descriptors using the VLFEAT library (Vedaldi & Fulkerson, 2010). These local features are computed at a single scale (s = 16 pixels) and with a constant step size d. For 15-scenes we use d = 8 pixels while d = 4 pixels for PPMI. Each region is represented by a BoW vector generated from SIFT descriptors (Sivic & Zisserman, 2003). We run a K-Means algorithm by randomly sampling about 1 million descriptors in each database to produce a dictionnary of M = 200 codeword elements. Each SIFT is then projected on the dictionary using hard assignement, and the codes are aggregated with sum pooling. The histogram is further (cid:96)2 normalized. Finally, we take the square root of each el- ement thus generating a Bahttacharyya kernel feature map.  The standard learning algorithm for fθ and πγ is a one- against-all hinge-loss perceptron learned with a gradient descent algorithm. The gradient descent step and number of iterations have been tuned over the training set in order to maximize the accuracy of the classiﬁer. We have chosen to create 10 sequences of regions for each training images, resulting in a training set of size 10 × (cid:96) for each classiﬁer – fθ and πγt – learned by our method. Performance with more samples has been computed but is not reported here since it is equivalent to the one obtained with 10 samples per image.  5.1. Experimental Results  In order to evaluate the performance of our method, we use the standard metrics for the two considered databases: multi-class accuracy for 15-scenes, and average accuracy over the 12 independently learned binary tasks for PPMI6. We randomly sample training/testing images on 5 splits of the data, and the ﬁnal performance corresponds to the av- erage performance obtained on the 5 runs. We run experiments with different values of B. The base- line model performance is the one obtained where B = 16 i.e. all the regions are acquired by the model and the classi- ﬁcation decision is based on the whole image. The results obtained with such a baseline approach are on par with previously published state-of-the art performances with a similar setup. For example, we reach 77.7% accuracy in the 15-Scene database, exactly matching the performances reported in (Parizi et al., 2012) 7. Although absolute per-  6Note that we do not report MAP metrics for PPMI, since our model produces a category label but not a score for each image. 7Their SBoW method matches our pipeline: mono-scale SIFT extracted with the same step size, same dictionary size, same cod- ing/pooling schemes on a 4 × 4 grid, and normalization policy.  605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659  660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714  Figure 4. Accuracy on the Guitar dataset from PPMI with varying values of B. formance can still be improved using more advanced low- level feature extraction or mid-level feature embedding, our main purpose here is to validate the relative performance advantage of the proposed method. Figures 5 and 6 show the accuracy obtained for the the PPMI dataset and the 15 Scenes dataset. These ﬁgures present two measures: the performance obtained using a uniformly sampled subset of B regions using fθ — in red —, and the performance when the regions are sampled fol- lowing the learned exploration policy πγ — in blue.  Performance on a small set of regions vs performance of the baseline model: When comparing the accuracy of our method with B < 16 to the performance of the base- line method (B = 16), one can see that given a reasonable value of B, our model is competitive with the classical ap- proach. For example, acquiring B = 10 for the 15 Scenes dataset and for B = 8 for the PPMI dataset allows one to obtain accuracy which is similar to the model with B = 16. This means that our method is able to classify as well as standard approach using only half of the regions. More- over, on PPMI, for B = 8, 10 and 12, our model clearly outperforms the baseline. This illustrates the ability of the learning algorithm to focus on relevant areas of the image, ignoring noisy or misleading regions. The generalization capacity of the prediction is thus improved.  Learned exploration policy vs random exploration pol- icy: Now, when comparing the performance of random exploration policy w.r.t. learned exploration policy, one can see that in almost all cases, the learned version is equivalent or better than the random one. For example, on the PPMI corpus with B = 8, learning how to acquire the regions allow one to obtain an improvement of about 4% in term of accuracy. This shows that our model is able, particularly on the PPMI dataset, to discover relevant regions depend- ing on their content. This improvement is less important  Figure 5. Mean accuracy on the PPMI dataset depending on the budget, B. We can see that the learned exploration policy (in red) has a better average accuracy for almost all budgets, especially when considering 8-12 regions.  when the number of acquired regions is large. This is due to the fact that, when acquiring for example B = 12 re- gions, even at random, the relevant information has a high chance of being acquired. The same effect also happens when B is low e.g. when B = 2, the information acquired is not sufﬁcient to allow for good classiﬁcation. This shows that our method is particularly interesting when the number of acquired regions is between about 30% and 60% of the overall image. On certain speciﬁc datasets performance gains with our method can be quit important. Performance for varying values of B for the Guitar dataset of PPMI is illustrated in Figure 4, where we can see that as the percentage of regions acquired decreases (decreasing B), the random ex- ploration policy’s performance degrades quickly whereas our method is able to maintain adequate performance, until B becomes too small. The difference in performance be- tween PPMI and 15 Scenes is likely linked to the fact that detecting instrument play in an image requires speciﬁc re- gions to be available (the face and the ﬂute for example), whereas the scene can be inferred from a wider array of regions.  Complexity Analysis: Let us denote C the cost of com- puting the features over a region of the image, and F the cost of computing fθ or πγi. The ﬁnal inference complex- ity is O(B(C + F )). As we use linear classiﬁers in all our experiments, classiﬁcation time is insigniﬁcant in com- parison to SIFT computation (F << C), and complexity is therefore reduced to O(BC). In comparison to the cost of a classical BoW model O(N M B), the proposed model thus results in a speed-up of N×M B .  Qualitative Results & Analysis: Figures 7 and 8 illus- trate the learned exploration policy for the class ﬂute of the  715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769  0.00.20.40.60.81.0% of regions0.500.550.600.650.700.750.800.850.900.95accuracyLearnedRandomGuitar6062646668707274767880246810121416Accuracy B Random Exploration PolicyLearned Exploration Policy770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824  Figure 8. Most frequent acquired regions for B = 4 (left) and B = 8 (right) on the PPMI Flute Dataset. The darker, the more frequent the region has been acquired for classifying.  Figure 6. Mean Accuracy on 15 Scenes dataset depending on the budget, B. On this dataset, the learned exploration policy (red) is only slightly better than  Figure 7. Trajectories computed on PPMI-Flute (test). Each node corresponds to a region — label is (x, y)-coordinates. Each edge i → j means that j has been acquired just after having acquired i. The color of the edge represents the proportion of infered tra- jectories that contain the i → j transition.  PPMI dataset. Figure 7 summarizes the regions visited over the testing images. Each region corresponds to a node of the graph, and an edge from region i to region j means that in at least one testing image, regions j has been acquired immediately after region i. We can see that the algorithm is focusing its attention around 5 regions that are certainly relevant for many pictures, i.e. (1; 3), (4; 2), (4; 3), (3; 3) and (3; 1). On the other hand, Figure 7 shows that at the beginning the algorithm tends to explore many different re- gions after the initial region. This shows that the the model starts by ﬁrst exploring the image — until having acquired 2 regions — before focusing its attention on the regions that are the most relevant for predicting the category. Figure 8 shows the average behavior of our algorithm for B = 4 and B = 8.We notice that about half of the acquired re- gions — 2 for B = 4 and 4 for B = 8 — are much more frequently explored than the other regions. These regions certainly correspond to regions that generally carry relevant information on all the images.  Figure 9. Two examples of regions acquired with B = 4 on the PPMI-Flute dataset. This example shows the ability of the model to adapt to different images where it is able to discover a ﬂute.  The images in Figure 8 can be interpreted as a spatial “prior” for a speciﬁc classiﬁcation task. For example, for discriminating playing v.s. holding ﬂute with B = 2, re- gions (3; 2) and (3; 1) are the more informative on aver- age. However, since the decision is instance-based in our method, this spatial prior is balanced with the speciﬁc vi- sual content of each test image. Our approach therefore shares some similarities with the reconﬁgurable model of (Parizi et al., 2012) using latent SVM. One example of instance-based classiﬁcation is illustrated in ﬁgure 9, where the regions visited with B = 4 are shown. We can see that the set of regions visited changes between the left and right image. This illustrates the ability of our method to auto- matically adapt its choice of representation to the content of the image it is classifying.  6. Conclusion In this paper, we introduced an adaptive representation pro- cess for image classiﬁcation. The presented strategy com- bines both an exploration strategy used to ﬁnd the best sub- set of regions for each image, and the ﬁnal classiﬁcation algorithm. New regions are iteratively selected based on the location and content of the previous ones. The result- ing scheme produces an effective instance-based classiﬁca- tion algorithm. We demonstrated the strategy’s pertinence on two different image classiﬁcation datasets. When using our exploration strategy limited to half of the regions of the  825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879  3035404550556065707580246810121416Accuracy B Random Exploration PolicyLearned Exploration PolicyLarochelle, H and Hinton, GE. Learning to combine foveal glimpses with a third-order Boltzmann machine. NIPS, pp. 1–9, 2010.  Lazebnik, S., Schmid, C., and Ponce, J. Beyond bags of features: Spatial pyramid matching for recognizing nat- ural scene categories. In CVPR, 2006.  Liu, L., Wang, L., and Liu, X.  assignment coding. In ICCV, 2011.  In defense of soft-  Mikolajczyk, Krystian and Schmid, Cordelia. A perfor- mance evaluation of local descriptors. IEEE Trans. Pat- tern Anal. Mach. Intell., 27(10):1615–1630, 2005.  Parizi, Sobhan Naderi, Oberlin, John G., and Felzenszwalb, Pedro F. Reconﬁgurable models for scene recognition. In CVPR, pp. 2775–2782, 2012.  R¨uckstieß, Thomas, Osendorfer, Christian, and van der Smagt, Patrick. Minimizing Data Consumption with Se- quential Online Feature Selection. Journal of Machine Learning and Cybernetics.  Sharma, Gaurav, Jurie, Fr´ed´eric, and Schmid, Cordelia. Discriminative spatial saliency for image classiﬁcation. In CVPR, pp. 3506–3513, 2012.  Sivic, J. and Zisserman, A. Video Google: A text retrieval approach to object matching in videos. In ICCV, 2003.  Su, Yu and Jurie, Fr´ed´eric. Improving image classiﬁcation using semantic attributes. International Journal of Com- puter Vision, 100(1):59–77, 2012.  van Gemert, Jan, Veenman, Cor J., Smeulders, Arnold W. M., and Geusebroek, Jan-Mark. Visual word am- biguity. IEEE Trans. Pattern Anal. Mach. Intell., 32(7): 1271–1283, 2010.  Vedaldi, A. and Fulkerson, B. Vlfeat – an open and portable In Proc. of the  library of computer vision algorithms. 18th annual ACM Intl. Conf. on Multimedia, 2010.  Yao, Bangpeng and Fei-Fei, Li. Grouplet: A structured image representation for recognizing human and object interactions. In CVPR, San Francisco, USA, June 2010.  880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934  images, we obtained a signiﬁcant gain relative to baseline methods.  References Borji, A., Sihite, D.N., and Itti, L. Salient object detection:  A benchmark. In ECCV, 2012.  Boureau, Y-Lan, Bach, Francis, LeCun, Yann, and Ponce, In  Jean. Learning mid-level features for recognition. CVPR, pp. 2559–2566, 2010.  Busa-Fekete, R., Benbouzid, D., and K´egl, B. Fast clas- siﬁcation using sparse decision DAGs. In ICML 2012, Edinburgh, Royaume-Uni, June 2012. LAL 12-223.  Chang, KY., Liu, T.L., Chen, H.T., and Lai, SH. Fusing generic objectness and visual saliency for salient object detection. In ICCV, 2011.  Chatﬁeld, K., Lempitsky, V., Vedaldi, A., and Zisserman, A. The devil is in the details: an evaluation of recent feature encoding methods. In Proceedings of the British Machine Vision Conference (BMVC), 2011.  Dimitrakakis, Christos and Lagoudakis, Michail G. Rollout sampling approximate policy iteration. Machine Learn- ing, 72(3):157–171, 2008.  Dulac-Arnold, Gabriel, Denoyer, Ludovic, and Gallinari, Patrick. Text classiﬁcation: A sequential reading ap- proach. In ECIR, pp. 411–423, 2011.  Dulac-Arnold, Gabriel, Denoyer, Ludovic, Preux, Philippe, and Gallinari, Patrick. Sequential approaches for learn- ing datum-wise sparse representations. Machine Learn- ing, 89(1-2):87–122, August 2012a. ISSN 0885-6125. doi: 10.1007/s10994-012-5306-7.  Dulac-Arnold, Gabriel, Denoyer, Ludovic, Preux, Philippe, and Gallinari, Patrick. Sequential approaches for learn- ing datum-wise sparse representations. Machine Learn- ing, 89(1-2):87–122, 2012b.  Ernst, Damien, Geurts, Pierre, and Wehenkel, Louis. Tree- J. Mach. ISSN 1532-  based batch mode reinforcement learning. Learn. Res., 6:503–556, December 2005. 4435.  Feng, Jiashi, Ni, Bingbing, Tian, Qi, and Yan, Shuicheng. Geometric lp-norm feature pooling for image classiﬁca- tion. In CVPR, pp. 2697–2704, 2011.  Karayev, Sergey, Baumgartner, Tobias, Fritz, Mario, and Darrell, Trevor. Timely object recognition. In NIPS, pp. 1–9, 2012.  935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989  ","In this paper, we investigate a new framework for image classification thatadaptively generates spatial representations. Our strategy is based on asequential process that learns to explore the different regions of any image inorder to infer its category. In particular, the choice of regions is specificto each image, directed by the actual content of previously selectedregions.The capacity of the system to handle incomplete image information aswell as its adaptive region selection allow the system to perform well inbudgeted classification tasks by exploiting a dynamicly generatedrepresentation of each image. We demonstrate the system's abilities in a seriesof image-based exploration and classification tasks that highlight its learnedexploration and inference abilities."
1309.1369,2014,Semistochastic Quadratic Bound Methods  ,"['Aleksandr Y. Aravkin', 'Anna Choromanska', 'Tony Jebara', 'Dimitri Kanevsky']",https://arxiv.org/pdf/1309.1369.pdf,"4 1 0 2     b e F 7 1         ] L M  . t a t s [      4 v 9 6 3 1  .  9 0 3 1 : v i X r a  Semistochastic quadratic bound methods  Aleksandr Aravkin  IBM T.J. Watson Research Center  Yorktown Heights, NY 10598 saravkin@us.ibm.com  Anna Choromanska Columbia University  NY, USA  aec2163@columbia.edu  Tony Jebara  Columbia University  NY, USA  Dimitri Kanevsky  IBM T.J. Watson Research Center  Yorktown Heights, NY 10598  jebara@cs.columbia.edu  dimitri.kanevsky@gmail.com  Abstract  Partition functions arise in a variety of settings, including conditional random ﬁelds, logistic regression, and latent gaussian models. In this paper, we con- sider semistochastic quadratic bound (SQB) methods for maximum likelihood es- timation based on partition function optimization. Batch methods based on the quadratic bound were recently proposed for this class of problems, and performed favorably in comparison to state-of-the-art techniques. Semistochastic methods fall in between batch algorithms, which use all the data, and stochastic gradient type methods, which use small random selections at each iteration. We build semistochastic quadratic bound-based methods, and prove both global conver- gence (to a stationary point) under very weak assumptions, and linear convergence rate under stronger assumptions on the objective. To make the proposed methods faster and more stable, we consider inexact subproblem minimization and batch- size selection schemes. The efﬁcacy of SQB methods is demonstrated via com- parison with several state-of-the-art techniques on commonly used datasets.  Introduction  1 The problem of optimizing a cost function expressed as the sum of a loss term over each sample in an input dataset is pervasive in machine learning. One example of a cost function of this type is the partition funtion. Partition function is a central quantity in many different learning tasks including training conditional random ﬁelds (CRFs) and log-linear models [1], and will be of central focus in this paper. Batch methods based on the quadratic bound were recently proposed [1] for the class of problems invoving the minimization of the partition function, and performed favorably in compar- ison to state-of-the-art techniques. This paper focuses on semistochastic extension of this recently developed optimization method. Standard learning systems based on batch methods such as BFGS and memory-limited L-BFGS, steepest descent (see e.g. [2]), conjugate gradient [3] or quadratic bound majorization method [1] need to make a full pass through an entire dataset before updating the parameter vector. Even though these methods can converge quickly (sometimes in several passes through the dataset), as datasets grow in size, this learning strategy becomes increasingly inefﬁcient. To faciliate learning on massive datasets, the community increasingly turns to stochastic methods. Stochastic optimization methods interleave the update of parameters after only processing a small mini-batch of examples (potentially as small as a single data-point), leading to signiﬁcant com- putational savings ([4, 5, 6]). Due to its simplicity and low computational cost, the most popular contemporary stochastic learning technique is stochastic gradient descent (SGD) [7, 4, 8]. SGD updates the parameter vector using the gradient of the objective function as evaluated on a single ex- ample (or, alternatively, a small mini-batch of examples). This algorithm admits multiple extensions,  1  including (i) stochastic average gradient method (SAG) that averages the most recently computed gradients for each training example [9], (ii) methods that compute the (weighted) average of all pre- vious gradients [10, 11], (iii) averaged stochastic gradient descent method (ASGD) that computes a running average of parameters obtained by SGD [12], (iv) stochastic dual coordinate ascent, that op- timizes the dual objective with respect to a single dual vector or a mini-batch of dual vectors chosen uniformly at random [13, 14], (v) variance reduction techniques [15, 16, 9, 17] (some do not require storage of gradients, c.f. [15]), (vi) majorization-minimization techniques that minimize a majoring surrogate of an objective function [18, 19] and (vii) gain adaptation techniques [20, 21]. Semistochastic methods can be viewed as an interpolation between the expensive reliable updates used by full batch methods, and inexpensive noisy updates used by stochastic methods. They in- herit the best of both worlds by approaching the solution more quickly when close to the optimum (like a full batch method) while simultaneously reducing the computational complexity per iteration (though less aggressively than stochastic methods). Several semistochastic extensions have been explored in previous works [22, 23, 24]. Recently, convergence theory and sampling strategies for these methods have been explored in [25, 26] and linked to results in ﬁnite sampling theory in [27]. Additionally, incorporating second-order information (i.e. Hessian) into the optimization problem ([20, 28, 29, 30, 31, 32]) was shown to often improve the performance of traditional SGD meth- ods which typically provide fast improvement initially, but are slow to converge near the optimum (see e.g. [25]), require step-size tuning and are difﬁcult to parallelize [33]. This paper focuses on semistochastic extension of a recently developed quadratic bound majorization technique [1], and we call the new algorithm semistochastic quadratic bound (SQB) method. The bound computes the update on the parameter vector using the product of the gradient of the objective function and an inverse of a second-order term that is a descriptor of the curvature of the objective function (dif- ferent than the Hessian). We discuss implementation details, in particular curvature approximation, inexact solvers, and batch-size selection strategies, which make the running time of our algorithm comparable to the gradient methods and also make the method easily parallelizable. We show global convergence of the method to a stationary point under very weak assumptions (in particular convex- ity is not required) and a linear convergence rate when the size of the mini-batch grows sufﬁciently fast, following the techniques of [25]. This rate of convergence matches state-of-the-art incremen- tal techniques [15, 13, 9, 18] (furthermore it is better than in case of standard stochastic gradient methods [7, 8] which typically have sublinear convergence rate [9, 34]). Compared to other exist- ing majorization-minimization incremental techniques [18], our approach uses much tighter bounds which, as shown in [1], can lead to faster convergence. The paper is organized as follows: Section 2 reviews quadratic bound majorization technique. Sec- tion 3 discusses stochastic and semistochastic extensions of the bound, and presents convergence theory for the proposed methods. In particular, we discuss very general stationary convergence the- ory under very weak assumptions, and also present a much stronger theory, including convergence rate analysis, for logistic regression. Section 4 discusses implementation details, and Section 5 shows numerical experiments illustrating the use of the proposed methods for l2-regularized logistic regression problems. Conclusions end the paper. The semistochastic quadratic bound majorization technique that we develop in this paper can be broadly applied to mixture models or models that induce representations. The advantages of this technique in the batch setting for learning mixture models and other latent models, was shown in the work of [1]. In particular, quadratic bound majorization was able to ﬁnd better local optima in non-convex problems than state-of-the art methods (and in less time). While theoretical guarantees for non-convex problems are hard to obtain, the broader convergence theory developed in this paper (ﬁnding a stationary point under weak assumptions) does carry over to the non-convex setting. 2 Quadratic bound methods Let Ω be a discrete probability space over the set of n elements, and take any log-linear density model  p(y|xj, θ) =  1  (1) parametrized by a vector θ ∈ Rd, where {(x1, y1), . . . , (xT , yT )} are iid input-output pairs, fxj : Ω (cid:55)→ Rd is a continuous vector-valued function mapping and hxj : Ω (cid:55)→ R+ is a ﬁxed non-negative measure. The partition function Zxj (θ) is a scalar that ensures that p(y|xj, θ) is a true density, so  Zxj (θ)  hxj (y) exp(cid:0)θ(cid:62)fxj (y)(cid:1)  2  in particular (1) integrates to 1:  Zxj (θ) =  (cid:88)  y  hxj (y) exp(θ(cid:62)fxj (y)) .  (2)  [1] propose a fast method to ﬁnd a tight quadratic bound for Zxj (θ), shown in the subroutine Bound Computation in Algorithm 1, which ﬁnds z, r, S so that  Zxj (θ) ≤ z exp( 1  2 (θ − ˜θ)(cid:62)S(θ − ˜θ) + (θ − ˜θ)(cid:62)r)  for any θ, ˜θ, fxj (y) ∈ Rd and hxj (y) ∈ R+ for all y ∈ Ω. The (regularized) maximum likelihood estimation problem is equivalent to  (cid:110)Lη(θ) := − 1  T  T(cid:88)  j=1  min  θ  log(p(yj|xj, θ)) +  η 2  (cid:107)θ(cid:107)2 ≈ 1 T  (cid:0)log(Zxj (θ)) − θ(cid:62)fxj (yj)(cid:1) +  T(cid:88)  j=1  (3)  (cid:107)θ(cid:107)2(cid:111)  ,  η 2  (4) where ≈ means equal up to an additive constant. The bound (3) suggests the iterative minimization scheme  θk+1 = θk − αk(Σk + ηI)−1(µk + ηθk).  (5) where Σk and µk are computed using Algorithm 1, η is the regularization term and αk is the step size at iteration k. In this paper, we consider applying the bound to randomly selected batches of data; any such selec- tion we denote T ⊂ [1, . . . , T ] or S ⊂ [1, . . . , T ].  Algorithm 1 Semistochastic Quadratic Bound (SQB) Input Parameters ˜θ, fxj (y) ∈ Rd and hxj (y) ∈ R+ for y ∈ Ω, j ∈ T Initialize µT = 0, ΣT = 0(d, d) For each j ∈ T  (fxj (y) − r)(fxj (y) − r)(cid:62)  Subroutine Bound Computation: z → 0+, r = 0, S = zI For each y ∈ Ω  α = hxj (y) exp( ˜θ(cid:62)fxj (y)) S+ = tanh( 1 r = z z+ = α  z+α r + α  z+α fxj (y)  2 log(α/z))  2 log(α/z)  Subroutine output z, r, S µT + = r − fxj (y) ΣT + = S  µT / = |T | ΣT / = |T | Output µT , ΣT  3 Stochastic and semistochastic extensions The bounding method proposed in [1] is summarized in Algorithm 1 with T = [1, . . . , T ] at every iteration. When T is large, this strategy can be expensive. In fact, computing the bound has com- plexity O(T nd2), since T n outer products must be summed to obtain Σ, and each other product has complexity O(d2). When the dimension d is large, considerable speedups can be gained by obtaining a factored form of Σ, as described in Section 4.1. Nonetheless, in either strategy, the size of T is a serious issue. A natural approach is to subsample a smaller selection T from the training set [1, . . . , T ], so that at each iteration, we run Algorithm 1 over T rather than over the full data to get µT , ΣT . When |T | is ﬁxed (and smaller than T ), we refer to the resulting method as a stochastic extension. If instead |T | is allowed to grow as iterations proceed, we call this method semistochastic; these methods are  3  analyzed in [25]. All of the numerical experiments we present focus on semistochastic methods. One can also decouple the computation of gradient and curvature approximations, using different data selections (which we call T and S). We show that this development is theoretically justiﬁable and practically very useful. For the stochastic and semistochastic methods discussed here, the quadratic bound property (3) does not hold for Z(θ), so the convergence analysis of [1] does not immediately apply. Nonetheless, it is possible to analyze the algorithm in terms of sampling strategies for T . The appeal of the stochastic modiﬁcation is that when |T | << T , the complexity O(|T |nd) of Algorithm 1 to compute µT , ΣS is much lower; and then we can still implement a (modiﬁed) iteration (5). Intuitively, one expects that even small samples from the data can give good updates for the overall problem. This intuition is supported by the experimental results, which show that in terms of effective passes through the data, SQB is competitive with state of the art methods. We now present the theoretical analysis of Algorithm 1. We ﬁrst prove that under very weak assump- tion, in particular using only the Lipschitz property, but not requiring convexity of the problem, the proposed algorithm converges to a stationary point. The proof technique easily carries over to other objectives, such as the ones used in maximum latent conditional likelihood problems (for details see [1]), since it relies mainly only on the sampling method used to obtain T . Then we focus on problem (4), which is convex, and strictly convex under appropriate assumptions on the data. We use the structure of (4) to prove much stronger results, and in particular analyze the rate of convergence of Algorithm 1.  3.1 General Convergence Theory  We present a general global convergence theory, that relies on the Lipschitz property of the objective and on the sampling strategy in the context of Algorithm 1. The end result we show here is that any limit point of the iterates is stationary. We begin with two simple preliminary results. Lemma 1 If every i ∈ [1, . . . , T ] is equally likely to appear in T , then E[µT ] = µ. Proof 1 Algorithm 1 returns µT = 1|T | each j has an equal chance to appear in T , then  (cid:80) j∈T ψj(θ), where ψj(θ) = −∇θ log(p(yj|xj, θ)). If (cid:88)  (cid:88)  (cid:88)  ψj(θ)  j∈T  1 |T |  j∈T  E[ψj(θ)] =  µ = µ .  1 |T |  j∈T   1  |T |  E   =  Note that the hypothesis here is very weak: there is no stipulation that the batch size be of a certain size, grow with iterations, etc. This lemma therefore applies to a wide class of randomized bound methods.  Lemma 2 Denote by λmin the inﬁmum over all possible eigenvalues of ΣS over all choices of batches (λmin may be 0). Then E[(ΣS + ηI)−1] satisﬁes I ≤ E[(ΣS + ηI)−1] ≤  I .  1  1  η + λmax  η + λmin  Proof 2 For any vector x and any realization of ΣS, we have  1  (cid:107)x(cid:107)2 ≤ xT (ΣS + ηI)−1x ≤  η + λmax  1  (cid:107)x(cid:107)2 ,  η + λmin  where λmax depends on the data. Taking the expectation over T of be above inequality gives the result.  Theorem 1 For any problem of form (4), apply iteration (5), where at each iteration µT , ΣS are obtained by Algorithm 1 for two independently drawn batches subsets T ,S ⊂ [1, . . . , T ] selected to satisfy the assumptions of Lemma 1. Finally, suppose also that the step sizes αk are square summable but not summable. Then Lη(θk) converges to a ﬁnite value, and ∇Lη(θk) → 0. Furthermore, every limit point of θk is a stationary point of Lη.  4  Theorem 1 states the conclusions of [35, Proposition 3], and so to prove it we need only check that the hypotheses of this proposition are satisﬁed.  Proof 3 [35] consider algorithms of the form  θk+1 = θk − αk(sk + wk) .  In the context of iteration (5), at each iteration we have  sk + wk = (ΣkS + λI)−1gkT ,  where gkT = µkT + ηθk, and gk is the full gradient of the regularized problem (4). We choose  sk = E[(ΣkS + ηI)−1]gk, wk = (ΣkS + ηI)−1gkT − sk.  We now have the following results:  1. Unbiased error:  E[wk] = E[(ΣkS + ηI)−1gkT − sk] = E[(ΣkS + ηI)−1]E[gkT ] − sk = 0 ,  (6) where the second equality is obtained by independence of the batches T and S, and the last equality uses Lemma 1.  2. Gradient related condition:  (gk)T sk = (gk)T E[(ΣkS + ηI)−1]gk ≥ (cid:107)gk(cid:107)2  η + λmax  .  3. Bounded direction:  4. Bounded second moment:  By part 1, we have  (cid:107)sk(cid:107) ≤ (cid:107)gk(cid:107)  η + λmin  .  E[(cid:107)wk(cid:107)2] ≤ E[(cid:107)(ΣkS + ηI)−1gkT (cid:107)2  ≤ E[(cid:107)gkT (cid:107)2] (η + λmin)2 =  tr(cov[gkT ]) + (cid:107)gk(cid:107)2  .  (η + λmin)2  (7)  (8)  (9)  The covariance matrix of gkT is proportional to the covariance matrix of the set of individual (data- point based) gradient contributions, and for problems of form (4) these contributions lie in the convex hull of the data, so in particular the trace of the covariance must be ﬁnite. Taken together, these results show all hypotheses of [35, Proposition 3] are satisﬁed, and the result follows.  Theorem (1) applies to any stochastic and semistochastic variant of the method. Note that two independent data samples T and S are required to prove (6). Computational complexity motivates different strategies for selecting choose T and S. In particular, it is natural to use larger mini- batches to estimate the gradient, and smaller mini-batch sizes for the estimation of the second-order curvature term. Algorithms of this kind have been explored in the context of stochastic Hessian methods [32]. We describe our implementation details in Section 4.  3.2 Rates of Convergence for Logistic Regression  The structure of objective (4) allows for a much stronger convergence theory. We ﬁrst present a lemma characterizing strong convexity and Lipschitz constant for (4). Both of these properties are crucial to the convergence theory. Lemma 3 The objective Lη in (4) has a gradient that is uniformly norm bounded, and Lipschitz continuous.  5  (10)  (11)  Proof 4 The function Lη has a Lipschitz continuous gradient if there exists an L such that  (cid:107)∇Lη(θ1) − ∇Lη(θ0)(cid:107) ≤ L(cid:107)θ1 − θ0(cid:107)  holds for all (θ1, θ0). Any uniform bound for trace(∇2Lη) is a Lipschitz bound for ∇Lη. Deﬁne  ay,j := hxj (y) exp(θ(cid:62)fxj (y)) ,  and note ay,j ≥ 0. Let pj be the empirical density where the probability of observing y is given by ay,j(cid:80)  . The gradient of (4) is given by  y ay,j  (cid:32)(cid:32)(cid:88)  T(cid:88)  j=1  y  1 T  (cid:80)  ay,jfxj (y) y ay,j  (cid:33)  (cid:33) − fxj (yj)  (cid:0)Epj [fxj (·)] − fxj (yj)(cid:1) + ηθ  T(cid:88)  j=1  + ηθ =  1 T  It is straightforward to check that the Hessian is given by  T(cid:88)  j=1  ∇2Lη =  1 T  covpj [fxj (·)] + ηI  where covpj [·] denotes the covariance matrix with respect to the empirical density function pj. Therefore a global bound for the Lipschitz constant L is given by maxy,j (cid:107)fxj (y)(cid:107)2 + ηI, which completes the proof. Note that Lη is strongly convex for any positive η. We now present a convergence rate result, using results from [25, Theorem 2.2].  Theorem 2 There exist µ, L > 0, ρ > 0 such that  (cid:107)∇Lη(θ1) − ∇Lη(θ2)(cid:107)∗∗ ≤ L(cid:107)θ2 − θ1(cid:107)∗ ρ(cid:107)θ2 − θ1(cid:107)∗  Lη(θ2) ≥ Lη(θ1) + (θ2 − θ1)T∇Lη(θ1) + θT (ΣkS + ηI)θ and (cid:107)θ(cid:107)∗∗ is the corresponding dual norm  (cid:113)  1 2  (cid:113)  where (cid:107)θ(cid:107)∗ = Furthermore, take αk = 1 the gradient at iteration k. Provided a batch growth schedule with limk→∞ Bk+1 Bk iteration (5) we have (for any (cid:15) > 0)  θT (ΣkS + ηI)−1θ. η − gkT (cid:107)2, the square error incurred in ≤ 1, for each  L in (5), and deﬁne Bk = (cid:107)∇Lk  (12)  Lη(θk) − Lη(θ∗) ≤(cid:16)  (cid:17)k  1 − ρ L  with Ck = max{Bk, (1 − ρ  L + (cid:15))k}.  [Lη(θ0) − Lη(θ∗)] + O(Ck) ,  (13)  Proof 5 Let ˜L denote the bound on the Lipschitz constant of g is provided in (10). By the conclusions ˜L. Let ˜ρ denote the minimum eigenvalue of (11) (note that of Lemma 2, we can take L = ˜ρ ≥ η). Then take ρ =  1√ η+λmin ˜ρ. The result follows immediately by [25, Theorem 2.2].  1√  η+λmax  Implementation details  4 In this section, we brieﬂy discuss important implementation details as well as describe the compara- tor methods we use for our algorithm.  4.1 Efﬁcient inexact solvers  The linear system we have to invert in iteration (5) has very special structure. The matrix Σ returned by Algorithm 1 may be written as Σ = SST , where each column of S is proportional to one of the vectors (fxj (y) − r) computed by the bound. When the dimensions of θ are large, it is not practical to compute the Σ explicitly. Instead, to compute the update in iteration (5), we take advantage of the fact that  Σx = S(ST x),  6  and use S (computed with a simple modiﬁcation to the bound method) to implement the action of Σ. When S ∈ Rd×k (k is a mini-batch size), the action of the transpose on a vector can be computed in O(dk), which is very efﬁcient for small k. The action of the regularized curvature approximation Σ + ηI follows immediately. Therefore, it is efﬁcient to use iterative minimization schemes, such as lsqr, conjugate gradient, or others to compute the updates. Moreover, using only a few iterations of these methods further regularizes the subproblems [36, 37]. It is interesting to note that even when η = 0, and ΣT is not invertible, it makes sense to consider inexact updates. To justify this approach, we present a range lemma. A similar lemma appears in [36] for a different quadratic approximation. Lemma 4 For any T , we have µT ∈ R(ΣT ). Proof 6 The matrix ΣT is formed by a sum of weighted outer products (fxj (y) − r)(fxj (y) − r)(cid:62). We can therefore write  ΣT = LDLT  where L = [l1, . . . , l|Ω|·|T |], lk = fxj (yk)−rk (k is the current iteration of the bound computation), , where the quantities αk, zk and D is a diagonal matrix with weights Dkk = 1|T | correspond to iterations in Algorithm (1). Since µ is in the range of L by construction, it must also be the range of ΣT .  2 log(αk/zk))  2 log(αk/zk)  tanh( 1  Lemma 4 tells us that there is always a solution to the linear system ΣT ∆θ = µT , even if ΣT is singular. In particular, a minimum norm solution can be found using the Moore-Penrose pseu- doinverse, or by simply applying lsqr or cg, which is useful in practice when the dimension d is large. For many problems, using a small number of cg iterations both speeds up the algorithm and serves as additional regularization at the earlier iterations, since the (highly variable) initially small problems are not fully solved.  4.2 Mini-batches selection scheme  In our experiments, we use a simple linear interpolation scheme to grow the batch sizes for both the gradient and curvature term approximations. In particular, each batch size (as a function of iteration k) is given by  bk = min(bcap, b1 + round((k − 1)γ)),  where bcap represents the cap on the maximum allowed size, b1 is the initial batch size, and γ gives the rate of increase. In order to specify the selections chosen, we will simply give values for each of µ on the gradient computation was the full training (b1 set, the cap bcap Σ were both set to 5. At each iteration of SQB, the parameter vector is updated as follows:  Σ for the curvature term was taken to be 200, initial b1  Σ, γµ, γΣ). For all experiments, the cap bcap  µ and b1  µ, b1  θk+1 = θk − ξk,  where ξk = α(ΣkS + ηI)−1(µkT + ηθk) (α is the step size; we use constant step size for SQB in our experiments). Notice that ξk is the solution to the linear system (ΣkS + ηI)ξk = µkT + ηθk and can be efﬁciently computed using the lsqr solver (or any other iterative solver). For all experiments, we ran a small number (l) iterations of lsqr, where l was chosen from the set {5, 10, 20}, before updating the parameter vector (this technique may be viewed as performing conjugate gradient on the bound), and chose l with the best performance (the fastest and most stable convergence).  4.3 Step size  One of the most signiﬁcant disadvantages of standard stochastic gradient methods [7, 8] is the choice of the step size. Stochastic gradient algorithms can achieve dramatic convergence rate if the step size is badly tuned [38, 9]. An advantage of computing updates using approximated curvature terms is that the inversion also establishes a scale for the problem, and requires minimal tuning. This is well known (phenomenologically) in inverse problems. In all experiments below, we used a constant step size; for well conditioned examples we used step size of 1, and otherwise 0.1.  7  4.4 Comparator methods  We compared SQB method with the variety of competitive state-of-the-art methods which we list below:  size;  • L-BFGS: limited-memory BFGS method (quasi-Newton method) tuned for log-linear models which uses both ﬁrst- and second-order information about the objective function (for L-BFGS this is gradient and approximation to the Hessian); we use the competitive implementation obtained from http://www.di.ens.fr/ mschmidt/Software/minFunc.html  • SGD: stochastic gradient descent method with constant step size; we use the competitive implementation obtained from http://www.di.ens.fr/ mschmidt/Software/SAG.html which is analogous to L. Bottou implementation but with pre-speciﬁed step size method implementation  constant step from http://www.di.ens.fr/ mschmidt/Software/SAG.html which is analogous to L. Bot- tou implementation but with pre-speciﬁed step size  gradient competitive  stochastic the use  averaged we  with obtained  • ASGD:  descent  • SAG: stochastic average gradient method using the estimate of Lipschitz constant Lk at iteration k set constant to the global Lipschitz constant; we use the competitive implemen- tation of [9] obtained from http://www.di.ens.fr/ mschmidt/Software/SAG.html  • SAGls: stochastic average gradient method with line search, we use the competitive im- plementation of [9] obtained from http://www.di.ens.fr/ mschmidt/Software/SAG.html; the algorithm adaptively estimates Lipschitz constant L with respect to the logistic loss func- tion using line-search Since our method uses the constant step size we chose to use the same scheme for the competitor methods like SGD, ASGD and SAG. For those methods we tuned the step size to achieve the best performance (the fastest and most stable convergence). Remaining comparators (L-BFGS and SAGls) use line-search.  5 Experiments We performed experiments with l2-regularized logistic regression on binary classiﬁcation task with regularization parameter η = 1 T . We report the results for six datasets. The ﬁrst three are sparse: rcv1 (T = 20242, d = 47236; SQB parameters: l = 5, γµ = 0.005, γΣ = 0.0003), adult (T = 32561, d = 123; SQB parameters: l = 5, γµ = 0.05, γΣ = 0.001) and sido (T = 12678, d = 4932; SQB parameters: l = 5, γµ = 0.01, γΣ = 0.0008). The remaining datasets are dense: covtype (T = 581012, d = 54; SQB parameters: l = 10, γµ = 0.0005, γΣ = 0.0003), pro- tein (T = 145751, d = 74; SQB parameters: l = 20, γµ = 0.005, γΣ = 0.001) and quantum (T = 50000, d = 78; SQB parameters: l = 5, γµ = 0.001, γΣ = 0.0008). Each dataset was split to training and testing datasets such that 90% of the original datasets was used for training and the remaining part for testing. Only sido and protein were split in half to training and testing datasets due to large disproportion of the number of datapoints belonging to each class. The ex- perimental results we obtained are shown in Figure 1. We report the training and testing costs as well as the testing error as a function of the number of effective passes through the data and thus the results do not rely on the implementation details. We would like to emphasize however that under current implementation the average running time for the bound method across the datasets is comparable to that of the competitor methods. All codes are released and are publicly available at www.columbia.edu/ aec2163/NonFlash/Papers/Papers.html. 6 Conclusions We have presented a new semistochastic quadratic bound (SQB) method, together with convergence theory and several numerical examples. The convergence theory is divided into two parts. First, we proved convergence to stationarity of the method under weak hypotheses (in particular, convexity is not required). Second, for the logistic regression problem, we provided a stronger convergence theory, including a rate of convergence analysis. The main contribution of this paper is to apply SQB methods in a semi-stochastic large-scale setting. In particular, we developed and analyzed a ﬂexible framework that allows sample-based approxima-  8  Figure 1: Comparison of optimization strategies for l2-regularized logistic regression. From left two right: training excess cost, testing cost and testing error. From top to bottom: rcv1 (αSGD = 10−1, αASGD = 1, αSQB = 10−1), adult (αSGD = 10−3, αASGD = 10−2, αSQB = 1), sido (αSGD = 10−3, αASGD = 10−2, αSQB = 1), covtype (αSGD = 10−4, αASGD = 10−3, αSQB = 10−1), protein (αSGD = 10−3, αASGD = 10−2, αSQB = 1) and quantum (αSGD = 10−4, αASGD = 10−2, αSQB = 10−1) datasets. This ﬁgure is best viewed in color.  tions of the bound from [1] that are appropriate in the large-scale setting, computationally efﬁcient, and competitive with state-of-the-art methods. Future work includes developing a fully stochastic version of SQB, as well as applying it to learn mixture models and other latent models, as well as models that induce representations, in the context of deep learning. References [1] T. Jebara and A. Choromanska. Majorization for CRFs and latent likelihoods. In NIPS, 2012.  [2] Y. Nesterov. Introductory lectures on convex optimization : a basic course. Applied optimization. Kluwer  Academic Publ., Boston, Dordrecht, London, 2004.  [3] M.R. Hestenes and E. Stiefel. Methods of conjugate gradients for solving linear systems. Journal of  Research of the National Bureau of Standards, 49(6):409–436, 1952.  9  51015202510−210−1Effective PassesObjective minus optimum (training)rcv1  LBFGSSGDASGDSAGSAGlsSQB510152010−1.810−1.7rcv1Test logistic lossEffective Passes  51015202533.23.43.63.844.2rcv1Test error [%]Effective Passes  51015202510−2Effective PassesObjective minus optimum (training)adult  51015202510−1.43310−1.43110−1.42910−1.42710−1.425adultTest logistic lossEffective Passes  51015202515.415.515.615.715.815.9adultTest error [%]Effective Passes  051015202510−410−310−210−1Effective PassesObjective minus optimum (training)sido  510152010−1sidoTest logistic lossEffective Passes  5101520252.62.833.23.43.63.84sidoTest error [%]Effective Passes  51015202510−2.2910−2.2710−2.2510−2.23Effective PassesObjective minus optimum (training)covtype  510152010−1.247610−1.247410−1.247210−1.247covtypeTest logistic lossEffective Passes  51015202523.8523.923.952424.0524.124.15covtypeTest error [%]Effective Passes  51015202510−410−310−2Effective PassesObjective minus optimum (training)protein  510152010−1.810−1.7proteinTest logistic lossEffective Passes  5101520250.260.280.30.320.34proteinTest error [%]Effective Passes  51015202510−2Effective PassesObjective minus optimum (training)quantum  51015202510−1.22310−1.22110−1.219quantumTest logistic lossEffective Passes  51015202929.129.229.329.429.529.629.729.8quantumTest error [%]Effective Passes  [4] L. Bottou. Online algorithms and stochastic approximations. In David Saad, editor, Online Learning and  Neural Networks. Cambridge University Press, Cambridge, UK, 1998.  [5] N. Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm.  Mach. Learn., 2(4):285–318, April 1988.  [6] F. Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the  brain. Psychological Review, 65(6):386–408, 1958.  [7] H. Robbins and S. Monro. A Stochastic Approximation Method. The Annals of Mathematical Statistics,  22(3):400–407, 1951.  [8] L. Bottou and Y. LeCun. Large scale online learning. In NIPS, 2003. [9] N. Le Roux, M. W. Schmidt, and F. Bach. A stochastic gradient method with an exponential convergence  rate for ﬁnite training sets. In NIPS, 2012.  [10] Y. Nesterov. Primal-dual subgradient methods for convex problems. Math. Program., 120(1):221–259,  2009.  [11] P. Tseng. An incremental gradient(-projection) method with momentum term and adaptive stepsize rule.  SIAM J. on Optimization, 8(2):506–531, February 1998.  [12] B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM J. Control  Optim., 30(4):838–855, July 1992.  [13] S. Shalev-Shwartz and T. Zhang. Proximal stochastic dual coordinate ascent. CoRR, abs/1211.2717,  2012.  [14] S. Shalev-Shwartz and T. Zhang. Accelerated mini-batch stochastic dual coordinate ascent. CoRR,  abs/1305.2581, 2013.  [15] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, NIPS, pages 315–323. 2013.  [16] C. Wang, X. Chen, A. Smola, and E. Xing. Variance reduction for stochastic gradient optimization. In  NIPS, pages 181–189. 2013.  [17] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss mini-  mization. CoRR, abs/1209.1873, 2012.  [18] J. Mairal. Optimization with ﬁrst-order surrogate functions. In ICML (3), pages 783–791, 2013. [19] J. Mairal. Stochastic majorization-minimization algorithms for large-scale optimization. In NIPS, pages  2283–2291. 2013.  [20] N. N. Schraudolph. Local gain adaptation in stochastic gradient descent. In ICANN, 1999. [21] N. N. Schraudolph. Fast curvature matrix-vector products for second-order gradient descent. Neural  Computation, 14:2002, 2002.  [22] A. Shaprio Y. Wardi. Convergence analysis of stochastic algorithms. Mathematics of Operations Re-  search, 21(3):615–628, 1996.  [23] Alexander Shapiro, Tito Homem de mello, and Pii S. On the rate of convergence of optimal solutions of  monte carlo approximations of stochastic programs. SIAM Journal on Optimization, 11:70–86, 2000.  [24] A. J. Kleywegt, A. Shapiro, and T. Homem-de Mello. The sample average approximation method for  stochastic discrete optimization. SIAM J. on Optimization, 12(2):479–502, February 2002.  [25] M. P. Friedlander and M. Schmidt. Hybrid deterministic-stochastic methods for data ﬁtting. SIAM J.  Scientiﬁc Computing, 34(3), 2012.  [26] R. H. Byrd, G. M. Chin, J. Nocedal, and Y. Wu. Sample size selection in optimization methods for  machine learning. Math. Program., 134(1):127–155, August 2012.  [27] A. Aravkin, M. P. Friedlander, F. Herrmann, and T. van Leeuwen. Robust inversion, dimensionality  reduction, and randomized sampling. Mathematical Programming, 134(1):101–125, 2012.  [28] N. N. Schraudolph, J. Yu, and S. G¨unter. A stochastic quasi-newton method for online convex optimiza-  tion. In AISTATS, 2007.  [29] Y. Le Cun, L. Bottou, G. B. Orr, and K.-R. M¨uller. Efﬁcient backprop. In Neural Networks, Tricks of the  Trade, Lecture Notes in Computer Science LNCS 1524. Springer Verlag, 1998.  [30] A. Bordes, L. Bottou, and P. Gallinari. Sgd-qn: Careful quasi-newton stochastic gradient descent. J.  Mach. Learn. Res., 10:1737–1754, December 2009.  [31] J. Martens. Deep learning via hessian-free optimization. In ICML, 2010. [32] R. H. Byrd, G. M. Chin, W. Neveitt, and J. Nocedal. On the use of stochastic hessian information in  optimization methods for machine learning. SIAM Journal on Optimization, 21(3):977–995, 2011.  [33] Q. V. Le, J. Ngiam, A. Coates, A. Lahiri, B. Prochnow, and A. Y. Ng. On optimization methods for deep  learning. In ICML, 2011.  [34] A. Agarwal, P. L. Bartlett, P. D. Ravikumar, and M. J. Wainwright. Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions on Information Theory, (5):3235–3249.  10  [35] D. P. Bertsekas and J. N. Tsitsiklis. Gradient convergence in gradient methods with errors. SIAM J. on  Optimization, 10(3):627–642, July 1999.  [36] James Martens and Ilya Sutskever. Training deep and recurrent networks with hessian-free optimization. In Grgoire Montavon, GeneviveB. Orr, and Klaus-Robert Mller, editors, Neural Networks: Tricks of the Trade, volume 7700 of Lecture Notes in Computer Science, pages 479–535. Springer Berlin Heidelberg, 2012.  [37] Curtis R. Vogel. Computational Methods for Inverse Problems. Society for Industrial and Applied Math-  ematics, 2002.  [38] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to  stochastic programming. SIAM J. on Optimization, 19(4):1574–1609, January 2009.  11  ","Partition functions arise in a variety of settings, including conditionalrandom fields, logistic regression, and latent gaussian models. In this paper,we consider semistochastic quadratic bound (SQB) methods for maximum likelihoodinference based on partition function optimization. Batch methods based on thequadratic bound were recently proposed for this class of problems, andperformed favorably in comparison to state-of-the-art techniques.Semistochastic methods fall in between batch algorithms, which use all thedata, and stochastic gradient type methods, which use small random selectionsat each iteration. We build semistochastic quadratic bound-based methods, andprove both global convergence (to a stationary point) under very weakassumptions, and linear convergence rate under stronger assumptions on theobjective. To make the proposed methods faster and more stable, we considerinexact subproblem minimization and batch-size selection schemes. The efficacyof SQB methods is demonstrated via comparison with several state-of-the-arttechniques on commonly used datasets."
1312.6190,2014,Adaptive Feature Ranking for Unsupervised Transfer Learning  ,"['Son N. Tran', ""Artur d'Avila Garcez""]",https://arxiv.org/pdf/1312.6190.pdf,"4 1 0 2     y a M 8 2         ]  G L . s c [      2 v 0 9 1 6  .  2 1 3 1 : v i X r a  Adaptive Feature Ranking for Unsupervised Transfer  Learning  Son N. Tran  Department of Computer Science  City University London London, UK, EC1V 0HB  Son.Tran.1@city.ac.uk  Artur d’Avila Garcez  Department of Computer Science  City University London London, UK, EC1V 0HB aag@soi.city.ac.uk  Abstract  Transfer Learning is concerned with the application of knowledge gained from solving a problem to a different but related problem domain. In this paper, we propose a method and efﬁcient algorithm for ranking and selecting representa- tions from a Restricted Boltzmann Machine trained on a source domain to be transferred onto a target domain. Experiments carried out using the MNIST, IC- DAR and TiCC image datasets show that the proposed adaptive feature ranking and transfer learning method offers statistically signiﬁcant improvements on the training of RBMs. Our method is general in that the knowledge chosen by the ranking function does not depend on its relation to any speciﬁc target domain, and it works with unsupervised learning and knowledge-based transfer. Keywords: Feature Selection, Transfer Learning, Restricted Boltzmann Machines  1  Introduction  Transfer Learning is concerned with the application of knowledge gained from solving a problem to a different but related problem domain. A number of researchers in Machine Learning have argued that the provision of supplementary knowledge should help improve learning performance [15, 2, 12, 1, 11, 14, 9, 3]. In Transfer Learning [10, 1, 11, 14], knowledge from a source domain can be used to improve performance in a target domain by assuming that related domains have some knowledge in common. In connectionist transfer learning, many approaches transfer the knowledge selected speciﬁcally based on the target, and (in some cases) with the provision of labels in the source domain [8, 16]. In constrast, we are interested in selecting the representations that can be transferred in general to the target without the provision of labels in the source domain, which is similar to self-taught learning [11, 6]. In addition, we propose to study how much knowledge should be transferred to the target domain, which has not been studied yet in self-taught mode. This paper introduces a method and efﬁcient algorithm for ranking and selecting representation knowledge from a Restricted Boltzmann Machine (RBM) [13, 4] trained on a source domain to be transferred onto a target domain. A ranking function is deﬁned that is shown to minimize infor- mation loss in the source RBM. High-ranking features are then transferred onto a target RBM by setting some of the network’s parameters. The target RBM is then trained to adapt a set of additional parameters on a target dataset, while keeping the parameters set by transfer learning ﬁxed. Experiments carried out using the MNIST, ICDAR and TiCC image datasets show that the proposed adaptive feature ranking and transfer learning method offers statistically signiﬁcant improvements on the training of RBMs in three out of four cases tested, where the MNIST dataset is used as source and either the ICDAR or TiCC dataset is used as target. As expected, our method improves on the predictive accuracy of the standard self-taught learning method for unsupervised transfer learning [11].  1  Our method is general in that the knowledge chosen by the ranking function does not depend on its relation to any speciﬁc target domain. In transfer learning, it is normal for the choice of the knowledge to be transferred from the source domain to rely on the nature of the target domain. For example, in [7], knowledge such as data samples in the source domain are transformed to a target domain to enrich supervised learning. In our approach, the transfer learning is unsupervised, as done in [11]. We are concerned with inductive transfer learning in that representation learned from a domain can be useful in analogous domains. For example, in [1] a common orthonormal matrix is learned to construct linear features among multiple tasks. Self-taught learning [11], on the other hand, applies sparse coding to transfer the representations learned from source data onto the target domain. Like self-taught, we are interested in unsupervised transfer learning using cross-domain features. The system has been implemented in MATLAB; all learning parameters and data folds are available upon request. The outline of the paper is as follows. In Section 2, we deﬁne feature selection by ranking in which high scoring features are associated with signiﬁcant part of the network. In Section 3, we introduce the adaptive feature learning method and algorithm to combine selective features in source domain with features in target domain. In Section 4, we present and discuss the experimental results. Section 5 concludes and discusses directions for future work.  2 Feature Selection by Ranking  In this section, we present a method for selecting representations from an RBM by ranking their features. We deﬁne a ranking function and show that it can capture the most signiﬁcant information in the RBM, according to a measure of information loss. In a trained RBM, we deﬁne cj to be a score for each unit j in the hidden layer. The score represents the weight-strength of a sub-network Nj (hj ∼ V ) consisting of all the connections from unit j to the visible layer. A score cj is a positive real number that can be seen as capturing the uncertainty in the sub-network. We expect to be able to replace all the weights of a network by appropriate cj or −cj with minimum information loss. We deﬁne the information loss as:  Iloss =  (cid:107)wj − cjsj(cid:107)2  (1)  (cid:88)  j  where wj is a vector of connection weights from unit j in the hidden layer to all the units in the visible layer, sj is a vector of the same size visN as wj, and sj ∈ [−1 Since equation (1) is a quadratic function, the value of cj that minimizes information loss can be found by setting the derivatives to zero, as follows:  1]visN .  (cid:88) (cid:88)  i  i  cj =  2(wij − cjsij)sij = 0 with all j wijsij − cj  ij = 0 with all j s2  (cid:88)  (cid:80) i wijsij(cid:80)  i s2 ij  i  Since sij = [−1  1] and from equation (1), we can see that:  (cid:107)wij − cjsij(cid:107)2 = (cid:107)abs(wij) − cj  sij  sign(wij)  (cid:107)2 ≥ (cid:107)abs(wij) − cj(cid:107)2  holds if and only if sij = sign(wij), which will also minimize Iloss. Applying equation (3) to (2), we obtain:  cj =  i abs(wij) visN  (cid:80)  2  (2)  (3)  (4)  We may notice that using cj instead of weights would result in a compression of the network; however, in this paper we focus on the scores only for selecting features to use for transfer learning. In what follows, we give a practical example which shows that low-score features are semantically less meaningful than high-scoring ones.  Example 2.1 We have trained an RBM with 10 hidden nodes to model the XOR function from its truth-table such that z = x XOR y. The true, f alse logical values are represented by integers 1, 0, respectively. After training the network, a score for each sub-network can be obtained. The score allows us to replace each real-value weight by its sign, and interpret those signs logically where a negative sign (−) represents logical negation (¬), as exempliﬁed in Table 1.  Network  Sub-network  Symbolic representation  2.970 : h2 ∼ {+x,−y, +z}  can be interpreted as  z = x ∧ ¬y  with score 2.970  Table 1: RBM trained on XOR function and one of its sub-networks with score value and logical interpretation  Table 2 shows all the sub-networks with associated scores. As one may recognize, each sub-network represents a logical rule learned from the RBM. However, not all of the rules are correct w.r.t. the XOR function. In particular, the sub-network scoring 0.158 encodes a rule ¬z = ¬x ∧ y, which is inconsistent with z = x XOR y. By ranking the sub-networks according to their scores, this can be identiﬁed: high-scoring sub-networks are consistent with the data, and low-scoring ones are not. We have repeated the training several times with different numbers of hidden units, obtaining similar intuitive results.  Score 1.340 2.970 6.165 0.158 2.481 1.677 2.544 7.355 6.540 4.868  Sub-network h1 ∼ {+x,−y, +z} h2 ∼ {+x,−y, +z} h3 ∼ {−x, +y, +z} h4 ∼ {−x, y,−z} h5 ∼ {+x,−y, +z} h6 ∼ {−x, +y, +z} h7 ∼ {+x,−y, +z} h8 ∼ {+x, +y,−z} h9 ∼ {−x,−y,−z} h10 ∼ {−x,−y,−z  Logical Representation  z = x ∧ ¬y z = x ∧ ¬y z = ¬x ∧ y ¬z = ¬x ∧ y z = x ∧ ¬y z = ¬x ∧ y z = x ∧ ¬y ¬z = x ∧ y ¬z = ¬x ∧ ¬y ¬z = ¬x ∧ ¬y  Table 2: Sub-networks and scores from RBM with 10 hidden units trained on XOR truth-table  In what follows, we will study the effect of pruning low-scoring sub-networks from an RBM trained on complex image domain data. So far, we have seen that our scoring can be useful at identifying relevant representations. In particular, if the score of a sub-network is small in relation to the average score of the network, they seem more likely to represent noise.  3 Adaptive Feature Learning  In this section, we introduce adaptive feature learning. Given an RBM trained on a dataset, we are interested in investigating whether the score ranking introduced in Section 2 can be useful at  3  improving the predictive accuracy of another RBM trained on a target domain. From a transfer learning perspective, we intend to produce a general transfer learning algorithm, ranking knowledge from a source domain to guide the learning on a target domain, using RBMs as transfer medium. In particular, we propose a transfer learning model as shown in Figure 1. Knowledge learned from a source domain is selected by the ranking function for transferring onto a target domain, as explained in what follows. The selection of features in the source domain is general in that it is independent from the data from the target domain.  Figure 1: General adaptive feature transfer mechanism for unsupervised learning  In the target domain, an RBM is trained given a number of transferred parameters (W (t)): a ﬁxed set of weights associated with high-ranking sub-networks from the source domain. The output of transferred hidden nodes in target domain is considered as self-taught features [11]. The connections between the visible layer and the hidden units transferred onto the target RBM can be seen as a set of up-weights and down-weights. How much the down-weights affect the learning in the target RBM depends on an inﬂuence factor θ; in this paper θ ∈ [0 1]. If θ = 0 then the features are combined but the transferred knowledge will not inﬂuence learning in the target domain. In this case, the result is a combination of self-taught features and target RBM features. Otherwise, if θ = 1 then the transferred knowledge will inﬂuence learning in the target domain. We refer to this case as adaptive feature learning, because the knowledge transferred from the source domain is used to guide the learning of new features in the target domain. The outputs of additional hidden nodes (associated with parameter U) in target RBM are called adaptive features. As usual, we train the target RBM to maximize the log-likelihood:  L(U|D; W (t))  (5)  using Contrastive Divergence [4].  Algorithm 1 Adaptive feature learning Require: A trained RBM N 1: Select a number of sub-networks N (t) ∈ N with the highest scores 2: Encode parameters W (t) from N (t) into a new RBM 3: Add hidden units (U) to the RBM 4: loop 5: % until convergence 6: 7: H+ ← p(H|V+); V− ← p(V | ˆH+); 8: 9: H− ← p(H| ˆV−) Y = U + η((cid:104)V T 10: 11: end loop  smp← p(H|V+); ˆH+ ˆV− smp← p(V | ˆH+)  + − ˆV T− H (t)− (cid:105))  V+ ⇐ X  + H (t)  4 Experimental Results  We start by evaluating the approach on the MNIST handwritten dataset. First, we want to check if hidden units with low scores are indeed less signiﬁcant in the case of image domains. Subsequently,  4  we show that knowledge from one image domain can be used to improve predictive accuracy in another image domain.  4.1 Feature Selection  We have trained an RBM with 500 hidden nodes on 20,000 samples from the MNIST dataset in order to visualized the ﬁlter bases of the 50 highest scoring sub-networks and the 50 lowest scoring sub- networks (each takes 10% of the network’s capacity). Figure 2 shows the result of using a standard RBM, and Figure 3 shows the result of using a sparse RBM [6]. As can be seen, in Figure 2, high scores are mostly associated with more concrete visualizations of the expected MNIST patterns, while low scores are mostly associated with fading or noisy patterns. In Figure 3, high-scores are associated with sparse representations, while low-scores produce less meaningful representations according to the way in which the RBM was trained. In sparse RBM we use PCA to reduce the dimensionality of the images to 69 and train the network with Gaussian visible units.  (a) Filter bases with high scores  (b) Filter bases with low scores  Figure 2: Features learned from RBM on MNIST dataset  (a) Filter bases with high scores  (b) Filter bases with low scores  Figure 3: Learned features from sparse RBM on MNIST dataset  We also examined visually the impact of the scores on an RBM with 500 hidden units trained on the MNIST dataset’s 10,000 examples. Sub-networks with the highest scores were gradually removed, and the pruned RBM was compared on the reconstruction of images with that of the original RBM, as illustrated in Figure 4. In Figure 5 we shows the reconstruction of test images from RBMs in which low-scored features were gradually removed. Finally, in order to obtain accuracy measures, we have provided the features obtained from the pruned RBMs as an input to an SVM classiﬁer. Figure 6 shows the drop in accuracy with the gradual pruning of the RBM. In case of pruning low-scored features, at ﬁrst, some removal of  5  Figure 4: Reconstructed test images from RBM in which high-scored features have been pruned. From left to right, number of hidden unit remain: 500 (full), 400 ,300, 200 and 100  Figure 5: Reconstructed test images from RBM in which low-scored features have been pruned. From left to right, number of hidden unit remain: 500 (full), 400 ,300, 200 and 100  units have produced a slight increase in accuracy. Then, the results indicate that it is possible to remove nodes and maintain performance almost unchanged (until relevant units start to be removed, at which point accuracy deteriorates, when more than half of the number of units is removed). In case of pruning high-scored features, the accuracy decreases signiﬁcantly when more than 10% of hidden units are removed.  Figure 6: Classiﬁcation accuracy of a pruned RBM, starting with 500 hidden units, on 10,000 MNIST test samples. The red line presents the pruning performance of low-scored features and the blue line presents the pruning performance of high-score features  4.2 Unsupervised Transfer Learning  In order to evaluate transfer learning in the context of images, we have transferred sub-networks from an RBM trained on the MNIST1 dataset to RBMs trained on the ICDAR2 and TiCC datasets3. Below, we use MNIST30k and MNIST05K to denote datasets with 30,000 and 5,000 samples from the MNIST data collection. For the TiCC collection, we denote TiCCd and TiCCa as the datasets of digits and letters, respectively. TiCCc and TiCCw are character samples from two different groups of writers. TiCCw A and TiCCw B are from the same group but the latter has a much smaller training set. Each column in Table 3 indicates a transfer experiment, e.g. MNIST30k : ICDARd uses the  1http://yann.lecun.com/exdb/mnist/ 2http://algoval.essex.ac.uk:8080/icdar2005/index.jsp?page=ocr.html 3http://homepage.tudelft.nl/19j49/Datasets.html  6  MNIST dataset as source domain and ICDAR as target domain. The percentages show the predictive accuracy on the target domain, as detailed in the sequel.  MNIST30k : ICDARd MNIST30k : TiCCw A MNIST05k : TiCCa MNIST05k : TiCCd  SVM PCA STL SC STL RBM STL RBM ASTL (α = 0) ASTL (α = 1)  39.04 39.38 46.23  30.47 ± 0.054 37.63± 0.505 36.66± 0.495 40.43 ± 0.328  73.44 68.36 70.06  72.88 ± 0.098 75.20 ± 0.745 76.49 ± 0.361 77.56 ± 0.564  59.16 57.90 55.82  58.13 ± 0.205 62.85 ± 0.079 63.21 ± 0.134 63.00 ± 0.160  60.34 56.29 57.78  62.08 ± 0.321 63.42 ± 0.090 65.04 ± 0.330 65.82 ± 0.262  Table 3: Transfer learning experimental results: each column indicates a transfer experiment, e.g. MNIST30k : ICDARd uses the MNIST dataset as source domain and ICDAR as target domain. The percentages show the predictive accuracy on the target domain. Results for SVMs are provided as a baseline. For the ”SVM” and ”RBM” lines, there is no transfer; for the other lines, transfer is carried out as described in Section ?? and Section ??. The percentages show the average results with 95% conﬁdence interval  In Table 3, we have measured classiﬁcation performance when using the knowledge from the source domain to guide the learning in the target domain. For each domain, the data is divided into training, validation and test sets. Each experiment is repeated 50 times and we use the validation set to se- lect the best model (number of transferred sub-networks, number of added units, learning rates and SVM hyper-parameters). The table reports the average results on the test sets. The results show that the proposed adaptive feature ranking and transfer learning method offers statistically signiﬁcant improvements on the training of RBMs in three out of four cases tested, where the MNIST dataset is used as source and either the ICDAR or TiCC dataset is used as target. As expected, our method improves on the predictive accuracy of the standard self-taught learning method for unsupervised transfer learning. In order to compare with transferring low-scored features, we performed exper- iments follows what we have done with transferring high-scored features, except that in this case the features are ranked from low-scores to high-scores. We observed that the highest accuracies were achieved only when a large number of high-scored features are among those which have been transferred.  SVM RBM STL RBM ASTL (α = 0) ASTL (α = 1)  59.16  TiCCd : TiCCa 60.65 ± 0.075 62.85 ± 0.079 62.41 ± 0.166 63.16 ± 0.120  60.34  TICCa : TiCCd 64.85 ± 0.227 63.42 ± 0.090 66.10 ± 0.137 66.25 ± 0.175  40.67  TICCc : TICCw B 47.46 ± 0.260 44.28 ± 0.323 51.07 ± 0.684 43.10 ± 0.332  Table 4: Transfer learning experimental results for datasets in TiCC collection. The percentages show the average predictive accuracy on the target domain with 95% conﬁdence interval  We also carried out experiments on the TiCC collection transferring from characters to digits, digits to characters, and from group of writers to group of other writers. The results in Table 4 show that in two out of three experiments adaptive learning and combining features did not gain advantages over combination of selective features from source domain and features from target domain. It may suggest that using selective combination of features would be better and more efﬁcient in a case that the source and target domains are considerably similar(i.e TiCCc:TICCw B) To compare our transfer learning approach with self-taught learning [11], we train an RBM in the source domain and use it to extract common features in the target domain for classiﬁcation. In the experiments where the domains have a close relation such as the same type of data (digits in MNIST05k:TiCCd) or in the same collections (TiCCd:TiCCa,TiCCd:TiCCd,TiCCc:TiCCwB), seﬂ- taught learning works very well especially when the training dataset in the target domain is small (TiCCc:TiCCwB). We also use sparse-coder [5] provided by Lee4 for self-taught learning as in [11], except that instead of using PCA for preprocessing data we apply the model directly to the raw  4http://ai.stanford.edu/˜hllee/softwares/nips06-sparsecoding.htm  7  (a) MNIST to ICDAR digits  (b) MNIST to TiCC letters  (c) MNIST to TiCC digits  (d) MNIST to TiCC writers  Figure 7: Performance of self-taught learning using RBM and sparse-coder regarding to number of bases/hidden units  pixels since that is what has been done with the RBM. Figure 7 shows the performance of self-taught learning on the datasets using RBMs and sparse-coder as feature learners.  (a) MNIST to ICDAR  (b) MNIST to TiCC digits  (c) MNIST to TiCC letters  (d) MNIST to TiCC writers  (e) TiCC letters to TiCC digits  (f) TiCC digits to TiCC letters  Figure 8: Performance of learning with guidance for different numbers of transferred knowledge rules and additional hidden units. The colour-bars map accuracy to the colour of the cells as shown so that the hotter the color, the higher the accuracy. With transfer, it is generally accepted that the performance of the model in a target domain will depend on the quality of the knowledge it received and the structure of the model. We then evaluated performance of the model using different sizes of transferred knowledge and number of units added to the hidden layer. Figure 8 shows that if the size of transferred knowledge is too small, it will be dominated by the data from the target domain. However, if the size of transferred knowledge is too large it can cause a drop in performance since the model will try to learn new knowledge mainly based on the transferred knowledge with little knowledge from the target domain.  8  5 Conclusion and Future Work  We have presented a method and efﬁcient algorithm for ranking and selecting representations from a Restricted Boltzmann Machine trained on a source domain to be transferred onto a target domain. The method is general in that the knowledge chosen by the ranking function does not depend on its relation to any speciﬁc target domain, and it works with unsupervised learning and knowledge- based transfer. Experiments carried out using the MNIST, ICDAR and TiCC image datasets show that the proposed adaptive feature ranking and transfer learning method offers statistically signiﬁcant improvements on the training of RBMs. In this paper we focus on selecting features from shallow network (RBM) for general transfer learn- ing. In future work we are interested in learning and transferring high-level features from deep networks selectively for a speciﬁc domain.  References  [1] Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Multi-task feature learning.  In Advances in Neural Information Processing Systems 19. MIT Press, 2007.  [2] Artur S. Avila Garcez and Gerson Zaverucha. The connectionist inductive learning and logic  programming system. Applied Intelligence, 11(1):5977, July 1999.  [3] Jesse Davis and Pedro Domingos. Deep transfer via second-order markov logic.  In Pro- ceedings of the 26th Annual International Conference on Machine Learning, ICML ’09, page 217224, New York, NY, USA, 2009. ACM.  [4] Geoffrey E. Hinton. Training products of experts by minimizing contrastive divergence. Neural  Comput., 14(8):1771–1800, August 2002.  [5] Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Y. Ng. Efﬁcient sparse coding algo- rithms. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 801–808. MIT Press, Cambridge, MA, 2007.  [6] Honglak Lee, Chaitanya Ekanadham, and Andrew Y. Ng. Sparse deep belief net model for  visual area v2. In Advances in Neural Information Processing Systems. MIT Press, 2008.  [7] Joseph J. Lim, Ruslan Salakhutdinov, and Antonio Torralba. Transfer learning by borrowing  examples for multiclass object detection. In NIPS, pages 118–126, 2011.  [8] Gr´egoire Mesnil, Yann Dauphin, Xavier Glorot, Salah Rifai, Yoshua Bengio, Ian J. Goodfel- low, Erick Lavoie, Xavier Muller, Guillaume Desjardins, David Warde-Farley, Pascal Vincent, Aaron C. Courville, and James Bergstra. Unsupervised and transfer learning challenge: a deep learning approach. In ICML Unsupervised and Transfer Learning, pages 97–110, 2012.  [9] Lilyana Mihalkova, Tuyen Huynh, and Raymond J. Mooney. Mapping and revising markov logic networks for transfer learning. In Proceedings of the 22nd national conference on Artiﬁ- cial intelligence - Volume 1, AAAI’07, page 608614. AAAI Press, 2007.  [10] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on Knowl-  edge and Data Engineering, 22(10):1345–1359, October 2010.  [11] Rajat Raina, Alexis Battle, Honglak Lee, Benjamin Packer, and Andrew Y. Ng. Self-taught learning: transfer learning from unlabeled data. In Proceedings of the 24th international con- ference on Machine learning, ICML ’07, page 759766, New York, NY, USA, 2007. ACM.  [12] Matthew Richardson and Pedro Domingos. Markov logic networks. Mach. Learn., 62(1-  2):107136, February 2006.  [13] Paul Smolensky. Information processing in dynamical systems: Foundations of harmony the- ory. In In Rumelhart, D. E. and McClelland, J. L., editors, Parallel Distributed Processing: Volume 1: Foundations, pages 194–281. MIT Press, Cambridge, 1986.  [14] Lisa Torrey, Jude W. Shavlik, Trevor Walker, and Richard Maclin. Transfer learning via advice  taking. In Advances in Machine Learning I, pages 147–170. Springer, 2010.  [15] Geoffrey G. Towell and Jude W. Shavlik. Knowledge-based artiﬁcial neural networks. Artiﬁ-  cial Intelligence, 70(1-2):119–165, 1994.  9  [16] Bin Wei and Christopher Pal. Heterogeneous transfer learning with rbms. In Wolfram Burgard  and Dan Roth, editors, AAAI. AAAI Press, 2011.  10  ","Transfer Learning is concerned with the application of knowledge gained fromsolving a problem to a different but related problem domain. In this paper, wepropose a method and efficient algorithm for ranking and selectingrepresentations from a Restricted Boltzmann Machine trained on a source domainto be transferred onto a target domain. Experiments carried out using theMNIST, ICDAR and TiCC image datasets show that the proposed adaptive featureranking and transfer learning method offers statistically significantimprovements on the training of RBMs. Our method is general in that theknowledge chosen by the ranking function does not depend on its relation to anyspecific target domain, and it works with unsupervised learning andknowledge-based transfer."
1312.6110,2014,Learning generative models with visual attention  ,"['Charlie Tang', 'Nitish Srivastava', 'Ruslan Salakhutdinov']",https://arxiv.org/pdf/1312.6110.pdf,"5 1 0 2     b e F 1 2         ]  V C . s c [      3 v 0 1 1 6  .  2 1 3 1 : v i X r a  Learning Generative Models with Visual Attention  Yichuan Tang, Nitish Srivastava, Ruslan Salakhutdinov  Department of Computer Science  University of Toronto  Toronto, Ontario, Canada  {tang,nitish,rsalakhu}@cs.toronto.edu  Abstract  Attention has long been proposed by psychologists to be important for efﬁciently dealing with the massive amounts of sensory stimulus in the neocortex. Inspired by the attention models in visual neuroscience and the need for object-centered data for generative models, we propose a deep-learning based generative frame- work using attention. The attentional mechanism propagates signals from the region of interest in a scene to an aligned canonical representation for genera- tive modeling. By ignoring scene background clutter, the generative model can concentrate its resources on the object of interest. A convolutional neural net is employed to provide good initializations during posterior inference which uses Hamiltonian Monte Carlo. Upon learning images of faces, our model can robustly attend to the face region of novel test subjects. More importantly, our model can learn generative models of new faces from a novel dataset of large images where the face locations are not known.1  Introduction  1 Building rich generative models that are capable of extracting useful, high-level latent represen- tations from high-dimensional sensory input lies at the core of solving many AI-related tasks, in- cluding object recognition, speech perception and language understanding. These models capture underlying structure in data by deﬁning ﬂexible probability distributions over high-dimensional data as part of a complex, partially observed system. Some of the successful generative models that are able to discover meaningful high-level latent representations include the Boltzmann Machine family of models: Restricted Boltzmann Machines, Deep Belief Nets [1], and Deep Boltzmann Ma- chines [2]. Mixture models, such as Mixtures of Factor Analyzers [3] and Mixtures of Gaussians, have also been used for modeling natural image patches [4]. More recently, denoising auto-encoders have been proposed as a way to model the transition operator that has the same invariant distribution as the data generating distribution [5]. Generative models have an advantage over discriminative models when part of the images are oc- cluded or missing. Occlusions are very common in realistic settings and have been largely ignored in recent literature on deep learning. In addition, prior knowledge can be easily incorporated in generative models in the forms of structured latent variables, such as lighting and deformable parts. However, the enormous amount of content in high-resolution images makes generative learning dif- ﬁcult [6, 7]. Therefore, generative models have found most success in learning to model small patches of natural images and objects: Zoran and Weiss [4] learned a mixture of Gaussians model over 8×8 image patches; Salakhutdinov and Hinton [2] used 64×64 centered and uncluttered stereo images of toy objects on a clear background; Tang et al. [8] used 24×24 images of centered and cropped faces. The fact that these models require curated training data limits their applicability on using the (virtually) unlimited unlabeled data. In this paper, we propose a framework to infer the region of interest in a big image for genera- tive modeling. This will allow us to learn a generative model of faces on a very large dataset of  1In the proceedings of Neural Information Processing Systems, 2014, Montréal, Québec, Canada.  1  (unlabeled) images containing faces. Our framework is able to dynamically route the relevant infor- mation to the generative model and can ignore the background clutter. The need to dynamically and selectively route information is also present in the biological brain. Plethora of evidence points to the presence of attention in the visual cortex [9, 10]. Recently, in visual neuroscience, attention has been shown to exist not only in extrastriate areas, but also all the way down to V1 [11]. Attention as a form of routing was originally proposed by Anderson and Van Essen [12] and then extended by Olshausen et al. [13]. Dynamic routing has been hypothesized as providing a way for achieving shift and size invariance in the visual cortex [14, 15]. Tsotsos et al. [16] proposed a model combining search and attention called the Selective Tuning model. Larochelle and Hinton [17] pro- posed a way of using third-order Boltzmann Machines to combine information gathered from many foveal glimpses. Their model chooses where to look next to ﬁnd locations that are most informative of the object class. Reichert et al. [18] proposed a hierarchical model to show that certain aspects of covert object-based attention can be modeled by Deep Boltzmann Machines. Several other related models attempt to learn where to look for objects [19, 20] and for video based tracking [21]. Inspired by Olshausen et al. [13], we use 2D similarity transformations to implement the scaling, rotation, and shift operation required for routing. Our main motivation is to enable the learning of generative models in big images where the location of the object of interest is unknown a-priori. 2 Gaussian Restricted Boltzmann Machines Before we describe our model, we brieﬂy review the Gaussian Restricted Boltzmann Machine (GRBM) [22], as it will serve as the building block for our attention-based model. GRBMs are a type of Markov Random Field model that has a bipartite structure with real-valued visible vari- ables v ∈ RD connected to binary stochastic hidden variables h ∈ {0, 1}H. The energy of the joint conﬁguration {v, h} of the Gaussian RBM is deﬁned as follows:  EGRBM (v, h; Θ) =  Wijvihj,  (1)  (cid:80) where Θ = {W, b, c, σ} are the model parameters. The marginal distribution over the visible vector h exp (−E(v, h; Θ)) and the corresponding conditional distributions take v is P (v; Θ) = 1Z(Θ) the following form:  ij  j  i  p(hj = 1|v) = 1/(cid:0)1 + exp(−(cid:88)  Wijvi − cj)(cid:1),  (cid:88)  1 2  (vi − bi)2  σ2 i  −(cid:88)  cjhj −(cid:88)  p(vi|h) = N (vi; µi, σ2  i ), where µi = bi + σ2 i  i  (cid:88)  j  Wijhj.  (2)  (3)  Observe that conditioned on the states of the hidden variables (Eq. 3), each visible unit is modeled by a Gaussian distribution, whose mean is shifted by the weighted combination of the hidden unit activations. Unlike directed models, an RBM’s conditional distribution over hidden nodes is factorial and can be easily computed. We can also add a binary RBM on top of the learned GRBM by treating the inferred h as the “visible” layer together with a second hidden layer h2. This results in a 2-layer Gaussian Deep Belief Network (GDBN) [1] that is a more powerful model of v. Speciﬁcally, in a GDBN model, p(h1, h2) is modeled by the energy function of the 2nd-layer RBM, while p(v1|h1) is given by Eq. 3. Efﬁcient inference can be performed using the greedy approach of [1] by treating each DBN layer as a separate RBM model. GDBNs have been applied to various tasks, including image classiﬁcation, video action and speech recognition [6, 23, 24, 25]. 3 The Model Let I be a high resolution image of a scene, e.g. a 256×256 image. We want to use attention to propagate regions of interest from I up to a canonical representation. For example, in order to learn a model of faces, the canonical representation could be a 24×24 aligned and cropped frontal face image. Let v ∈ RD represent this low resolution canonical image. In this work, we focus on a Deep Belief Network2 to model v.  2Other generative models can also be used with our attention framework.  2  Figure 1: Left: The Shifter Circuit, a well-known neuroscience model for visual attention [13]; Right: The proposed model uses 2D similarity transformations from geometry and a Gaussian DBN to model canonical face images. Associative memory corresponds to the DBN, object-centered frame correspond to the visible layer and the attentional mechanism is modeled by 2D similarity transformations.  This is illustrated in the diagrams of Fig. 1. The left panel displays the model of Olshausen et.al. [13], whereas the right panel shows a graphical diagram of our proposed generative model with an atten- tional mechanism. Here, h1 and h2 represent the latent hidden variables of the DBN model, and (cid:52)x,(cid:52)y,(cid:52)θ,(cid:52)s (position, rotation, and scale) are the parameters of the 2D similarity transforma- tion. The 2D similarity transformation is used to rotate, scale, and translate the canonical image v onto the canvas that we denote by I. Let p = [x y]T be a pixel coordinate (e.g. [0, 0] or [0, 1]) of the canonical image v. Let {p} be the set of all coordinates of v. For example, if v is 24×24, then {p} ranges from [0, 0] to [23, 23]. Let the “gaze” variables u ∈ R4 ≡ [(cid:52)x,(cid:52)y,(cid:52)θ,(cid:52)s] be the parameter of the Similarity transformation. In order to simplify derivations and to make transformations be the transformation parameters, we can equivalently redeﬁne u = [a, b, (cid:52)x, (cid:52)y], linear w.r.t. where a = s sin(θ) − 1 and b = s cos(θ) (see [26] for details). We further deﬁne a function w := w(p, u) → p(cid:48) as the transformation function to warp points p to p(cid:48):  p(cid:48) (cid:44)(cid:104) x(cid:48)  (cid:105)  (cid:104) 1 + a −b  (cid:105)(cid:104) x  (cid:105)  (4) We use the notation I({p}) to denote the bilinear interpolation of I at coordinates {p} with anti- aliasing. Let x(u) be the extracted low-resolution image at warped locations p(cid:48):  1 + a  y(cid:48)  +  =  y  b  .  x(u) (cid:44) I(w({p}, u)).  (5) Intuitively, x(u) is a patch extracted from I according to the shift, rotation and scale parameters of u, as shown in Fig. 1, right panel. It is this patch of data that we seek to model generatively. Note that the dimensionality of x(u) is equal to the cardinality of {p}, where {p} denotes the set of pixel coordinates of the canonical image v. Unlike standard generative learning tasks, the data x(u) is not static but changes with the latent variables u. Given v and u, we model the top-down generative process over3 x with a Gaussian distribution having a diagonal covariance matrix σ2I:  (cid:105)  (cid:104) (cid:52)x(cid:52)y  p(x|v, u,I) ∝ exp  i  .  σ2 i  (6) The fact that we do not seek to model the rest of the regions/pixels of I is by design. By using 2D similarity transformation to mimic attention, we can discard the complex background of the scene and let the generative model focus on the object of interest. The proposed generative model takes the following form:  p(x, v, u|I) = p(x|v, u,I)p(v)p(u),  (7) where for p(u) we use a ﬂat prior that is constant for all u, and p(v) is deﬁned by a 2-layer Gaussian Deep Belief Network. The conditional p(x|v, u,I) is given by a Gaussian distribution as in Eq. 6. To simplify the inference procedure, p(x|v, u,I) and the GDBN model of v, p(v), will share the same noise parameters σi.  (cid:18)  − 1 2  (cid:88)  (cid:19)  (xi(u) − vi)2  3We will often omit dependence of x on u for clarity of presentation.  3  Olshausen et al. 93Our model2d similaritytransformationInference  4 While the generative equations in the last section are straightforward and intuitive, inference in these models is typically intractable due to the complicated energy landscape of the posterior. During inference, we wish to compute the distribution over the gaze variables u and canonical object v given the big image I. Unlike in standard RBMs and DBNs, there are no simplifying factorial assumptions about the conditional distribution of the latent variable u. Having a 2D similarity transformation is reminiscent of third-order Boltzmann machines with u performing top-down multiplicative gating of the connections between v and I. It is well known that inference in these higher-order models is rather complicated. One way to perform inference in our model is to resort to Gibbs sampling by computing the set of alternating conditional posteriors: The conditional distribution over the canonical image v takes the following form:  p(v|u, h1,I) = N(cid:16) µ + x(u)  ; σ2(cid:17)  (8) j is the top-down inﬂuence of the DBN. Note that if we know the where µi = bi + σ2 i gaze variable u and the ﬁrst layer of hidden variables h1, then v is simply deﬁned by a Gaussian distribution, where the mean is given by the average of the top-down inﬂuence and bottom-up in- formation from x. The conditional distributions over h1 and h2 given v are given by the standard DBN inference equations [1]. The conditional posterior over the gaze variables u is given by:  j Wijh1  2  ,  (cid:80)  p(u|x, v) =  p(x|u, v)p(u)  p(x|v)  ,  (cid:88)  (xi(u) − vi)2  i  1 2  σ2 i  + const.  log p(u|x, v) ∝ log p(x|u, v) + log p(u) =  (9) Using Bayes’ rule, the unnormalized log probability of p(u|x, v) is deﬁned in Eq. 9. We stress that this equation is atypical in that the random variable of interest u actually affects the conditioning variable x (see Eq. 5) We can explore the gaze variables using Hamiltonian Monte Carlo (HMC) algorithm [27, 28]. Intuitively, conditioned on the canonical object v that our model has in “mind”, HMC searches over the entire image I to ﬁnd a region x with a good match to v. If the goal is only to ﬁnd the MAP estimate of p(u|x, v), then we may want to use second-order methods for optimizing u. This would be equivalent to the Lucas-Kanade framework in computer vision, developed for image alignment [29]. However, HMC has the advantage of being a proper MCMC sampler that satisﬁes detailed balance and ﬁts nicely with our probabilistic framework. The HMC algorithm ﬁrst speciﬁes the Hamiltonian over the position variables u and auxiliary momentum variables r: H(u, r) = U (u) + K(r), where the potential function is deﬁned by i . The dy- U (u) = 1 2 namics of the system is deﬁned by:  and the kinetic energy function is given by K(r) = 1 2  (xi(u)−vi)2  (cid:80)  (cid:80)  i r2  σ2 i  i  ∂u ∂t (x(u) − v)  σ2 ∂x  ∂w({p}, u)  = r,  ,  ∂x(u) ∂u ∂w({p}, u)  ∂u  ∂H ∂u ∂x ∂u  =  =  ∂u  = − ∂H (cid:88)  ∂r ∂t  =  ∂xi  ∂w(pi, u)  ∂w(pi, u)  ∂u  i  (10)  (11)  (12)  .  Observe that Eq. 12 decomposes into sums over single coordinate positions pi = [x y]T. Let us denote p(cid:48) i = w(pi, u) to be the coordinate pi warped by u. For the ﬁrst term on the RHS of Eq. 12,  (13) i) denotes the sampling of the gradient images of I at the warped location pi. For the  where ∇I(p(cid:48) second term on the RHS of Eq. 12, we note that we can re-write Eq. 4 as:  (dimension 1 by 2 )  ∂w(pi, u)  i),  ∂xi  = ∇I(p(cid:48)  (cid:105)  (cid:104) x(cid:48)  y(cid:48)  (cid:104) x −y  y  x  =  (cid:35)  (cid:105)(cid:34) a b(cid:52)x(cid:52)y  (cid:105)  (cid:104) x  y  1 0  0 1  4  +  ,  (14)  giving us  (cid:104) x −y  y  x  (cid:105)  .  1 0  0 1  ∂w(pi, u)  ∂u  =  (15)  HMC simulates the discretized system by performing leap-frog updates of u and r using Eq. 10. Additional hyperparameters that need to be speciﬁed include the step size (cid:15), number of leap-frog steps, and the mass of the variables (see [28] for details).  (a)  (b)  4.1 Approximate Inference HMC essentially performs gradient descent with momentum, therefore it is prone to getting stuck at local optimums. This is especially a problem for our task of ﬁnding the best trans- formation parameters. While the posterior over u should be unimodal near the optimum, many local minima exist away from the global optimum. For example, in Fig. 2(a), the big image I is enclosed by the blue box, and the canonical image v is enclosed by the green box. The current setting of u aligns together the wrong eyes. However, it is hard to move the green box to the left due to the local optima created by the dark in- tensities of the eye. Resampling the momentum variable every iteration in HMC does not help signiﬁcantly because we are modeling real-valued images using a Gaussian distribution as the residual, leading to quadratic costs in the difference be- tween x(u) and v (see Eq. 9). This makes the energy barriers between modes extremely high. To alleviate this problem we need to ﬁnd good initializations of u. We use a Convolutional Network (ConvNet) to per- form efﬁcient approximate inference, resulting in good initial guesses. Speciﬁcally, given v, u and I, we predict the change in u that will lead to the maximum log p(u|x, v). In other words, instead of using the gradient ﬁeld for updating u, we learn a ConvNet to output a better vector ﬁeld in the space of u. We used a fairly standard ConvNet architecture and the standard stochastic gradient descent learning procedure. We note that standard feedforward face detectors seek to model p(u|I), while completely ignoring the canonical face v. In contrast, here we take v into account as well. The ConvNet is used to initial- ize u for the HMC algorithm. This is important in a proper generative model because conditioning on v is appealing when multiple faces are present in the scene. Fig. 2(b) is a hypothesized Euclidean space of v, where the black manifold represents canonical faces and the blue manifold represents cropped faces x(u). The blue manifold has a low intrinsic dimensionality of 4, spanned by u. At A and B, the blue comes close to black manifold. This means that there are at least two modes in the posterior over u. By conditioning on v, we can narrow the posterior to a single mode, depending on whom we want to focus our attention. We demonstrate this exact capability in Sec. 6.3. Fig. 3 demonstrates the iterative process of how approximate inference works in our model. Specif- ically, based on u, the ConvNet takes a window patch around x(u) (72×72) and v (24×24) as input, and predicts the output [(cid:52)x,(cid:52)y,(cid:52)θ,(cid:52)s]. In step 2, u is updated accordingly, followed by step 3 of alternating Gibbs updates of v and h, as discussed in Sec. 4. The process is repeated. For the details of the ConvNet see the supplementary materials. 5 Learning While inference in our framework localizes objects of interest and is akin to object detection, it is not the main objective. Our motivation is not to compete with state-of-the-art object detectors but rather propose a probabilistic generative framework capable of generative modeling of objects which are at unknown locations in big images. This is because labels are expensive to obtain and are often not available for images in an unconstrained environment. To learn generatively without labels we propose a simple Monte Carlo based Expectation- Maximization algorithm. This algorithm is an unbiased estimator of the maximum likelihood objec-  Figure 2: (a) HMC can easily get stuck at local optima. (b) Importance of modeling p(u|v,I). Best in color.  5  AverageABFigure 3: Inference process: u in step 1 is randomly initialized. The average v and the extracted x(u) form the input to a ConvNet for approximate inference, giving a new u. The new u is used to sample p(v|I, u, h). In step 3, one step of Gibbs sampling of the GDBN is performed. Step 4 repeats the approximate inference using the updated v and x(u).  Figure 4: Example of an inference step. v is 24×24, x is 72×72. Approximate inference quickly ﬁnds a good initialization for u, while HMC provides further adjustments. Intermediate inference steps on the right are subsampled from 10 actual iterations.  tive. During the E-step, we use the Gibbs sampling algorithm developed in Sec. 4 to draw samples from the posterior over the latent gaze variables u, the canonical variables v, and the hidden vari- ables h1, h2 of a Gaussian DBN model. During the M-step, we can update the weights of the Gaussian DBN by using the posterior samples as its training data. In addition, we can update the parameters of the ConvNet that performs approximate inference. Due to the fact that the ﬁrst E-step requires a good inference algorithm, we need to pretrain the ConvNet using labeled gaze data as part of a bootstrap process. Obtaining training data for this initial phase is not a problem as we can jitter/rotate/scale to create data. In Sec. 6.2, we demonstrate the ability to learn a good generative model of face images from the CMU Multi-PIE dataset. 6 Experiments We used two face datasets in our experiments. The ﬁrst dataset is a frontal face dataset, called the Caltech Faces from 1999, collected by Markus Weber. In this dataset, there are 450 faces of 27 unique individuals under different lighting conditions, expressions, and backgrounds. We downsam- pled the images from their native 896 by 692 by a factor of 2. The dataset also contains manually labeled eyes and mouth coordinates, which will serve as the gaze labels. We also used the CMU Multi-PIE dataset [30], which contains 337 subjects, captured under 15 viewpoints and 19 illumi- nation conditions in four recording sessions for a total of more than 750,000 images. We demon- strate our model’s ability to perform approximate inference, to learn without labels, and to perform identity-based attention given an image with two people. 6.1 Approximate inference We ﬁrst investigate the critical inference algorithm of p(u|v,I) on the Caltech Faces dataset. We run 4 steps of approximate inference detailed in Sec. 4.1 and diagrammed in Fig. 3, followed by three iterations of 20 leap-frog steps of HMC. Since we do not initially know the correct v, we initialize v to be the average face across all subjects. Fig. 4 shows the image of v and x during inference for a test subject. The initial gaze box is colored yellow on the left. Subsequent gaze updates progress from yellow to blue. Once ConvNet-based approximate inference gives a good initialization, starting from step 5, ﬁve iterations of 20 leap-frog steps of HMC are used to sample from the the posterior. Fig. 5 shows the quantitative results of Intersection over Union (IOU) of the ground truth face box and the inferred face box. The results show that inference is very robust to initialization and requires  6  ConvNetConvNetStep 1Step 2Step 3Step 41 Gibbs stepInference steps123456HMCVX(a)  (b)  (c)  Figure 5: (a) Accuracy as a function of gaze initialization (pixel offset). Blue curve is the percentage success of at least 50% IOU. Red curve is the average IOU. (b) Accuracy as a function of the number of approximate inference steps when initializing 50 pixels away. (c) Accuracy improvements of HMC as a function of gaze initializations.  (a) DBN trained on Caltech  (b) DBN updated with Multi-PIE  Figure 6: Left: Samples from a 2-layer DBN trained on Caltech. Right: samples from an updated DBN after training on CMU Multi-PIE without labels. Samples highlighted in green are similar to faces from CMU.  only a few steps of approximate inference to converge. HMC clearly improves model performance, resulting in an IOU increase of about 5% for localization. This is impressive given that none of the test subjects were part of the training and the background is different from backgrounds in the training set.  97%  78%  93%  97% O(c)  Our method OpenCV NCC template  IOU > 0.5 # evaluations O(whs) O(whs) O(whs) Table 1: Face localization accuracy. w: image width; h: image height; s: image scales; c: number of inference steps used.  We also compared our inference algorithm to the template matching in the task of face de- tection. We took the ﬁrst 5 subjects as test subjects and the rest as training. We can lo- calize with 97% accuracy (IOU > 0.5) us- ing our inference algorithm4. In comparison, a near state-of-the-art face detection system from OpenCV 2.4.9 obtains the same 97% ac- curacy. It uses Haar Cascades, which is a form of AdaBoost5. Normalized Cross Correlation [31] obtained 93% accuracy, while Euclidean distance template matching achieved an accuracy of only 78%. However, note that our algorithm looks at a constant number of windows while the other baselines are all based on scanning windows. 6.2 Generative learning without labels  nats  log ˆZ  627±0.5 503±1.8 499±0.1 387±0.3 687.8  617±0.4 512±1.1 96±0.8 85±0.5 454.6  569±0.6 494±1.7 594±0.5 503±0.7 694.2  No CMU training CMU w/o labels CMU w/ labels  Caltech Train Caltech Valid CMU Train CMU Valid  The main advantage of our model is that it can learn on large images of faces without lo- calization label information (no manual cropping required). To demonstrate, we use both the Caltech and the CMU faces Table 2: Variational lower-bound estimates on the log-density of the dataset. For the CMU faces, a Gaussian DBNs (higher is better). subset of 2526 frontal faces with ground truth labels are used. We split the Caltech dataset into a training and a validation set. For the CMU faces, we ﬁrst took 10% of the images as training cases for the ConvNet for approximate inference. This is needed due to the completely different backgrounds of the Caltech and CMU datasets. The remaining 90% of the CMU faces are split into a training and validation set. We ﬁrst trained a GDBN with 1024 h1 and 256 h2 hidden units on the Caltech training set. We also trained  4u is randomly initialized at ± 30 pixels, scale range from 0.5 to 1.5. 5OpenCV detection uses pretrained model from haarcascade_frontalface_default.xml, scaleFactor=1.1,  minNeighbors=3 and minSize=30.  7  0204060801000.50.60.70.80.911.1Accuracy of Approximate InferenceInitial Pixel OffsetAccuracy  Trials with IOU > 0.5Average IOU05101500.20.40.60.81Accuracy of Approximate Inference# of Inference StepsAccuracy  Trials with IOU > 0.5Average IOU020406080100−0.2−0.100.10.20.3Accuracy ImprovementsInitial Pixel OffsetAccuracy  Average IOU ImprovementsFigure 7: Left: Conditioned on different v will result in a different (cid:52)u. Note that the initial u is exactly the same for two trials. Right: Additional examples. The only difference between the top and bottom panels is the conditioned v. Best viewed in color.  Inference with ambiguity  a ConvNet for approximate inference using the Caltech training set and 10% of the CMU training images. Table 2 shows the estimates of the variational lower-bounds on the average log-density (higher is better) that the GDBN models assign to the ground-truth cropped face images from the training/test sets under different scenarios. In the left column, the model is only trained on Caltech faces. Thus it gives very low probabilities to the CMU faces. Indeed, GDBNs achieve a variational lower-bound of only 85 nats per test image. In the middle column, we use our approximate inference to estimate the location of the CMU training faces and further trained the GDBN on the newly localized faces. This gives a dramatic increase of the model performance on the CMU Validation set6, achieving a lower- bound of 387 nats per test image. The right column gives the best possible results if we can train with the CMU manual localization labels. In this case, GDBNs achieve a lower-bound of 503 nats. We used Annealed Importance Sampling (AIS) to estimate the partition function for the top-layer RBM. Details on estimating the variational lower bound are in the supplementary materials. Fig. 6(a) further shows samples drawn from the Caltech trained DBN, whereas Fig. 6(b) shows samples after training with the CMU dataset using estimated u. Observe that samples in Fig. 6(b) show a more diverse set of faces. We trained GDBNs using a greedy, layer-wise algorithm of [1]. For the top layer we use Fast Persistent Contrastive Divergence [32], which substantially improved generative performance of GDBNs (see supplementary material for more details). 6.3 Our attentional mechanism can also be useful when multiple objects/faces are present in the scene. Indeed, the posterior p(u|x, v) is conditioned on v, which means that where to attend is a func- tion of the canonical object v the model has in “mind” (see Fig. 2(b)). To explore this, we ﬁrst synthetically generate a dataset by concatenating together two faces from the Caltech dataset. We then train approximate inference ConvNet as in Sec. 4.1 and test on the held-out subjects. Indeed, as predicted, Fig. 7 shows that depending on which canonical image is conditioned, the same exact gaze initialization leads to two very different gaze shifts. Note that this phenomenon is observed across different scales and location of the initial gaze. For example, in Fig. 7, right-bottom panel, the initialized yellow box is mostly on the female’s face to the left, but because the conditioned canonical face v is that of the right male, attention is shifted to the right. 7 Conclusion In this paper we have proposed a probabilistic graphical model framework for learning generative models using attention. Experiments on face modeling have shown that ConvNet based approximate inference combined with HMC sampling is sufﬁcient to explore the complicated posterior distribu- tion. More importantly, we can generatively learn objects of interest from novel big images. Future work will include experimenting with faces as well as other objects in a large scene. Currently the ConvNet approximate inference is trained in a supervised manner, but reinforcement learning could also be used instead. Acknowledgements The authors gratefully acknowledge the support and generosity from Samsung, Google, and ONR grant N00014-14-1-0232.  6We note that we still made use of labels coming from the 10% of CMU Multi-PIE training set in order to  pretrain our ConvNet. ""w/o labels"" here means that no labels for the CMU Train/Valid images are given.  8  References [1] G. E. Hinton, S. Osindero, and Y. W. Teh. A fast learning algorithm for deep belief nets. Neural Compu-  tation, 18(7):1527–1554, 2006.  [2] R. Salakhutdinov and G. Hinton. Deep Boltzmann machines. In AISTATS, 2009. [3] Geoffrey E. Hinton, Peter Dayan, and Michael Revow. Modeling the manifolds of images of handwritten  digits. IEEE Transactions on Neural Networks, 8(1):65–74, 1997.  [4] Daniel Zoran and Yair Weiss. From learning models of natural image patches to whole image restoration.  In ICCV. IEEE, 2011.  [5] Yoshua Bengio, Li Yao, Guillaume Alain, and Pascal Vincent. Generalized denoising auto-encoders as  generative models. In Advances in Neural Information Processing Systems 26, 2013.  [6] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolutional deep belief networks for scalable unsu-  pervised learning of hierarchical representations. In ICML, pages 609–616, 2009.  [7] Marc’Aurelio Ranzato, Joshua Susskind, Volodymyr Mnih, and Geoffrey Hinton. On Deep Generative  Models with Applications to Recognition. In CVPR, 2011.  [8] Yichuan Tang, Ruslan Salakhutdinov, and Geoffrey E. Hinton. Deep mixtures of factor analysers.  In  [9] M. I. Posner and C. D. Gilbert. Attention and primary visual cortex. Proc. of the National Academy of  ICML. icml.cc / Omnipress, 2012.  Sciences, 96(6), March 1999.  [10] E. A. Buffalo, P. Fries, R. Landman, H. Liang, and R. Desimone. A backward progression of attentional  effects in the ventral stream. PNAS, 107(1):361–365, Jan. 2010.  [11] N Kanwisher and E Wojciulik. Visual attention: Insights from brain imaging. Nature Reviews Neuro-  science, 1:91–100, 2000.  [12] C. H. Anderson and D. C. Van Essen. Shifter circuits: A computational strategy for dynamic aspects of  visual processing. National Academy of Sciences, 84:6297–6301, 1987.  [13] B. A. Olshausen, C. H. Anderson, and D. C. Van Essen. A neurobiological model of visual attention and invariant pattern recognition based on dynamic routing of information. The Journal of neuroscience : the ofﬁcial journal of the Society for Neuroscience, 13(11):4700–4719, 1993.  [14] Laurenz Wiskott. How does our visual system achieve shift and size invariance?, 2004. [15] S. Chikkerur, T. Serre, C. Tan, and T. Poggio. What and where: a Bayesian inference theory of attention.  Vision Research, 50(22):2233–2247, October 2010.  [16] J. K. Tsotsos, S. M. Culhane, W. Y. K. Wai, Y. H. Lai, N. Davis, and F. Nuﬂo. Modeling visual-attention  via selective tuning. Artiﬁcial Intelligence, 78(1-2):507–545, October 1995.  [17] Hugo Larochelle and Geoffrey E. Hinton. Learning to combine foveal glimpses with a third-order boltz-  mann machine. In NIPS, pages 1243–1251. Curran Associates, Inc., 2010.  [18] D. P. Reichert, P. Seriès, and A. J. Storkey. A hierarchical generative model of recurrent object-based  attention in the visual cortex. In ICANN (1), volume 6791, pages 18–25. Springer, 2011.  [19] B. Alexe, N. Heess, Y. W. Teh, and V. Ferrari. Searching for objects driven by context. In NIPS 2012,  December 2012.  [20] Marc’Aurelio Ranzato. On learning where to look. arXiv, arXiv:1405.5488, 2014. [21] M. Denil, L. Bazzani, H. Larochelle, and N. de Freitas. Learning where to attend with deep architectures  for image tracking. Neural Computation, 28:2151–2184, 2012.  [22] G. E. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science,  [23] A. Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, University of  313:504–507, 2006.  Toronto, Toronto, Ontario, Canada, 2009.  temporal features. In ECCV 2010. Springer, 2010.  on Audio, Speech, and Language Processing, 2011.  [24] Graham W. Taylor, Rob Fergus, Yann LeCun, and Christoph Bregler. Convolutional learning of spatio-  [25] A. Mohamed, G. Dahl, and G. Hinton. Acoustic modeling using deep belief networks. IEEE Transactions  [26] Richard Szeliski. Computer Vision - Algorithms and Applications. Texts in Computer Science. Springer,  [27] S. Duane, A. D. Kennedy, B. J Pendleton, and D. Roweth. Hybrid Monte Carlo. Physics Letters B,  2011.  195(2):216–222, 1987.  [28] R. M. Neal. MCMC using Hamiltonian dynamics. in Handbook of Markov Chain Monte Carlo (eds S.  Brooks, A. Gelman, G. Jones, XL Meng). Chapman and Hall/CRC Press, 2010.  [29] Simon Baker and Iain Matthews. Lucas-kanade 20 years on: A unifying framework. International Journal  [30] Ralph Gross, Iain Matthews, Jeffrey F. Cohn, Takeo Kanade, and Simon Baker. Multi-pie. Image Vision  of Computer Vision, 56:221–255, 2002.  Comput., 28(5):807–813, 2010.  [31] J. P. Lewis. Fast normalized cross-correlation, 1995. [32] T. Tieleman and G. E. Hinton. Using fast weights to improve persistent contrastive divergence. In Pro- ceedings of the 26th Annual International Conference on Machine Learning, ICML 2009, volume 382, page 130. ACM, 2009.  9  Figure 8: A visual diagram of the convolutional net used for approximate inference.  APPENDIX  Convolutional Neural Network  The training of ConvNet for approximate inference is standard and did not involve any special ’tricks’. We used SGD with minibatch size of 128 samples. We used a standard ConvNet architecture with convolution C layers followed by max-pooling S layers. The ConvNet takes as input x and v to predict change in u such that to maximize log p(u|x, v). In order to better predict change of u, x as well as a bigger border around x are used as the input to the ConvNet. Therefore, x has resolution 72×72 and v has resolution of 24×24. Two handle two different inputs with different resolutions, two different “streams"" are used in this ConvNet architecture. One stream will process x and another one for v. These two streams will be combined multi- plicatively after subsampling the x stream by a factor of 3. The rest of the ConvNet is same as the standard classiﬁcation ConvNets, except that we use mean squared error as our cost function. See Figure 8 for a visual diagram of what the convolutional neural network architecture used.  layer 0 1 2 3 4 5 6 7  latent variables type maps:3 72x72 input x maps:3 24x24 input v maps:16 66x66 Conv of layer 0 maps:16 22x22 Pooling Conv of layer 1 maps:16 22x22 Combine layers 3,4 maps:16 22x22 Fully connected Fully connected  1024 4  ﬁlter size  # weights  - - 7x7 3x3 5x5 - - -  - -  -  -  2352  1200  7.9M 4K  Table 3: Model architectures of the convolutional neural network used during approximate inference.  Table 3 details the exact model architecture used. In layer 5, the two streams have the same number of hidden maps and hidden topography. We combine these two multiplicatively by multiplying their activation element- wise. This creates a third-order ﬂavor and is more powerful for the task of determining where to shift attention to next.  10  72x7224x2466x66convpool22x22fully connectedconv22x221024multiplyGaussian Deep Belief Network  The training of the Gaussian Deep Belief Network is performed in a standard greedy layerwise fashion. The ﬁrst layer Gaussian Restricted Boltzmann Machine is trained with the Contrastive Divergence algorithm where the standard deviation of each visible unit is learned as well. After training the ﬁrst layer, we use Eq. 3 to obtain ﬁrst hidden layer binary probabilities. We then train a 2nd binary-binary Restricted Boltzmann Machine using the fast persistent contrastive divergence learning algorithm. This greedy training leads us to the Gaussian Deep Belief Network. No ﬁnetuning of the entire network is performed.  Quantitative evaluation for Gaussian Deep Belief Network  For quantitative evaluation, we approximate the standard variational lower bound on the log likelihood of the Gaussian Deep Belief Network. The model is a directed model:  p(v, h1, h2) = p(v|h1)p(h1, h2)  (16)  For any approximating posterior distribution q(h1|v), the GDBN’s log-likelihood has this lower variational bound:  q(h1|v)[log p(v|h1) + log p  ∗  (h1)] − log Z + H(q(h1|v))  log  (17)  The entropy H(q(h1|v)) can be calculated since we made the factorial assumption on q(h1|v).  (cid:88)  h1  p(v, h1) ≥(cid:88) log p(v|h1) = −(cid:88)  h1  D(cid:88)  (x − µ)2  ∗  log p  (h1) = bTh1 + log  log σi − D 2  log 2π − 1 2  (cid:88)  j  σ2 i exp{h1Wj + cj}  i  In order to calculate the expectation of the approximating posteriors, we use Monte Carlo sampling.  (cid:88)  h1  q(h1|v) log p  ∗  (v, h1) ≈ 1 M  ∗  log p  (v, h1(m))  M(cid:88) = −(cid:88)  m=1  D(cid:88)  i  (x − µ)2  σ2 i  log σi − D 2  log 2π − 1 2  (cid:88)  + bTh1 + log  exp{h1Wj + cj}  j  where h1(m) is the m-th sample from the posterior q(h1|v). In order to calculate the partition function of the top-most layer of the GDBN, we use Annealed Importance Sampling (AIS). We used 100 chains with 50,000 intermediate distributions to estimate the partition function of the binary-binary RBM which forms the top layer of the GDBN. Even though AIS is an unbiased estimator of the partition function, it is prone to under-estimating it due to bad mixing of the chains. This causes the log probability to be over-estimated. Therefore the variational lower bounds reported in our paper are not strictly guaranteed to be lower bounds and are subject to errors. However, we believe that the margin of error is unlikely to be high enough to affect our conclusions.  Additional Results  We present some more examples of inference process of our framework. Below, we show some success cases and a failure case for inference on the CMU Multi-PIE dataset. The initial gaze variables of u are highlighted in yellow and later iterations are highlighted with color gradually changing to blue.  11  (18)  (19)  (20)  (21)  (22)  Figure 9: Example of an approximate inference steps. v is 24×24, x is 72×72. Approximate inference quickly ﬁnds a good initialization for u, while HMC makes small adjustments.  (a) Ex. 1  (b) Ex. 2  (c) Ex. 3  (d) Ex. 4  Figure 10: E-step for learning on CMU Multi-PIE. (a),(b),(c) are successful. (d) is a failure case.  12  Inference steps123456HMCVXInference steps123456HMCVX","Attention has long been proposed by psychologists as important foreffectively dealing with the enormous sensory stimulus available in theneocortex. Inspired by the visual attention models in computationalneuroscience and the need of object-centric data for generative models, wedescribe for generative learning framework using attentional mechanisms.Attentional mechanisms can propagate signals from region of interest in a sceneto an aligned canonical representation, where generative modeling takes place.By ignoring background clutter, generative models can concentrate theirresources on the object of interest. Our model is a proper graphical modelwhere the 2D Similarity transformation is a part of the top-down process. AConvNet is employed to provide good initializations during posterior inferencewhich is based on Hamiltonian Monte Carlo. Upon learning images of faces, ourmodel can robustly attend to face regions of novel test subjects. Moreimportantly, our model can learn generative models of new faces from a noveldataset of large images where the face locations are not known."
1312.6199,2014,Intriguing properties of neural networks  ,"['Joan Bruna', 'Christian Szegedy', 'Ilya Sutskever', 'Ian Goodfellow', 'Wojciech Zaremba', 'Rob Fergus', 'Dumitru Erhan']",https://arxiv.org/pdf/1312.6199.pdf,"4 1 0 2     b e F 9 1         ]  V C . s c [      4 v 9 9 1 6  .  2 1 3 1 : v i X r a  Intriguing properties of neural networks  Christian Szegedy  Google Inc.  Wojciech Zaremba New York University  Ilya Sutskever  Joan Bruna  Google Inc.  New York University  Dumitru Erhan  Google Inc.  Ian Goodfellow  Rob Fergus  University of Montreal  New York University  Facebook Inc.  Abstract  Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninter- pretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we ﬁnd that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we ﬁnd that deep neural networks learn input-output mappings that are fairly discontinuous to a signiﬁcant extent. We can cause the network to misclas- sify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network’s prediction error. In addition, the speciﬁc nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.  1  Introduction  Deep neural networks are powerful learning models that achieve excellent performance on visual and speech recognition problems [9, 8]. Neural networks achieve high performance because they can express arbitrary computation that consists of a modest number of massively parallel nonlinear steps. But as the resulting computation is automatically discovered by backpropagation via supervised learning, it can be difﬁcult to interpret and can have counter-intuitive properties. In this paper, we discuss two counter-intuitive properties of deep neural networks. The ﬁrst property is concerned with the semantic meaning of individual units. Previous works [6, 13, 7] analyzed the semantic meaning of various units by ﬁnding the set of inputs that maximally activate a given unit. The inspection of individual units makes the implicit assumption that the units of the last feature layer form a distinguished basis which is particularly useful for extracting seman- tic information. Instead, we show in section 3 that random projections of φ(x) are semantically indistinguishable from the coordinates of φ(x). This puts into question the conjecture that neural networks disentangle variation factors across coordinates. Generally, it seems that it is the entire space of activations, rather than the individual units, that contains the bulk of the semantic informa- tion. A similar, but even stronger conclusion was reached recently by Mikolov et al. [12] for word representations, where the various directions in the vector space representing the words are shown to give rise to a surprisingly rich semantic encoding of relations and analogies. At the same time,  1  the vector representations are stable up to a rotation of the space, so the individual units of the vector representations are unlikely to contain semantic information. The second property is concerned with the stability of neural networks with respect to small per- turbations to their inputs. Consider a state-of-the-art deep neural network that generalizes well on an object recognition task. We expect such network to be robust to small perturbations of its in- put, because small perturbation cannot change the object category of an image. However, we ﬁnd that applying an imperceptible non-random perturbation to a test image, it is possible to arbitrarily change the network’s prediction (see ﬁgure 5). These perturbations are found by optimizing the input to maximize the prediction error. We term the so perturbed examples “adversarial examples”. It is natural to expect that the precise conﬁguration of the minimal necessary perturbations is a random artifact of the normal variability that arises in different runs of backpropagation learning. Yet, we found that adversarial examples are relatively robust, and are shared by neural networks with varied number of layers, activations or trained on different subsets of the training data. That is, if we use one neural net to generate a set of adversarial examples, we ﬁnd that these examples are still statistically hard for another neural network even when it was trained with different hyperparameters or, most surprisingly, when it was trained on a different set of examples. These results suggest that the deep neural networks that are learned by backpropagation have nonin- tuitive characteristics and intrinsic blind spots, whose structure is connected to the data distribution in a non-obvious way.  2 Framework Notation We denote by x ∈ Rm an input image, and φ(x) activation values of some layer. We ﬁrst examine properties of the image of φ(x), and then we search for its blind spots. We perform a number of experiments on a few different networks and three datasets :  • For the MNIST dataset, we used the following architectures [11]  – A simple fully connected network with one or more hidden layers and a Softmax  classiﬁer. We refer to this network as “FC”.  – A classiﬁer trained on top of an autoencoder. We refer to this network as “AE”.  • The ImageNet dataset [3].  – Krizhevsky et. al architecture [9]. We refer to it as “AlexNet”.  • ∼ 10M image samples from Youtube (see [10])  – Unsupervised trained network with ∼ 1 billion learnable parameters. We refer to it as  “QuocNet”.  For the MNIST experiments, we use regularization with a weight decay of λ. Moreover, in some experiments we split the MNIST training dataset into two disjoint datasets P1, and P2, each with 30000 training cases.  3 Units of: φ(x)  Traditional computer vision systems rely on feature extraction: often a single feature is easily inter- pretable, e.g. a histogram of colors, or quantized local derivatives. This allows one to inspect the individual coordinates of the feature space, and link them back to meaningful variations in the input domain. Similar reasoning was used in previous work that attempted to analyze neural networks that were applied to computer vision problems. These works interpret an activation of a hidden unit as a meaningful feature. They look for input images which maximize the activation value of this single feature [6, 13, 7, 4]. The aforementioned technique can be formally stated as visual inspection of images x(cid:48), which satisfy (or are close to maximum attainable value):  x(cid:48) = arg max  (cid:104)φ(x), ei(cid:105)  x∈I  2  (a) Unit sensitive to lower round stroke.  (b) Unit sensitive to upper round stroke, or lower straight stroke.  (c) Unit senstive to left, upper round stroke.  (d) Unit senstive to diagonal straight stroke.  Figure 1: An MNIST experiment. The ﬁgure shows images that maximize the activation of various units (maximum stimulation in the natural basis direction). Images within each row share semantic properties.  (a) Direction sensitive to upper straight stroke, or lower round stroke.  (b) Direction sensitive to lower left loop.  (c) Direction senstive to round top stroke.  (d) Direction sensitive to right, upper round stroke.  Figure 2: An MNIST experiment. The ﬁgure shows images that maximize the activations in a random direction (maximum stimulation in a random basis). Images within each row share semantic properties.  where I is a held-out set of images from the data distribution that the network was not trained on and ei is the natural basis vector associated with the i-th hidden unit. Our experiments show that any random direction v ∈ Rn gives rise to similarly interpretable se- mantic properties. More formally, we ﬁnd that images x(cid:48) are semantically related to each other, for many x(cid:48) such that  x(cid:48) = arg max  x∈I  (cid:104)φ(x), v(cid:105)  This suggests that the natural basis is not better than a random basis for inspecting the properties of φ(x). This puts into question the notion that neural networks disentangle variation factors across coordinates. First, we evaluated the above claim using a convolutional neural network trained on MNIST. We used the MNIST test set for I. Figure 1 shows images that maximize the activations in the natural basis, and Figure 2 shows images that maximize the activation in random directions. In both cases the resulting images share many high-level similarities. Next, we repeated our experiment on an AlexNet, where we used the validation set as I. Figures 3 and 4 compare the natural basis to the random basis on the trained network. The rows appear to be semantically meaningful for both the single unit and the combination of units. Although such analysis gives insight on the capacity of φ to generate invariance on a particular subset of the input distribution, it does not explain the behavior on the rest of its domain. We shall see in the next section that φ has counterintuitive properties in the neighbourhood of almost every point form data distribution.  4 Blind Spots in Neural Networks  So far, unit-level inspection methods had relatively little utility beyond conﬁrming certain intuitions regarding the complexity of the representations learned by a deep neural network [6, 13, 7, 4]. Global, network level inspection methods can be useful in the context of explaining classiﬁcation decisions made by a model [1] and can be used to, for instance, identify the parts of the input which led to a correct classiﬁcation of a given visual input instance (in other words, one can use a trained  3  (a) Unit sensitive to white ﬂowers.  (b) Unit sensitive to postures.  (c) Unit senstive to round, spiky ﬂowers.  (d) Unit senstive to round green or yellow objects.  Figure 3: Experiment performed on ImageNet. Images stimulating single unit most (maximum stimulation in natural basis direction). Images within each row share many semantic properties.  (a) Direction sensitive to white, spread ﬂowers.  (b) Direction sensitive to white dogs.  (c) Direction sensitive to spread shapes.  (d) Direction sensitive to dogs with brown heads.  Figure 4: Experiment performed on ImageNet. Images giving rise to maximum activations in a random direc- tion (maximum stimulation in a random basis). Images within each row share many semantic properties.  model for weakly-supervised localization). Such global analyses are useful in that they can make us understand better the input-to-output mapping represented by the trained network. Generally speaking, the output layer unit of a neural network is a highly nonlinear function of its input. When it is trained with the cross-entropy loss (using the Softmax activation function), it represents a conditional distribution of the label given the input (and the training set presented so far). It has been argued [2] that the deep stack of non-linear layers in between the input and the output unit of a neural network are a way for the model to encode a non-local generalization prior over the input space. In other words, it is assumed that is possible for the output unit to assign non- signiﬁcant (and, presumably, non-epsilon) probabilities to regions of the input space that contain no training examples in their vicinity. Such regions can represent, for instance, the same objects from different viewpoints, which are relatively far (in pixel space), but which share nonetheless both the label and the statistical structure of the original inputs. It is implicit in such arguments that local generalization—in the very proximity of the training examples—works as expected. And that in particular, for a small enough radius ε > 0 in the vicinity of a given training input x, an x + r satisfying ||r|| < ε will get assigned a high probability of the correct class by the model. This kind of smoothness prior is typically valid for computer vision problems. In general, imperceptibly tiny perturbations of a given image do not normally change the underlying class. Our main result is that for deep neural networks, the smoothness assumption that underlies many kernel methods does not hold. Speciﬁcally, we show that by using a simple optimization procedure, we are able to ﬁnd adversarial examples, which are obtained by imperceptibly small perturbations to a correctly classiﬁed input image, so that it is no longer classiﬁed correctly. In some sense, what we describe is a way to traverse the manifold represented by the network in an efﬁcient way (by optimization) and ﬁnding adversarial examples in the input space. The adversarial examples represent low-probability (high-dimensional) “pockets” in the manifold, which are hard to efﬁciently ﬁnd by simply randomly sampling the input around a given example. Already, a variety of recent state of the art computer vision models employ input deformations during training for  4  increasing the robustness and convergence speed of the models [9, 13]. These deformations are, however, statistically inefﬁcient, for a given example: they are highly correlated and are drawn from the same distribution throughout the entire training of the model. We propose a scheme to make this process adaptive in a way that exploits the model and its deﬁciencies in modeling the local space around the training data. We make the connection with hard-negative mining explicitly, as it is close in spirit: hard-negative mining, in computer vision, consists of identifying training set examples (or portions thereof) which are given low probabilities by the model, but which should be high probability instead, cf. [5]. The training set distribution is then changed to emphasize such hard negatives and a further round of model training is performed. As shall be described, the optimization problem proposed in this work can also be used in a constructive way, similar to the hard-negative mining principle.  4.1 Formal description We denote by f : Rm −→ {1 . . . k} a classiﬁer mapping image pixel value vectors to a discrete label set. We also assume that f has an associated continuous loss function denoted by lossf : Rm × {1 . . . k} −→ R+. For a given x ∈ Rm image and target label l ∈ {1 . . . k}, we aim to solve the following box-constrained optimization problem:  • Minimize (cid:107)r(cid:107)2 subject to:  1. f (x + r) = l 2. x + r ∈ [0, 1]m  The minimizer r might not be unique, but we denote one such x + r for an arbitrarily chosen minimizer by D(x, l). Informally, x + r is the closest image to x classiﬁed as l by f. Obviously, D(x, f (x)) = f (x), so this task is non-trivial only if f (x) (cid:54)= l. In general, the exact computation of D(x, l) is a hard problem, so we approximate it by using a box-constrained L-BFGS. Concretely, we ﬁnd an approximation of D(x, l) by performing line-search to ﬁnd the minimum c > 0 for which the minimizer r of the following problem satisﬁes f (x + r) = l.  • Minimize c|r| + lossf (x + r, l) subject to x + r ∈ [0, 1]m  This penalty function method would yield the exact solution for D(X, l) in the case of convex losses, however neural networks are non-convex in general, so we end up with an approximation in this case.  4.2 Experimental results  Our “minimimum distortion” function D has the following intriguing properties which we will sup- port by informal evidence and quantitative experiments in this section:  1. For all the networks we studied (MNIST, QuocNet [10], AlexNet [9]), for each sam- ple, we have always managed to generate very close, visually hard to distinguish, ad- versarial examples that are misclassiﬁed by the original network (see ﬁgure 5 and http://goo.gl/huaGPb for examples).  2. Cross model generalization: a relatively large fraction of examples will be misclassiﬁed by networks trained from scratch with different hyper-parameters (number of layers, regular- ization or initial weights).  3. Cross training-set generalization a relatively large fraction of examples will be misclassi-  ﬁed by networks trained from scratch on a disjoint training set.  The above observations suggest that adversarial examples are somewhat universal and not just the results of overﬁtting to a particular model or to the speciﬁc selection of the training set. They also suggest that back-feeding adversarial examples to training might improve generalization of the re- sulting models. Our preliminary experiments have yielded positive evidence on MNIST to support this hypothesis as well: We have successfully trained a two layer 100-100-10 non-convolutional neu- ral network with a test error below 1.2% by keeping a pool of adversarial examples a random subset of which is continuously replaced by newly generated adversarial examples and which is mixed into  5  (a)  (b)  Figure 5: Adversarial examples generated for AlexNet [9].(Left) is a correctly predicted sample, (center) dif- ference between correct image, and image predicted incorrectly magniﬁed by 10x (values shifted by 128 and clamped), (right) adversarial example. All images in the right column are predicted to be an “ostrich, Struthio camelus”. Average distortion based on 64 examples is 0.006508. Plase refer to http://goo.gl/huaGPb for full resolution images. The examples are strictly randomly chosen. There is not any postselection involved.  (a)  (b)  Figure 6: Adversarial examples for QuocNet [10]. A binary car classiﬁer was trained on top of the last layer features without ﬁne-tuning. The randomly chosen examples on the left are recognized correctly as cars, while the images in the middle are not recognized. The rightmost column is the magniﬁed absolute value of the difference between the two images.  the original training set all the time. We used weight decay, but no dropout for this network. For comparison, a network of this size gets to 1.6% errors when regularized by weight decay alone and can be improved to around 1.3% by using carefully applied dropout. A subtle, but essential detail is that we only got improvements by generating adversarial examples for each layer outputs which were used to train all the layers above. The network was trained in an alternating fashion, maintain- ing and updating a pool of adversarial examples for each layer separately in addition to the original training set. According to our initial observations, adversarial examples for the higher layers seemed to be signiﬁcantly more useful than those on the input or lower layers. In our future work, we plan to compare these effects in a systematic manner. For space considerations, we just present results for a representative subset (see Table 1) of the MNIST experiments we performed. The results presented here are consistent with those on a larger variety of non-convolutional models. For MNIST, we do not have results for convolutional mod- els yet, but our ﬁrst qualitative experiments with AlexNet gives us reason to believe that convolu- tional networks may behave similarly as well. Each of our models were trained with L-BFGS until convergence. The ﬁrst three models are linear classiﬁers that work on the pixel level with various weight decay parameters λ. All our examples use quadratic weight decay on the connection weights: i /k added to the total loss, where k is the number of units in the layer. Three of our models are simple linear (softmax) classiﬁer without hidden units (FC10(λ)). One of them, FC10(1), is trained with extremely high λ = 1 in order to test whether it is still possible to generate adversarial examples in this extreme setting as well.Two other models are a simple sigmoidal neural network with two hidden layers and a classiﬁer. The last model, AE400-10, consists of a single layer sparse autoencoder with sigmoid activations and 400 nodes with a Softmax classiﬁer. This network has been trained until it got very high quality ﬁrst layer ﬁlters and this layer was not ﬁne-tuned. The last column measures the minimum average pixel level distortion necessary to reach 0% accuracy between the original x and distorted on the training set. The distortion is measure by  lossdecay = λ(cid:80) w2  (cid:113)(cid:80)(x(cid:48)  i−xi)2  n  6  (a) Even columns: adver- sarial examples for a lin- ear (std- dev=0.06)  (FC) classiﬁer  (b) Even columns: adver- sarial examples for a 200- 200-10 sigmoid network (stddev=0.063)  (c) Randomly distorted samples by Gaussian noise with stddev=1. Accuracy: 51%.  Figure 7: Adversarial examples for a randomly chosen subset of MNIST compared with randomly distorted examples. Odd columns correspond to original images, and even columns correspond to distorted counterparts. The adversarial examples generated for the speciﬁc model have accuracy 0% for the respective model. Note that while the randomly distorted examples are hardly readable, still they are classiﬁed correctly in half of the cases, while the adversarial examples are never classiﬁed correctly.  Model Name FC10(10−4) FC10(10−2) FC10(1) FC100-100-10 FC200-200-10 AE400-10  Description Softmax with λ = 10−4 Softmax with λ = 10−2 Softmax with λ = 1 Sigmoid network λ = 10−5, 10−5, 10−6 Sigmoid network λ = 10−5, 10−5, 10−6 Autoencoder with Softmax λ = 10−6  Training error  Test error  Av. min. distortion  6.7% 10% 21.2% 0% 0% 0.57%  7.4% 9.4% 20% 1.64% 1.54% 1.9%  0.062 0.1 0.14 0.058 0.065 0.086  Table 1: Tests of the generalization of adversarial instances on MNIST.  FC10(10−4) FC10(10−2) FC10(1) FC100-100-10 FC200-200-10 AE400-10 Gaussian noise, stddev=0.1 Gaussian noise, stddev=0.3  FC10(10−4) 100% 87.1% 71.9% 28.9% 38.2% 23.4% 5.0% 15.6%  FC10(10−2) 11.7% 100% 76.2% 13.7% 14% 16% 10.1% 11.3%  FC10(1) 22.7% 35.2% 100% 21.1% 23.8% 24.8% 18.3% 22.7%  FC100-100-10 2% 35.9% 48.1% 100% 20.3% 9.4% 0% 5%  FC200-200-10 3.9% 27.3% 47% 6.6% 100% 6.6% 0% 4.3%  AE400-10 2.7% 9.8% 34.4% 2% 2.7% 100% 0.8% 3.1%  Av. distortion  0.062 0.1 0.14 0.058 0.065 0.086 0.1 0.3  Table 2: Cross-model generalization of adversarial examples. The columns of the Tables show the error induced by distorted examples fed to the given model. The last column shows average distortion wrt. original training set.  x(cid:48) images, where n = 784 is the number of image pixels. The pixel intensities are scaled to be in the range [0, 1]. In our ﬁrst experiment, we generated a set of adversarial instances for a given network and fed these examples for each other network to measure the proportion of misclassiﬁed instances. The last column shows the average minimum distortion that was necessary to reach 0% accuracy on the whole training set. The experimental results are presented in Table 2. The columns of Table 2 show the error (proportion of misclassiﬁed instances) on the so distorted training sets. The last two rows are given for reference showing the error induced when distorting by the given amounts of Gaussian noise. Note that even the noise with stddev 0.1 is greater than the stddev of our adversarial noise for all but one of the models. Figure 7 shows a visualization of the generated adversarial instances for two of the networks used in this experiment The general conclusion is that adversarial examples tend to stay hard even for models trained with different hyperparameters. Although the autoencoder based version seems most resilient to adversarial examples, it is not fully immune either. Still, this experiment leaves open the question of dependence over the training set. Does the hardness of the generated examples rely solely on the particular choice of our training set as a sample or does this effect generalize even to models trained on completely different training sets?  7  Model  Error on P1  Error on P2  Error on Test  Min Av. Distortion  FC100-100-10: 100-100-10 trained on P1 FC123-456-10: 123-456-10 trained on P1 FC100-100-10’ trained on P2  0% 0% 2.3%  2.4% 2.5% 0%  2% 2.1% 2.1%  0.062 0.059 0.058  Table 3: Models trained to study cross-training-set generalization of the generated adversarial examples. Errors presented in Table correpond to original not-distorted data, to provide a baseline.  Distorted for FC100-100-10 (av. stddev=0.062) Distorted for FC123-456-10 (av. stddev=0.059) Distorted for FC100-100-10’ (av. stddev=0.058) Gaussian noise with stddev=0.06  FC100-100-10 100% 6.25% 8.2% 2.2%  FC123-456-10 26.2% 100% 8.2% 2.6%  FC100-100-10’ 5.9% 5.1% 100% 2.4%  Distorted for FC100-100-10 ampliﬁed to stddev=0.1 Distorted for FC123-456-10 ampliﬁed to stddev=0.1 Distorted for FC100-100-10’ ampliﬁed to stddev=0.1 Gaussian noise with stddev=0.1  100% 96% 27% 2.6%  98% 100% 50% 2.8%  43% 22% 100% 2.7%  Table 4: Cross-training-set generalization error rate for the set of adversarial examples generated for different models. The error induced by a random distortion to the same examples is displayed in the last row.  To study cross-training-set generalization, we have partitioned the 60000 MNIST training images into two parts P1 and P2 of size 30000 each and trained three non-convolutional networks with sigmoid activations on them: Two, FC100-100-10 and FC123-456-10, on P1 and FC100-100-10 on P2. The reason we trained two networks for P1 is to study the cumulative effect of changing the hypermarameters and the training sets at the same time. Models FC100-100-10 and FC100-100- 10 share the same hyperparameters: both of them are 100-100-10 networks, while FC123-456-10 has different number of hidden units. In this experiment, we were distorting the elements of the test set rather than the training set. Table 3 summarizes the basic facts about these models. After we generate adversarial examples with 100% error rates with minimum distortion for the test set, we feed these examples to the each of the models. The error for each model is displayed in the corresponding column of the upper part of Table 4. In the last experiment, we magnify the effect of our distortion by using the examples x + 0.1 x(cid:48)−x rather than x(cid:48). This magniﬁes the distortion (cid:107)x(cid:48)−x(cid:107)2 on average by 40%, from stddev 0.06 to 0.1. The so distorted examples are fed back to each of the models and the error rates are displayed in the lower part of Table 4. The intriguing conclusion is that the adversarial examples remain hard for models trained even on a disjoint training set, although their effectiveness decreases considerably.  4.3 Spectral Analysis of Unstability  The previous section showed examples of deep networks resulting from purely supervised training which are unstable with respect to a peculiar form of small perturbations. Independently of their generalisation properties across networks and training sets, the adversarial examples show that there exist small additive perturbations of the input (in Euclidean sense) that produce large perturbations at the output of the last layer. This section describes a simple procedure to measure and control the additive stability of the network by measuring the spectrum of each rectiﬁed layer. Mathematically, if φ(x) denotes the output of a network of K layers corresponding to input x and trained parameters W , we write  φ(x) = φK(φK−1(. . . φ1(x; W1); W2) . . . ; WK) ,  where φk denotes the operator mapping layer k − 1 to layer k. The unstability of φ(x) can be explained by inspecting the upper Lipschitz constant of each layer k = 1 . . . K, deﬁned as the constant Lk > 0 such that  The resulting network thus satsiﬁes (cid:107)φ(x) − φ(x + r)(cid:107) ≤ L(cid:107)r(cid:107), with L =(cid:81)K  ∀ x, r , (cid:107)φk(x; Wk) − φk(x + r; Wk)(cid:107) ≤ Lk(cid:107)r(cid:107) .  k=1 Lk.  A half-rectiﬁed layer (both convolutional or fully connected) is deﬁned by the mapping φk(x; Wk, bk) = max(0, Wkx+bk). Let (cid:107)W(cid:107) denote the operator norm of W (i.e., its largest singu-  8  Layer Conv. 1 Conv. 2 Conv. 3 Conv. 4 Conv. 5 FC. 1 FC. 2 FC. 3  Size  3 × 11 × 11 × 96 96 × 5 × 5 × 256 256 × 3 × 3 × 384 384 × 3 × 3 × 384 384 × 3 × 3 × 256  9216 × 4096 4096 × 4096 4096 × 1000  Stride  Upper bound  4  1  1  1  1 N/A N/A N/A  2.75  10  7  7.5  11  3.12  4  4  Table 5: Frame Bounds of each rectiﬁed layer of the network from [9].  lar value). Since the non-linearity ρ(x) = max(0, x) is contractive, i.e. satisﬁes (cid:107)ρ(x)−ρ(x+r)(cid:107) ≤ (cid:107)r(cid:107) for all x, r; it follows that (cid:107)φk(x; Wk)−φk(x+r; Wk)(cid:107) = (cid:107) max(0, Wkx+bk)−max(0, Wk(x+r)+bk)(cid:107) ≤ (cid:107)Wkr(cid:107) ≤ (cid:107)Wk(cid:107)(cid:107)r(cid:107) , and hence Lk ≤ (cid:107)Wk(cid:107). On the other hand, a max-pooling layer φk is contractive:  ∀ x , r , (cid:107)φk(x) − φk(x + r)(cid:107) ≤ (cid:107)r(cid:107) ,  since its Jacobian is a projection onto a subset of the input coordinates and hence does not expand the gradients. Finally, if φk is a contrast-normalization layer  (cid:16)  x  (cid:15) + (cid:107)x(cid:107)2  (cid:17)γ ,  φk(x) =  one can verify that ∀ x , r , (cid:107)φk(x) − φk(x + r)(cid:107) ≤ (cid:15)−γ(cid:107)r(cid:107) for γ ∈ [0.5, 1], which corresponds to most common operating regimes. It results that a conservative measure of the unstability of the network can be obtained by simply computing the operator norm of each fully connected and convolutional layer. The fully connected case is trivial since the norm is directly given by the largest singular value of the fully connected matrix. Let us describe the convolutional case. If W denotes a generic 4-tensor, implementing a convolutional layer with C input features, D output features, support N × N and spatial stride ∆,  (cid:41)  W x =  xc (cid:63) wc,d(n1∆, n2∆) ; d = 1 . . . , D  ,  (cid:40) C(cid:88)  c=1  where xc denotes the c-th input feature image, and wc,d is the spatial kernel corresponding to input feature c and output feature d, by applying Parseval’s formula we obtain that its operator norm is given by  (cid:107)W(cid:107) =  sup  (cid:107)A(ξ)(cid:107) ,  (1)  ξ∈[0,N ∆−1)2 where A(ξ) is a D × (C · ∆2) matrix whose rows are  ∀ d = 1 . . . D , A(ξ)d =  and (cid:100)wc,d is the 2-D Fourier transform of wc,d: (cid:88)  (cid:16) ∆−2(cid:100)wc,d(ξ + l · N · ∆−1) ; c = 1 . . . C , l = (0 . . . ∆ − 1)2(cid:17) (cid:100)wc,d(ξ) =  wc,d(u)e−2πi(u·ξ)/N 2  .  ,  u∈[0,N )2  Table 5 shows the upper Lipschitz bounds computed from the ImageNet deep convolutional network of [9], using (1). It shows that instabilities can appear as soon as in the ﬁrst convolutional layer. These results are consistent with the exsitence of blind spots constructed in the previous section, but they don’t attempt to explain why these examples generalize across different hyperparameters or training sets. We emphasize that we compute upper bounds: large bounds do not automatically translate into existence of adversarial examples; however, small bounds guarantee that no such ex- amples can appear. This suggests a simple regularization of the parameters, consisting in penalizing each upper Lipschitz bound, which might help improve the generalisation error of the networks.  9  5 Discussion  We demonstrated that deep neural networks have counter-intuitive properties both with respect to the semantic meaning of individual units and with respect to their discontinuities. The existence of the adversarial negatives appears to be in contradiction with the network’s ability to achieve high generalization performance. Indeed, if the network can generalize well, how can it be confused by these adversarial negatives, which are indistinguishable from the regular examples? Possible explanation is that the set of adversarial negatives is of extremely low probability, and thus is never (or rarely) observed in the test set, yet it is dense (much like the rational numbers), and so it is found near every virtually every test case. However, we don’t have a deep understanding of how often adversarial negatives appears, and thus this issue should be addressed in a future research.  References  [1] David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, and Klaus- Robert M¨uller. How to explain individual classiﬁcation decisions. The Journal of Machine Learning Research, 99:1803–1831, 2010.  [2] Yoshua Bengio. Learning deep architectures for ai. Foundations and trends® in Machine Learning,  2(1):1–127, 2009.  [3] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchi- cal image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248–255. IEEE, 2009.  [4] Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. Visualizing higher-layer features of a deep network. Technical Report 1341, University of Montreal, June 2009. Also presented at the ICML 2009 Workshop on Learning Feature Hierarchies, Montr´eal, Canada.  [5] Pedro Felzenszwalb, David McAllester, and Deva Ramanan. A discriminatively trained, multiscale, de- formable part model. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1–8. IEEE, 2008.  [6] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate  object detection and semantic segmentation. arXiv preprint arXiv:1311.2524, 2013.  [7] Ian Goodfellow, Quoc Le, Andrew Saxe, Honglak Lee, and Andrew Y Ng. Measuring invariances in  deep networks. Advances in neural information processing systems, 22:646–654, 2009.  [8] Geoffrey E. Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, and Brian Kingsbury. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Process. Mag., 29(6):82–97, 2012.  [9] Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.  Imagenet classiﬁcation with deep convolutional  neural networks. In Advances in Neural Information Processing Systems 25, pages 1106–1114, 2012.  [10] Quoc V Le, Marc’Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S Corrado, Jeff Dean, and Andrew Y Ng. Building high-level features using large scale unsupervised learning. arXiv preprint arXiv:1112.6209, 2011.  [11] Yann LeCun and Corinna Cortes. The mnist database of handwritten digits, 1998. [12] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word representations  in vector space. arXiv preprint arXiv:1301.3781, 2013.  [13] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional neural networks. arXiv  preprint arXiv:1311.2901, 2013.  10  ","Deep neural networks are highly expressive models that have recently achievedstate of the art performance on speech and visual recognition tasks. Whiletheir expressiveness is the reason they succeed, it also causes them to learnuninterpretable solutions that could have counter-intuitive properties. In thispaper we report two such properties.First, we find that there is no distinction between individual high levelunits and random linear combinations of high level units, according to variousmethods of unit analysis. It suggests that it is the space, rather than theindividual units, that contains of the semantic information in the high layersof neural networks.Second, we find that deep neural networks learn input-output mappings thatare fairly discontinuous to a significant extend. We can cause the network tomisclassify an image by applying a certain imperceptible perturbation, which isfound by maximizing the network's prediction error. In addition, the specificnature of these perturbations is not a random artifact of learning: the sameperturbation can cause a different network, that was trained on a differentsubset of the dataset, to misclassify the same input."
1312.5650,2014,Zero-Shot Learning by Convex Combination of Semantic Embeddings  ,"['Tomas Mikolov', 'Andrea Frome', 'Samy Bengio', 'Jonathon Shlens', 'Yoram Singer', 'Greg S. Corrado', 'Jeffrey Dean', 'Mohammad Norouzi']",https://arxiv.org/pdf/1312.5650.pdf,"4 1 0 2    r a     M 1 2      ]  G L . s c [      3 v 0 5 6 5  .  2 1 3 1 : v i X r a  Zero-Shot Learning by Convex Combination of  Semantic Embeddings  Mohammad Norouzi∗, Tomas Mikolov, Samy Bengio, Yoram Singer, Jonathon Shlens, Andrea Frome, Greg S. Corrado, Jeffrey Dean  norouzi@cs.toronto.edu, {tmikolov, bengio, singer}@google.com  {shlens, afrome, gcorrado, jeff}@google.com ∗University of Toronto  Google, Inc.  ON, Canada  Mountain View, CA, USA  Abstract  Several recent publications have proposed methods for mapping images into con- tinuous semantic embedding spaces. In some cases the embedding space is trained jointly with the image transformation. In other cases the semantic embedding space is established by an independent natural language processing task, and then the image transformation into that space is learned in a second stage. Proponents of these image embedding systems have stressed their advantages over the tradi- tional n-way classiﬁcation framing of image understanding, particularly in terms of the promise for zero-shot learning – the ability to correctly annotate images of previously unseen object categories. In this paper, we propose a simple method for constructing an image embedding system from any existing n-way image clas- siﬁer and a semantic word embedding model, which contains the n class labels in its vocabulary. Our method maps images into the semantic embedding space via convex combination of the class label embedding vectors, and requires no addi- tional training. We show that this simple and direct method confers many of the advantages associated with more complex image embedding schemes, and indeed outperforms state of the art methods on the ImageNet zero-shot learning task.  1  Introduction  The classic machine learning approach to object recognition presupposes the existence of a large la- beled training dataset to optimize the free parameters of an image classiﬁer. There have been contin- ued efforts in collecting larger image corpora with a broader coverage of object categories (e.g., [3]), thereby enabling image classiﬁcation with many classes. While annotating more object categories in images can lead to a ﬁner granularity of image classiﬁcation, creating high quality ﬁne grained image annotations is challenging, expensive, and time consuming. Moreover, as new visual entities emerge over time, the annotations should be revised, and the classiﬁers should be re-trained. Motivated by the challenges facing standard machine learning framework for n-way classiﬁcation, especially when n (the number of class labels) is large, several recent papers have proposed meth- ods for mapping images into semantic embedding spaces [14, 4, 9, 6, 18, 19]. In doing so, it is hoped that by resorting to nearest neighbor search in the embedding space with respect to a set of label embedding vectors, one can address zero-shot learning – annotation of images with new la- bels corresponding to previously unseen object categories. While the common practice for image embedding is to learn a regression model from images into a semantic embedding space, it has been unclear whether there exists a more direct way to transform any probabilistic n-way classiﬁer into  ∗Part of this work was done while Mohammad Norouzi was at Google.  1  an image embedding model, which can be used for zero-shot learning. In this work, we present a simple method for constructing an image embedding system by combining any existing probabilistic n-way image classiﬁer with an existing word embedding model, which contains the n class labels in its vocabulary. We show that our simple method confers many of the advantages associated with more complex image embedding schemes. Recently, zero-shot learning [10, 14] has received a growing amount of attention [16, 11, 6, 18]. A key to zero-shot learning is the use of a set of semantic embedding vectors associated with the class labels. These semantic embedding vectors might be obtained from human-labeled object at- tributes [4, 9], or they might be learned from a text corpus in an unsupervised fashion, based on an independent natural language modeling task [6, 18, 12]. Regardless of the way the label embedding vectors are obtained, previous work casts zero-shot learning as a regression problem from the input space into the embedding space. In contrast, given a pre-trained standard classiﬁer, our method maps images into the semantic embedding space via the convex combination of the class label embedding vectors. The values of a given classiﬁer’s predictive probabilities for different training labels are used to compute a weighted combination of the label embeddings in the semantic space. This provides a continuous embedding vector for each image, which is then used for extrapolating the pre-trained classiﬁer’s predictions beyond the training labels, into a set of test labels. The effectiveness of our method called “convex combination of semantic embeddings” (ConSE) is evaluated on ImageNet zero-shot learning task. By employing a convolutional neural network [7] trained only on 1000 object categories from ImageNet, the ConSE model is able to achieve 9.4% hit@1 and 24.7% hit@5 on 1600 unseen objects categories, which were omitted from the training dataset. When the number of test classes gets larger, and they get further from the training classes in the ImageNet category hierarchy, the zero-shot classiﬁcation results get worse, as expected, but still our model outperforms a recent state-of-the-art model [6] applied to the same task.  2 Previous work  Zero-shot learning is closely related to one-shot learning [13, 5, 1, 8], where the goal is to learn object classiﬁers based on a few labeled training exemplars. The key difference in zero-shot learning is that no training images are provided for a held-out set of test categories. Thus, zero-shot learning is more challenging, and the use of side information about the interactions between the class labels is more essential in this setting. Nevertheless, we expect that advances in zero-shot learning will beneﬁt one-shot learning, and visual recognition in general, by providing better ways to incorporate prior knowledge about the relationships between the object categories. A key component of zero-shot learning is the way a semantic space of class label embeddings is deﬁned. In computer vision, there has been a body of work on the use of human-labeled visual attributes [4, 9] to help detecting unseen object categories. Binary attributes are most commonly used to encode presence and absence of a set of visual characteristics within instances of an object category. Some examples of these attributes include different types of materials, different colors, textures, and object parts. More recently, relative attributes [15] are proposed to strengthen the at- tribute based representations. In attribute based approaches, each class label is represented by a vector of attributes, instead of the standard one-of-n encoding. And multiple classiﬁers are trained for predicting each object attribute. While this is closely related to our approach, the main issue with attribute-based classiﬁcation is its lack of scalability to large-scale tasks. Annotating thousands of attributes for thousands of object classes is an ambiguous and challenging task in itself, and the ap- plicability of supervised attributes to large-scale zero-shot learning is limited. There has been some recent work showing good zero-shot classiﬁcation performance on visual recognition tasks [17, 11], but these methods also rely on the use of knowledge bases containing descriptive properties of object classes, and the WordNet hierarchy. A more scalable approach to semantic embeddings of class labels builds upon the recent advances in unsupervised neural language modeling [2]. In this approach, a set of multi-dimensional embedding vectors are learned for each word in a text corpus. The word embeddings are optimized to increase the predictability of each word given its context [12]. Essentially, the words that cooccur in similar contexts, are mapped onto similar embedding vectors. Frome et al. [6] and Socher et al. [18] exploit such word embeddings to embed textual names of object class labels into a continuous semantic space. In this work, we also use the skip-gram model [12] to learn the class label embeddings.  2  j, y(cid:48)  j)}m(cid:48)  j=1 is provided, where x(cid:48)  3 Problem Statement Assume that a labeled training dataset of images D0 ≡ {(xi, yi)}m i=1 is given, where each image is represented by a p-dimensional feature vector, xi ∈ Rp. For generality we denote X def= Rp. There are n0 distinct class labels available for training, i.e., yi ∈ Y0 ≡ {1, . . . , n0}. In addition, a test dataset denoted D1 ≡ {(x(cid:48) j ∈ Y1 ≡ j ∈ X as above, while y(cid:48) {n0 + 1, . . . , n0 + n1}. The test set contains n1 distinct class labels, which are omitted from the training set. Let n = n0 +n1 denote the total number of labels in the training and test sets. The goal of zero-shot learning is to train a classiﬁer on the training set D0, which performs reason- ably well on the unseen test set D1. Clearly, without any side information about the relationships between the labels in Y0 and Y1, zero-shot learning is infeasible as Y0 ∩ Y1 = ∅. However, to mitigate zero-shot learning, one typically assumes that each class label y (1 ≤ y ≤ n) is associated with a semantic embedding vector s(y) ∈ S ≡ Rq. The semantic embedding vectors are such that two labels y and y(cid:48) are similar if and only if their semantic embeddings s(y) and s(y(cid:48)) are close in S, e.g., (cid:104)s(y), s(y(cid:48))(cid:105)S is large. Clearly, given an embedding of training and test class labels into a joint semantic space i.e., {s(y); y ∈ Y0 ∪ Y1}, the training and test labels become related, and one can hope to learn from the training labels to predict the test labels. Previous work (e.g., [6, 18]) has addressed zero-shot classiﬁcation by learning a mapping from input features to semantic label embedding vectors using a multivariate regression model. Accordingly, during training instead of learning an n0-way classiﬁer from inputs to training labels (X → Y0), a regression model is learned from inputs to the semantic embedding space (X → S). A training dataset of inputs paired with semantic embeddings, i.e., {(xi, s(yi)); (xi, yi) ∈ D0}, is constructed to train a regression function f : X → S that aims to map xi to s(yi). Once f (·) is learned, it is applied to a test image x(cid:48) j is then compared with the test label embedding vectors, {s(y(cid:48)); y(cid:48) ∈ Y1}, to ﬁnd the most relevant test labels. Thus, instead of directly mapping from X → Y1, which seems impossible, zero-shot learning methods ﬁrst learn a mapping X → S, and then a deterministic mapping such as k-nearest neighbor search in the semantic space is used to map a point in S to a ranked list of labels in Y1.  j), and this continuous semantic embedding for x(cid:48)  j to obtain f (x(cid:48)  4 ConSE: Convex combination of semantic embeddings  4.1 Model Description  classiﬁer beyond the training labels, to a set of test labels.  In contrast to previous work which casts zero-shot learning as a regression problem from the input space to the semantic label embedding space, in this work, we do not explicitly learn a regres- sion function f : X → S. Instead, we follow the classic machine learning approach, and learn (cid:80)n0 a classiﬁer from training inputs to training labels. To this end, a classiﬁer p0 is trained on D0 to estimate the probability of an image x belonging to a class label y ∈ Y0, denoted p0(y | x), where y=1 p0(y | x) = 1. Given p0, we propose a method to transfer the probabilistic predictions of the Let (cid:98)y0(x, 1) denote the most likely training label for an image x according to the classiﬁer p0. Analogously, let (cid:98)y0(x, t) denote the tth most likely training label for x according to p0. In other words, p0((cid:98)y0(x, t) | x) is the tth largest value among {p0(y | x); y ∈ Y0}. Given the top T predic- for an input x, as the convex combination of the semantic embeddings {s((cid:98)y0(x, t))}T  tions of p0 for an input x, our model deterministically predicts a semantic embedding vector f (x) t=1 weighted  (cid:98)y0(x, 1) ≡ argmax  Formally, we denote  p0(y | x) .  y∈Y0  (1)  by their corresponding probabilities. More formally,  T(cid:88)  1 Z  p((cid:98)y0(x, t) | x) · s((cid:98)y0(x, t)) ,  where Z is a normalization factor given by Z =(cid:80)T  f (x) =  t=1  t=1 p((cid:98)y0(x, t) | x), and T is a hyper-parameter  (2)  controlling the maximum number of embedding vectors to be considered. If the classiﬁer is very  3  2 s(lion) + 1  2 s(tiger), then it is likely that f (x) ≈ s(liger).  conﬁdent in its prediction of a label y for x, i.e., p0(y | x) ≈ 1, then f (x) ≈ s(y). However, if the classiﬁer have doubts whether an image contains a “lion” or a “tiger”, e.g., p0(lion | x) = 0.6 and p0(tiger | x) = 0.4, then our predicted semantic embedding, f (x) = 0.6 · s(lion) + 0.4 · s(tiger), will be something between lion and tiger in the semantic space. Even though “liger” (a hybrid cross between a lion and a tiger) might not be among the training labels, because it is likely that s(liger) ≈ 1 Given the predicted embedding of x in the semantic space, i.e., f (x), we perform zero-shot classi- ﬁcation by ﬁnding the class labels with embeddings nearest to f (x) in the semantic space. The top  prediction of our model for an image x from the test label set, denoted(cid:98)y1(x, 1), is given by where we use cosine similarity to rank the embedding vectors. Moreover, let(cid:98)y1(x, k) denote the kth most likely test label predicted for x. Then, (cid:98)y1(x, k) is deﬁned as the label y(cid:48) ∈ Y1 with the  kth largest value of cosine similarity in {cos(f (x), s(y(cid:48))); y(cid:48) ∈ Y1}. Note that previous work on zero-shot learning also uses a similar k-nearest neighbor procedure in the semantic space to perform label extrapolation. The key difference in our work is that we deﬁne the embedding prediction f (x) based on a standard classiﬁer as in Eq. (2), and not based on a learned regression model. For the speciﬁc choice of cosine similarity to measure closeness in the embedding space, the norm of f (x) does not matter, and we could drop the normalization factor (1/Z) in Eq. (2).  (cid:98)y1(x, 1) ≡ argmax  cos(f (x), s(y(cid:48))) ,  y(cid:48)∈Y1  (3)  4.2 Difference with DeViSE  Our model is inspired by a technique recently proposed for image embedding, called “Deep Visual- Semantic Embedding” (DeViSE) [6]. Both DeViSE and ConSE models beneﬁt from the convolu- tional neural network classiﬁer of Krizhevsky et al. [7], but there is an important difference in the way they employ the neural net. The DeViSE model replaces the last layer of the convolutional net, the Softmax layer, with a linear transformation layer. The new transformation layer is trained using a ranking objective to map training inputs close to continuous embedding vectors corresponding to correct labels. Subsequently, the lower layers of the convolutional neural network are ﬁne-tuned us- ing the ranking objective to produce better results. In contrast, the ConSE model keeps the Softmax layer of the convolutional net intact, and it does not train the neural network any further. Given a test image, the ConSE simply runs the convolutional classiﬁer and considers the top T predictions of the model. Then, the convex combination of the corresponding T semantic embedding vectors in the semantic space (see Eq. (2)) is computed, which deﬁnes a deterministic transformation from the outputs of the Softmax classiﬁer into the embedding space.  5 Experiments  We compare our approach, “convex combination of semantic embedding” (ConSE), with a state- of-the-art method called “Deep Visual-Semantic Embedding” (DeViSE) [6] on the ImageNet dataset [3]. Both of the ConSE and DeViSE models make use of the same skipgram text model [12] to deﬁne the semantic label embedding space. The skipgram model was trained on 5.4 billion words from Wikipedia.org to construct 500 dimensional word embedding vectors. The embedding vectors are then normalized to have a unit norm. The convolutional neural network of [7], used in both ConSE and DeViSE, is trained on ImageNet 2012 1K set with 1000 training labels. Because the image classiﬁer, and the label embedding vectors are identical in the ConSE and DeViSE models, we can perform a direct comparison between the two embedding techniques. We mirror the ImageNet zero-shot learning experiments of [6]. Accordingly, we report the zero-shot generalization performance of the models on three test datasets with increasing degree of difﬁculty. The ﬁrst test dataset, called “2-hops” includes labels from the 2011 21K set which are visually and semantically similar to the training labels in the ImageNet 2012 1K set. This dataset only includes labels within 2 tree hops of the ImageNet 2012 1K labels. A more difﬁcult dataset including labels within 3 hops of the training labels is created in a similar way and referred to as “3-hops”. Finally, a dataset of all the labels in the ImageNet 2011 21K set is created. The three test datasets respectively include 1, 589, 7, 860, and 20, 900 labels. These test datasets do not include any image labeled with any of the 1000 training labels.  4  Test Image  Softmax Baseline [7]  DeViSE [6]  ConSE (10)  wig fur coat Saluki, gazelle hound Afghan hound, Afghan stole  water spaniel tea gown bridal gown, wedding gown spaniel tights, leotards  business suit dress, frock hairpiece, false hair, postiche swimsuit, swimwear, bathing suit kit, outﬁt  ostrich, Struthio camelus black stork, Ciconia nigra vulture crane peacock  heron owl, bird of Minerva, bird of night hawk bird of prey, raptor, raptorial bird ﬁnch  ratite, ratite bird, ﬂightless bird peafowl, bird of Juno common spoonbill New World vulture, cathartid Greek partridge, rock partridge  sea lion plane, carpenter’s plane cowboy boot loggerhead, loggerhead turtle goose  elephant turtle turtleneck, turtle, polo-neck ﬂip-ﬂop, thong handcart, pushcart, cart, go-cart  California sea lion Steller sea lion Australian sea lion South American sea lion eared seal  hamster broccoli Pomeranian capuchin, ringtail weasel  golden hamster, Syrian hamster rhesus, rhesus monkey pipe shaker American mink, Mustela vison  golden hamster, Syrian hamster rodent, gnawer Eurasian hamster rhesus, rhesus monkey rabbit, coney, cony  thresher, threshing machine tractor harvester, reaper half track snowplow, snowplough  truck, motortruck skidder tank car, tank automatic riﬂe, machine riﬂe trailer, house trailer  ﬂatcar, ﬂatbed, ﬂat truck, motortruck tracked vehicle bulldozer, dozer wheeled vehicle  Tibetan mastiff titi, titi monkey koala, koala bear, kangaroo bear llama chow, chow chow  kernel littoral, litoral, littoral zone, sands carillon Cabernet, Cabernet Sauvignon poodle, poodle dog  dog, domestic dog domestic cat, house cat schnauzer Belgian sheepdog domestic llama, Lama peruana  (farm machine)  (alpaca, Lama pacos)  Figure 1: Zero-shot test images from ImageNet, and their corresponding top 5 labels predicted by the Softmax Baseline [7], DeViSE [6], and ConSE(T = 10). The labels predicted by the Softmax baseline are the labels used for training, and the labels predicted by the other two models are not seen during training of the image classiﬁers. The correct labels are shown in blue. Examples are hand-picked to illustrate the cases that the ConSE(10) performs well, and a few failure cases.  Fig. 1 depicts some qualitative results. The ﬁrst column shows the top 5 predictions of the convolu- tional net, referred to as the Softmax baseline [7]. The second and third columns show the zero-shot predictions by the DeViSE and ConSE(10) models. The ConSE(10) model uses the top T = 10 predictions of the Softmax baseline to generate convex combination of embeddings. Fig. 1 shows that the labels predicted by the ConSE(10) model are generally coherent and they include very few outliers. In contrast, the top 5 labels predicted by the DeViSE model include more outliers such as “ﬂip-ﬂop” predicted for a “Steller sea lion”, “pipe” and “shaker” predicted for a “hamster”, and “automatic riﬂe” predicted for a “farm machine”.  5  Test Label Set  2-hops  2-hops (+1K)  3-hops  3-hops (+1K)  # Candidate  Labels  1, 589  1, 589 +1000  7, 860  7, 860 +1000  ImageNet 2011 21K  20, 841  ImageNet 2011 21K (+1K)  20, 841 +1000  Model DeViSE ConSE(1) ConSE(10) ConSE(1000) DeViSE ConSE(1) ConSE(10) ConSE(1000) DeViSE ConSE(1) ConSE(10) ConSE(1000) DeViSE ConSE(1) ConSE(10) ConSE(1000) DeViSE ConSE(1) ConSE(10) ConSE(1000) DeViSE ConSE(1) ConSE(10) ConSE(1000)  1 6.0 9.3 9.4 9.2 0.8 0.2 0.3 0.3 1.7 2.6 2.7 2.6 0.5 0.2 0.2 0.2 0.8 1.3 1.4 1.3 0.3 0.1 0.2 0.2  5  Flat hit@k (%) 2 10 26.4 30.8 32.7 32.1 14.2 24.0 24.9 24.5 8.2 10.8 11.5 11.3 5.9 9.3 9.7 9.5 3.9 5.4 5.8 5.6 3.2 4.8 5.0 4.9  18.1 23.7 24.7 24.1 7.9 17.2 17.0 16.7 5.3 7.3 7.8 7.6 3.4 5.9 5.9 5.8 2.5 3.6 3.9 3.8 1.9 3.0 3.0 3.0  10.0 14.4 15.1 14.8 2.7 7.1 6.2 6.2 2.9 4.2 4.4 4.3 1.4 2.4 2.2 2.2 1.4 2.1 2.2 2.1 0.8 1.2 1.2 1.2  20 36.4 38.7 41.8 41.1 22.7 31.8 33.5 32.9 12.5 14.8 16.1 15.7 9.7 13.4 14.3 14.0 6.0 7.6 8.3 8.1 5.3 7.0 7.5 7.3  Table 1: Flat hit@k performance of DeViSE [6] and ConSE (T ) for T = 1, 10, 1000 on ImageNet zero-shot learning task. When testing the methods with the datasets indicated with (+1K), training labels are also included as potential labels within the nearest neighbor classiﬁer, hence the number of candidate labels is 1000 more. In all cases, zero-shot classes did not occur in the training set, and none of the test images is annotated with any of the training labels.  The high level of annotation granularity in Imagenet, e.g., different types of sea lions, creates chal- lenges for recognition systems which are based solely on visual cues. Using models such as ConSE and DeViSE, one can leverage the similarity between the class labels to expand the original predic- tions of the image classiﬁers to a list of similar labels, hence better retrieval rates can be achieved. We report quantitative results in terms of two metrics: “ﬂat” hit@k and “hierarchical” precision@k. Flat hit@k is the percentage of test images for which the model returns the one true label in its top k predictions. Hierarchical precision@k uses the ImageNet category hierarchy to penalize the predictions that are semantically far from the correct labels more than the predictions that are close. Hierarchical precision@k measures, on average, what fraction of the model’s top k predictions are among the k most relevant labels for each test image, where the relevance of the labels is mea- sure by their distance in the Imagenet category hierarchy. A more formal deﬁnition of hierarchical precision@k is included in the supplementary material of [6]. Hierarchical precision@1 is always equivalent to ﬂat hit@1. Table 1 shows ﬂat hit@k results for the DeViSE and three versions of the ConSE model. The ConSE model has a hyper-parameter T that controls the number of training labels used for the convex combination of semantic embeddings. We report the results for T = 1, 10, 1000 as ConSE (T ) in Table 1. Because there are only 1000 training labels, T is bounded by 1 ≤ T ≤ 1000. The results are reported on the three test datasets; the dataset difﬁculty increases from top to bottom in Table 1. For each dataset, we consider including and excluding the training labels within the label candidates used for k-nearest neighbor label ranking (i.e., Y1 in Eq. (3)). None of the images in the test set are labeled as training labels, so including training labels in the label candidate set for ranking hurts the performance as ﬁnding the correct labels is harder in a larger set. Datasets that include training labels in their label candidate set are marked by “(+1K)”. The results demonstrate  6  Test Label Set 2-hops  2-hops (+1K)  3-hops  3-hops (+1K)  ImageNet 2011 21K  ImageNet 2011 21K (+1K)  Model DeViSE ConSE(10) Softmax baseline DeViSE ConSE(10) DeViSE ConSE(10) Softmax baseline DeViSE ConSE(10) DeViSE ConSE(10) Softmax baseline DeViSE ConSE(10)  1  0  0.06 0.094  0.008 0.003 0.017 0.027  0  0.005 0.002 0.008 0.014  0  0.003 0.002  Hierarchical precision@k  2  0.152 0.214 0.236 0.204 0.234 0.037 0.053 0.053 0.053 0.061 0.017 0.025 0.023 0.025 0.029  5  0.192 0.247 0.181 0.196 0.254 0.191 0.202 0.157 0.192 0.211 0.072 0.078 0.071 0.083 0.086  10  0.217 0.269 0.174 0.201 0.260 0.214 0.224 0.143 0.201 0.225 0.085 0.092 0.069 0.092 0.097  20  0.233 0.284 0.179 0.214 0.271 0.236 0.247 0.130 0.214 0.240 0.096 0.104 0.065 0.101 0.105  Table 2: Hierarchical precision@k performance of Softmax baseline [7], DeViSE [6], and ConSE(10) on ImageNet zero-shot learning task.  that the ConSE model consistently outperforms the DeViSE on all of the datasets for all values of T . Among different versions of the ConSE, ConSE(10) performs the best. We do not directly compare against the method of Socher et al. [18], but Frome et al. [6] reported that the ranking loss used within the DeViSE signiﬁcantly outperforms the the squared loss used in [18]. Not surprisingly, the performance of the models is best when training labels are excluded from the label candidate set. All of the models tend to predict training labels more often than test labels, espe- cially at their ﬁrst few predictions. For example, when training labels are included, the performance of ConSE(10) drops from 9.4% hit@1 to 0.3% on the 2-hops dataset. This suggests that a procedure better than vanilla k-nearest neighbor search needs to be employed in order to distinguish images that do not belong to the training labels. We note that the DeViSE has a slightly lower bias towards training labels as the performance drop after inclusion of training labels is slightly smaller than the performance drop in the ConSE model. Table 2 shows hierarchical precision@k results for the Softmax baseline, DeViSE, and ConSE(10) on the zero-shot learning task. The results are only reported for ConSE (10) because T = 10 seems to perform the best among T = 1, 10, 1000. The hierarchical metric also conﬁrms that the ConSE improves upon the DeViSE for zero-shot learning. We did not compare against the Softmax baseline on the ﬂat hit@k measure, because the Softmax model cannot predict any of the test labels. However, using the hierarchical metric, we can now compare with the Softmax baseline when the training labels are also included in the label candidate set (+1K). We ﬁnd that the top k predictions of the ConSE outperform the Softmax baseline in hierarchical precision@k. Even though the ConSE model is proposed for zero-shot learning, we assess how the ConSE com- pares with the DeViSE and the Softmax baseline on the standard classiﬁcation task with the training 1000 labels, i.e., the training and test labels are the same. Table 3 and 4 show the ﬂat hit@k and hi- erarchical precision@k rates on the 1000-class learning task. According to Table 3, the ConSE(10) model improves upon the Softmax baseline in hierarchical precision at 5, 10, and 20, suggesting that the mistakes made by the ConSE model are on average more semantically consistent with the correct class labels, than the Softmax baseline. This improvement is due to the use of label em- bedding vectors learned from Wikipedia articles. However, on the 1000-class learning task, the ConSE(10) model underperforms the DeViSE model. We note that the DeViSE model is trained with respect to a k-nearest neighbor retrieval objective on the same speciﬁc set of 1000 labels, so its better performance on this task is expected. Although the DeViSE model performs better than the ConSE on the original 1000-class learning task (Table 3, 4), it does not generalize as well as the ConSE model to the unseen zero-shot learning categories (Table 1, 2). Based on this observation, we conclude that a better k-nearest neighbor  7  Hierarchical precision@k  Test Label Set  ImageNet 2011 1K  Model Softmax baseline DeViSE ConSE (1) ConSE (10) ConSE (1000)  1  0.556 0.532 0.551 0.543 0.539  2  0.452 0.447 0.422 0.447 0.442  5  0.342 0.352 0.32 0.348 0.344  10  0.313 0.331 0.297 0.322 0.319  20  0.319 0.341 0.313 0.337 0.335  Table 3: Hierarchical precision@k performance of Softmax baseline [7], DeViSE [6], and ConSE on ImageNet original 1000-class learning task.  Test Label Set  ImageNet 2011 1K  Model Softmax baseline DeViSE ConSE (1) ConSE (10) ConSE (1000)  1 55.6 53.2 55.1 54.3 53.9  Flat hit@k (%)  2 67.4 65.2 57.7 61.9 61.1  5 78.5 76.7 60.9 68.0 67.0  10 85.0 83.3 63.5 71.6 70.6  Table 4: Flat hit@k performance of Softmax baseline [7], DeViSE [6], and ConSE on ImageNet original 1000-class learning task.  classiﬁcation on the training labels, does not automatically translate into a better k-nearest neighbor classiﬁcation on a zero-shot learning task. We believe that the DeViSE model suffers from a variant of overﬁtting, which is the model has learned a highly non-linear and complex embedding function for images. This complex embedding function is well suited for predicting the training label embed- dings, but it does not generalize well to novel unseen label embedding vectors. In contrast, a simpler embedding model based on convex combination of semantic embeddings (ConSE) generalizes more reliably to unseen zero-shot classes, with little chance of overﬁtting. Implementation details. The ConSE(1) model takes the top-1 prediction of the convolutional net, and expands it to a list of labels based on the similarity of the label embedding vectors. To implement ConSE(1) efﬁciently, one can pre-compute a list of test labels for each training label, and simply predict the corresponding list based on the top prediction of the convolutional net. The top prediction of the ConSE(1) occasionally differs from the top prediction of the Softmax baseline due to a detail of our implementation. In the Imagenet experiments, following the setup of the DeViSE model, there is not a one-to-one correspondence between the class labels and the word embedding vectors. Rather, because of the way the Imagenet synsets are deﬁned, each class label is associated with several synonym terms, and hence several word embedding vectors. In the process of mapping the Softmax scores to an embedding vector, the ConSE model ﬁrst averages the word vectors associated with each class label, and then linearly combine the average vectors according to the Softmax scores. However, when we rank the word vectors to ﬁnd the k most likely class labels, we search over individual word vectors, without any averaging of the synonym words. Thus, the ConSE(1) might produce an average embedding which is not the closest vector to any of the word vectors corresponding to the original class label, and this results in a slight difference in the hit@1 scores for ConSE(1) and the Softmax baseline. While other alternatives exist for this part of the algorithm, we intentionally kept the ranking procedure exactly the same as the DeViSE model to perform a direct comparison.  6 Conclusion  The ConSE approach to mapping images into a semantic embedding space is deceptively simple. Treating classiﬁer scores as weights in a convex combination of word vectors is perhaps the most direct method imaginable for recasting an n-way image classiﬁcation system as image embedding system. Yet this method outperforms more elaborate joint training approaches both on zero-short learning and on performance metrics which weight errors based on semantic quality. The success of this method undoubtedly lays is its ability to leverage the strengths inherent in the state-of-the-art image classiﬁer and the state-of-the-art text embedding system from which it was constructed.  8  While it draws from their strengths, we have no reason to believe that ConSE depends on the details the visual and text models from which it is constructed. In particular, though we used a deep con- volutional network with a Softmax classiﬁer to generate the weights for our linear combination, any visual object classiﬁcation system which produces relative scores over a set of classes is compatible with the ConSE framework. Similarly, though we used semantic embedding vectors which were the side product of an unsupervised natural language processing task, the ConSE framework is applica- ble to other alternative representation of text in which similar concepts are nearby in vector space. The choice of the training corpus for the word embeddings affects the results too. One feature of the ConSE model which we did not exploit in our experiments is its natural represen- tation of conﬁdence. The norm of the vector that ConSE assigns to an image is a implicit expression of the model’s conﬁdence in the embedding of that image. Label assignments about which the Softmax classiﬁer is uncertain be given lower scores, which naturally reduces the magnitude of the ConSE linear combination, particularly if Softmax probabilities are used as weights without renor- malization. Moreover, linear combinations of labels with disparate semantics under the text model will have a lower magnitude than linear combinations of the same number of closely related labels. These two effects combine such that ConSE only produces embeddings with an L2-norm near 1.0 for images which were either nearly completely unambiguous under the image model or which were assigned a small number of nearly synonymous text labels. We believe that this property could be fruitfully exploited in settings where conﬁdence is a useful signal.  replacement. CVPR, 2005.  Innovations in Machine Learning, 2006.  database. CVPR, 2009.  References [1] E. Bart and S. Ullman. Cross-generalization: learning novel classes from a single example by feature  [2] Y. Bengio, H. Schwenk, J.-S. Sen´ecal, F. Morin, and J.-L. Gauvain. Neural probabilistic language models.  [3] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image  [4] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describing objects by their attributes. CVPR, 2009. [5] L. Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object categories. IEEE Trans. PAMI, 28:594–  611, 2006.  [6] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, M. Ranzato, and T. Mikolov. Devise: A deep  visual-semantic embedding model. NIPS, 2013.  [7] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deep convolutional neural net-  [8] B. M. Lake, R. Salakhutdinov, J. Gross, and J. B. Tenenbaum. One shot learning of simple visual concepts.  [9] C. H. Lampert, H. Nickisch, and S. Harmeling. Learning to detect unseen object classes by between-class  works. NIPS, 2012.  CogSci, 2011.  attribute transfer. CVPR, 2009.  [10] H. Larochelle, D. Erhan, and Y. Bengio. Zero-data learning of new tasks. AAAI, 2008. [11] T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka. Metric learning for large scale image classiﬁcation:  Generalizing to new classes at near-zero cost. ECCV, 2012.  [12] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient estimation of word representations in vector  [13] E. G. Miller, N. E. Matsakis, and P. A. Viola. Learning from one example through shared densities on  [14] M. Palatucci, D. Pomerleau, G. E. Hinton, and T. M. Mitchell. Zero-shot learning with semantic output  space. ICLR, 2013.  transforms. CVPR, 2000.  codes. NIPS, 2009.  [15] D. Parikh and K. Grauman. Relative attributes. ICCV, 2011. [16] M. Rohrbach, M. Stark, and B. Schiele. Evaluating knowledge transfer and zero-shot learning in a large-  [17] M. Rohrbach, M. Stark, and B. Schiele. Evaluating knowledge transfer and zero-shot learning in a large-  [18] R. Socher, M. Ganjoo, H. Sridhar, O. Bastani, C. D. Manning, and A. Y. Ng. Zero-shot learning through  [19] J. Weston, S. Bengio, and N. Usunier. Wsabie: Scaling up to large vocabulary image annotation. IJCAI,  scale setting. CVPR, 2011.  scale setting. CVPR, 2011.  cross-modal transfer. NIPS, 2013.  2011.  9  ","Several recent publications have proposed methods for mapping images intocontinuous semantic embedding spaces. In some cases the embedding space istrained jointly with the image transformation. In other cases the semanticembedding space is established by an independent natural language processingtask, and then the image transformation into that space is learned in a secondstage. Proponents of these image embedding systems have stressed theiradvantages over the traditional \nway{} classification framing of imageunderstanding, particularly in terms of the promise for zero-shot learning --the ability to correctly annotate images of previously unseen objectcategories. In this paper, we propose a simple method for constructing an imageembedding system from any existing \nway{} image classifier and a semantic wordembedding model, which contains the $\n$ class labels in its vocabulary. Ourmethod maps images into the semantic embedding space via convex combination ofthe class label embedding vectors, and requires no additional training. We showthat this simple and direct method confers many of the advantages associatedwith more complex image embedding schemes, and indeed outperforms state of theart methods on the ImageNet zero-shot learning task."
1312.6204,2014,One-Shot Adaptation of Supervised Deep Convolutional Models  ,"['Trevor Darrell', 'Eric Tzeng', 'Yangqing Jia', 'Judy Hoffman', 'Kate Saenko', 'Jeff Donahue']",https://arxiv.org/pdf/1312.6204.pdf,"4 1 0 2     b e F 8 1         ]  V C . s c [      2 v 4 0 2 6  .  2 1 3 1 : v i X r a  One-Shot Adaptation of Supervised Deep  Convolutional Models  Judy Hoffman, Eric Tzeng, Jeff Donahue  {jhoffman,etzeng,jdonahue}@eecs.berkeley.edu  UC Berkeley, EECS & ICSI  Yangqing Jia∗ Google Research  jiayq@google.com  Kate Saenko  UMass Lowell, CS & ICSI  saenko@cs.uml.edu  Trevor Darrell  UC Berkeley, EECS & ICSI  trevor@eecs.berkeley.edu  Abstract  Dataset bias remains a signiﬁcant barrier towards solving real world computer vi- sion tasks. Though deep convolutional networks have proven to be a competitive approach for image classiﬁcation, a question remains: have these models have solved the dataset bias problem? In general, training or ﬁne-tuning a state-of- the-art deep model on a new domain requires a signiﬁcant amount of data, which for many applications is simply not available. Transfer of models directly to new domains without adaptation has historically led to poor recognition performance. In this paper, we pose the following question: is a single image dataset, much larger than previously explored for adaptation, comprehensive enough to learn general deep models that may be effectively applied to new image domains? In other words, are deep CNNs trained on large amounts of labeled data as suscep- tible to dataset bias as previous methods have been shown to be? We show that a generic supervised deep CNN model trained on a large dataset reduces, but does not remove, dataset bias. Furthermore, we propose several methods for adaptation with deep models that are able to operate with little (one example per category) or no labeled domain speciﬁc data. Our experiments show that adaptation of deep models on benchmark visual domain adaptation datasets can provide a signiﬁcant performance boost.  1  Introduction  Supervised deep convolutional neural networks (CNNs) trained on large-scale classiﬁcation tasks have been shown to learn impressive mid-level structures and obtain high levels of performance on contemporary classiﬁcation challenges [3, 23]. These models generally assume extensive training using labeled data, and testing is limited to data from the same domain. In practice, however, the images we would like to classify are often produced under different imaging conditions or drawn from a different distribution, leading to a domain shift. Scaling such models to new domains remains an open challenge. Deep CNNs require large amounts of training data to learn good mid-level convolutional models and ﬁnal fully-connected classiﬁer stages. While the continuing expansion of web-based datasets like ImageNet [3] promises to produce labeled data for almost any desired category, such large-scale supervised datasets may not include images of the category across all domains of practical interest. Earlier deep learning efforts addressed this challenge by learning layers in an unsupervised fashion using unlabeled data to discover salient mid-level structures [6, 8]. While such approaches are  ∗This work was completed while Yangqing Jia was a graduate student at UC Berkeley  1  appealing, they have heretofore been unable to match the level of performance of supervised models, and unsupervised training of networks with the same level of depth as [17] remains a challenge. Unfortunately, image datasets are inherently biased [21]. Theoretical [2, 4] and practical results from [20, 21] have shown that supervised methods’ test error increases in proportion to the difference between the test and training input distribution. Many visual domain adaptation methods have been put forth to compensate for dataset bias [7, 22, 1, 20, 18, 16, 13, 12, 14, 15], but are limited to shallow models. Evaluation for image category classiﬁcation across visually distinct domains has focused on the Ofﬁce dataset, which contains 31 image categories and 3 domains [20]. Recently, [9] showed that using the deep mid-level features learned on ImageNet, instead of the more conventional bag-of-words features, effectively removed the bias in some of the domain adaptation settings in the Ofﬁce dataset [20]. However, [9] limited their experiments to small-scale source domains found only in Ofﬁce, and evaluated on only a subset of relevant layers. Yet until now, almost none of the previous domain adaptation studies used ImageNet as the source domain, nor utilized the full set of parameters of a deep CNN trained on source data. Recent work by Rodner et al. [19] attempted to adapt from ImageNet to the SUN dataset, but did not take advantage of deep convolutional features. In this paper, we ask the question: will deep models still suffer from dataset bias when trained with all layers of the CNN and a truly large scale source dataset? Here, we provide the ﬁrst evaluation of domain adaptation with deep learned representations in its most natural setting, in which all of ImageNet is used as source data for a target category. We use the 1.2 million labeled images available in the 2012 ImageNet 1000-way classiﬁcation dataset [3] to train the model in [17] and evaluate its generalization to the Ofﬁce dataset. This constitutes a three orders of magnitude increase in source data compared to the several thousand images available for the largest domain in Ofﬁce. We ﬁnd that it is easier to adapt from ImageNet than from previous smaller source domains, but that dataset bias remains a major issue. Fine-tuning the parameters on the small amount of labeled target data (we consider one-shot adaptation) turns out to be unsurprisingly problematic. Instead, we propose a simple yet intuitive adaptation method: train a ﬁnal domain-adapted classiﬁcation “layer” using various layers of the pre-trained network as features, without any ﬁne-tuning its parameters. We provide a comprehensive evaluation of existing methods for classiﬁer adaptation as applied to each of the fully connected layers of the network, including the last, task-speciﬁc classiﬁcation layer. When adapting from ImageNet to Ofﬁce, it turns out to be possible to achieve target domain performance on par with source domain performance using only a single labeled example per target category. We examine both the setting where there are a few labeled examples from the target domain (super- vised adaptation) and the setting where there are no labeled target examples (unsupervised adap- tation). We also describe practical solutions for choosing between the various adaptation methods based on experimental constraints such as limited computation time.  2 Background: Deep Domain Adaptation Approaches  For our task we consider adapting between a large source domain and a target domain with few or or no labeled examples. A typical approach to domain adaptation or transfer learning with deep architectures is to take the representation learned via back-propagation on a large dataset, and then transfer the representation to a smaller dataset by ﬁne-tuning, i.e. backpropagation at a lower learn- ing rate [11, 23]. However, ﬁne-tuning requires an ample amount of labeled target data and so should not be expected to work well when we consider the very sparse label condition, such as the one-shot learning scenario we evaluate below, where we have just one labeled example per category in the target domain. In fact, in our experiments under this setting, ﬁne-tuning actually reduces performance. Speciﬁcally, on the ImageNet→Webcam task reported in Section 4, using the ﬁnal output layer as a predictor in the target domain received 66% accuracy, while using the ﬁnal output layer after ﬁne tuning produced a degraded accuracy of 61%. A separate method that was recently proposed for deep adaptation is called Deep Learning for do- main adaptation by Interpolating between Domains (DLID) [5]. This method learns multiple unsu- pervised deep models directly on the source, target, and combined datasets and uses a representation  2  which is the concatenation of the outputs of each model as its adaptation approach. While this was shown to be an interesting approach, it is limited by its use of unsupervised deep structures. In general, unsupervised deep convolutional models have been unable to achieve the performance of supervised deep CNNs. However, training a supervised deep model requires sufﬁcient labeled data. Our insight is that the extensive labeled data available in the source domain can be exploited using a supervised model without requiring a signiﬁcant amount of labeled target data. Therefore, we propose using a supervised deep source model with supervised or unsupervised adap- tation algorithms that are applied to models learned on the target data directly. This hybrid approach will utilize the strong representation available from the supervised deep model trained on a large source dataset while requiring only enough target labeled data to train a shallow model with far fewer parameters. Speciﬁcally, we consider training a convolutional neural network (CNN) on the source domain and using that network to extract features on the target data that can then be used to train an auxiliary shallow learner. For extracting features from the deep source model, we follow the setup of Donahue et al. [9], which extracts a visual feature DeCAF from the ImageNet-trained architecture of [17].  3 Adapting Deep CNNs with Few Labeled Target Examples  We propose a general framework for selectively adapting the parameters of a convolutional neural network (CNN) whose representation and classiﬁer weights are trained on a large-scale source do- main, such as ImageNet. Our framework adds a ﬁnal domain-adaptive classiﬁcation “layer” that takes the activations of one of the existing network’s layers as input features. Note that the net- work cannot be effectively ﬁne-tuned without access to more labeled target data. This adapted layer is a linear classiﬁer that combines source and target training data using an adaptation method. To demonstrate the generality of our framework, we select a representative set of popular linear clas- siﬁer adaptation approaches that we empirically evaluate in Section 4. We separate our discussion into the set of supervised and unsupervised adaptation settings. Below we denote the features extracted over the source domain as X and the features extracted over the target domain as ˜X. Similarly, we denote the source domain image classiﬁer as θ and the target domain image classiﬁer as ˜θ.  3.1 Unsupervised Adaptation  Many unsupervised adaptation techniques seek to minimize the distance between subspaces that represent the source and target domains. We denote these subspaces as U and ˜U, respectively. GFK [12] The Geodesic Flow Kernel (GFK) method [12] is an unsupervised domain adaptation approach which seeks embeddings for the source and target points that minimize domain shift. Inputs to the method are U and ˜U, lower-dimensional embeddings of the source and target domains (e.g. from principal component analysis). The method constructs the geodesic ﬂow φ(t) along the manifold of subspaces such that U = φ(0) and ˜U = φ(1). Finally, a transformation G is dt using a closed-form solution, and classiﬁcation is  constructed by computing G = (cid:82) 1  (cid:124) 0 φ(t)φ(t)  performed by training an SVM on the source data X and transformed target data G ˜X.  SA [10] The Subspace Alignment (SA) method [10] also begins with low-dimensional em- beddings of the source and target domains U and ˜U, respectively. It seeks to minimize in M, a transformation matrix, the objective (cid:107)U M − ˜U(cid:107)2 F . The analytical solution to this objective (cid:124) ˜U. Given M∗, an SVM is trained on source data X and transformed target data is M∗ = U (cid:124) ˜X. U M∗ ˜U  3.2 Supervised Adaptation  Late Fusion Perhaps the simplest supervised adaptation method is to independently train a source and target classiﬁer and combine the scores of the two to create a ﬁnal scoring function. We call this approach Late Fusion. It has been explored by many for a simple adaptation approach. Let us  3  denote the score from the source classiﬁer as vs and the score from the target classiﬁer as vt. For our experiments we explore two methods of combining these scores, which are described below:  • Max: Produce the scores of both the source and target classiﬁer and simply choose the max of the two as the ﬁnal score for each example. Therefore, vadapt = max(vs, vt). • Linear Interpolation: Set the score for a particular example to equal the convex combi- nation of the source and target classiﬁer scores, vadapt = (1 − α)vs + αvt. This method requires setting a hyperparameter, α, which determines the weights of the source and target classiﬁers.  Late Fusion has two major advantages: it is easy to implement, and the source classiﬁer it uses may be precomputed to make adaptation very fast. In the case of the linear interpolation combination rule, however, this method can potentially suffer from having a sensitive hyperparameter. We show a hyperparameter analysis in Section 4. Daum´e III [7] This simple feature replication method was proposed for domain adaptation by [7]. The method augments feature vectors with a source component, a target component, and a shared component. Each source data point x is augmented to x(cid:48) = (x; x; 0), and each target data point ˜x is augmented to ˜x(cid:48) = ( ˜x; 0; ˜x). Finally, an SVM is trained on the augmented source and target data—a relatively expensive procedure given the potentially large size of the source domain and the tripled augmented feature dimensionality. PMT [1] This classiﬁer adaptation method, Projective Model Transfer (PMT), proposed by [1], is a variant of adaptive SVM. It takes as input a classiﬁer θ pre-trained on the source domain. PMT- SVM learns a target domain classiﬁer ˜θ by adding an extra term to the usual SVM objective which between the target and source hyperplanes. This  regularizes the angle α( ˜θ, θ) = cos−1(cid:16) θ  (cid:17)  (cid:124) ˜θ (cid:107)θ(cid:107)(cid:107) ˜θ(cid:107)  results in the following loss function: (cid:107) ˜θ(cid:107)2  LP M T ( ˜θ) =  1 2  2 +  (cid:107) ˜θ(cid:107)2  2 sin2 α( ˜θ, θ) + (cid:96)hinge( ˜X, ˜Y ; ˜θ) ,  Γ 2  (1)  where (cid:96)hinge(X, Y ; θ) denotes the SVM hinge loss of a data matrix X, label vector Y , and classi- ﬁer hyperplane θ, and Γ is a hyperparameter which, as it increases, enforces more transfer from the source classiﬁer. MMDT [15] The Max-margin Domain Transforms (MMDT) method from [15] jointly optimizes an SVM-like objective over a feature transformation matrix A mapping target points to the source feature space and classiﬁer parameters θ in the source feature space. In particular, MMDT mini- mizes the following loss function (assuming a binary classiﬁcation task to simplify notation, and with (cid:96)hinge deﬁned as in PMT):  LMM DT (θ, A) =  (cid:107)θ(cid:107)2  2 +  1 2  1 2  (cid:107)A − I(cid:107)2  F + Cs(cid:96)hinge(X, Y ; θ) + Ct(cid:96)hinge(A ˜X, ˜Y ; θ) ,  (2)  where Cs and Ct are hyperparameters controlling the importance of correctly classifying the source and target points (respectively).  4 Evaluation  4.1 Datasets  The Ofﬁce [20] dataset is a collection of images from three distinct domains: Amazon, DSLR, and Webcam. The 31 categories in the dataset consist of objects commonly encountered in ofﬁce settings, such as keyboards, ﬁle cabinets, and laptops. Of these 31 categories, 16 overlap with the categories present in the 1000-category ImageNet classiﬁcation task1. Thus, for our experiments, we limit ourselves to these 16 classes. In our experiments using Amazon as a source domain, we follow the standard training protocol for this dataset of using 20 source examples per category [20, 12], for a total of 320 images.  1 The 16 overlapping categories are backpack, bike helmet, bottle, desk lamp, desktop computer, ﬁle cabinet, keyboard, laptop computer, mobile phone, mouse, printer, projector, ring binder, ruler, speaker, and trash can.  4  ImageNet [3] is the largest available dataset of image category labels. We use 1000 categories’ worth of data (1.2M images) to train the network, and use the 16 categories that overlap with Ofﬁce (approximately 1200 examples per category or ≈20K images total) as labeled source classiﬁer data.  4.2 Experimental Setup & Baselines  For our experiments, we use the fully trained deep CNN model described in Section 2, extracting feature representations from three different layers of the CNN. We then train a source classiﬁer using these features on one of two source domains, and adapt to the target domain. The source domains we consider are either the Amazon domain, or the corresponding 16-category ImageNet subset where each category has many more examples. We focus on the Webcam domain as our target (test) domain, as Amazon-to-Webcam was shown to be the only challenging shift in [9] (the DSLR domain is much more similar to Webcam and did not require adaptation when using deep mid-level features). This combination exempliﬁes the shift from online web images to real- world images taken in typical ofﬁce/home environments. Note that, regardless of the source domain chosen to learn the classiﬁer, ImageNet data from all 1000 categories was used to train the network. In addition, for the supervised adaptation setting we assume access to only a single example per category from the target domain (Webcam). Each method is then evaluated across 20 random train/test splits, and we report averages and standard errors for each setting. For each random train/test split we choose one example for training and 10 other examples for testing (so there is a balanced test set across categories). Therefore, each test split has 160 examples. The unsupervised adaptation methods operate in a transductive setting, so the target subspaces are learned from the unlabeled test data.  Non-adaptive Baselines evaluate using the following non-adaptive baselines.  In addition to the adaptation methods outlined in Section 3, we also  • SVM (source only): A support vector machine trained only on source data. • SVM (target only): A support vector machine trained only on target data. • SVM (source and target): A support vector machine trained on both source and target data. To account for the large discrepancy between the number of training data points in the source and target domains, we weighted the data points such that the constraints from the source and target domains effectively contribute equally to the optimization problem. Speciﬁcally, each source data point receives a weight of , and each target data point receives a weight of , where ns, nt denote the number of data points in the source and target, respectively.  ns+nt  ns+nt  ns  nt  Many of the adaptation methods we evaluate have hyperparameters that must be cross-validated for use in practice, so we set the parameters of the adaptation techniques as follows. First, the C value used for C-SVM in the classiﬁer for all methods is set to C = 1. Without any validation data we are not able to tune this parameter properly, so we choose to leave it as the default value. Since all methods we report require setting of this parameter, we feel that the relative comparisons between methods is sound even if the absolute numbers could be improved with a new setting for C. For Daum´e III and MMDT, which look at the source and target data simultaneously, we use the same weighting scheme as we did for the source and target SVM. Late Fusion with the linear interpolation combination rule is reported across hyperparameter settings in Figure 1(a) to help understand how performance varies as we trade off emphasis between the learned classiﬁers from the source and target domains. Again, we do not have the validation data to tune this parameter so we report in the tables the performance averaged across parameter settings. The plot vs α indicates that there is usually a best parameter setting that could be learned with more available data. For PMT, we choose Γ = 1000, which corresponds to allowing a large amount of transfer from the source classiﬁer to the target classiﬁer. We do this because the source-only classiﬁer is stronger than the target-only classiﬁer (with ImageNet source). For the unsupervised methods GFK and SA, again we evaluated a variety of subspace dimensionalities and Figure 1(b) shows that the overall method performance does not vary signiﬁcantly with the dimensionality choice.  5  Training Data  Amazon Webcam Amazon Amazon  Adaptation Method SVM (source only) SVM (target only) GFK [12] SA [10] SVM (source and target) Late Fusion (Max) Late Fusion (Lin. Int. Avg) Daum´e III [7] PMT [1] MMDT [15] Late Fusion (Lin. Int. Oracle) Amazon+Webcam  DeCAF6 50.28 ± 1.8 62.28 ± 1.8 53.13 ± 1.1 51.74 ± 1.2 Amazon+Webcam 62.91 ± 1.8 Amazon+Webcam 65.35 ± 1.7 Amazon+Webcam 63.23 ± 1.4 Amazon+Webcam 68.89 ± 1.9 Amazon+Webcam 64.84 ± 1.5 Amazon+Webcam 65.47 ± 1.8 71.1 ± 1.7  DeCAF7  54.08 ± 1.7 64.97 ± 1.8 53.39 ± 1.1 53.86 ± 1.0 65.82 ± 1.4 58.42 ± 1.1 64.29 ± 1.3 72.09 ± 1.4 65.63 ± 1.8 68.10 ± 1.5 72.82 ± 1.4  Table 1: Amazon→Webcam adaptation experiment. We show here multiclass accuracy on the target domain test set for both supervised and unsupervised adaptation experiments across the two fully connected layer features (similar to [9], but with one labeled target example). The best performing unsupervised adaptation algorithms are shown in blue and the best performing supervised adaptation algorithms are shown in red.  4.3 Effect of Source Domain Size  Previous studies considered source domains from the Ofﬁce dataset. In this section, we ask what happens when an orders-of-magnitute larger source dataset is used. For completeness we begin by evaluating Amazon as a source domain. Preliminary results on this setting are reported in [9], here we extend the comparison here by presenting the results with more adaptation algorithms and more complete evaluation of hyperparameter settings. Table 1 presents multiclass accuracies for each algorithm using either layer 6 or 7 from the deep network, which corresponds to the output from each of the fully connected layers. An SVM trained using only Amazon data achieves 78.6% in-domain accuracy (tested on the same domain) when using the DeCAF6 feature and 80.2% in-domain accuracy when using the DeCAF7 feature. These numbers are signiﬁcantly higher than the performance of the same classiﬁer on Webcam test data, indicating that even with the DeCAF features, there is a still a domain shift between the Amazon and Webcam datasets. Next, we consider an unsupervised adaptation setting where no labeled examples are available from the target dataset. In this scenario, we apply two state-of-the-art unsupervised adaptation methods, GFK [12] and SA [10]. Both of these methods make use of a subspace dimensionality hyperpa- rameter. We show the results using a 100-dimensional subspace and leave the discussion of setting this parameter until Section 4.6. For this shift the adaptation algorithms increase performance when using the layer 6 feature, but offer no additional improvement when using the layer 7 feature. We ﬁnally assume that a single example per category is available in the target domain. As the bottom rows of Table 1 show, supervised adaptation algorithms are able to provide signiﬁcant improvement regardless of the feature space chosen, even in the one-shot scenario. For this experiment we noticed that using the second fully connected layer (DeCAF7) was a stronger overall feature in general.  4.4 Adapting with a Large Scale Source Domain  We next address one of the main questions of this paper: Is there still a domain shift when using a large source dataset such as ImageNet? To begin to answer this question we follow the same experimental paradigm as the previous experiment, but use ImageNet as our source dataset. The results are shown in Table 2. Again, we ﬁrst verify that the source only SVM achieves higher performance when tested on in- domain data than on Webcam data. Indeed, for the 16 overlapping labels, the source SVM pro- duces 62.50% accuracy on ImageNet data using DeCAF6 features and 74.50% accuracy when using  6  Adaptation Method SVM (source only) SVM (target only) GFK [12] SA [10] SVM (source and target) Late Fusion (Max) Late Fusion (Lin. Int. Avg) Daum´e III [7] PMT [1] MMDT [15] Late Fusion (Lin. Int. Oracle)  Training Data  ImageNet Webcam ImageNet ImageNet  DeCAF6 53.51 ± 1.1 62.28 ± 1.8 65.16 ± 1.1 59.30 ± 1.4 ImageNet+Webcam 56.68 ± 1.2 ImageNet+Webcam 59.59 ± 1.3 ImageNet+Webcam 60.64 ± 1.3 ImageNet+Webcam 59.21 ± 1.7 ImageNet+Webcam 66.30 ± 2.1 ImageNet+Webcam 59.21 ± 1.3 ImageNet+Webcam 71.65 ± 2.0  DeCAF7 59.15 ± 1.1 64.97 ± 1.8 67.97 ± 1.4 66.08 ± 1.4 66.93 ± 1.3 68.86 ± 1.2 66.45 ± 1.1 71.39 ± 1.5 69.81 ± 1.8 67.75 ± 1.4 76.76 ± 1.3  Table 2: ImageNet→Webcam adaptation experiment. Comparison of unsupervised and supervised adaptation algorithms on the ImageNet to Webcam domain shift. Results are computed using the outputs of each of the fully connected layers as features. The best supervised adaptation performance is indicated in red and the best unsupervised adaptation performance is highlighted in blue.  Adaptation Method SVM (source only) SVM (target only) GFK [12] SA [10] SVM (source and target) Late Fusion (Max) Late Fusion (LinInt Avg) Daum´e III [7] PMT [1] MMDT [15] Late Fusion (Lin. Int. Oracle)  Training Data  Source Webcam Source Source  Source+Webcam Source+Webcam Source+Webcam Source+Webcam Source+Webcam Source+Webcam Source+Webcam  Source=ImageNet  66.23 ± 0.8 63.13 ± 1.9 68.73 ± 1.1 66.08 ± 1.1 75.13 ± 1.1 71.77 ± 1.4 70.56 ± 1.2 77.15 ± 1.1 70.28 ± 1.8 73.96 ± 1.2 76.61 ± 1.5  Source=Amazon  53.23 ± 1.6 63.13 ± 1.9 54.56 ± 1.2 55.98 ± 1.0 63.20 ± 1.7 62.25 ± 0.8 64.56 ± 1.3 70.51 ± 1.7 66.77 ± 2.1 66.23 ± 1.4 71.49 ± 1.3  Table 3: ImageNet→Webcam and Amazon→Webcam adaptation experiments using DeCAF8, the label activations of the CNN trained on the full ImageNet data. Again, we compare multiclass accu- racy of various unsupervised and supervised adaptation methods. The best performing unsupervised adaptation algorithm is shown in blue and the best performing supervised adaptation algorithms are shown in red.  DeCAF7 features. Compare this to the 54% and 59% for Webcam evaluation and a dataset bias is still clearly evident. Note that when using ImageNet as a source domain, overall performance of all algorithms improves. In addition, unsupervised adaptation approaches are more effective than for the smaller source do- main experiment.  4.5 Adapting a Pre-trained Classiﬁer to a New Label Set  DeCAF8 differs from the other DeCAF features in that it constitutes the 1000 activations corre- sponding to the 1000 labels in the ImageNet classiﬁcation task. In the CNN proposed by [17], these activations are fed into a softmax unit to compute the label probabilities. We instead experiment with using the DeCAF8 activations directly as a feature representation, which is akin to training another classiﬁer using the output of the 1000-way CNN classiﬁer. Table 3 shows results for various adaptation techniques using both ImageNet and Amazon as source domains. We use the same setup as before, but instead use DeCAF8 as the feature representation.  7  The ImageNet results are uniformly better with DeCAF8 than with DeCAF6 or DeCAF7, likely due to the fact that DeCAF8 was explicitly trained on ImageNet data to effectively discriminate between ImageNet categories. Because it can more effectively classify images from the source domain, it is able to better adapt from the source domain to the target domain. However, we see a negligible difference in performance for Amazon, with performance actually de- creasing with respect to DeCAF7 for certain adaptation methods. We believe this is because the ﬁnal activation vector is too speciﬁc to the 1000-way ImageNet task, and that DeCAF7 provides a more general representation that is better suited to the Amazon domain. This, in turn, results in improved adaptation. In general, however, the difference between the various DeCAF representations with Amazon as a source are small enough to be insigniﬁcant.  4.6 Analysis and Practical Considerations  Our adaptation experiments show that, despite its large size, even ImageNet is not large enough to cover all domains, and that traditional domain adaptation methods go a long way in increasing performance and mitigating the effects of this shift. Depending on the characteristics of the problem at hand, our results suggest different methods may be most suitable. If no labels exist in the target domain, then there are unsupervised adaptation algorithms that are easy to use and fast to compute at adaptation time, yet still achieve increased performance over source- only methods. For this scenario, we experimented with two subspace alignment based methods that both require setting a parameter that indicates the dimensionality of the input subspaces. Figure 1(b) shows the effect that changing the subspace dimensionality has on the overall method performance. In general, we noticed that these methods were not particularly sensitive to this parameter so long as the dimensionality remains larger than the number of categories in our label set. Below this threshold, the subspace is less likely to capture all important discriminative information needed for classiﬁcation. In the case where we have a large source dataset and a limited number of labeled target examples, it may be preferable to compute source classiﬁer parameters in advance, then examine only the source parameters and the target data at adaptation time. Examples of these kinds of methods are Late Fusion and PMT. These methods are unaffected by the number of data points in the source domain at adaptation time, and can thus be applied quickly. In our experiments, we found that a properly tuned Late Fusion classiﬁer with linear interpolation was the fastest and most effective approach. Figure 1(a) shows the performance of linear interpolation Late Fusion as we vary the hyperparameter α. Although the method is sensitive to α, we found that for both source domains, the basic strategy of setting α around 0.8 provides a close approximation to optimal performance. This setting can be interpreted as trusting the target classiﬁer more than the source, but not so much as to completely discount the information available from the source classiﬁer. In each table we report both the performance of linear interpolation both averaged across hyper parameter settings α ∈ [0, 1] as well as the performance of linear interpolation with the best possible setting of α per experiment – this is denoted as “Oracle” performance. If there are no computational constraints and there are very few labels in the target domain, the best-performing method seems to be the “frustratingly easy” approach originally proposed by Daum´e III [7] and applied again for deep models in [5]. Finally, we found that feature representation can have a signiﬁcant impact on adaptation perfor- mance. Our results show that ImageNet as source performs best with the DeCAF8 representation, whereas Amazon as source performs best with the DeCAF7 representation. This, combined with our intuition, seems to indicate that for adaptation from source domains other than ImageNet, an intermediate representation other than DeCAF8 is more powerful for adaptation, whereas ImageNet classiﬁcation works best with the full representation that was trained on it.  5 Conclusion  In this paper, we presented the ﬁrst evaluation of domain adaptation from a large-scale source dataset with deep features. We demonstrated that, although using ImageNet as a source domain generalizes  8  (a) Late Fusion with Linear Interpolation  (b) Unsupervised Methods  Figure 1: Evaluation of hyperparameters for domain adaptation methods. (a) Analysis of the com- bination hyperparameter α for Late Fusion with linear interpolation. (b) Analysis of the subspace dimensionality for the unsupervised adaptation algorithms  better than other smaller source domains, there is still a domain shift when adapting to other visual domains. Our experimental results show that deep adaptation methods can go a long way in mitigating the ef- fects of this domain shift. Based on our results, we also provided a set of practical recommendations for choosing a feature representation and adaptation method accounting for constraints on runtime and accuracy. There are a number of interesting directions to take given our results. First we notice that though DeCAF8 is the strongest feature to use for learning a classiﬁer on ImageNet data, DeCAF7 is actually a better feature to use with the Amazon source domain and the Webcam target domain. This could lead to a hybrid approach where one uses different feature representations for the various domains and produces a combined adapted model. Another interesting direction that should be explored is to integrate the adaption algorithms into the deep models explicitly and even allow for feedback between the two stages. Current deep models although allow information ﬂow between the ﬁnal classiﬁer and the representation learning architecture. We feel that the next step is to have a separate task speciﬁc adaptable layer that does not simply learn a new ﬁnal layer, but instead learns a separate, but equivalent ﬁnal layer, that is regularized by the ﬁnal layer learned on the source dataset. This future work is a natural extension of the result we have shown in this paper: that pre-trained deep representations with large source domains can be effectively adapted to new target domains using only shallow, linear adaptation methods, and that in cases where the target data is limited, this approach is the best way to mitigate dataset bias.  References [1] Y. Aytar and A. Zisserman. Tabula rasa: Model transfer for object category detection. In Proc. ICCV,  2011.  [2] Shai Ben-David, John Blitzer, Koby Crammer, Fernando Pereira, et al. Analysis of representations for  domain adaptation. Proc. NIPS, 2007.  [3] A. Berg, J. Deng, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge 2012. 2012. [4] John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman. Learning bounds  for domain adaptation. In Proc. NIPS, 2007.  [5] S. Chopra, S. Balakrishnan, and R. Gopalan. DLID: Deep learning for domain adaptation by interpolating  between domains. In ICML Workshop on Challenges in Representation Learning, 2013.  [6] A. Coates, A. Karpathy, and A. Ng. Emergence of object-selective features in unsupervised feature  learning. In Proc. NIPS, 2012.  9  00.20.40.60.8150556065707580αMulticlass Accuracy  LinInt: α svmwebcam + (1−α)svmImageNetLinInt: α svmwebcam + (1−α) svmamazonsvmImageNetsvmamazonsvmwebcam102030405060708090100354045505560657075Subspace DimensionalityMulticlass Accuracy  ImageNet Source: SAImageNet Source: GFKAmazon Source: SAAmazon Source: GFK[7] H. Daum´e III. Frustratingly easy domain adaptation. In ACL, 2007. [8] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. Le, M. Mao, M. Ranzato, A. Senior, P. Tucker,  K. Yang, and A. Ng. Large scale distributed deep networks. In Proc. NIPS, 2012.  [9] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. DeCAF: A Deep Convo-  lutional Activation Feature for Generic Visual Recognition. arXiv e-prints, 2013.  [10] B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars. Unsupervised visual domain adaptation using  subspace alignment. In Proc. ICCV, 2013.  [11] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection  and semantic segmentation. arXiv e-prints, 2013.  [12] B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic ﬂow kernel for unsupervised domain adaptation. In  Proc. CVPR, 2012.  [13] R. Gopalan, R. Li, and R. Chellappa. Domain adaptation for object recognition: An unsupervised ap-  proach. In Proc. ICCV, 2011.  [14] J. Hoffman, B. Kulis, T. Darrell, and K. Saenko. Discovering latent domains for multisource domain  adaptation. In Proc. ECCV, 2012.  [15] J. Hoffman, E. Rodner, J. Donahue, K. Saenko, and T. Darrell. Efﬁcient learning of domain-invariant  image representations. In Proc. ICLR, 2013.  [16] A. Khosla, T. Zhou, T. Malisiewicz, A. Efros, and A. Torralba. Undoing the damage of dataset bias. In  Proc. ECCV, 2012.  [17] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classiﬁcation with deep convolutional neural  networks. In Proc. NIPS, 2012.  [18] B. Kulis, K. Saenko, and T. Darrell. What you saw is not what you get: Domain adaptation using  asymmetric kernel transforms. In Proc. CVPR, 2011.  [19] Erik Rodner, Judy Hoffman, Jeff Donahue, Trevor Darrell, and Kate Saenko. Towards adapting imagenet to reality: Scalable domain adaptation with implicit low-rank transformations. CoRR, abs/1308.4200, 2013.  [20] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual category models to new domains. In Proc.  ECCV, 2010.  [21] A. Torralba and A. Efros. Unbiased look at dataset bias. In Proc. CVPR, 2011. [22] J. Yang, R. Yan, and A. Hauptmann. Adapting SVM classiﬁers to data with shifted distributions. In ICDM  Workshops, 2007.  [23] M. Zeiler and R. Fergus. Visualizing and Understanding Convolutional Networks. ArXiv e-prints, 2013.  10  ","Dataset bias remains a significant barrier towards solving real worldcomputer vision tasks. Though deep convolutional networks have proven to be acompetitive approach for image classification, a question remains: have thesemodels have solved the dataset bias problem? In general, training orfine-tuning a state-of-the-art deep model on a new domain requires asignificant amount of data, which for many applications is simply notavailable. Transfer of models directly to new domains without adaptation hashistorically led to poor recognition performance. In this paper, we pose thefollowing question: is a single image dataset, much larger than previouslyexplored for adaptation, comprehensive enough to learn general deep models thatmay be effectively applied to new image domains? In other words, are deep CNNstrained on large amounts of labeled data as susceptible to dataset bias asprevious methods have been shown to be? We show that a generic supervised deepCNN model trained on a large dataset reduces, but does not remove, datasetbias. Furthermore, we propose several methods for adaptation with deep modelsthat are able to operate with little (one example per category) or no labeleddomain specific data. Our experiments show that adaptation of deep models onbenchmark visual domain adaptation datasets can provide a significantperformance boost."
1312.6461,2014,Nonparametric Weight Initialization of Neural Networks via Integral Representation  ,"['Sho Sonoda', 'Noboru Murata']",https://arxiv.org/pdf/1312.6461.pdf,"4 1 0 2     b e F 9 1         ]  G L . s c [      3 v 1 6 4 6  .  2 1 3 1 : v i X r a  Nonparametric Weight Initialization of Neural  Networks via Integral Representation  Sho Sonoda, Noboru Murata  Schools of Advanced Science and Engineering  Waseda University  Shinjuku, Tokyo 169-8555  s.sonoda0110@toki.waseda.jp, noboru.murata@eb.waseda.ac.jp  Abstract  A new initialization method for hidden parameters in a neural network is pro- posed. Derived from the integral representation of neural networks, a nonparamet- ric probability distribution of hidden parameters is introduced. In this proposal, hidden parameters are initialized by samples drawn from this distribution, and output parameters are ﬁtted by ordinary linear regression. Numerical experiments show that backpropagation with proposed initialization converges faster than uni- formly random initialization. Also it is shown that the proposed method achieves enough accuracy by itself without backpropagation in some cases.  1  Introduction  In the backpropagation learning of a neural network, the initial weight parameters are crucial to its ﬁnal estimates. Since hidden parameters are put inside nonlinear activation functions, simultaneous learning of all parameters by backpropagation is accompanied by a non-convex optimization prob- lem. When the machine starts from an initial point far from the goal, the learning curve easily gets stuck in local minima or lost in plateaus, and the machine fails to provide good performance. Recently deep learning schemes draw tremendous attention for their overwhelming high perfor- mances for real world problems[1, 2]. Deep learning schemes consist of two stages: pre-training and ﬁne-tuning. The pre-training stage plays an important role for the convergence of the follow- ing ﬁne-tuning stage. In pre-training, the weight parameters are constructed layer by layer, by stacking unsupervised learning machines such as restricted Boltzmann machines[3] or denoising autoencoders[4]. Despite the brilliant progress in application ﬁelds, theoretical interpretation of the schemes is still an open question[5]. In this paper we introduce a new initialization/pre-training scheme which could avoid the non- convex optimization problem. The key concept is the probability distribution of weight parameters derived from Murata’s integral representation of neural networks[6]. The distribution gives an intu- itive idea what the parameters represent and contains information about where efﬁcient parameters exist. Sampling from this distribution, we can initialize weight parameters more efﬁciently than just sampling from a uniform distribution. In fact, for relatively simple or low dimensional problems, our method by itself attains a high accuracy solution without backpropagation. De Freitas et al.[7] also introduced a series of stochastic learning methods for neural networks based on the Sequential Monte Carlo (SMC). In their methods the learning process is iterative and initial parameters are given by less informative distributions such as normal distributions. On the other hand we could draw the parameters from a data dependent distribution. Furthermore, in SMC, the number of hidden units must be determined before the learning, while it is determined naturally in our method.  1  2 Back ground and related works One of the most naive initialization heuristics is to draw samples uniformly from an interval [−α, α]. Nguyen and Widrow[8] gave two fundamental points of view. First, since a typical activation func- tion such as sigmoid and hyperbolic tangent is approximated as a linear function at its inﬂection point, one should initialize the hidden parameters in such a way that the inputs for each hidden unit are in the linear region. Second, since each hidden unit determines the slice of the Fourier trans- formed input space, that is, each individual hidden unit responds selectively to only the inputs whose spatial frequency is in a particular band, one should initialize hidden parameters in such a way that the corresponding frequency bands cover the possible input frequencies. LeCun et al.[9] also emphasized the need to preset parameters in the linear region because param- eters outside the linear region have small gradients and stray into more difﬁcult nonlinear regions. They focused on the curvature of input vectors and proposed to use α ∝ m−1/2, where m is the fan- in, or the dimensionality of input vectors. Shimodaira[10] proposed to initialize parameters such that corresponding activation regions to cover whole the possible inputs. Linear algebraic techniques are also employed. For example, Shepanski[11] used the pseudo inverse to determine the parameters of linear approximated neural networks, and Yam and Chow[12] used the QR decomposition. Integral transform viewpoints originated from more theoretical backgrounds than linear region view- points: the theoretical evaluation of the approximation power of neural networks. In the earliest stage, purely functional analysis methods were employed. In 1957 Kolmogorov[13] showed that any multivariate continuous functions can be exactly represented by sums of compositions of dif- ferent continuous functions of only one variable. Inspired by the Kolmogorov’s theorem, Hecht- Nielsen[14] and K˚urkov´a[15] applied the idea to neural networks, which are sums of compositions of the same sigmoid function. Sprecher[16] gave more constructive version of the proof and later implemented the improved proof as a learning algorithm of neural networks[17]. In 1989 the universal approximation property of single layer neural networks has been inves- tigated and the integral transform aspects emerged. Carroll and Dickinson[18] introduced the Radon transform and Funahashi[19] used the Fourier analysis and the Paley-Weiner theory, whereas Cybenko[20] employed the Hahn-Banach and Riesz Representation theorems. In the following years, upper bounds of the approximation error were investigated[21, 22, 6]. Barron[22] reﬁned the Jones’ result[21] using the weighted Fourier transform. K˚urkov´a[15] later developed the gen- eral theory of integral transforms. Inspired by the Barron’s result, Murata[6] introduced a family of integral transforms deﬁned by ridge functions, which are regarded as a hybrid of the Radon and wavelet transforms. Cand´es[23] inherited Murata’s transforms and developed ridgelets, which was the beginning of the series of multiscale “-lets” analysis[24]. Those multiscale viewpoints also inherits the selective activation properties of neural networks. De- noeux and Lengell´e[25] proposed to collect K prototype vectors as initial hidden parameters. Each prototype pk is drawn from its corresponding cluster Ck, where the clusters {Ck}K k=1 are formed in a stereographically projected input space. In this manner each prototype pk comes to selectively respond to the input vectors x which belongs to the cluster Ck. This study is based on the integral transform viewpoint, and proposes a new way for practical im- plementation. Although integral transforms have been well studied as theoretical integral represen- tations of neural networks, practical implementations for training have been merely done. However integral representations have big advantage over linear region viewpoints in that they can give global directions how each neural units should behave, while the latter only give local directions.  3 Nonparametric weight initialization via integral transform  3.1 Sampling based two-stage learning Let g : Rm → R be a neural network with a single hidden layer expressed as  g(x) =  wjφ (aj · x − bj) + w0,  (1)  J(cid:88)  j=1  2  where the map φ is called the activation function; aj and bj are called hidden parameters, and wj are output parameters. With an ordinary sigmoid function σ(z) := 1+exp(−z), the activation function φ is supposed to be the sigmoid pair in the form  1  1 H  {σ(z + h) − σ(z − h)} ,  φ(z) :=  (h > 0), where H := σ(h) − σ(−h) normalizes the maximum value of φ to be one. We consider an oracle distribution p(a, b) of hidden parameters. If such a distribution exists, we can sample and ﬁx these hidden parameters according to p(a, b) ﬁrst, and then we could ﬁt the rest output parameters by ordinary linear regression. We call this two-stage framework as Sampling Regression (SR) learning. The candidates of p(a, b) could be some parametric distributions such as normal distributions or uniform distributions. In the following sections we derive a data dependent distribution from an integral representation of neural networks.  (2)  (cid:90)  3.2 Integral representations of neural networks Consider approximating a map f : Rm → R with a neural network. Murata[6] deﬁned an integral transform T of f with respect to a decomposing kernel φd as  T (a, b) :=  1 C  Rm  φd(a · x − b)f (x)dx,  (3)  (cid:90)  where C is a normalizing constant. Murata also showed that given the decomposing kernel φd, there exists the associating composing kernel φc such that for any f ∈ L1(Rm) ∩ Lp(Rm)(1 ≤ p ≤ ∞), the inversion formula  c (a · x − b)T (a, b)e−(cid:15)|a|2 φ∗  Rm+1  f (x) = lim (cid:15)→0  (4) holds (Th.1 in [6]) where ·∗ denotes the complex conjugate. The convergence factor e−(cid:15)|a|2 is omitted when T ∈ L1(Rm+1), which is attained when f is compactly supported and C m,α-H¨older continuous with 0 < α ≤ 1 (Th.3 in [6]), or compactly supported and bounded C m+1-smooth (Cor.2 in [6]). In particular one can set a composing kernel φc as a sigmoid pair φ given in Eq.2 and the associating decomposing kernel as:  in Lp,  dadb  if m is even  φd(z) =  ρ(m+1)(z) otherwise  (5) where ρ is a nonnegative C∞-smooth function whose support is in the interval [−1, 1]. Such a ρ does exist and is known as a molliﬁer[26].The standard molliﬁer ρ(z) = exp is a well-known example. Hereafter we assume φc is a sigmoid pair and φd is the corresponding derivative of the standard molliﬁer. We also assume that our target f is a bounded and compactly supported C (m+1)-smooth function. Then the integral transform T of f is absolutely integrable and the inversion formula is  (cid:16) 1  (cid:17)  z2−1  ,  (cid:26)ρ(m)(z)  Rm+1 φ∗  c (a · x − b)T (a, b)dadb.  Let τ (a, b) be a probability distribution function over Rm+1 which is proportional to |T (a, b)|, and c(a, b) be satisfying c(a, b)τ (a, b) = T (a, b) for all (a, b) ∈ Rm+1. With this notations, the inversion formula is rewritten as the expectation form with respect to τ (a, b), that is,  reduced to the direct form f (x) =(cid:82) (cid:90)  c(a, b)φc(a · x − b)τ (a, b)dadb.  (6)  The expression implies the ﬁnite sum  f (x) =  Rm+1  J(cid:88)  1 J  c(aj, bj)φc(aj · x − bj),  (aj, bj) i.i.d.∼ τ (a, b)  gJ (x) :=  (7) converges to f in mean square as J → ∞, i.e. E[gJ ] = f and Var[gJ ] < ∞ holds for any J (Th.2 in [6]). Here gJ is a neural network with 2J hidden units, therefore we can regard the inversion formula as an integral representation of neural networks.  j=1  3  3.3 Practical calculation of the integral transform Now we attempt to make use of the integral transform |T (a, b)| as an oracle distribution p(a, b) of hidden parameters. Although the distribution is given in the explicit form as we saw in the preceding section, further reﬁnements are required for practical calculation. Given a set {(xn, yn)}N as  n=1 ⊂ Rm×R of input and output pairs, T (a, b) is empirically approximated  T (a, b) ≈ 1 Z  φd(a · xn − b)yn,  (8)  N(cid:88)  n=1  with some constant Z > 0 which is hard to calculate exactly. In fact sampling algorithms such as the acceptance-rejection method[27] and Markov chain Monte Carlo method[27] work with any unnormarized distribution because they only evaluate the ratio between probability values. Note that the approximation converges to the exact T (a, b) in probability by the law of large numbers only when the input vectors are i.i.d. samples from a uniform distribution. As a decomposing kernel φd we make use of the k-th order derivative of the standard molliﬁer z2−1 where k = m if m is even and k = m + 1 otherwise. The k-th derivative ρ(k)(z) ρ(z) = exp 1 of the molliﬁer takes the form  ρ(k)(z) =  Pk(z)  (z2 − 1)2k ρ(z)  (k = 0, 1, 2,··· ),  (9)  Pk+1(z) = P (cid:48)  P0(z) ≡ 1 (const.),  where Pk(z) denotes a polynomial of z which is calculated by the following recurrence formula:  k(z)(z4 − 2z2 + 1) + Pk(z)(cid:8)−4kz3 + 2(2k − 1)z(cid:9) .  (10) (11) The higher order derivatives of a molliﬁer has more rapid oscillations in the neighbourhoods of both edges of its support. Given a data set D := {(xn, yn)}N as below:  n=1 ⊂ Rm × R, our Sampling Regression method is summarized  1. Sampling stage: Draw J samples {(aj, bj)}J  0. Preliminary stage: Calculate ρ(k)(z) according to Eq.9, Eq.10 and Eq.11, where k = m if m is even and k = m + 1 otherwise. Then T (a, b) is calculated by Eq.8 with setting φd = ρ(k). As we noted above, one can choose arbitrary Z > 0. j=1 from the probability distribution τ (a, b) ∝ |T (a, b)| by acceptance-rejection method, where J denotes the number of hidden (sigmoid pair) units. Then we obtain the hidden parameters {(aj, bj)}J  Solve the system of linear equations yn =(cid:80)J  2. Regression stage: Let φjn := φd(aj · xn − bj) for all j = 1,··· , J and n = 1,··· , N. j=1 wjφjn + w0 (n = 1··· N ) with respect  j=1.  to {wj}J  j=0. Then we obtain the output parameters {wj}J  j=0.  3.4 For more efﬁcient sampling Generally |T (a, b)| is ill-shaped and sampling from the distribution is difﬁcult. For example in Fig.1 Left, samples drawn from |T (a, b)| of f (x) = sin 2πx with x ∈ [−1, 1] is plotted. Whereas in Fig.1 Right, the same distribution is transformed to another (α, β)-coordinate system (which is explained below). The support of the distribution is reshaped into a rectangular, which implies sampling from |T (α, β)| is easier than doing from |T (a, b)|. This ill-shapeness is formulated as following proposition. Proposition 3.1. Suppose the objective function f (x) has a compact support, then the support of its transform T (a, b) is in the region Ω := {(a, b)||b| ≤ M(cid:107)a(cid:107) + 1} with M := maxx∈supp f (cid:107)x(cid:107). Proof. Recall the support of φd is included in the interval [−1, 1], therefore for any a, b and x, φd(a · x − b) (cid:54)= 0 implies |a · x − b| < 1. The latter condition is equivalently deformed to a · x − 1 < b < a · x + 1, which implies |b| < |a · x| + 1. By the compact support assumption of f,  4  Figure 1: Sample parameters drawn from |T (a, b)| of sin 2πx case. Red lines indicate the theo- retical boundary b = ±(M(cid:107)a(cid:107) + 1) of the support of |T (a, b)|. Left: |T (a, b)| has a non-convex support, in which case sampling is inefﬁcient. Right: The same sample points are plotted in the coordinate transformed (α, β)-space. Coordinate lines are deformed to lattice lines, and |T (α, β)| has a rectangular support.  taking the maximum with respect to x leads to |b| < M(cid:107)a(cid:107) + 1. By tracking back the inferences, for any a, b and x ∈ supp f,  (12) Since for any x /∈ supp f, the integrand of T (a, b) is always zero, the integration domain of T (a, b) can be restricted into supp f. Therefore by Eq.12,  (a, b) /∈ Ω ⇒ φd(a · x − b) = 0.  T (a, b) (cid:54)= 0 ⇒ (a, b) ∈ Ω  (13)  holds, which comes to the conclusion: supp T ⊂ Ω.  (cid:18)a  (cid:19)  (cid:18)  α  (cid:19)  In a relatively high dimensional input case, sampling in the coordinate transformed (α, β)-space  ,  b  =  (M(cid:107)α(cid:107) + 1)β  (14) is more efﬁcient than sampling in the (a, b)-space because the shape of the support of |T (a, b)| in the (α, β)-space is rectangular (see, Fig.1) and therefore the proposal distribution is expected to reduce miss proposals, out of the support. In case that the coordinate transform technique is not enough, it is worth sampling from each com- ponent distribution. Namely, the empirically approximated |T (a, b)| is bounded above by a mixture distribution:  |T (a, b)| ≈ 1 Z  ynφd(a · xn − b)  |yn||φd(a · xn − b)|,  (cid:12)(cid:12)(cid:12) N(cid:88)  n=1  ηnpn(a, b),  (15)  where pn(a, b) ∝ |φd(a·xn−b)| is a component distribution and ηn ∝ |yn| is a mixing probabilities. In addition, an upper bound of φd is given by the form  n=1  log |φd(z)| ≤ Az2 + B,  (16)  for some A > 0 and B.  5  (cid:12)(cid:12)(cid:12) ≤ 1 N(cid:88) ∝ N(cid:88)  n=1  Z  4 Experimental results  We conducted three sets of experiments comparing three types of learning methods:  BackPropagation.  BP Whole parameters are initialized by samples from a uniform distribution, and trained by SBP Hidden parameters are initialized by Sampling from |T (a, b)|; and the rest output param- eters are initialized by samples from a uniform distribution. Then whole parameters are trained by BackPropagation. SR Hidden parameters are determined by Sampling from |T (a, b)|; the rest output parameters  are ﬁtted by linear Regression.  In order to compare the ability of the three methods, we conducted three experiments on three differ- ent problems: One-dimensional complicated curve regression, Multidimensional Boolean functions approximation and Real world data classiﬁcation.  4.1 One-dimensional complicated curve regression - Topologist’s sine curve sin 2π/x  Figure 2: Training results of three methods for ﬁtting the topologist’s sine curve sin 2π/x. Left: SR (solid black line) by itself achieved the highest accuracy without the iterative learning, whereas SBP (dashed red line) converged to lower RMSE than BP (dotted green line). Right: The original curve (upper left) has high frequencies around the origin. SR (upper right) followed such a dynamic variation of frequency better than other two methods. SBP (lower left) roughly approximated the curve with noise. BP (lower right) only ﬁtted moderate part of the curve.  First we performed one-dimensional curve regression. The objective function is a two-sided topol- ogists’s sine curve (TSC) f (x) := sin 2π/x deﬁned on the interval [−1, 1] whose indeﬁniteness at zero is removed by deﬁning f (0) = 0. The TSC is such a complicated curve whose spatial frequency gets arbitrary high as x tends to zero. For training, 201 points were sampled from the domain [−1, 1] in equidistant manner. The number of hidden parameters were ﬁxed to 100 in each model. Note that relatively redundant quantity of parameters are needed for our sampling initialization scheme to ob- tain good parameters. The output function was set to linear and the batch learning was performed by BFGS quasi-Newton method. Uniformly random initialization parameters for BP and SBP were drawn from the interval [−1, 1]. Sampling from |T (a, b)| was performed by acceptance-rejection method. In Fig.2 Left, the Root Mean Squared Error (RMSE) in training phase of three methods are shown. The solid black line corresponds to the result by SR, which by itself achieved the highest accuracy without iterative learnings. The dashed red line corresponds to the result by SBP, and it converged to lower RMSE than that of BP depicted in the dotted green line. In Fig.2 Right, ﬁtting results of the three methods are shown. As we noted the original curve (upper left) has numerical instability  6  around the origin, therefore it is difﬁcult to ﬁt the curve. SR (upper right) approximated the original curve well except around the origin, while other two methods, SBP (lower left) and BP (lower right) could just partly ﬁt the original curve. In this experiment, we examined the ﬂexibility of our method by ﬁtting a complicated curve. The experimental result supports that the oracle distribution gave advantageous directions.  4.2 Multidimensional Boolean functions approximation - Combined AND, OR and XOR  Second we performed a binary problem with two- dimensional input and three-dimensional output. Out- put vectors are composed of three logical functions: F (x, y) := (xANDy, xORy, xXORy). Therefore (x, y) ∈ the total number of data is just four: {(0, 0), (0, 1), (1, 0), (1, 1)}. The number of hidden units were ﬁxed to 10. The output function was set to sigmoid and the loss function was set to cross-entropy. Uniformly random initialization parameters for BP and SBP were drawn from the interval [−1, 1]. Sampling from |T (a, b)| was performed by acceptance-rejection method. In Fig.3 both the cross-entropy curves and classiﬁca- tion error rates are depicted in thin and thick lines re- spectively. The solid black line corresponds to the re- sults by SR, which achieved the perfectly correct an- swer from the beginning. The dashed red line cor- responds to the results by SBP, which also attained the perfect solution faster than BP. The dotted green line corresponds to the results by BP, which cost 100 iterations of learning to give the correct answer. In this experiment we have validated that the proposed method works well with multiclass classiﬁcation prob- lems. The quick convergence of SBP indicates that |T (a, b)| contains advantageous information on the training examples to the uniform distribution.  Figure 3: Cross-entropy curves (thin lines) and classiﬁcation error rates (thick lines). SR (solid black line) achieved the perfectly correct answer from the beginning. SBP (dashed red line) also attained the perfect solution faster than BP. BP (dotted green line) costed 100 iterations of learning to give the correct answer.  4.3 MNIST  Finally we examined a real classiﬁcation problem us- ing the MNIST data set[28]. The data set consists of 60, 000 training examples and 10, 000 test exam- ples. Each input vector is a 256-level gray-scaled (28 × 28 =)784-pixel image of a handwritten digit. The corresponding label is one of 10 digits. We imple- mented these labels as 10-dimensional binary vectors whose components are chosen randomly with equiva- lent probability for one and zero. We used randomly sampled 15, 000 training examples for training and whole 10, 000 testing examples for testing. The num- ber of hidden units were ﬁxed to 300, which is the same size as used in the previous study of LeCun et al.[29]. Note that J sigmoid pairs corresponds to 2J sigmoid units, therefore we used 150 sigmoid pairs for SR and SBP, and 300 sigmoid units for BP. The out- put function was set to sigmoid and the loss function was set to cross-entropy. In obedience to LeCun et al.[9], input vectors were normalized and randomly ini- tialized parameters for BP and SBP were drawn from uniform distribution with mean zero and standard de- viation 784−1/2 ≈ 0.0357.  7  Figure 4: Test classiﬁcation error rates for MNIST dataset. SR (black real line) marked 23.0% at the beginning and ﬁn- ished 9.94%, the error reascent suggests that SR may have overﬁtted. SBP (red dashed line) reduced the fastest and ﬁn- ished 8.30%. BP (green dotted line) de- clined the slowest and ﬁnished 8.77%.  (|z| < 1),  Direct sampling from |T (a, b)| is numerically difﬁcult because the differential order of its decompos- ing kernel φd piles up as high as 784-th order. We abandoned rigorous sampling and tried sampling from a mixture annealed distribution. As described in Eq.15, we regarded |T (a, b)| as a mixture of |φd(a · xn − b)|. By making use of the log boundary given by Eq.16, we numerically approximated φd(z) from above  log |φd(z)| ≤ 2800z2 − 800,  (17) and drew samples from an easier component distribution pn(a, b) ∝ exp{2800(a · xn − b)2 − 800}. Details of the sampling technique is explained in A.2. The sampling procedure scales linearly with the dimensionality of the input space (784) and the number of required hidden units (150) respectively. In particular it scales constantly with the number of the training examples. The following linear regression was conducted by singular value demcomposition (SVD), which generally costs O(mn2) operations, assuming m ≥ n, for decomposing a m× n-matrix. In our case m corresponds to the number of the training examples (15, 000) and n corresponds to the number of hidden units (300). At last backpropagation learning was performed by stochastic gradient descent (SGD) with adaptive learning rates and diagonal approximated Hessian[30]. The experiment was performed in R[31] on a Xeon X5660 2.8GHz with 50GB memory. In Fig.4 the classiﬁcation error rates for test examples are depicted. The black real line corresponds to the results by SR, which marked the lowest error rate (23.0%) of the three at the beginning, and ﬁnished 9.94% after 45, 000 iterations of SGD training. The training process was not monotonically decreasing in the early stage of training, it appears that the SR initialization overﬁtted to some extent. The red dashed line corresponds to the results by SBP, which marked the steepest error reduction in the ﬁrst 5, 000 iterations of SGD training and ﬁnished 8.30%. The green dotted line corresponds to the results by BP, which declined the slowest in the early stage of training and ﬁnished 8.77%. In Tab.1 the training time from initialization through SGD training is listed. The sampling step in SR ran faster than the following regression and SGD steps. In addition, the sampling time of SR and SBP was as fast as the sampling time of BP. As we expected, the regression step in SR, which scales linearly with the amount of the data, cost much more time than the sampling step did. The SGD step also cost, however each step cost around merely 0.05 seconds, and it would be shorten if the initial parameters had better accuracy.  Table 1: Training Times for MNIST  Method SR SBP BP  Sampling [s] Regression [s] BP by SGD (45, 000 itrs.) [s] 1.15 × 10−2 1.14 × 10−2 1.15 × 10−2  2.00 × 103 2.31 × 103 2.67 × 103  2.60  - -  In this experiment, we conﬁrmed that the proposed method still works for real world data with the aid of an annealed sampling technique. Although SR showed an overﬁtting aspects, the fastest con- vergence of SBP supports that the oracle distribution gave meaningful parameters, and the annealed sampling technique could draw meaningful samples. Hence the overﬁtting of SR possibly comes from regression step, which suggests the necessity for further blushing up of regression technique. In addition, our further experiments also indicated that when the number of hidden units increased to 6, 000, the initial test error rate scored 3.66%, which is smaller than the previously reported error rates 4.7% by LeCun et al.[29] with 300 hidden units.  5 Conclusion and future directions  In this paper, we introduced a two-stage weight initialization method for backpropagation: sam- pling hidden parameters from the oracle distribution and ﬁtting output parameters by ordinary lin- ear regression. Based on the integral representation of neural networks, we constructed our oracle distributions from given data in a nonparametric way. Since the shapes of those distributions are not simple in high dimensional input cases, we also discussed some numerical techniques such as the coordinate transform and the mixture approximation of the oracle distributions. We performed three numerical experiments: complicated curve regression, Boolean function approximation, and  8  handwritten digit classiﬁcation. Those experiments show that our initialization method works well with backpropagation. In particular for the low dimensional problems, well-sampled parameters by themselves achieve good accuracy without any parameter updates by backpropagation. For the handwritten digit classiﬁcation problem, the proposed method works better than random initializa- tion. Sampling learning methods inevitably come with redundant hidden units since drawing good sam- ples usually requires a large quantity of trial. Therefore the model shrinking algorithms such as pruning, sparse regression, dimension reduction and feature selection are naturally compatible to the proposed method. Although plenty of integral transforms have been used for theoretical analysis of neural networks, numerical implementations, in particular sampling approaches are merely done. Even theoretical calculations often lack practical applicability, for example a higher order of derivative in our case, each integral representation interprets different aspects of neural networks. Further Monte Carlo discretization of other integral representations is an important future work. In the deep learning context, it is said that the deep structure remedies the difﬁculty of a problem by multilayered superpositions of simple information transformations. We conjecture that the complex- ity of high dimensional oracle distributions can be decomposed into relatively simple distributions in each layer of the deep structure. Therefore, extending our method to the multilayered structure is our important future work.  Acknowledgments  The authors are grateful to Hideitsu Hino for his incisive comments on the paper. They also thank to Mitsuhiro Seki for having constructive discussions with them.  References  [1] Q. Le, M. Ranzato, R. Monga, M. Devin, K. Chen, G. Corrado, J. Dean, and A. Ng. Building high-level features using large scale unsupervised learning. In J. Langford and J. Pineau, editors, ICML2012, pages 81–88, 2012.  [2] G. E. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent pre-trained deep neural networks for large- IEEE Transactions on Audio, Speech & Language Processing, 20(1),  vocabulary speech recognition. 2012.  [3] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural Compu-  tation, 18(7):1527–1554, 2006.  [4] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. In  Bernhard Scholkopf, John Platt, and Thomas Hoffman, editors, NIPS ’06, pages 153–160, 2007.  [5] Y. Bengio., A. Courville., and P. Vincent. Representation learning: A review and new perspectives. PAMI,  35(8):1798–1828, 2013.  [6] N. Murata. An integral representation of functions using three-layered networks and their approximation  bounds. Neural Networks, 9(6):947–956, 1996.  [7] J. F. G. de Freitas, M. Niranjan, A. H. Gee, and A. Doucet. Sequential monte carlo methods to train neural  network models. Neural Computation, 12(4):955–993, 2000.  [8] D. Nguyen and B. Widrow. Improving the learning speed of 2-layer neural networks by choosing initial  values of the adaptive weights. In IJCNN 1990, volume 3, pages 21–26, 1990.  [9] Y. LeCun, L. Bottou, G. B. Orr, and K.-R. M¨uller. Efﬁcient backprop. In G. Montavon, G. B. Orr, and  K.-R. M¨uller, editors, Neural Networks: Tricks of the Trade (2nd ed.), pages 9–48. Springer, 2012.  [10] H. Shimodaira. A weight value initialization method for improving learning performance of the back-  propagation algorithm in neural networks. In ICTAI1994, pages 672–675, 1994.  [11] J. F. Shepanski. Fast learning in artiﬁcial neural systems: multilayer perceptron training using optimal  estimation. In ICNN1988, volume 1, pages 465–472, 1988.  [12] J. Y. F. Yam and T. W. S. Chow. A weight initialization method for improving training speed in feedfor-  ward neural network. Neurocomputing, 30(1-4):219–232, 2000.  [13] A. N. Kolmogorov. On the representation of continuous functions of several variables by superposition of continuous functions of one variable and addition. Doklady Akademii Nauk SSSR, 114:369–373, 1957.  9  [14] R. Hecht-Nielsen. Kolmogorov’s mapping neural network existence theorem. In ICNN1987, volume III,  pages 11–13, 1987.  [15] V. K˚urkov´a. Kolmogorov’s theorem is relevant. Neural Computation, 3(4):617–622, 1991. [16] D. A. Sprecher. On the structure of continuous functions of several variables. Transactions of the AMS,  115:340–355, 1965.  [17] D. A. Sprecher. A numerical implementation of kolmogorov’s superpositions. Neural Networks,  9(5):765–772, 1996.  [18] S. M. Carroll and B. W. Dickinson. Construction of neural nets using the radon transform. In IJCNN  1989, volume 1, pages 607–611, 1989.  [19] K. Funahashi. On the approximate realization of continuous mappings by neural networks. Neural  Networks, 2(3):183–192, 1989.  [20] G. Cybenko. Approximation by superpositions of a sigmoidal function. MCSS, 2(4):303–314, 1992. [21] L. K. Jones. A simple lemma on greedy approximation in hilbert space and convergence rates for projec-  tion pursuit regression and neural network training. The Annals of Statistics, 20(1):608–613, 1992.  [22] A.R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transac-  tions on Information Theory, 39(3):930–945, 1993.  [23] E. J. Candes. Ridgelets: Theory and Applications. PhD thesis, Standford University, 1998. [24] J. Fadili and J. L. Starck. Curvelets and ridgelets. In R. A. Meyers, editor, Encyclopedia of Complexity  and Systems Science, volume 3, pages 1718–1738. Springer New York, 2009.  [25] T. Denoeux and R. Lengelle. Initializing back propagation networks with prototypes. Neural Networks,  6(3):351–363, 1993.  [26] K. O. Friedrichs. The identity of weak and strong extensions of differential operators. Transactions of the  AMS, 55(1):132–151, 1944.  [27] C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006. [28] C. J. C. Burges Y. LeCun, C. Cortes. The mnist database of handwritten digits. http://yann.lecun.  com/exdb/mnist/.  [29] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.  In Proceedings of the IEEE, volume 11, pages 2278–2324, 1998.  [30] L. Bottou. Online algorithms and stochastic approximations. In D. Saad, editor, Online Learning and  Neural Networks. Cambridge University Press, 1998.  [31] R Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical  Computing, Vienna, Austria, 2013.  Supplementary materials  A Sampling recipes  Sampling hidden parameter (a, b)’s from the oracle distribution p(a, b) demands a little ingenuity. In our exper- iments, we have implemented two sampling procedures: a rigorous but naive, computationally inefﬁcient way and an approximative/ad hoc but quick and well-performing way. Although both work quickly and accurately in a low dimensional input problem, only the latter works in a high dimensional problem such as MNIST.  A.1 Sampling from rigorous oracle distribution  Given a decomposing kernel φd(z) := ρ(m)(z), we employed acceptance-rejection (AR) method directly on rigorous sampling from p(a, b) On a proposal distribution q(a, b), we employed uniform distribution. We assume here that the support Ω of proposal distribution q(a, b) has been adjusted to cover the mass of p(a, b) as tight as possible, and the inﬁmum k := inf p(a, b)/q(a, b) has been estimated. Then our sampling procedure is conducted according to the following Alg.1. Note that in a high dimensional case, the estimation accuracy of k and the tightness of Ω affects the sampling efﬁciency and accuracy materially. In fact, the expectation number of trial to obtain one sample by AR is k times, which gets exponentially large as the dimensionality increases. Since the support of the oracle distri- bution p(a, b) is not rectangular, sampling from coordinate transformed p(α, β) remedies the difﬁculty. In addition, the high order differentiation in the decomposing kernel φd cause numerical unstability.  10  Algorithm 1 Rigorous sampling according to ordinary acceptance-rejection method.  repeat  draw proposal point (a∗, b∗) ∼ q(a, b). draw uniformly random value u from the interval [0, 1]. if u ≤ p(a∗,b∗)  kq(a∗,b∗) then  return (a∗, b∗) {accept} do nothing {reject}  else  end if  until acceptance occurs.  A.2 Sampling from mixture annealed distribution  distribution p(a, b) ≈(cid:80)N  In order to overcome the high dimensional sampling difﬁculty, we approximately regarded p(a, b) as a mixture n=1 ηnpn(a, b) (as described in Eq.15) and conducted two-step sampling: ﬁrst choose one component distribution pn(a, b) according to the mixing probability ηn ∝ |yn|, second draw a sample (a, b) from chosen component distribution pn(a, b). Sampling from pn(a, b) ∝ |φd(a·xn−b)| holds another difﬁculty due to its high order differentiation in φd(z). According to its upper bound evaluation (Eq.16), a high order derivative ρ(m)(z)(= φd(z)) has its almost all mass around both edge of its domain interval [−1, 1] and almost no mass in the middle of the domain (see Fig.5 Left). Hence we approximated, or annealed, ρ(m)(z) by a beta distribution, which could model extreme skewness of ρ(m)(z) (e.g., Beta(z; 100, 3); see Fig.5 Right). Then we conducted further steps of sampling: ﬁrst sample z ∈ [−1, 1] according to the annealing beta distribution, then sample a and b under the restriction z = a · xn − b.  Figure 5: 10-th order derivative ρ(10)(z) of molliﬁer. Left: ρ(10)(z) has almost all mass, with high frequency, at both ends, and no mass in the middle of domain. Right: The right half of |ρ(10)(z)| is approximated by beta distribution Beta(z; 100, 3) (red line).  Obviously the mixture approximation gives rise to poor restriction and virtual indeﬁniteness of (a, b). Since the rigorous computation establishes all relations between (a, b) and all xn’s, whereas the mixture approximation does just one relation between (a, b) and one particular xn. We introduced two additional assumptions. First, a is parallel to given xn. Since a always appears in the form a · xn, only the parallel component of a could have any effect (on one particular xn), hence we eliminated the extra freedom in the orthogonal component. Second, the norm a := (cid:107)a(cid:107) has similar scale to the distances (cid:107)xn − xm(cid:107) between input vectors. Since a controls the spatial frequency of a hidden unit, it determines how broad the hidden unit covers the part of the input space. Namely, a controls which input vectors are selectively responded by the unit. Therefore, in order to avoid such an isolation case that an unit responds for only one input vector, we assumed a is no smaller than the distance between input vectors. In this procedure we set a as a distance (cid:107)xn − xm(cid:107) of randomly selected two input examples xn and xm. We denote this procedure simply by a ∼ p((cid:107)x − x(cid:48)(cid:107)). Once a is ﬁxed with these assumptions, b is determined as b = a · xn − z.  11  Given shape parameters α, β of the beta distribution Beta(z; α, β), one cycle of our second sampling method is summarized as Alg.2. This method consists of no more expensive steps. It scales linearly with the dimen- sionality of the input space and the number of required sample parameters respectively. Moreover, it does not depends on the size of the training data.  Algorithm 2 Quick sampling from mixture annealed distribution (for high dimensional use.)  choose a sufﬁx n of xn according to the mixing probability ηn. draw ζ ∼ Beta(z; α, β) and k ∼ Bernoulli(k; p = 0.5) z ← (−1)kζ set length a ∼ p(x − x(cid:48)). a ← axn/(cid:107)xn(cid:107). b ← a · xn − z.  12  ","A new initialization method for hidden parameters in a neural network isproposed. Derived from the integral representation of the neural network, anonparametric probability distribution of hidden parameters is introduced. Inthis proposal, hidden parameters are initialized by samples drawn from thisdistribution, and output parameters are fitted by ordinary linear regression.Numerical experiments show that backpropagation with proposed initializationconverges faster than uniformly random initialization. Also it is shown thatthe proposed method achieves enough accuracy by itself without backpropagationin some cases."
1312.6229,2014,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks  ","['Michael Mathieu', 'Yann LeCun', 'Rob Fergus', 'David Eigen', 'Pierre Sermanet', 'Xiang Zhang']",https://arxiv.org/pdf/1312.6229.pdf,"4 1 0 2     b e F 4 2         ]  V C . s c [      4 v 9 2 2 6  .  2 1 3 1 : v i X r a  OverFeat:  Integrated Recognition, Localization and Detection  using Convolutional Networks  Pierre Sermanet David Eigen  Xiang Zhang Michael Mathieu Rob Fergus Yann LeCun Courant Institute of Mathematical Sciences, New York University  719 Broadway, 12th Floor, New York, NY 10003  sermanet,deigen,xiang,mathieu,fergus,yann@cs.nyu.edu  Abstract  We present an integrated framework for using Convolutional Networks for classi- ﬁcation, localization and detection. We show how a multiscale and sliding window approach can be efﬁciently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object bound- aries. Bounding boxes are then accumulated rather than suppressed in order to increase detection conﬁdence. We show that different tasks can be learned simul- taneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classiﬁcations tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.  1 Introduction  Recognizing the category of the dominant object in an image is a tasks to which Convolutional Networks (ConvNets) [17] have been applied for many years, whether the objects were handwritten characters [16], house numbers [24], textureless toys [18], trafﬁc signs [3, 26], objects from the Caltech-101 dataset [14], or objects from the 1000-category ImageNet dataset [15]. The accuracy of ConvNets on small datasets such as Caltech-101, while decent, has not been record-breaking. However, the advent of larger datasets has enabled ConvNets to signiﬁcantly advance the state of the art on datasets such as the 1000-category ImageNet [5].  The main advantage of ConvNets for many such tasks is that the entire system is trained end to end, from raw pixels to ultimate categories, thereby alleviating the requirement to manually design a suitable feature extractor. The main disadvantage is their ravenous appetite for labeled training samples.  The main point of this paper is to show that training a convolutional network to simultaneously classify, locate and detect objects in images can boost the classiﬁcation accuracy and the detection and localization accuracy of all tasks. The paper proposes a new integrated approach to object detection, recognition, and localization with a single ConvNet. We also introduce a novel method for localization and detection by accumulating predicted bounding boxes. We suggest that by combining many localization predictions, detection can be performed without training on background samples and that it is possible to avoid the time-consuming and complicated bootstrapping training passes. Not training on background also lets the network focus solely on positive classes for higher accuracy.  1  Experiments are conducted on the ImageNet ILSVRC 2012 and 2013 datasets and establish state of the art results on the ILSVRC 2013 localization and detection tasks.  While images from the ImageNet classiﬁcation dataset are largely chosen to contain a roughly- centered object that ﬁlls much of the image, objects of interest sometimes vary signiﬁcantly in size and position within the image. The ﬁrst idea in addressing this is to apply a ConvNet at multiple locations in the image, in a sliding window fashion, and over multiple scales. Even with this, however, many viewing windows may contain a perfectly identiﬁable portion of the object (say, the head of a dog), but not the entire object, nor even the center of the object. This leads to decent classiﬁcation but poor localization and detection. Thus, the second idea is to train the system to not only produce a distribution over categories for each window, but also to produce a prediction of the location and size of the bounding box containing the object relative to the window. The third idea is to accumulate the evidence for each category at each location and size.  Many authors have proposed to use ConvNets for detection and localization with a sliding window over multiple scales, going back to the early 1990’s for multi-character strings [20], faces [30], and hands [22]. More recently, ConvNets have been shown to yield state of the art performance on text detection in natural images [4], face detection [8, 23] and pedestrian detection [25].  Several authors have also proposed to train ConvNets to directly predict the instantiation parameters of the objects to be located, such as the position relative to the viewing window, or the pose of the object. For example Osadchy et al. [23] describe a ConvNet for simultaneous face detection and pose estimation. Faces are represented by a 3D manifold in the nine-dimensional output space. Positions on the manifold indicate the pose (pitch, yaw, and roll). When the training image is a face, the network is trained to produce a point on the manifold at the location of the known pose. If the image is not a face, the output is pushed away from the manifold. At test time, the distance to the manifold indicate whether the image contains a face, and the position of the closest point on the manifold indicates pose. Taylor et al. [27, 28] use a ConvNet to estimate the location of body parts (hands, head, etc) so as to derive the human body pose. They use a metric learning criterion to train the network to produce points on a body pose manifold. Hinton et al. have also proposed to train networks to compute explicit instantiation parameters of features as part of a recognition process [12].  Other authors have proposed to perform object localization via ConvNet-based segmentation. The simplest approach consists in training the ConvNet to classify the central pixel (or voxel for vol- umetric images) of its viewing window as a boundary between regions or not [13]. But when the regions must be categorized, it is preferable to perform semantic segmentation. The main idea is to train the ConvNet to classify the central pixel of the viewing window with the category of the ob- ject it belongs to, using the window as context for the decision. Applications range from biological image analysis [21], to obstacle tagging for mobile robots [10] to tagging of photos [7]. The ad- vantage of this approach is that the bounding contours need not be rectangles, and the regions need not be well-circumscribed objects. The disadvantage is that it requires dense pixel-level labels for training. This segmentation pre-processing or object proposal step has recently gained popularity in traditional computer vision to reduce the search space of position, scale and aspect ratio for detec- tion [19, 2, 6, 29]. Hence an expensive classiﬁcation method can be applied at the optimal location in the search space, thus increasing recognition accuracy. Additionally, [29, 1] suggest that these methods improve accuracy by drastically reducing unlikely object regions, hence reducing potential false positives. Our dense sliding window method, however, is able to outperform object proposal methods on the ILSVRC13 detection dataset.  Krizhevsky et al. [15] recently demonstrated impressive classiﬁcation performance using a large ConvNet. The authors also entered the ImageNet 2012 competition, winning both the classiﬁcation and localization challenges. Although they demonstrated an impressive localization performance, there has been no published work describing how their approach. Our paper is thus the ﬁrst to provide a clear explanation how ConvNets can be used for localization and detection for ImageNet data.  In this paper we use the terms localization and detection in a way that is consistent with their use in the ImageNet 2013 competition, namely that the only difference is the evaluation criterion used and both involve predicting the bounding box for each object in the image.  2  Figure 1: Localization (top) and detection tasks (bottom). The left images contains our predic- tions (ordered by decreasing conﬁdence) while the right images show the groundtruth labels. The detection image (bottom) illustrates the higher difﬁculty of the detection dataset, which can contain many small objects while the classiﬁcation and localization images typically contain a single large object.  2 Vision Tasks  In this paper, we explore three computer vision tasks in increasing order of difﬁculty: (i) classi- ﬁcation, (ii) localization, and (iii) detection. Each task is a sub-task of the next. While all tasks are adressed using a single framework and a shared feature learning base, we will describe them separately in the following sections.  Throughout the paper, we report results on the 2013 ImageNet Large Scale Visual Recognition Chal- lenge (ILSVRC2013). In the classiﬁcation task of this challenge, each image is assigned a single label corresponding to the main object in the image. Five guesses are allowed to ﬁnd the correct answer (this is because images can also contain multiple unlabeled objects). The localization task is similar in that 5 guesses are allowed per image, but in addition, a bounding box for the predicted object must be returned with each guess. To be considered correct, the predicted box must match the groundtruth by at least 50% (using the PASCAL criterion of union over intersection), as well as be labeled with the correct class (i.e. each prediction is a label and bounding box that are associated together). The detection task differs from localization in that there can be any number of objects in each image (including zero), and false positives are penalized by the mean average precision  3  (mAP) measure. The localization task is a convenient intermediate step between classiﬁcation and detection, and allows us to evaluate our localization method independently of challenges speciﬁc to detection (such as learning a background class). In Fig. 1, we show examples of images with our localization/detection predictions as well as corresponding groundtruth. Note that classiﬁcation and localization share the same dataset, while detection also has additional data where objects can be smaller. The detection data also contain a set of images where certain objects are absent. This can be used for bootstrapping, but we have not made use of it in this work.  3 Classiﬁcation  Our classiﬁcation architecture is similar to the best ILSVRC12 architecture by Krizhevsky et al. [15]. However, we improve on the network design and the inference step. Because of time constraints, some of the training features in Krizhevsky’s model were not explored, and so we expect our results can be improved even further. These are discussed in the future work section 6  Figure 2: Layer 1 (top) and layer 2 ﬁlters (bottom).  3.1 Model Design and Training  We train the network on the ImageNet 2012 training set (1.2 million images and C = 1000 classes) [5]. Our model uses the same ﬁxed input size approach proposed by Krizhevsky et al. [15] during training but turns to multi-scale for classiﬁcation as described in the next section. Each image is downsampled so that the smallest dimension is 256 pixels. We then extract 5 random crops (and their horizontal ﬂips) of size 221x221 pixels and present these to the network in mini-batches of size 128. The weights in the network are initialized randomly with (µ, σ) = (0, 1 × 10−2). They are then updated by stochastic gradient descent, accompanied by momentum term of 0.6 and an ℓ2 weight decay of 1 × 10−5. The learning rate is initially 5 × 10−2 and is successively decreased by a factor of 0.5 after (30, 50, 60, 70, 80) epochs. DropOut [11] with a rate of 0.5 is employed on the fully connected layers (6th and 7th) in the classiﬁer.  We detail the architecture sizes in tables 1 and 3. Note that during training, we treat this architecture as non-spatial (output maps of size 1x1), as opposed to the inference step, which produces spatial outputs. Layers 1-5 are similar to Krizhevsky et al. [15], using rectiﬁcation (“relu”) non-linearities and max pooling, but with the following differences: (i) no contrast normalization is used; (ii) pooling regions are non-overlapping and (iii) our model has larger 1st and 2nd layer feature maps, thanks to a smaller stride (2 instead of 4). A larger stride is beneﬁcial for speed but will hurt accuracy.  4  Layer Stage # channels Filter size Conv. stride Pooling size Pooling stride Zero-Padding size Spatial input size  1  2  conv + max  conv + max  96  11x11 4x4 2x2 2x2  -  256 5x5 1x1 2x2 2x2  -  231x231  24x24  3  conv 512 3x3 1x1  - -  4  conv 1024 3x3 1x1  - -  5  conv + max  1024 3x3 1x1 2x2 2x2  1x1x1x1 12x12  1x1x1x1 12x12  1x1x1x1 12x12  6 full 3072  7 full 4096  - - - - -  - - - - -  Output  8 full 1000  - - - - -  6x6  1x1  1x1  Table 1: Architecture speciﬁcs for fast model. The spatial size of the feature maps depends on the input image size, which varies during our inference step (see Table 5 in the Appendix). Here we show training spatial sizes. Layer 5 is the top convolutional layer. Subsequent layers are fully connected, and applied in sliding window fashion at test time. The fully-connected layers can also be seen as 1x1 convolutions in a spatial setting. Similar sizes for accurate model can be found in the Appendix.  In Fig. 2, we show the ﬁlter coefﬁcients from the ﬁrst two convolutional layers. The ﬁrst layer ﬁlters capture orientated edges, patterns and blobs. In the second layer, the ﬁlters have a variety of forms, some diffuse, others with strong line structures or oriented edges.  3.2 Feature Extractor  Along with this paper, we release a feature extractor named “OverFeat” 1 in order to provide power- ful features for computer vision research. Two models are provided, a fast and accurate one. Each architecture is described in tables 1 and 3. We also compare their sizes in Table 4 in terms of param- eters and connections. The accurate model is more accurate than the fast one (14.18% classiﬁcation error as opposed to 16.39% in Table 2), however it requires nearly twice as many connections. Using a committee of 7 accurate models reaches 13.6% classiﬁcation error as shown in Fig. 4.  3.3 Multi-Scale Classiﬁcation  In [15], multi-view voting is used to boost performance: a ﬁxed set of 10 views (4 corners and center, with horizontal ﬂip) is averaged. However, this approach can ignore many regions of the image, and is computationally redundant when views overlap. Additionally, it is only applied at a single scale, which may not be the scale at which the ConvNet will respond with optimal conﬁdence.  Instead, we explore the entire image by densely running the network at each location and at multiple scales. While the sliding window approach may be computationally prohibitive for certain types of model, it is inherently efﬁcient in the case of ConvNets (see section 3.5). This approach yields signiﬁcantly more views for voting, which increases robustness while remaining efﬁcient. The result of convolving a ConvNet on an image of arbitrary size is a spatial map of C-dimensional vectors at each scale.  However, the total subsampling ratio in the network described above is 2x3x2x3, or 36. Hence when applied densely, this architecture can only produce a classiﬁcation vector every 36 pixels in the input dimension along each axis. This coarse distribution of outputs decreases performance compared to the 10-view scheme because the network windows are not well aligned with the objects in the images. The better aligned the network window and the object, the strongest the conﬁdence of the network response. To circumvent this problem, we take an approach similar to that introduced by Giusti et al. [9], and apply the last subsampling operation at every offset. This removes the loss of resolution from this layer, yielding a total subsampling ratio of x12 instead of x36.  We now explain in detail how the resolution augmentation is performed. We use 6 scales of input which result in unpooled layer 5 maps of varying resolution (see Table 5 for details). These are then pooled and presented to the classiﬁer using the following procedure, illustrated in Fig. 3:  (a) For a single image, at a given scale, we start with the unpooled layer 5 feature maps.  1http://cilvr.nyu.edu/doku.php?id=software:overfeat:start  5  (b) Each of unpooled maps undergoes a 3x3 max pooling operation (non-overlapping regions),  repeated 3x3 times for (∆x, ∆y) pixel offsets of {0, 1, 2}.  (c) This produces a set of pooled feature maps, replicated (3x3) times for different (∆x, ∆y) com-  binations.  (d) The classiﬁer (layers 6,7,8) has a ﬁxed input size of 5x5 and produces a C-dimensional output vector for each location within the pooled maps. The classiﬁer is applied in sliding-window fashion to the pooled maps, yielding C-dimensional output maps (for a given (∆x, ∆y) combi- nation).  (e) The output maps for different (∆x, ∆y) combinations are reshaped into a single 3D output map  (two spatial dimensions x C classes).  Figure 3: 1D illustration (to scale) of output map computation for classiﬁcation, using y-dimension from scale 2 as an example (see Table 5). (a): 20 pixel unpooled layer 5 feature map. (b): max pooling over non-overlapping 3 pixel groups, using offsets of ∆ = {0, 1, 2} pixels (red, green, blue respectively). (c): The resulting 6 pixel pooled maps, for different ∆. (d): 5 pixel classiﬁer (layers 6,7) is applied in sliding window fashion to pooled maps, yielding 2 pixel by C maps for each ∆. (e): reshaped into 6 pixel by C output maps.  These operations can be viewed as shifting the classiﬁer’s viewing window by 1 pixel through pool- ing layers without subsampling and using skip-kernels in the following layer (where values in the neighborhood are non-adjacent). Or equivalently, as applying the ﬁnal pooling layer and fully- connected stack at every possible offset, and assembling the results by interleaving the outputs.  The procedure above is repeated for the horizontally ﬂipped version of each image. We then produce the ﬁnal classiﬁcation by (i) taking the spatial max for each class, at each scale and ﬂip; (ii) averaging the resulting C-dimensional vectors from different scales and ﬂips and (iii) taking the top-1 or top-5 elements (depending on the evaluation criterion) from the mean class vector.  At an intuitive level, the two halves of the network — i.e. feature extraction layers (1-5) and classiﬁer layers (6-output) — are used in opposite ways. In the feature extraction portion, the ﬁlters are convolved across the entire image in one pass. From a computational perspective, this is far more efﬁcient than sliding a ﬁxed-size feature extractor over the image and then aggregating the results from different locations2. However, these principles are reversed for the classiﬁer portion of the network. Here, we want to hunt for a ﬁxed-size representation in the layer 5 feature maps across different positions and scales. Thus the classiﬁer has a ﬁxed-size 5x5 input and is exhaustively applied to the layer 5 maps. The exhaustive pooling scheme (with single pixel shifts (∆x, ∆y)) ensures that we can obtain ﬁne alignment between the classiﬁer and the representation of the object in the feature map.  3.4 Results  In Table 2, we experiment with different approaches, and compare them to the single network model of Krizhevsky et al. [15] for reference. The approach described above, with 6 scales, achieves a top-5 error rate of 13.6%. As might be expected, using fewer scales hurts performance: the single- scale model is worse with 16.97% top-5 error. The ﬁne stride technique illustrated in Fig. 3 brings a relatively small improvement in the single scale regime, but is also of importance for the multi-scale gains shown here.  2Our network with 6 scales takes around 2 secs on a K20x GPU to process one image  6  Approach Krizhevsky et al. [15] OverFeat - 1 fast model, scale 1, coarse stride OverFeat - 1 fast model, scale 1, ﬁne stride OverFeat - 1 fast model, 4 scales (1,2,4,6), ﬁne stride OverFeat - 1 fast model, 6 scales (1-6), ﬁne stride OverFeat - 1 accurate model, 4 corners + center + ﬂip OverFeat - 1 accurate model, 4 scales, ﬁne stride OverFeat - 7 fast models, 4 scales, ﬁne stride OverFeat - 7 accurate models, 4 scales, ﬁne stride  Top-1 Top-5 error % error %  40.7 39.28 39.01 38.57 38.12 35.60 35.74 35.10 33.96  18.2 17.12 16.97 16.39 16.27 14.71 14.18 13.86 13.24  Table 2: Classiﬁcation experiments on validation set. Fine/coarse stride refers to the number of ∆ values used when applying the classiﬁer. Fine: ∆ = 0, 1, 2; coarse: ∆ = 0.  Figure 4: Test set classiﬁcation results. During the competition, OverFeat yielded 14.2% top 5 error rate using an average of 7 fast models. In post-competition work, OverFeat ranks ﬁfth with 13.6% error using bigger models (more features and more layers).  We report the test set results of the 2013 competition in Fig. 4 where our model (OverFeat) obtained 14.2% accuracy by voting of 7 ConvNets (each trained with different initializations) and ranked 5th out of 18 teams. The best accuracy using only ILSVRC13 data was 11.7%. Pre-training with extra data from the ImageNet Fall11 dataset improved this number to 11.2%. In post-competition work, we improve the OverFeat results down to 13.6% error by using bigger models (more features and more layers). Due to time constraints, these bigger models are not fully trained, more improvements are expected to appear in time.  3.5 ConvNets and Sliding Window Efﬁciency  In contrast to many sliding-window approaches that compute an entire pipeline for each window of the input one at a time, ConvNets are inherently efﬁcient when applied in a sliding fashion because they naturally share computations common to overlapping regions. When applying our network to larger images at test time, we simply apply each convolution over the extent of the full image. This extends the output of each layer to cover the new image size, eventually producing a map of output class predictions, with one spatial location for each “window” (ﬁeld of view) of input. This  7  convolution  pooling  conv  conv  conv  input  1st stage  classifier  output  convolution  pooling  conv  conv  conv  input  1st stage  classifier  output  Figure 5: The efﬁciency of ConvNets for detection. During training, a ConvNet produces only a single spatial output (top). But when applied at test time over a larger image, it produces a spatial output map, e.g. 2x2 (bottom). Since all layers are applied convolutionally, the extra computa- tion required for the larger image is limited to the yellow regions. This diagram omits the feature dimension for simplicity.  is diagrammed in Fig. 5. Convolutions are applied bottom-up, so that the computations common to neighboring windows need only be done once.  Note that the last layers of our architecture are fully connected linear layers. At test time, these layers are effectively replaced by convolution operations with kernels of 1x1 spatial extent. The entire ConvNet is then simply a sequence of convolutions, max-pooling and thresholding operations exclusively.  4 Localization  Starting from our classiﬁcation-trained network, we replace the classiﬁer layers by a regression network and train it to predict object bounding boxes at each spatial location and scale. We then combine the regression predictions together, along with the classiﬁcation results at each location, as we now describe.  4.1 Generating Predictions  To generate object bounding box predictions, we simultaneously run the classiﬁer and regressor networks across all locations and scales. Since these share the same feature extraction layers, only the ﬁnal regression layers need to be recomputed after computing the classiﬁcation network. The output of the ﬁnal softmax layer for a class c at each location provides a score of conﬁdence that an object of class c is present (though not necessarily fully contained) in the corresponding ﬁeld of view. Thus we can assign a conﬁdence to each bounding box.  4.2 Regressor Training  The regression network takes as input the pooled feature maps from layer 5. It has 2 fully-connected hidden layers of size 4096 and 1024 channels, respectively. The ﬁnal output layer has 4 units which specify the coordinates for the bounding box edges. As with classiﬁcation, there are (3x3) copies throughout, resulting from the ∆x, ∆y shifts. The architecture is shown in Fig. 8.  8  Figure 6: Localization/Detection pipeline. The raw classiﬁer/detector outputs a class and a con- ﬁdence for each location (1st diagram). The resolution of these predictions can be increased using the method described in section 3.3 (2nd diagram). The regression then predicts the location scale of the object with respect to each window (3rd diagram). These bounding boxes are then merge and accumulated to a small number of objects (4th diagram).  9  Figure 7: Examples of bounding boxes produced by the regression network, before being com- bined into ﬁnal predictions. The examples shown here are at a single scale. Predictions may be more optimal at other scales depending on the objects. Here, most of the bounding boxes which are initially organized as a grid, converge to a single location and scale. This indicates that the network is very conﬁdent in the location of the object, as opposed to being spread out randomly. The top left image shows that it can also correctly identify multiple location if several objects are present. The various aspect ratios of the predicted bounding boxes shows that the network is able to cope with various object poses.  We ﬁx the feature extraction layers (1-5) from the classiﬁcation network and train the regression network using an ℓ2 loss between the predicted and true bounding box for each example. The ﬁnal regressor layer is class-speciﬁc, having 1000 different versions, one for each class. We train this network using the same set of scales as described in Section 3. We compare the prediction of the regressor net at each spatial location with the ground-truth bounding box, shifted into the frame of reference of the regressor’s translation offset within the convolution (see Fig. 8). However, we do not train the regressor on bounding boxes with less than 50% overlap with the input ﬁeld of view: since the object is mostly outside of these locations, it will be better handled by regression windows that do contain the object.  Training the regressors in a multi-scale manner is important for the across-scale prediction combi- nation. Training on a single scale will perform well on that scale and still perform reasonably on other scales. However training multi-scale will make predictions match correctly across scales and exponentially increase the conﬁdence of the merged predictions. In turn, this allows us to perform well with a few scales only, rather than many scales as is typically the case in detection. The typical ratio from one scale to another in pedestrian detection [25] is about 1.05 to 1.1, here however we use a large ratio of approximately 1.4 (this number differs for each scale since dimensions are adjusted to ﬁt exactly the stride of our network) which allows us to run our system faster.  4.3 Combining Predictions  We combine the individual predictions (see Fig. 7) via a greedy merge strategy applied to the regres- sor bounding boxes, using the following algorithm.  (a) Assign to Cs the set of classes in the top k for each scale s ∈ 1 . . . 6, found by taking the  maximum detection class outputs across spatial locations for that scale.  (b) Assign to Bs the set of bounding boxes predicted by the regressor network for each class in Cs,  across all spatial locations at scale s.  10  Figure 8: Application of the regression network to layer 5 features, at scale 2, for example. (a) The input to the regressor at this scale are 6x7 pixels spatially by 256 channels for each of the (3x3) ∆x, ∆y shifts. (b) Each unit in the 1st layer of the regression net is connected to a 5x5 spatial neighborhood in the layer 5 maps, as well as all 256 channels. Shifting the 5x5 neighborhood around results in a map of 2x3 spatial extent, for each of the 4096 channels in the layer, and for each of the (3x3) ∆x, ∆y shifts. (c) The 2nd regression layer has 1024 units and is fully connected (i.e. the purple element only connects to the purple element in (b), across all 4096 channels). (d) The output of the regression network is a 4-vector (specifying the edges of the bounding box) for each location in the 2x3 map, and for each of the (3x3) ∆x, ∆y shifts.  (c) Assign B ← Ss Bs (d) Repeat merging until done:  (e)  (f)  (g)  (b∗  1, b∗  2) = argminb16=b2∈Bmatch score(b1, b2)  If match score(b∗  1, b∗  2) > t , stop.  Otherwise, set B ← B\{b∗  1, b∗  2} ∪ box merge(b∗  1, b∗ 2)  In the above, we compute match score using the sum of the distance between centers of the two bounding boxes and the intersection area of the boxes. box merge compute the average of the bounding boxes’ coordinates.  The ﬁnal prediction is given by taking the merged bounding boxes with maximum class scores. This is computed by cumulatively adding the detection class outputs associated with the input windows from which each bounding box was predicted. See Fig. 6 for an example of bounding boxes merged into a single high-conﬁdence bounding box. In that example, some turtle and whale bounding boxes appear in the intermediate multi-scale steps, but disappear in the ﬁnal detection image. Not only do these bounding boxes have low classiﬁcation conﬁdence (at most 0.11 and 0.12 respectively), their collection is not as coherent as the bear bounding boxes to get a signiﬁcant conﬁdence boost. The bear boxes have a strong conﬁdence (approximately 0.5 on average per scale) and high matching scores. Hence after merging, many bear bounding boxes are fused into a single very high conﬁdence box, while false positives disappear below the detection threshold due their lack of bounding box coherence and conﬁdence. This analysis suggest that our approach is naturally more robust to false positives coming from the pure-classiﬁcation model than traditional non-maximum suppression, by rewarding bounding box coherence.  11  Figure 9: Localization experiments on ILSVRC12 validation set. We experiment with different number of scales and with the use of single-class regression (SCR) or per-class regression (PCR).  4.4 Experiments  We apply our network to the Imagenet 2012 validation set using the localization criterion speciﬁed for the competition. The results for this are shown in Fig. 9. Fig. 10 shows the results of the 2012 and 2013 localization competitions (the train and test data are the same for both of these years). Our method is the winner of the 2013 competition with 29.9% error.  Our multiscale and multi-view approach was critical to obtaining good performance, as can be seen in Fig. 9: Using only a single centered crop, our regressor network achieves an error rate of 40%. By combining regressor predictions from all spatial locations at two scales, we achieve a vastly better error rate of 31.5%. Adding a third and fourth scale further improves performance to 30.0% error.  Using a different top layer for each class in the regressor network for each class (Per-Class Regres- sor (PCR) in Fig. 9) surprisingly did not outperform using only a single network shared among all classes (44.1% vs. 31.3%). This may be because there are relatively few examples per class an- notated with bounding boxes in the training set, while the network has 1000 times more top-layer parameters, resulting in insufﬁcient training. It is possible this approach may be improved by shar- ing parameters only among similar classes (e.g. training one network for all classes of dogs, another for vehicles, etc.).  5 Detection  Detection training is similar to classiﬁcation training but in a spatial manner. Multiple location of an image may be trained simultaneously. Since the model is convolutional, all weights are shared among all locations. The main difference with the localization task, is the necessity to predict a background class when no object is present. Traditionally, negative examples are initially taken at random for training. Then the most offending negative errors are added to the training set in boot- strapping passes. Independent bootstrapping passes render training complicated and risk potential mismatches between the negative examples collection and training times. Additionally, the size of bootstrapping passes needs to be tuned to make sure training does not overﬁt on a small set. To cir- cumvent all these problems, we perform negative training on the ﬂy, by selecting a few interesting negative examples per image such as random ones or most offending ones. This approach is more computationally expensive, but renders the procedure much simpler. And since the feature extraction is initially trained with the classiﬁcation task, the detection ﬁne-tuning is not as long anyway.  In Fig. 11, we report the results of the ILSVRC 2013 competition where our detection system ranked 3rd with 19.4% mean average precision (mAP). We later established a new detection state of the art with 24.3% mAP. Note that there is a large gap between the top 3 methods and other teams (the 4th  12  Figure 10: ILSVRC12 and ILSVRC13 competitions results (test set). Our entry is the winner of the ILSVRC13 localization competition with 29.9% error (top 5). Note that training and testing data is the same for both years. The OverFeat entry uses 4 scales and a single-class regression approach.  Figure 11: ILSVRC13 test set Detection results. During the competition, UvA ranked ﬁrst with 22.6% mAP. In post competition work, we establish a new state of the art with 24.3% mAP. Systems marked with * were pre-trained with the ILSVRC12 classiﬁcation data.  method yields 11.5% mAP). Additionally, our approach is considerably different from the top 2 other systems which use an initial segmentation step to reduce candidate windows from approximately 200,000 to 2,000. This technique speeds up inference and substantially reduces the number of potential false positives. [29, 1] suggest that detection accuracy drops when using dense sliding window as opposed to selective search which discards unlikely object locations hence reducing false positives. Combined with our method, we may observe similar improvements as seen here between traditional dense methods and segmentation based methods. It should also be noted that  13  we did not ﬁne tune on the detection validation set as NEC and UvA did. The validation and test set distributions differ signiﬁcantly enough from the training set that this alone improves results by approximately 1 point. The improvement between the two OverFeat results in Fig. 11 are due to longer training times and the use of context, i.e. each scale also uses lower resolution scales as input.  6 Discussion  We have presented a multi-scale, sliding window approach that can be used for classiﬁcation, lo- calization and detection. We applied it to the ILSVRC 2013 datasets, and it currently ranks 4th in classiﬁcation, 1st in localization and 1st in detection. A second important contribution of our paper is explaining how ConvNets can be effectively used for detection and localization tasks. These were never addressed in [15] and thus we are the ﬁrst to explain how this can be done in the context of Im- ageNet 2012. The scheme we propose involves substantial modiﬁcations to networks designed for classiﬁcation, but clearly demonstrate that ConvNets are capable of these more challenging tasks. Our localization approach won the 2013 ILSVRC competition and signiﬁcantly outperformed all 2012 and 2013 approaches. The detection model was among the top performers during the compe- tition, and ranks ﬁrst in post-competition results. We have proposed an integrated pipeline that can perform different tasks while sharing a common feature extraction base, entirely learned directly from the pixels.  Our approach might still be improved in several ways. (i) For localization, we are not currently back-propping through the whole network; doing so is likely to improve performance. (ii) We are using ℓ2 loss, rather than directly optimizing the intersection-over-union (IOU) criterion on which performance is measured. Swapping the loss to this should be possible since IOU is still differen- tiable, provided there is some overlap. (iii) Alternate parameterizations of the bounding box may help to decorrelate the outputs, which will aid network training.  References  [1] J. Carreira, F. Li, and C. Sminchisescu. Object recognition by sequential ﬁgure-ground ranking. Interna-  tional journal of computer vision, 98(3):243–262, 2012.  [2] J. Carreira and C. Sminchisescu. Constrained parametric min-cuts for automatic object segmentation,  release 1. http://sminchisescu.ins.uni-bonn.de/code/cpmc/.  [3] D. C. Ciresan, J. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classiﬁcation.  In CVPR, 2012.  [4] M. Delakis and C. Garcia. Text detection with convolutional neural networks. In International Conference  on Computer Vision Theory and Applications (VISAPP 2008), 2008.  [5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical  Image Database. In CVPR09, 2009.  [6] I. Endres and D. Hoiem. Category independent object proposals. In Computer Vision–ECCV 2010, pages  575–588. Springer, 2010.  [7] C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learning hierarchical features for scene labeling. IEEE  Transactions on Pattern Analysis and Machine Intelligence, 2013. in press.  [8] C. Garcia and M. Delakis. Convolutional face ﬁnder: A neural architecture for fast and robust face  detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2004.  [9] A. Giusti, D. C. Ciresan, J. Masci, L. M. Gambardella, and J. Schmidhuber. Fast image scanning with deep max-pooling convolutional neural networks. In International Conference on Image Processing (ICIP), 2013.  [10] R. Hadsell, P. Sermanet, M. Scofﬁer, A. Erkan, K. Kavackuoglu, U. Muller, and Y. LeCun. Learning long-range vision for autonomous off-road driving. Journal of Field Robotics, 26(2):120–144, February 2009.  [11] G. Hinton, N. Srivastave, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving neural net-  works by preventing co-adaptation of feature detectors. arXiv:1207.0580, 2012.  [12] G. E. Hinton, A. Krizhevsky, and S. D. Wang. Transforming auto-encoders. In Artiﬁcial Neural Networks  and Machine Learning–ICANN 2011, pages 44–51. Springer Berlin Heidelberg, 2011.  [13] V. Jain, J. F. Murray, F. Roth, S. Turaga, V. Zhigulin, K. Briggman, M. Helmstaedter, W. Denk, and H. S.  Seung. Supervised learning of image restoration with convolutional networks. In ICCV’07.  [14] K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun. What is the best multi-stage architecture for  object recognition? In Proc. International Conference on Computer Vision (ICCV’09). IEEE, 2009.  14  [15] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deep convolutional neural net-  works. In NIPS, 2012.  [16] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Hand- written digit recognition with a back-propagation network. In D. Touretzky, editor, Advances in Neural Information Processing Systems (NIPS 1989), volume 2, Denver, CO, 1990. Morgan Kaufman.  [17] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.  Proceedings of the IEEE, 86(11):2278–2324, November 1998.  [18] Y. LeCun, F.-J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance  to pose and lighting. In Proceedings of CVPR’04. IEEE Press, 2004.  [19] S. Manen, M. Guillaumin, and L. Van Gool. Prime object proposals with randomized prims algorithm. In  International Conference on Computer Vision (ICCV), 2013.  [20] O. Matan, J. Bromley, C. Burges, J. Denker, L. Jackel, Y. LeCun, E. Pednault, W. Satterﬁeld, C. Stenard, and T. Thompson. Reading handwritten digits: A zip code recognition system. IEEE Computer, 25(7):59– 63, July 1992.  [21] F. Ning, D. Delhomme, Y. LeCun, F. Piano, L. Bottou, and P. Barbano. Toward automatic phenotyping of developing embryos from videos. IEEE Transactions on Image Processing, 14(9):1360–1371, September 2005. Special issue on Molecular and Cellular Bioimaging.  [22] S. Nowlan and J. Platt. A convolutional neural network hand tracker. pages 901–908, San Mateo, CA,  1995. Morgan Kaufmann.  [23] M. Osadchy, Y. LeCun, and M. Miller. Synergistic face detection and pose estimation with energy-based  models. Journal of Machine Learning Research, 8:1197–1215, May 2007.  [24] P. Sermanet, S. Chintala, and Y. LeCun. Convolutional neural networks applied to house numbers digit  classiﬁcation. In International Conference on Pattern Recognition (ICPR 2012), 2012.  [25] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun. Pedestrian detection with unsupervised multi- stage feature learning. In Proc. International Conference on Computer Vision and Pattern Recognition (CVPR’13). IEEE, June 2013.  [26] P. Sermanet and Y. LeCun. Trafﬁc sign recognition with multi-scale convolutional networks. In Proceed-  ings of International Joint Conference on Neural Networks (IJCNN’11), 2011.  [27] G. Taylor, R. Fergus, G. Williams, I. Spiro, and C. Bregler. Pose-sensitive embedding by nonlinear nca  regression. In NIPS, 2011.  [28] G. Taylor, I. Spiro, C. Bregler, and R. Fergus. Learning invarance through imitation. In CVPR, 2011. [29] J. R. R. Uijlings, K. E. A. van de Sande, T. Gevers, and A. W. M. Smeulders. Selective search for object  recognition. International Journal of Computer Vision, 104(2):154–171, 2013.  [30] R. Vaillant, C. Monrocq, and Y. LeCun. Original approach for the localisation of objects in images. IEE  Proc on Vision, Image, and Signal Processing, 141(4):245–250, August 1994.  15  Appendix: Additional Model Details  Layer Stage # channels Filter size Conv. stride Pooling size Pooling stride Zero-Padding size Spatial input size  1  2  conv + max  conv + max  96 7x7 2x2 3x3 3x3  -  256 7x7 1x1 2x2 2x2  -  221x221  36x36  3  conv 512 3x3 1x1  - -  4  conv 512 3x3 1x1  - -  5  conv 1024 3x3 1x1  - -  6  conv + max  1024 3x3 1x1 3x3 3x3  1x1x1x1 15x15  1x1x1x1 15x15  1x1x1x1 15x15  1x1x1x1 15x15  7 full 4096  8 full 4096  - - - - -  - - - - -  Output  9 full 1000  - - - - -  5x5  1x1  1x1  Table 3: Architecture speciﬁcs for accurate model. It differs from the fast model mainly in the stride of the ﬁrst convolution, the number of stages and the number of feature maps.  model  Krizhevsky  fast  accurate  # parameters (in millions)  # connections (in millions)  60 145 144  -  2810 5369  Table 4: Number of parameters and connections for different models.  Scale  1 2 3 4 5 6  Input size  245x245 281x317 317x389 389x461 425x497 461x569  Layer 5 pre-pool 17x17 20x23 23x29 29x35 32x35 35x44  Layer 5 post-pool (5x5)x(3x3) (6x7)x(3x3) (7x9)x(3x3) (9x11)x(3x3) (10x11)x(3x3) (11x14)x(3x3)  Classiﬁer  map (pre-reshape)  (1x1)x(3x3)xC (2x3)x(3x3)xC (3x5)x(3x3)xC (5x7)x(3x3)xC (6x7)x(3x3)xC (7x10)x(3x3)xC  Classiﬁer map size 3x3xC 6x9xC 9x15xC 15x21xC 18x24xC 21x30xC  Table 5: Spatial dimensions of our multi-scale approach. 6 different sizes of input images are used, resulting in layer 5 unpooled feature maps of differing spatial resolution (although not indi- cated in the table, all have 256 feature channels). The (3x3) results from our dense pooling operation with (∆x, ∆y) = {0, 1, 2}. See text and Fig. 3 for details for how these are converted into output maps.  16  ","We present an integrated framework for using Convolutional Networks forclassification, localization and detection. We show how a multiscale andsliding window approach can be efficiently implemented within a ConvNet. Wealso introduce a novel deep learning approach to localization by learning topredict object boundaries. Bounding boxes are then accumulated rather thansuppressed in order to increase detection confidence. We show that differenttasks can be learned simultaneously using a single shared network. Thisintegrated framework is the winner of the localization task of the ImageNetLarge Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained verycompetitive results for the detection and classifications tasks. Inpost-competition work, we establish a new state of the art for the detectiontask. Finally, we release a feature extractor from our best model calledOverFeat."
1312.6159,2014,Learned versus Hand-Designed Feature Representations for 3d Agglomeration  ,"['John A. Bogovic', 'Gary B. Huang', 'Viren Jain']",https://arxiv.org/pdf/1312.6159.pdf,"3 1 0 2   c e D 0 2         ]  V C . s c [      1 v 9 5 1 6  .  2 1 3 1 : v i X r a  Learned versus Hand-Designed Feature Representations for 3d Agglomeration  John A. Bogovic, Gary B. Huang & Viren Jain  Janelia Farm Research Campus Howard Hughes Medical Institute  19700 Helix Drive, Ashburn, VA, USA  {bogovicj, huangg, jainv}@janelia.hhmi.org  Abstract  For image recognition and labeling tasks, recent results suggest that machine learning methods that rely on manually speciﬁed feature representations may be outperformed by methods that automatically derive feature representations based on the data. Yet for problems that involve analysis of 3d objects, such as mesh segmentation, shape retrieval, or neuron fragment agglomeration, there remains a strong reliance on hand-designed feature descriptors. In this paper, we evaluate a large set of hand-designed 3d feature descriptors alongside features learned from the raw data using both end-to-end and unsupervised learning techniques, in the context of agglomeration of 3d neuron fragments. By combining unsupervised learning techniques with a novel dynamic pooling scheme, we show how pure learning-based methods are for the ﬁrst time competitive with hand-designed 3d shape descriptors. We investigate data augmentation strategies for dramatically increasing the size of the training set, and show how combining both learned and hand-designed features leads to the highest accuracy.  1  Introduction  A core issue underlying any machine learning approach is the choice of feature representation. Tra- ditionally, features have been hand-designed according to domain knowledge and experience (for example, Gabor ﬁlters for image analysis or cepstral coefﬁcients for automatic speech recognition). Recently, it has become more common to attempt to learn features based on supervised or unsuper- vised learning methods [22, 15, 4, 7, 8, 26, 11]. These automatically derived feature representations have the advantage of not requiring domain expertise and potentially yielding a much larger set of features for a classiﬁer. Perhaps most importantly, however, automatic methods may discover fea- tures that are more ﬁnely tuned for the particular problem being solved and thus lead to improved accuracy. For many problems that involve analysis of 3d objects there remains a strong reliance on hand- designed feature descriptors even when machine learning is used in conjunction with such descrip- tors. For example, the ﬁeld of 3d shape retrieval has a substantial history of benchmarking hand- designed shape descriptors [23, 31]. Mesh segmentation has recently been addressed using a con- ditional random ﬁeld with energy terms based on a hand-curated set of shape-based features [20]. Supervoxel agglomeration for connectomic reconstruction of neurons has, thus far, also been largely dependent on manually speciﬁed feature representations [1, 19]. Designing features for representing speciﬁc kinds of 3d objects is arguably more intuitive as com- pared to hand-designing representations for more low-level data (such as raw image patches). For example, describing a neuron fragment in terms of quantities such as curvature, volume, and orien- tation seems natural. On the other hand, it is less clear this intuitive appeal is a good justiﬁcation  1  (a) Positive Example  (b) Negative Example  Figure 1: Renderings of canonical positive and negative edge examples from the training set. We denote a pair of supervoxels which are subject to binary classiﬁcation as a single edge in the overall agglomeration task [19]. The small black sphere indicates the decision point around which most computations are centered, and the dotted box indicates a cube with a length of 30 pixels in each dimension.  for such a feature representation in a speciﬁc task such as neuron fragment agglomeration (Figure 1 and supplementary Figure 5). The primary contributions of our work are:  1. A large set of diverse hand-designed 3d shape descriptors that dramatically improve perfor- mance over simple baseline features used in prior work. We evaluate each feature individu- ally, evaluate an ensemble set of all hand-designed features, and compare the computational cost of the features.  2. An unsupervised learning approach for deriving 3d feature descriptors that, when combined with a novel dynamic pooling scheme, yields performance comparable to an ensemble set of all hand-designed features. To our knowledge, this is the ﬁrst time purely learned features have been shown to provide competitive performance on a task involving analysis or classiﬁcation of 3d shapes.  3. An end-to-end supervised learning approach for deriving 3d feature descriptors. We intro- duce data augmentation strategies that dramatically expand the size of the training set and thus improve generalization performance of the end-to-end feature learning scheme.  2 Agglomeration of 3d Neuron Fragments  We focus on the application domain of segmentation of large-scale electron microscopy data for the purposes of ‘connectomic’ reconstruction of nervous system structure. Mapping neural circuit connectivity at the resolution of individual synapses is an important goal for neurobiology, which requires nanometer resolution imaging of tissue over large ﬁelds of view [14]. Interpreting the resulting tera- or peta-voxel sized datasets currently involves substantial human effort, and thus increased or complete automation through highly accurate computational reconstruction would be ideal [18]. Automated pipelines for segmentation of both natural and non-natural images have converged on a broadly similar set of steps: boundary prediction, oversegmentation, and agglomeration of seg- ments [12, 11, 19, 1, 5]. In this section we describe the source of the raw data, the creation of 3d segments, and the machine learning problem of fragment agglomeration. Electron microscopy images: Tissue from a drosophila melanogaster brain was imaged using fo- cused ion-beam scanning electron microscopy (FIB-SEM [21]) at a resolution of 8 × 8 × 8 nm. The tissue was prepared using high-pressure freeze substitution and stained with heavy metals for contrast during electron microscopy. As compared to traditional electron microscopy methods such as serial-section transmission electron microscopy (ssTEM), FIB-SEM provides the ability to image tissue at very high resolution in all three spatial dimensions. Isotropic resolution at the sub-10nm scale is particularly advantageous in drosophila due to the small neurite size that is typical through- out the neuropil. Boundary prediction: We trained a deep and wide multiscale recursive (DAWMR) network [17] to generate afﬁnity graphs from the electron microscopy data. Afﬁnity graphs are similar to pixel-wise  2  boundary prediction maps, except that they encode connectivity relationships between neighbor- ing pixels (in our case, 6-connectivity due to the 3d image space) [32]. We supplied the DAWMR network with 120 megavoxels of hand-segmented image data for training (with rotation and x-y re- ﬂection augmentations further increasing the total amount of data seen during training). The network uses a total ﬁeld of view of 50 × 50 × 50 pixels in the prediction of any single afﬁnity edge. The ground truth afﬁnity graphs are binary representations where 0 represents the case where two pixels are disconnected (belong to different objects, or are both part of ‘outside’ space unassigned to any object), and 1 represents the case where two pixels are part of the same object. The DAWMR networks, trained on this ground truth, generate analog [0, 1]-valued afﬁnity graphs. Oversegmentation: The DAWMR-generated afﬁnity graph is thresholded at a value of 0.9 and objects are ‘grown’ by a seeded watershed procedure to an afﬁnity value of 0.8. The afﬁnity graph is then re-segmented at 0.8, new objects are added into the overall segmentation, and all objects are grown to a threshold of 0.7. This procedure is repeated for thresholds 0.6 and 0.5. A distance- transform based object-breaking watershed procedure is then applied that slightly reduced the rate of undersegmentation in large objects. Finally, all objects are grown to a threshold of 0.2. Training and test sets: Two separate 200 megavoxel volumes were processed by the DAWMR network and oversegmented according to the procedure described above. Neither volume contained data used to train the boundary predictor. Pairs of segments within 1 pixel of each other (we refer to these identiﬁed segment-pairs as edges) were labeled by humans as to whether the segments belong to the same or different neuron. One of the two volumes was randomly chosen to be the training set (14, 522 edges: 7968 positive and 6584 negative), and the other volume serves as a test set (14, 829 edges: 8342 positive and 6487 negative). Figure 1 shows examples of both positive and negative segment-pairs. Learning binary agglomeration decisions: Superpixel agglomeration has recently been posed as a machine learning problem; some methods attempt to optimize classiﬁer performance over a se- quence of predictions that reﬂect, for example, variable ordering of agglomeration decisions based on classiﬁer conﬁdence [27, 19]. In this work, we simply train a classiﬁer on a one-step cost func- tion that reﬂects the ground truth binary edge assignments. This is designed to simplify the in- terpretation of feature contributions and ease the computational burden of the many classiﬁcation experiments we perform. We learn binary agglomeration decisions using a dropout multilayer per- ceptron (MLP) [16], and for comparison provide certain results using a decision-stump boosting classiﬁer [13].  3 Hand-Designed Features  In this section we describe the proposed hand-designed features and evaluate the performance of each feature by measuring its accuracy on the agglomeration classiﬁcation problem. The features for a given pair of segments are computed from a ﬁxed-radius subvolume centered around a ‘decision point’ between the two segments (Figure 1). The subvolume consists of the raw image values as well as the afﬁnity graph produced by the DAWMR network. For simplicity, we often collapse the afﬁnity graph by averaging over the three edge directions, which we refer to as the ‘boundary map.’ The decision point is deﬁned as the midpoint of the shortest line segment that touches both seg- ments. The motivation for this scheme lies in the intuition (based on observing human classiﬁcation strategies) that the relevant image and object information required to decide whether two segments should be merged is concentrated near the interface between the two segments.  3.1 Feature Descriptions  Boundary map statistics: after identifying a set of pixels that constitute the interface between two segments, we compute a number of statistics of the boundary map values over these pixels: mean, median, moments (variance, skewness, kurtosis), quartiles, length, minimum value, and maximum value. We also compute these statistics from the ﬁrst and second derivative of the boundary map. This follows many previous approaches that identify some type of interface between segments, mea- sure statistics at boundary map locations along this interface, and use these statistics as features to  3  (a) Ray Features  (b) Level Set Initialization  (c) Level Set Evolution  Figure 2: Demonstration of ray and level set features on positive edge examples. Rays originate in the red segment and penetrate the blue segment. The surfaces in (b) show the initialization state of the level set, and the multiple green surfaces in (c) show the results after various amounts of level set evolution.  train an agglomeration classiﬁer [1, 11, 19]. The boundary map is obtained by averaging the edges of the afﬁnity graph. As noted in Table 1, we consider the ensemble of these statistics (experiment 6) as a baseline feature set. Size: the volumes of both segments, and their log value. Proximity: a scalar giving the shortest distance from a voxel assigned to one segment to a voxel assigned to the other segment. Growth: segments are isolated within the component mask and are grown via a seeded watershed transform until they share a catchment basin. The afﬁnity graph value at which this occurs yields the ﬁrst growth feature. The second growth feature is given by the distance from the decision point to the location at which the catchment basins merge. Rays: lines are propagated from the centroid of a segment until they terminate [30]. The features describe the average distance these rays travel before termination under one of two conditions: the afﬁnity graph value falls below a speciﬁed threshold, or the ray exits a mask deﬁned by the union of the two segments. We seed rays from both segments and use ﬁve choices of afﬁnity graph threshold. Our experiments used 42 rays uniformly distributed over the sphere. Another type of ray feature describes the average distance rays travel through one segment when seeded from the other segment. Figure 2(a) shows an example of the rays used for this feature. SIFT: scale-invariant feature transform (SIFT) descriptors are computed that summarize the image gradient magnitudes and orientations near the decision point [24]. We cluster the descriptors using k-means with 50 clusters and represent each descriptor as a feature vector based on a soft vector quantization encoding. SIFT features are computed using both the image data and the afﬁnity graph. Angles: we compute two vectors, vo1 and vo2, giving the orientation of each of the segments and a third vector, vc, that points from the center of mass of one segment to the center of mass of the other. The orientation of each segment is computed from a smooth vector ﬁeld determined by the largest eigenvector of a windowed segments’ second-moment matrix (see the Appendix of [19] for details). Features include the length of vc, and the angles formed by vo1 and vo2 with vc. This procedure is repeated with downsampled object masks, and objects grown using the afﬁnity graph watershed transform (as for growth features) with 9 choices of threshold, yielding 33 angle features in total. Intuitively, we expect that two segments should be merged if the orientation of one segment is parallel with the vector pointing to the other segment. Level sets: a segment is eroded to produce a contour that initializes a level set [25]. It is then evolved under a speed function that should, ideally, result in the deformed segment moving towards and into the other segment if those segments belong together. The speed function determining the evolution consists of orientation and gradient vector ﬂow ﬁelds. The orientation vector ﬁeld is computed from the primary eigenvalue of the second-image-moment matrix. This ﬁeld serves to move the initial contour from one segment to the other and provides evidence for positive examples. The eigenvalues of the moment matrix describe how tubular, ﬂat, or  4  spherical each segment is. Therefore, we also compute the mean and standard deviation of the three eigenvalues yielding 6 orientation features. A gradient vector ﬂow (GVF) ﬁeld [33] is computed from the boundary prediction map in a manner similar to [34]. This ﬁeld can prevent the contour from crossing the boundary between segments and serves as evidence for negative examples. We compute the mean and standard deviation of the curl and divergence of the gradient vector ﬁeld over the interface between segments, yielding 4 GVF features. The level set overlap feature is the number of pixels belonging to both the level set result and the other segment. This process is repeated in reverse (starting the evolution from the other segment). We use these two overlap quantities, along with the mean, minimum, maximum and absolute differ- ence between the two results, to yield 6 overlap features in total. Shape diameter function: the local width of each segment, represented via statistics on the shape diameter function as deﬁned in [29]. The shape diameter function has been widely used for 3d mesh analysis and segmentation. We include both moments (8 features) and quantile-based statistics (10 features). Shape context: the local shape of each segment using a 3d implementation of [3]. In particular, we consider the shape to be the set of all points inside the window and on the boundary of either segment. Shape context is computed using the window’s central point as a reference, and a histogram with 5 radial, 12 polar angle and 12 azimuth angle bins. We cluster these quantities using k-means (20 clusters) then represent the feature using soft vector quantization.  Training set  Testing set  Exp. Feature Set Description  ) 1 bm mean, median, interface len. m b (  2 exp 1 + bm moments 3 exp 1 + bm quantiles 4 exp 1 + bm quantiles, min/max 5 exp 1 + bm deriv. mean, median 6 exp 1 + all bm deriv. stats  7 baseline ((cid:83) exp 1:6) 8 baseline ((cid:83) exp 1:6) boosting  9 exp 7 + growth 10 exp 7 + proximity 11 exp 7 + angles 12 exp 7 + size 13 exp 7 + rays 14 exp 7 + shape diam. quantiles 15 exp 7 + shape diam. moments 16 exp 7 + shape context 17 exp 7 + convex hull 18 exp 7 + level sets overlap 19 exp 7 + level sets gradient v.f. 20 exp 7 + level sets orientation 21 exp 7 + SIFT soft v.q. 22 exp 7 + image moments 23 exp 7 + image deriv. stats 24 exp 7 + image stats  25 all hand-designed ((cid:83) exp 1:24)  p a M y r a d n u o B  t c e j b O  e g a m  I  ACC(%) AUC(%) ACC(%) AUC(%) Dim. Cost 5.7 6 5.7 9 5.7 8 7.9 10 7.9 14 14.0 42 14.0 49 - 49 51 1.0 50 489.1 13.0 82 1.9 53 91 44.2 59 402.5 57 402.5 5.5 69 57 8.7 55 464.0 53 35.0 55 229.4 56.0 149 4.1 53 85 5.8 5.8 94 363 -  83.64 85.27 84.54 85.03 90.31 91.85 92.30 92.17 92.55 92.18 95.74 93.31 94.36 93.52 94.26 94.71 93.47 92.23 93.20 93.74 99.04 93.16 95.58 96.42 99.98  92.44 93.39 93.11 93.36 96.71 97.61 97.85 97.88 98.09 97.85 99.25 98.43 98.92 98.56 98.72 99.03 98.56 97.90 98.28 98.61 99.93 98.29 99.22 99.43 99.98  90.46 91.64 91.33 91.63 95.73 96.05 96.05 95.36 96.32 96.02 96.27 96.61 96.52 96.46 92.14 96.50 96.74 96.08 96.17 96.75 95.67 96.12 95.61 95.73 97.61  80.92 82.68 82.04 82.69 88.64 89.11 89.41 88.56 89.67 89.09 89.65 90.28 90.06 89.82 86.32 89.91 89.97 89.16 89.59 90.13 88.75 89.26 89.09 88.85 92.33  Table 1: Classiﬁcation experiments with hand-designed features. For each experiment, we provide the training and test accuracy (ACC), area under the ROC curve (AUC), number of total dimensions in the input feature vector, and relative computation time for each individual feature. The notation ‘exp X +’ denotes that the feature set from experiment X was added (i.e., set union) to the feature set in that experiment. All experiments except 8 used a drop-out multilayer perceptron as the classiﬁer.  5  (a) Hand-Designed and Learned Experiments  (b) Unsupervised Learning Experiments  Figure 3: Precision-recall curves comparing , and (a) hand-designed, end-to-end, and unsupervised feature learning schemes (b) different pooling schemes for unsupervised features. Unsupervised representation learn- ing combined with dynamic pooling (unsupervised dyn–obj dyn–bnd) yields comparable performance to an ensemble of all hand-designed features (all hand–designed), while combining both learned and hand-designed features yields the best performance (unsupervised dyn–obj dyn–bnd, hand-designed).  Convex hull: the number of pixels in the convex hull of each segment contained inside and outside of the segment and the log values of these quantities [2].  3.2 Classiﬁcation Experiments and Results  We performed a variety of classiﬁcation experiments in which we varied the set of hand-designed features provided to the classiﬁer, as summarized in Table 1. The ‘Cost’ column represents the wall- clock time taken to compute each feature set, normalized by the time taken for the fastest feature (‘growth’). Figure 3 shows the precision-recall curve for experiments using hand-designed features as well as results from experiments using the feature learning schemes described in subsequent sections. As our classiﬁer, we use a drop-out multilayer perceptron (200 hidden units, 500, 000 weight up- dates, rectiﬁed linear hidden units) [16], but also present results using a decision-stump boosting classiﬁer for comparison (experiment 8). Substantial improvement in performance results as the feature set increases from a simple set of 6 features derived from boundary map values (experiment 1: 80.92% test set classiﬁcation accuracy) to the combined set of all hand-designed features (experiment 25: 92.33% accuracy). Interestingly, when considered in isolation, some of the simplest features, such as size and convex hull, provide some of the largest improvements in accuracy. However, using all the hand-designed features to- gether yields signiﬁcantly higher accuracy and improved precision as compared to any individual feature.  4 Learned Features  In this section, we describe two data-driven feature representations. In contrast to hand-designed features, these representations do not require domain knowledge specialized to the data set being considered, and can therefore be easily adapted to new types of data. In addition, they are tuned to the statistics of the particular problem being solved, and may therefore prove to be complementary to or exceed the performance of hand-designed features.  6  0.750.80.850.90.9510.550.60.650.70.750.80.850.90.951RecallPrecisionPrecision vs Recall (Testing Set)  baselineall hand−designedend−to−end, no augend−to−end, all augunsupervised midpointunsupervised dyn−obj + dyn−bndunsupervised dyn−obj + dyn−bnd, hand−designed0.750.80.850.90.9510.550.60.650.70.750.80.850.90.951RecallPrecisionUnsupervised Features: Precision vs Recall (Testing Set)  unsupervised midpointunsupervised static−allunsupervised dyn−obj + dyn−bndunsupervised dyn−obj + dyn−bnd, hand−designedc i m a n y D  t c e j b O  g n i l o o p  c i m a n y D  y r a d n u o B  g n i l o o p  Segments  Pooling region  Rendering  Figure 4: Examples of dynamic pooling: the top row shows object pooling for the positive edge segments shown in Figure 1, and the bottom row shows boundary pooling for the negative edge segments. The left column shows 2d x-y slices of the segmentation, and the center column shows the corresponding raw image data with an overlay of the pooling region, where the dynamic pooling regions correspond to using a window of radius 10 voxels, and the total slice area corresponds to the context needed to generate the feature representation for all locations in the window. The right column gives a rendering of the 3d pooling regions, where the pooling window is given by the bounding box indicated by dashed lines.  4.1 End-to-end Learning  A naive but powerful approach is to simply provide the raw input signal values to the classiﬁer. In such an approach, the classiﬁer generally consists of multiple non-linear processing layers, and the classiﬁer is tasked with mapping the raw input signal to intermediate hidden representations that improve overall classiﬁcation performance. This approach, sometimes called ‘end-to-end’ learning, has achieved state-of-the-art performance on a variety of vision problems using multi-layer percep- trons and convolutional neural networks [6, 22]. We implement the end-to-end learning approach in the context of 3d agglomeration by creating, for each edge, a feature vector that contains image, segment, and boundary information within a 3d bounding box centered around the ‘decision point’ (as deﬁned at the beginning of Section 3). Speciﬁcally, we provide raw image values from the electron microscopy data, boundary map values, and two binary segment masks. A particular mask is non-zero only where a given segment belonging to the edge is present. A multiscale representation of the region around the decision point can be obtained by extracting the raw voxel values using multiple windows of varying radii. Further, to control the dimensionality of the input when using a large window radius, the subvolume of raw values can be downsampled by some factor d in each spatial dimension. As a result, for a particular scale consisting of a bounding box of radius r and downsampling of d, the total dimensionality of the feature vector is 4×(2 r d +1)3.  4.2 Unsupervised Feature Learning  End-to-end learning can be particularly difﬁcult when the size of the training set is limited (relative to the dimensionality of the data), as the classiﬁer must discover useful patterns and invariances in the original data representation from a limited amount of supervised signal. However, the original (unlabeled) data itself can be useful as an additional signal, by learning representations that are capable of reproducing the data. These ‘unsupervised’ approaches learn feature representations by optimizing models that reconstruct the raw data in the presence of various forms of regularization such as a bottleneck or sparsity [15, 28]. We experimented with using the unsupervised feature learning and extraction module used in DAWMR networks and adapting it to the agglomeration task. The core of this module consists of vector quantization (VQ) of 53 patches of the data, where the dictionary is learned using orthogonal matching pursuit (OMP-1), and encoding is performed using soft-thresholding with reverse polarity.  7  Exp. Feature Set Description  d 1 r=5, d=1, 20 hidden units  2 r=5, d=1 3 r=5, d=1, 400 hidden units 4 exp 2 + (r=10, d=2) 5 exp 3 + (r=10, d=2)  d 6 midpoint  7 midpoint + static-all (r=10) 8 midpoint + dyn-obj (r=10) 9 dyn-bnd (r=10) 10 dyn-obj + dyn-bnd (r=4 + r=10)  n e - o t - d n e  e s i v r e p u s n u  Training set  Testing set  ACC(%) AUC(%) ACC(%) AUC(%) Dim. Cost 7.6 7.6 7.6 12.6 12.6 9.9 16,000 371.7 16,000 368.0 8000 371.2 16,000 246.6  98.87 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0  99.64 99.98 99.98 99.98 99.98 99.98 99.98 99.98 99.98 99.98  82.62 84.34 85.02 84.50 85.54 87.84 88.85 89.65 91.24 91.38  92.36 93.53 93.62 93.75 93.99 95.19 95.89 96.28 96.96 97.14  5324 5324 5324 10,648 10,648 8000  Table 2: Classiﬁcation experiments with learned features. Dynamic pooling strategies (dyn-obj and dyn-bnd) are critical to achieving accuracy levels competitive with hand-designed features.  This core component is performed at two scales (original resolution and downsampling by two in each spatial dimension), and a foveated representation is produced by concatenating the encoding produced at a center location with a max-pooled encoding over all locations within a radius of two of the center. Therefore, a 93 support region is used to produce the representation centered at a given voxel. A straightforward method of adapting this feature representation to the problem of 3d agglomeration is to simply extract the feature representation at the decision point, which we will refer to as simply the ‘midpoint’ feature. Similar to end-to-end learning described above, the input data to the feature learning and extraction module consists of the raw image values, boundary map values, and a single binary segment mask that is non-zero only where either segment belonging to the edge is present. (We found that a single binary segment mask gave comparable performance to using two separate masks as used in end-to-end learning.) However, the agglomeration task of deciding whether or not to merge two segments likely requires a greater context than the boundary prediction problem that the DAWMR feature representation was originally designed for. Therefore, we also consider extracting the foveated feature representation from every location within a ﬁxed-radius window of the decision point, and average-pool these features. We refer to this as ‘static-all’ pooling, and concatenate this feature with the midpoint feature to obtain the ‘midpoint + all’ feature set. We further introduce the notion of dynamic pooling, where the region to pool over is dependent on the segments themselves. For instance, rather than average pooling over all features within a window as in ‘all’ pooling, we can restrict the average pooling to be over only features corresponding to locations in either of the two segments (within a ﬁxed-radius window of the decision point). This procedure, which we term ‘dyn-obj’ dynamic object pooling, may improve results over ‘all’ pooling by ignoring locations that are irrelevant to the agglomeration decision. Another approach to dynamic pooling is to focus on those locations whose interpretation would change as a result of the agglomeration decision. In particular, the interpretation of those locations ‘in-between’ the two segments would change depending on whether the two segments were merged into a single object or kept as two separate segments. Therefore, we introduce the notion of dynamic pooling along the boundary between two segments, which we refer to as ‘dyn-bnd’ pooling. This is done by dilating each segment by a ﬁxed amount (in our experiments, by half the radius used for the window around the decision point), and then considering those locations in the intersection of the two dilated segments. Both dynamic pooling methods are illustrated in Figure 4. Finally, similar to end-to-end learning above, we consider multiscale dynamic pooling representa- tions given by extracting features within windows of differing radii.  8  4.3 Classiﬁcation Experiments and Results  Results using the two data-driven representations above are presented in Table 2 and Figure 3(a). As with the hand-designed features, classiﬁcation was performed using a drop-out multilayer percep- tron, with 200 hidden units unless otherwise speciﬁed. The end-to-end features outperform rudimen- tary boundary map features for the test set but not the larger feature set. The unsupervised feature set achieves much better test set performance, approaching that of the all hand-designed features. Figure 3(b) demonstrates the improvements that dynamic pooling methods can achieve.  5 Training Set Augmentation  Next, we describe experiments designed to improve generalization performance through synthetic augmentation of the training set. The motivation behind this methodology comes from work such as Decoste and Sch¨olkopf [9] and Drucker et al. [10], which create ‘virtual’ examples by applying some set of transformations to examples in the original training set and use these examples during classiﬁer training. Thus, the training procedure is more likely to produce a classiﬁer invariant to the given transformations. In this work, we experiment with ‘swap,’ ‘isometry,’ and ‘jitter’ augmentations of the training data. The ‘swap’ transformations exchange the identities of the ﬁrst and second segments (i.e. swapping the ordering). The ‘isometry’ augmentation considers all possible isometries of the underlying data. The image data is slightly anisotropic, as the z-axis corresponds to the milling direction in FIB-SEM, orthogonal to the imaging plane (see Section 2, Electron microscopy images). Distance-preserving maps of the data therefore include four 90◦ rotations of the x-y plane, reﬂection of the x-axis, and reﬂection of the z-axis. In total, these transforms form a group of order 16, equivalent to the isometries of a square prism, or D4h. Finally, ‘jitter’ augmentations slightly shift the location of the decision point. In this work, our experiments use 27 different decision points, where the original decision point is offset by all combinations of {−1, 0, 1} in all coordinates.  5.1 Augmentation Results  We experimented with hand-designed, end-to-end, and unsupervised features using training set aug- mentation. We also explored using a more powerful classiﬁer with two hidden layers; this deeper classiﬁer could be especially important when augmentation is used, as the amount of training data increases dramatically. Table 4 in the supplementary gives the full results for experiments with different types of augmen- tation. Although augmented training examples had a slightly detrimental effect on the classiﬁcation results when using hand-designed features, the end-to-end features beneﬁted signiﬁcantly by using all augmentation types simultaneously (thus expanding training set size from 14, 552 to 12, 572, 928 examples). The two-layer MLP classiﬁer further improved performance for end-to-end features using all augmentations. Overall, however, even after including augmentations in the end-to-end experiments, generalization performance was still much worse as compared to unsupervised feature experiments performed without augmentation. The unsupervised feature experiments that included augmentations saw minimal effects on generalization performance.  6 Feature Combination and Selection  We explore combining various learned feature schemes with hand-designed features, with the hy- pothesis that the hand-designed features may more easily capture higher-level or non-linear edge or segment characteristics than the learned methods. We use all training set augmentations for these experiments, since this case markedly improved the end-to-end feature learning approach. For com- putational reasons, we omitted the most expensive hand-designed features, namely, SIFT features, shape diameter (moments, and quantiles), level set overlap, and level set orientation. Proximity was not omitted because it is necessary for other aspects of the pipeline. Results of these experiments are given in Table 3. Test set accuracy improves for both end-to-end and unsupervised learned features when used in combination with hand-designed features, though the improvement is more marked for end-to-end features.  9  Training set  Testing set  Exp. Feature Set Description  1 2  hand-designed + end-to-end hand-designed + unsupervised  ACC(%) AUC(%) ACC(%) AUC(%) Dim. Training Ex. 5546 12,572,928 1,571,616  95.06 100.0  99.07 99.98  92.09 92.21  97.74 97.67  16,222  Table 3: Classiﬁcation experiments using a combination of hand-designed and learned features.  7 Discussion  We have demonstrated that features derived purely from learning algorithms can provide highly informative representations for a classiﬁcation task involving 3d objects. The key innovation in achieving this result was a type of dynamic pooling that selectively pools feature representations from different spatial locations in a manner (dynamically) dependent on the shape of the underlying objects involved in the classiﬁcation. We were able to implement this strategy in a straightforward way using an unsupervised learning approach, as the feature learning phase was separated from the encoding stage in which the pooling is performed. These methods and results are a starting point for further work involving feature learning methods applied to 3d objects. In particular, the results motivate a more sophisticated end-to-end strategy that also incorporates dynamic pooling. Learning such an architecture will be more involved than in the unsupervised case, as the variations in spatial pooling (from one example to the next) will need to be incorporated into the learning algorithm. Another open question is whether learning architectures for these types of problems would beneﬁt from more complicated non-linearities or recurrent/recursive structure; some of the hand-designed features that appear to provide predictive beneﬁt are based on highly non-linear iterative methods (e.g, level sets) or ray-tracing (e.g., ray features and shape diameter function), both of which are computations that might be difﬁcult for a typical multilayer network architecture to emulate. Adding speciﬁc representational capacity motivated by these hand-designed strategies while preserving the ability to train most details of the architecture could offer a superior approach. Acknowledgements: We thank Zhiyuan Lu for sample preparation, Shan Xu and Harald Hess for FIB-SEM imaging, and Corey Fisher and Chris Ordish for data annotation.  References [1] B. Andres, U. Koethe, M. Helmstaedter, W. Denk, and F. Hamprecht. Segmentation of SBFSEM volume In Proceedings of the 30th DAGM symposium on  data of neural tissue by hierarchical classiﬁcation. Pattern Recognition, pages 142–152. Springer, 2008. 1, 2, 3.1  [2] E. Bas, M. G. Uzunbas, D. Metaxas, and E. Myers. Contextual grouping in a concept : a multistage  decision strategy for EM segmentation. In ISBI, pages 1–8, 2012. 3.1  [3] S. Belongie, J. Malik, and J. Puzicha. Shape matching and object recognition using shape contexts. IEEE [4] Y. Bengio. Learning deep architectures for ai. Foundations and trends R(cid:13) in Machine Learning, 2(1):1–  PAMI, 24(4):509–522, Apr. 2002. 3.1  127, 2009. 1  [5] D. Chklovskii, S. Vitaladevuni, and L. Scheffer. Semi-automated reconstruction of neural circuits using  electron microscopy. Current Opinion in Neurobiology, 2010. 2  [6] D. C. Cires¸an, U. Meier, L. M. Gambardella, and J. Schmidhuber. Deep, big, simple neural nets for  handwritten digit recognition. Neural computation, 22(12):3207–3220, 2010. 4.1  [7] A. Coates, A. Karpathy, and A. Ng. Emergence of object-selective features in unsupervised feature  learning. In Advances in Neural Information Processing Systems 25, pages 2690–2698, 2012. 1  [8] A. Coates, A. Y. Ng, and H. Lee. An analysis of single-layer networks in unsupervised feature learning.  In International Conference on Artiﬁcial Intelligence and Statistics, pages 215–223, 2011. 1  [9] D. Decoste and B. Sch¨olkopf. Training Invariant Support Vector Machines. Machine Learning, 46:161–  190, 2002. 5  [10] H. Drucker, R. Schapire, and P. Simard. Boosting Performance in Neural Networks. International Journal  of Pattern Recognition and Artiﬁcial Intelligence, 07(04):705–719, Aug. 1993. 5  [11] C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Scene parsing with multiscale feature learning, purity  trees, and optimal covers. arXiv preprint arXiv:1202.2160, 2012. 1, 2, 3.1  10  [12] C. Fowlkes, D. Martin, and J. Malik. Learning afﬁnity functions for image segmentation: Combining patch-based and gradient-based approaches. In IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR 2003), 2003. 2  [13] Y. Freund and L. Mason. The alternating decision tree learning algorithm. In Machine Learning: Pro-  ceedings of the Sixteenth International Conference, pages 124–133, 1999. 2  [14] M. Helmstaedter, K. L. Briggman, and W. Denk. 3D structural imaging of the brain with photons and  electrons. Current Opinion in Neurobiology, 18(6):633–641, 2008. 2  [15] G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science,  313(5786):504, 2006. 1, 4.2  [16] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov.  Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012. 2, 3.2 [17] G. B. Huang and V. Jain. Deep and wide multiscale recursive networks for robust image labeling. arXiv  preprint arXiv:1310.0354, 2013. 2  [18] V. Jain, H. Seung, and S. Turaga. Machines that learn to segment images: a crucial technology for  connectomics. Current opinion in neurobiology, 2010. 2  [19] V. Jain, S. C. Turaga, K. Briggman, M. N. Helmstaedter, W. Denk, and H. S. Seung. Learning to agglom- In Advances in Neural Information Processing Systems, pages 648–656,  erate superpixel hierarchies. 2011. 1, 1, 2, 3.1  [20] E. Kalogerakis, A. Hertzmann, and K. Singh. Learning 3d mesh segmentation and labeling. ACM Trans-  actions on Graphics (TOG), 29(4):102, 2010. 1  [21] G. Knott, H. Marchman, D. Wall, and B. Lich. Serial section scanning electron microscopy of adult brain  tissue using focused ion beam milling. Journal of Neuroscience, 28(12):2959, 2008. 2  [22] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.  Proceedings of the IEEE, 86(11):2278–2324, 1998. 1, 4.1  [23] B. Li, A. Godil, M. Aono, X. Bai, T. Furuya, L. Li, R. L´opez-Sastre, H. Johan, R. Ohbuchi, C. Redondo- In Proceedings of the 5th Eurographics  Cabrera, et al. Shrec’12 track: Generic 3d shape retrieval. conference on 3D Object Retrieval, pages 119–126. Eurographics Association, 2012. 1  [24] D. G. Lowe. Distinctive Image Features from Scale-Invariant Keypoints. International Journal of Com-  puter Vision, 60(2):91–110, Nov. 2004. 3.1  [25] R. Malladi, J. A. Sethian, and B. C. Vemuri. Shape modeling with front propagation: a level set approach.  IEEE PAMI, 17(2):158–175, 1995. 3.1  [26] Y. Marc’Aurelio Ranzato, L. Boureau, and Y. LeCun. Sparse feature learning for deep belief networks.  Advances in neural information processing systems, 20:1185–1192, 2007. 1  [27] J. Nunez-Iglesias, R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. Machine learning of hierarchical  clustering to segment 2D and 3D images. In CVPR, pages 1–15, 2013. 2  [28] B. A. Olshausen et al. Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for  natural images. Nature, 381(6583):607–609, 1996. 4.2  [29] L. Shapira, A. Shamir, and D. Cohen-Or. Consistent mesh partitioning and skeletonisation using the shape  diameter function. The Visual Computer, 24(4):249–259, Jan. 2008. 3.1  [30] K. Smith, A. Carleton, and V. Lepetit. Fast Ray Features for Learning Irregular Shapes. IEEE Computer  Vision, pages 397–404, 2009. 3.1  [31] J. W. Tangelder and R. C. Veltkamp. A survey of content based 3d shape retrieval methods. Multimedia  tools and applications, 39(3):441–471, 2008. 1  [32] S. C. Turaga, J. F. Murray, V. Jain, F. Roth, M. Helmstaedter, K. Briggman, W. Denk, and H. S. Seung. Convolutional networks can learn to generate afﬁnity graphs for image segmentation. Neural Computa- tion, 22(2):511–538, 2010. 2  [33] C. Xu and J. L. Prince. Snakes, shapes, and gradient vector ﬂow. IEEE Transactions on Image Processing,  7(3):359–69, Jan. 1998. 3.1  [34] Z. Yang, J. A. Bogovic, A. Carass, M. Ye, P. C. Searson, and J. L. Prince. Automatic cell segmentation in ﬂuorescence images of conﬂuent cell monolayers using multi-object geometric deformable model. In SPIE Medical Imaging, volume 8669, page 866904, Mar. 2013. 3.1  11  A Supplementary: Edge examples  e v i t i s o P  e g d E  e v i t a g e N  e g d E  Rendering  Slice  Figure 5: Complex 3d segment shapes and spatial relationships makes agglomeration more chal- lenging. The interdigitation of the segments in the positive example create a complex interface. The negative edge demonstrates that very thin structures can abut large segments, creating a interface relative to the smaller object. Furthermore, since much of the larger segment lies outside of the decision window, global properties (e.g., orientation) cannot be computed.  B Supplementary: Error Analysis  In this section, we explore potential causes of classiﬁer errors and make suggestions for future improvements of the pipeline. Speciﬁcally, we examine a set of 50 edges, half of which were false positives, half of which were false negatives, for which the both the MLP classiﬁer and human expert were conﬁdent. We manually examined the segments, image, and afﬁnity graph for these edges in an attempt to glean potential patterns that might help drive future improvements.  (a) False positive  (b) False negative  Figure 6: Examples of false positive and false negative edges for experiment 25 in Table 1. The yellow line in (a) shows the true boundary between cells that have been undersegmented.  One characteristic that seems common among errors is the presence of ‘undersegmentation,’ the presence of segments that overlap more than one true object. Undersegmentation appear in 15 of  12  the 50 error cases; 13 of those examples are false positives. It is possible that these errors are due to segments that erroneously grow across cell boundaries. This can cause segments to become adjacent when the true objects are not, thereby confusing the classiﬁer. An example of this phenomenon is shown in Figure 6(a). Another property of some errors seems to be that they occur near boundaries of internal cell struc- tures, such as mitochondria. Figure 6(b) shows an example of such a false negative edge. Notice that in this example, the red segment lies inside a mitochondrion, the blue segment consists of part of the cell outside the mitochondrion, and the two segments share a mutual boundary. The patterns of error we observed above suggest some improvements for future work. First, the undersegmentations could be ameliorated either by reﬁnements to the boundary prediction or to the procedure that generates segments from the boundaries. Segments that are too large could cause some of the false positives we have observed, and suggests that using a more conservative overseg- mentation scheme that yields smaller objects might be preferable. Of course, whether this approach would cause false negatives would need exploration. Secondly, the errors occurring near mitochondrial and other intra-cellular boundaries suggest that our methodology might beneﬁt from a framework that explicitly identiﬁes the locations of these problem areas. This new information could improve agglomeration, boundary prediction, or both.  C Supplementary: Precision-Recall Plots  Figure 7(a) shows the precision-recall curves in the low recall region. Experiments with very high training-set accuracy do not achieve high precision on the testing set due to overﬁtting. Figure 7(b) shows the precision-recall curves for end-to-end feature learning. Including training set augmenta- tion improves performance much more than a multi-scale approach. Combining end-to-end features with hand-designed features improved the end-to-end features signiﬁcantly.  (a) Low-recall region  (b) End-to-end  Figure 7: Precision-recall curves in (a) low-recall regions and (b) for the end-to-end learned feature scheme.  13  00.10.20.30.40.50.60.70.90.920.940.960.981RecallPrecisionPrecision vs Recall (Testing Set)  baselineall hand−designedend−to−end, no augend−to−end, all augunsupervised midpointunsupervised dyn−obj + dyn−bndunsupervised dyn−obj + dyn−bnd, hand−designed0.750.80.850.90.9510.550.60.650.70.750.80.850.90.951RecallPrecisionEnd−to−end Features: Precision vs Recall (Testing Set)  end−to−end, no augend−to−end, multi−scale, no augend−to−end, all augend−to−end + hand−designed, all augD Supplementary: Full Augmentation Results  Training set  Testing set  n o N  Aug. Exp. Feature Set Description ACC(%) AUC(%) ACC(%) AUC(%) Dim. Training Ex. 14,552 e 1 All Hand-sel. 14,552 14,552 29,104 29,104  2 End-end 3 Unsup. Learned  363 5324 16,000 363 5324  99.98 100.0 100.0 100.0 100.0  91.04 84.34 91.38 90.91 84.93  99.98 99.98 99.98 99.98 99.98  96.33 93.53 97.14 96.39 93.07  p 4 All Hand-sel. a w S  5 End-end 6 Unsup. Learned  invariant to segment order  y 7 All Hand-sel. r t e m o s I  8 End-end 9 Unsup. Learned  r 10 All Hand-sel.  e t t i J  11 End-end 12 Unsup. Learned  l 13 End-end A  l  14 Unsup. Learned  e 15 All Hand-sel. (MLP2)  n o N  16 End-end (MLP2) 17 Unsup. Learned (MLP2)  l  l 18 End-end (MLP2) A  19 Unsup. Learned (MLP2)  99.98 97.91 99.84 100.0 99.96 100.0 89.48 100.0 100.0 100.0 100.0 91.99 99.98  99.98 99.77 99.98 99.98 99.98 99.98 96.32 99.98 99.98 99.98 99.98 97.54 99.98  91.13 84.85 91.36 90.06 85.37 91.35 86.38 91.57 91.07 84.48 91.77 88.00 91.50  96.49 93.30 97.27 95.18 93.51 97.18 94.55 97.41 96.01 92.67 97.13 95.34 97.24  363 5324 16,000 363 5324 16,000  232,832 232,832 232,832 392,904 392,904 392,904 5324 12,572,928 1,571,616 14,552 14,552 14,552 5324 12,572,928 1,571,616  16,000 363 5324 16,000  16,000  Table 4: Classiﬁcation experiments using augmented training data and MLP’s with two hidden layers. The hand-designed feature set is comparable to experiment 25 in Table 1, end-to-end features are are compara- ble to experiment 2 in Table 2, and unsupervised features are comparable to experiment 10 in Table 2. For computational reasons, we omit the ‘all augmentations’ experiment using all hand-designed features.  14  ","For image recognition and labeling tasks, recent results suggest that machinelearning methods that rely on manually specified feature representations may beoutperformed by methods that automatically derive feature representations basedon the data. Yet for problems that involve analysis of 3d objects, such as meshsegmentation, shape retrieval, or neuron fragment agglomeration, there remainsa strong reliance on hand-designed feature descriptors. In this paper, weevaluate a large set of hand-designed 3d feature descriptors alongside featureslearned from the raw data using both end-to-end and unsupervised learningtechniques, in the context of agglomeration of 3d neuron fragments. Bycombining unsupervised learning techniques with a novel dynamic pooling scheme,we show how pure learning-based methods are for the first time competitive withhand-designed 3d shape descriptors. We investigate data augmentation strategiesfor dramatically increasing the size of the training set, and show howcombining both learned and hand-designed features leads to the highestaccuracy."
1312.6211,2014,An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks  ,"['Yoshua Bengio', 'Mehdi Mirza', 'Ian Goodfellow', 'Aaron Courville', 'Xia Da']",https://arxiv.org/pdf/1312.6211.pdf,"An Empirical Investigation of Catastrophic Forgetting in  Gradient-Based Neural Networks  Ian J. Goodfellow Mehdi Mirza Da Xiao Aaron Courville Yoshua Bengio  Abstract  Catastrophic forgetting is a problem faced by many machine learning models and al- gorithms. When trained on one task, then trained on a second task, many machine learning models “forget” how to perform the ﬁrst task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catas- trophic forgetting problem occurs for mod- ern neural networks, comparing both estab- lished and recent gradient-based training al- gorithms and activation functions. We also examine the eﬀect of the relationship between the ﬁrst task and the second task on catas- trophic forgetting. We ﬁnd that it is always best to train using the dropout algorithm– the dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoﬀ curve be- tween these two extremes. We ﬁnd that dif- ferent tasks and relationships between tasks result in very diﬀerent rankings of activation function performance. This suggests that the choice of activation function should always be cross-validated.  5 1 0 2    r a  M 4         ] L M  . t a t s [      3 v 1 1 2 6  .  2 1 3 1 : v i X r a  1. Introduction  Catastrophic forgetting(McCloskey & Cohen, 1989; Ratcliﬀ, 1990) is a problem that aﬀects neural net- works, as well as other learning systems, including both biological and machine learning systems. When a learning system is ﬁrst trained on one task, then trained on a second task, it may forget how to perform  Code associated with this paper is available at https:// github.com/goodfeli/forgetting.  goodfeli@iro.umontreal.ca mirzamom@iro.umontreal.ca xiaoda99@bupt.edu.cn aaron.courville@umontreal.ca yoshua.bengio@umontreal.ca  the ﬁrst task. For example, a machine learning system trained with a convex objective will always reach the same conﬁguration at the end of training on the sec- ond task, regardless of how it was initialized. This means that an SVM that is trained on two diﬀerent tasks will completely forget how to perform the ﬁrst task. Whenever the SVM is able to correctly classify an example from the original task, it is only due to chance similarities between the two tasks.  A well-supported model of biological learning in hu- man beings suggests that neocortical neurons learn using an algorithm that is prone to catastrophic for- getting, and that the neocortical learning algorithm is complemented by a virtual experience system that re- plays memories stored in the hippocampus in order to continually reinforce tasks that have not been recently performed (McClelland et al., 1995). As machine learning researchers, the lesson we can glean from this is that it is acceptable for our learning algorithms to suﬀer from forgetting, but they may need complemen- tary algorithms to reduce the information loss. De- signing such complementary algorithms depends on understanding the characteristics of the forgetting ex- perienced by our contemporary primary learning algo- rithms.  In this paper we investigate the extent to which catas- trophic forgetting aﬀects a variety of learning algo- rithms and neural network activation functions. Neu- roscientiﬁc evidence suggests that the relationship be- tween the old and new task strongly inﬂuences the out- come of the two successive learning experiences (Mc- Clelland). Consequently, we examine three diﬀerent types of relationship between tasks: one in which the tasks are functionally identical but with diﬀerent for- mats of the input, one in which the tasks are similar, and one in which the tasks are dissimilar.  We ﬁnd that dropout (Hinton et al., 2012) is consis- tently the best training algorithm for modern feedfor-  Catastrophic Forgetting  ward neural nets. The choice of activation function has a less consistent eﬀect–diﬀerent activation func- tions are preferable depending on the task and rela- tionship between tasks, as well as whether one places greater emphasis on adapting to the new task or re- taining performance on the old task. When training with dropout, maxout (Goodfellow et al., 2013b) is the only activation function to consistently appear some- where on the frontier of performance tradeoﬀs for all tasks we considered. However, maxout is not the best function at all points along the tradeoﬀ curve, and does not have as consistent performance when trained with- out dropout, so it is still advisable to cross-validate the choice of activation function, particularly when train- ing without dropout.  We ﬁnd that in most cases, dropout increases the opti- mal size of the net, so the resistance to forgetting may be explained mostly by the larger nets having greater capacity. However, this eﬀect is not consistent, and when using dissimilar task pairs, dropout usually de- creases the size of the net. This suggests dropout may have other more subtle beneﬁcial eﬀects to character- ize in the future.  2. Related work  Catastrophic forgetting has not been a well-studied property of neural networks in recent years. This property was well-studied in the past, but has not re- ceived much attention since the deep learning renais- sance that began in 2006. Srivastava et al. (2013) re- popularized the idea of studying this aspect of modern deep neural nets.  However, the main focus of this work was not to study catastrophic forgetting, so the experiments were lim- ited. Only one neural network was trained in each case. The networks all used the same hyperparameters, and the same heuristically chosen stopping point. Only one pair of tasks was employed, so it is not clear whether the ﬁndings apply only to pairs of tasks with the same kind and degree of similarity or whether the ﬁndings generalize to many kinds of pairs of tasks. Only one training algorithm, standard gradient descent was em- ployed. We move beyond all of these limitations by training multiple nets with diﬀerent hyperparameters, stopping using a validation set, evaluating using three task pairs with diﬀerent task similarity proﬁles, and including the dropout algorithm in our set of experi- ments.  3. Methods  In this section, we describe the basic algorithms and techniques used in our experiments.  3.1. Dropout  Dropout (Hinton et al., 2012; Srivastava, 2013) is a recently introduced training algorithm for neural net- works. Dropout is designed to regularize neural net- works in order to improve their generalization perfor- mance.  Dropout training is a modiﬁcation to standard stochastic gradient descent training. When each ex- ample is presented to the network during learning, the input states and hidden unit states of the network are multiplied by a binary mask. The zeros in the mask cause some units to be removed from the net- work. This mask is generated randomly each time an example is presented. Each element of the mask is sampled independently of the others, using some ﬁxed probability p. At test time, no units are dropped, and the weights going out of each unit are multiplied by p to compensate for that unit being present more often than it was during training.  Dropout can be seen as an extremely eﬃcient means of training exponentially many neural networks that share weights, then averaging together their predic- tions. This procedure resembles bagging, which helps to reduce the generalization error. The fact that the learned features must work well in the context of many diﬀerent models also helps to regularize the model.  Dropout is a very eﬀective regularizer. Prior to the introduction of dropout, one of the main ways of re- ducing the generalization error of a neural network was simply to restrict its capacity by using a small number of hidden units. Dropout enables training of noticeably larger networks. As an example, we per- formed random hyperparameter search with 25 exper- iments in each case to ﬁnd the best two-layer recti- ﬁer network (Glorot et al., 2011a) for classifying the MNIST dataset. When training with dropout, the best network according to the validation set had 56.48% more parameters than the best network trained with- out dropout.  We hypothesize that the increased size of optimally functioning dropout nets means that they are less prone to the catastrophic forgetting problem than tra- ditional neural nets, which were regularized by con- straining the capacity to be just barely suﬃcient to perform the ﬁrst task.  Catastrophic Forgetting  3.2. Activation functions  Each of the hidden layers of our neural networks trans- forms some input vector x into an output vector h. In all cases, this is done by ﬁrst computing a presynaptic activation z = W x+b where W is a matrix of learnable parameters and b is a vector of learnable parameters. The presynaptic activation z is then transformed into a post-synaptic activation h by an activation function: h = f (z). h is then provided as the input to the next layer.  We studied the following activation functions:  1. Logistic sigmoid:  ∀i, f (z)i =  1  1 + exp(−zi)  2. Rectiﬁed linear (Jarrett et al., 2009; Glorot et al.,  2011a):  ∀i, f (z)i = max(0, zi)  3. Hard Local Winner Take All (LWTA) (Srivastava  et al., 2013):  ∀i, f (z)i = g(i, z)zi.  Here g is a gating function. z is divided into dis- joint blocks of size k, and g(i, z) is 1 if zi is the maximal element of its group. If more than one element is tied for the maximum, we break the tie uniformly at random 1. Otherwise g(i, z) is 0.  4. Maxout (Goodfellow et al., 2013b):  ∀i, f (z)i = maxj{zki, . . . , zk(i+1)−1}  We trained each of these four activation functions with each of the two algorithms we considered, for a total of eight distinct methods.  3.3. Random hyperparameter search  Making fair comparisons between diﬀerent deep learn- ing methods is diﬃcult. The performance of most deep learning methods is a complicated non-linear function of multiple hyperparameters. For many applications, the state of the art performance is obtained by a hu- man practitioner selecting hyperparameters for some  1This is a deviation from the implementation of Srivas- tava et al. (2013), who break ties by using the smallest index. We used this approach because it is easier to imple- ment in Theano. We think our deviation from the previous implementation is acceptable because we are able to repro- duce the previously reported classiﬁcation performance.  deep learning method. Human selection is problem- atic for comparing methods because the human prac- titioner may be more skillful at selecting hyperparam- eters for methods that he or she is familiar with. Hu- man practitioners may also have a conﬂict of interest predisposing them to selecting better hyperparameters for methods that they prefer.  Automated selection of hyperparameters allows more fair comparison of methods with a complicated de- pendence on hyperparameters. However, automated selection of hyperparameters is challenging. Grid search suﬀers from the curse of dimensionality, requir- ing exponentially many experiments to explore high- dimensional hyperparameter spaces. In this work, we use random hyperparameter search (Bergstra & Ben- gio, 2012) instead. This method is simple to implement and obtains roughly state of the art results using only 25 experiments on simple datasets such as MNIST.  Other more sophisticated methods of hyperparameter search, such as Bayesian optimization, may be able to obtain better results, but we found that random search was able to obtain state of the art performance on the tasks we consider, so we did not think that the greater complication of using these methods was jus- tiﬁed. More sophisticated methods of hyperparameter feedback may also introduce some sort of bias into the experiment, if one of the methods we study satisﬁes more of the modeling assumptions of the hyperparam- eter selector.  4. Experiments  All of our experiments follow the same basic form. For each experiment, we deﬁne two tasks: the “old task” and the “new task.” We examine the behavior of neu- ral networks that are trained on the old task, then trained on the new task.  For each deﬁnition of the tasks, we run the same suite of experiments for two kinds of algorithms: stochastic gradient descent training, and dropout training. For each of these algorithms, we try four diﬀerent activa- tion functions: logistic sigmoid, rectiﬁer, hard LWTA, and maxout.  For each of these eight conditions, we randomly gen- erate 25 random sets of hyperparameters. See the code accompanying the paper for details. In all cases, we use a model with two hidden layers followed by a softmax classiﬁcation layer. The hyperparameters we search over include the magnitude of the max- norm constraint (Srebro & Shraibman, 2005) for each layer, the method used to initialize the weights for each layer and any hyper-parameters associated with such  Catastrophic Forgetting  method, the initial biases for each layer, the parame- ters controlling a saturating linear learning rate decay and momentum increase schedule, and the size of each layer.  We did not search over some hyperparameters for which good values are reasonably well-known. For example, for dropout, the best probability of drop- ping a hidden unit is known to usually be around 0.5, and the best probability of dropping a visible unit is known to usually be around 0.2. We used these well- known constants on all experiments. This may reduce the maximum possible performance we are able to ob- tain using our search, but it makes the search function much better with only 25 experiments since fewer of the experiments fail dramatically.  We did our best to keep the hyperparameter searches comparable between diﬀerent methods. We always used the same hyperparameter search for SGD as for dropout. For the diﬀerent activation functions, there are some slight diﬀerences between the hyperameter searches. All of these diﬀerences are related to param- eter initialization schemes. For LWTA and maxout, we always set the initial biases to 0, since randomly initializing a bias for each unit can make one unit within a group win the max too often, resulting in dead ﬁlters. For rectiﬁers and sigmoids, we randomly select the initial biases, but using diﬀerent distribu- tions. Sigmoid networks can beneﬁt from signiﬁcantly negative initial biases, since this encourages sparsity, but these initializations are fatal to rectiﬁer networks, since a signiﬁcantly negative initial bias can prevent a unit’s parameters from ever receiving non-zero gra- dient. Rectiﬁer units can also beneﬁt from slightly positive initial biases, because they help prevent rec- tiﬁer units from getting stuck, but there is no known reason to believe this helps sigmoid units. We thus use a diﬀerent range of initial biases for the rectiﬁers and the sigmoids. This was necessary to make sure that each method is able to achieve roughly state of the art performance with only 25 experiments in the random search. Likewise, there are some diﬀerences in the way we initialize the weights for each activation function. For all activation functions, we initialize the weights from a uniform distribution over small values, in at least some cases. For maxout and LWTA, this is always the method we use. For rectiﬁers and sig- moids, the hyperparameter search may also choose to use the initialization method advocated by Martens & Sutskever (2011). In this method, all but k of the weights going into a unit are set to 0, while the re- maining k are set to relatively large random values. For maxout and LWTA, this method performs poorly because diﬀerent ﬁlters within the same group can be  initialized to have extremely dissimilar semantics.  In all cases, we ﬁrst train on the “old task” until the validation set error has not improved in the last 100 epochs. Then we restore the parameters corresponding to the best validation set error, and begin training on the “new task”. We train until the error on the union of the old validation set and new validation set has not improved for 100 epochs.  After running all 25 randomly conﬁgured experiments for all 8 conditions, we make a possibilities frontier curve showing the minimum amount of test error on the new task obtaining for each amount of test error on the old task. Speciﬁcally, these plots are made by drawing a curve that traces out the lower left frontier of the cloud of points of all (old task test error, new task test error) pairs encountered by all 25 models dur- ing the course of training on the new task, with one point generated after each pass through the training set. Note that these test set errors are computed after training on only a subset of the training data, because we do not train on the validation set. It is possible to improve further by also training on the validation set, but we do not do so here because we only care about the relative performance of the diﬀerent methods, not necessarily obtaining state of the art results.  (Usually possibilities frontier curves are used in sce- narios where higher values are better, and the curves trace out the higher edge of a convex hull of scatter- plot. Here, we are plotting error rates, so the lower values are better and the curves trace out the lower edge of a convex hull of a scatterplot. We used error rather than accuracy so that log scale plots would com- press regions of bad performance and expand regions of good performance, in order to highlight the diﬀerences between the best-performing methods. Note that the log scaling sometimes makes the convex regions apear non-convex)  4.1. Input reformatting  Many naturally occurring tasks are highly similar to each other in terms of the underlying structure that must be understood, but have the input presented in a diﬀerent format.  For example, consider learning to understand Italian after already learning to understand Spanish. Both tasks share the deeper underlying structure of being a natural language understanding problem, and fur- thermore, Italian and Spanish have similar grammar. However, the speciﬁc words in each language are diﬀer- ent. A person learning Italian thus beneﬁts from hav- ing a pre-existing representation of the general struc-  Catastrophic Forgetting  ture of the language. The challenge is to learn to map the new words into these structures (e.g., to attach the Italian word “sei” to the pre-existing concept of the second person conjugation of the verb “to be”) without damaging the ability to understand Spanish. The ability to understand Spanish could diminish if the learning algorithm inadvertently modiﬁes the more abstract deﬁnition of language in general (i.e., if neu- rons that were used for verb conjugation before now get re-purposed for plurality agreement) rather than exploiting the pre-existing deﬁnition, or if the learning algorithm removes the associations between individual Spanish words and these pre-existing concepts (e.g., if the net retains the concept of there being a second per- son conjugation of the verb “to be” but forgets that the Spanish word “eres” corresponds to it).  To test this kind of learning problem, we designed a simple pair of tasks, where the tasks are the same, but with diﬀerent ways of formatting the input. Specif- ically, we used MNIST classiﬁcation, but with a dif- ferent permutation of the pixels for the old task and the new task. Both tasks thus beneﬁt from having concepts like penstroke detectors, or the concept of penstrokes being combined to form digits. However, the meaning of any individual pixel is diﬀerent. The net must learn to associate new collections of pixels to penstrokes, without signiﬁcantly disrupting the old higher level concepts, or erasing the old connections between pixels and penstrokes.  The classiﬁcation performance results are presented in Fig. 1. Using dropout improved the two-task valida- tion set performance for all models on this task pair. We show the eﬀect of dropout on the optimal model size in Fig. 2. While the nets were able to basically succeed at this task, we don’t believe that they did so by mapping diﬀerent sets of pixels into pre-existing concepts. We visualized the ﬁrst layer weights of the best net (in terms of combined validation set error) and their apparent semantics do not noticeably change between when training on the old task concludes and training on the new task begins. This suggests that the higher layers of the net changed to be able to ac- comodate a relatively arbitrary projection of the input, rather than remaining the same while the lower layers adapted to the new input format.  4.2. Similar tasks  We next considered what happens when the two tasks are not exactly the same, but semantically similar, and using the same input format. To test this case, we used sentiment analysis of two product categories of Amazon reviews (Blitzer et al., 2007) as the two tasks.  Figure 2. Optimal model size with and without dropout on the input reformatting tasks.  Figure 4. Optimal model size with and without dropout on the similar tasks experiment.  The task is just to classify the text of a product review as positive or negative in sentiment. We used the same preprocessing as (Glorot et al., 2011b).  The classiﬁcation performance results are presented in Fig. 3. Using dropout improved the two-task valida- tion set performance for all models on this task pair. We show the eﬀect of dropout on the optimal model size in Fig. 6.  4.3. Dissimilar tasks  We next considered what happens when the two tasks are semantically similar. To test this case, we used Amazon reviews as one task, and MNIST classiﬁca- tion as another. In order to give both tasks the same output size, we used only two classes of the MNIST dataset. To give them the same validation set size, we randomly subsampled the remaining examples of the MNIST validation set (since the MNIST validation set was originally larger than the Amazon validation set, and we don’t want the estimate of the performance on the Amazon dataset to have higher variance than the MNIST one). The Amazon dataset as we preprocessed it earlier has 5,000 input features, while MNIST has only 784. To give the two tasks the same input size, we reduced the dimensionality of the Amazon data with PCA.  Classiﬁcation performance results are presented in  Catastrophic Forgetting  Figure 1. Possibilities frontiers for the input reformatting experiment.  Figure 3. Possibilities frontiers for the similar tasks experiment.  Catastrophic Forgetting  Figure 5. Possibilities frontiers for the dissimilar tasks experiment.  to forgetting may be explained in part by the large model sizes that can be trained with dropout. On the input-reformatted task pair and the similar task pair, dropout never decreased the size of the optimal model for any of the four activation functions we tried. How- ever, dropout seems to have additional properties that can help prevent forgetting that we do not yet have an explanation for. On the dissimilar tasks experiment, dropout improved performance but reduced the size of the optimal model for most of the activation func- tions, and on the other task pairs, it occasionally had no eﬀect on the optimal model size.  The only recent previous work on catastrophic forget- ting(Srivastava et al., 2013) argued that the choice of activation function has a signiﬁcant eﬀect on the catas- trophic forgetting properties of a net, and in particu- lar that hard LWTA outperforms logistic sigmoid and rectiﬁed linear units in this respect when trained with stochastic gradient descent.  In our more extensive experiments we found that the choice of activation function has a less consistent eﬀect than the choice of training algorithm. When we per- formed experiments with diﬀerent kinds of task pairs, we found that the ranking of the activation functions is very problem dependent. For example, logistic sig- moid is the worst under some conditions but the best  Figure 6. Optimal model size with and without dropout on the disimilar tasks experiment.  Fig. 5. Using dropout improved the two-task valida- tion set performance for all models on this task pair. We show the eﬀect of dropout on the optimal model size in Fig. 6.  5. Discussion  Our experiments have shown that training with dropout is always beneﬁcial, at least on the relatively small datasets we used in this paper. Dropout im- proved performance for all eight methods on all three task pairs. Dropout works the best in terms of perfor- mance on the new task, performance on the old task, and points along the tradeoﬀ curve balancing these two extremes, for all three task pairs. Dropout’s resistance  Catastrophic Forgetting  under other conditions. This suggests that one should always cross-validate the choice of activation function, as long as it is computationally feasible. We also re- ject the idea that hard LWTA is particular resistant to catastrophic forgetting in general, or that it makes the standard SGD training algorithm more resistant to catastrophic forgetting. For example, when train- ing with SGD on the input reformatting task pair, hard LWTA’s possibilities frontier is worse than all activa- tion functions except sigmoid for most points along the curve. On the similar task pair, LWTA with SGD is the worst of all eight methods we considered, in terms of best performance on the new task, best performance on the old task, and in terms of attaining points close to the origin of the possibilities frontier plot. How- ever, hard LWTA does perform the best in some cir- cumstances (it has the best performance on the new task for the dissimilar task pair ). This suggests that it is worth including hard LWTA as one of many ac- tivation functions in a hyperparameter search. LWTA is however never the leftmost point in any of our three task pairs, so it is probably only useful in sequential task settings where forgetting is an issue.  When computational resources are too limited to ex- periment with multiple activation functions, we rec- ommend using the maxout activation function trained with dropout. This is the only method that appears on the lower-left frontier of the performance tradeoﬀ plots for all three task pairs we considered.  Acknowledgments  to  the  like  thank  developers  We would of Theano (Bergstra et al., 2010; Bastien et al., 2012), Pylearn2 (Goodfellow et al., 2013a). We would also like to thank NSERC, Compute Canada, and Calcul Qu´ebec for providing computational resources. Ian Goodfellow is supported by the 2013 Google Fellowship in Deep Learning.  References  Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Raz- van, Bergstra, James, Goodfellow, Ian J., Bergeron, Arnaud, Bouchard, Nicolas, and Bengio, Yoshua. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.  Bergstra, James and Bengio, Yoshua.  Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13:281–305, Febru- ary 2012.  Bergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric,  Lamblin, Pascal, Pascanu, Razvan, Desjardins, Guillaume, Turian, Joseph, Warde-Farley, David, a CPU and and Bengio, Yoshua. GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Conference (SciPy), June 2010. Oral Presentation.  Theano:  Blitzer, John, Dredze, Mark, and Pereira, Fernando. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classiﬁcation. In ACL ’07, pp. 440–447, 2007.  Glorot, Xavier, Bordes, Antoine, and Bengio, Yoshua. In JMLR Deep sparse rectiﬁer neural networks. W&CP: Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2011), April 2011a.  Glorot, Xavier, Bordes, Antoine, and Bengio, Yoshua. Domain adaptation for large-scale sentiment classi- ﬁcation: A deep learning approach. In Proceedings of theTwenty-eight International Conference on Ma- chine Learning (ICML’11), volume 27, pp. 97–110, June 2011b.  Goodfellow, Ian J., Warde-Farley, David, Lamblin, Pascal, Dumoulin, Vincent, Mirza, Mehdi, Pascanu, Razvan, Bergstra, James, Bastien, Fr´ed´eric, and Bengio, Yoshua. Pylearn2: a machine learning research library. arXiv preprint arXiv:1308.4214, 2013a.  Goodfellow,  Ian J., Warde-Farley, David, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua. Max- out networks. In Dasgupta, Sanjoy and McAllester, David (eds.), Proceedings of the 30th International Conference on Machine Learning (ICML’13), pp. 13191327. ACM, 2013b. URL http://icml.cc/ 2013/.  Hinton, Geoﬀrey E., Srivastava, Nitish, Krizhevsky, Ilya, and Salakhutdinov, Rus- Alex, Sutskever, lan. Improving neural networks by preventing co- adaptation of feature detectors. Technical report, arXiv:1207.0580, 2012.  Jarrett, Kevin, Kavukcuoglu, Koray, Ranzato, Marc’Aurelio, and LeCun, Yann. What is the best multi-stage architecture for object recognition? In Proc. International Conference on Computer Vision (ICCV’09), pp. 2146–2153. IEEE, 2009.  Martens, James and Sutskever, Ilya. Learning recur- rent neural networks with Hessian-free optimization. In Proc. ICML’2011. ACM, 2011.  Catastrophic Forgetting  McClelland, J. L., McNaughton, B. L., and O’Reilly, R. C. Why there are complementary learning sys- tems in the hippocampus and neocortex: Insights from the successes and failures of connectionist mod- els of learning and memory. Psychological Review, 102:419–457, 1995.  McClelland, James L.  McCloskey, M. and Cohen, N. J. Catastrophic inter- ference in connectionist networks: The sequential learning problem. In Bower, G. H. (ed.), The Psy- chology of Learning and Motivation, Vol. 24, pp. 109–164. Academic Press, San Diego, CA, 1989.  Ratcliﬀ, R. Connectionist models of recognition mem- ory: constraints imposed by learning and forget- ting functions. Psychological review, 97(2):285–308, April 1990. ISSN 0033-295X. URL http://view. ncbi.nlm.nih.gov/pubmed/2186426.  Srebro, Nathan and Shraibman, Adi. Rank, trace- norm and max-norm. In Proceedings of the 18th Annual Conference on Learning Theory, pp. 545– 560. Springer-Verlag, 2005.  Srivastava, Nitish.  Improving neural networks with  dropout. Master’s thesis, U. Toronto, 2013.  Srivastava, Rupesh K, Masci, Jonathan, Kazerou- nian, Sohrob, Gomez, Faustino, and Schmidhu- ber, J¨urgen. Compete to compute. In Burges, C.J.C., Bottou, L., Welling, M., Ghahramani, Z., and Weinberger, K.Q. (eds.), Advances in Neural Information Processing Systems 26, pp. 2310–2318. 2013. URL http://media.nips.cc/nipsbooks/ nipspapers/paper_files/nips26/1109.pdf.  ","Catastrophic forgetting is a problem faced by many machine learning modelsand algorithms. When trained on one task, then trained on a second task, manymachine learning models ""forget"" how to perform the first task. This is widelybelieved to be a serious problem for neural networks. Here, we investigate theextent to which the catastrophic forgetting problem occurs for modern neuralnetworks, comparing both established and recent gradient-based trainingalgorithms and activation functions. We also examine the effect of therelationship between the first task and the second task on catastrophicforgetting. We find that it is always best to train using the dropoutalgorithm--the dropout algorithm is consistently best at adapting to the newtask, remembering the old task, and has the best tradeoff curve between thesetwo extremes. We find that different tasks and relationships between tasksresult in very different rankings of activation function performance. Thissuggests the choice of activation function should always be cross-validated."
1312.6192,2014,Can recursive neural tensor networks learn logical reasoning?  ,['Samuel R. Bowman'],https://arxiv.org/pdf/1312.6192.pdf,"4 1 0 2     b e F 5 1         ] L C . s c [      4 v 2 9 1 6  .  2 1 3 1 : v i X r a  Can recursive neural tensor networks  learn logical reasoning?  Samuel R. Bowman  NLP Group, Dept. of Linguistics  Stanford University  Stanford, CA 94305-2150  sbowman@stanford.edu  Abstract  Recursive neural network models and their accompanying vector representations for words have seen success in an array of increasingly semantically sophisticated tasks, but almost nothing is known about their ability to accurately capture the aspects of linguistic meaning that are necessary for interpretation or reasoning. To evaluate this, I train a recursive model on a new corpus of constructed examples of logical reasoning in short sentences, like the inference of some animal walks from some dog walks or some cat walks, given that dogs and cats are animals. This model learns representations that generalize well to new types of reasoning pattern in all but a few cases, a result which is promising for the ability of learned representation models to capture logical reasoning.  1 Introduction  Deep learning methods in NLP which learn vector representations for words have seen successful uses in recent years on increasingly sophisticated tasks [1, 2, 3, 4]. Given the still modest per- formance of semantically rich NLP systems in many domains—question answering and machine translation, for instance—it is worth exploring the degree to which learned vectors can serve as gen- eral purpose semantic representations. Much of the work to date analyzing vector representations for words (see [5]) has focused on lexical semantic behaviors—like the similarity between words like Paris and France. Good similarity functions are valuable for many NLP tasks, but there are real use cases for which it is necessary to know how words relate to one another or to some extrinsic representation, rather than just how similar they are to one another: deciding whether an inference holds for instance, such as that John went to Paris means that John went to France but not vice versa. This paper explores the ability of linguistic representations developed using supervised deep learning techniques to support interpretation and reasoning.  Natural language inference (NLI), the ability to reason about the truth of a statement on the basis of some premise, is among the clearest examples of a task that requires comprehensive and accurate natural language understanding [6]. I borrow the structure of the task from MacCartney [6]. In it, the model is presented with a pair of sentences, and made to label the logical relation between the sentences as equivalence, entailment, or any of ﬁve other classes, as here:  At least three dogs bark entails that at least two dogs bark. All dogs bark is equivalent to no dogs don’t bark. Most dogs bark is incompatible with no dogs bark.  The success of an inference system most crucially depends on the use of a representation of phrase and sentence meaning that supports the accurate computation of inferential consequences. The  1  framework of monotonicity inference [7, 8] delimits a means of computing inferences in a broad set of cases, and I focus in this paper on reproducing this kind of inference in a learned model.  NLI involves many aspects of sentence understanding, and it is useful to be able to isolate inference from all the other ways that sentence understanding can go wrong: even if a model can learn to per- form inference correctly, a failure of coreference resolution, word sense disambiguation, or parsing could supply it with inaccurate input and cause it to fail on an evaluation. To sidestep this potential problem, I propose a simpliﬁed variant of the task below that uses hand-constructed, unambiguous inference problems, and construct a dataset of these problems.  In the following sections, I present the task of strict unambiguous NLI and present a recursive neural tensor network model conﬁgured for the task but closely modeled after those used in practice by Socher et al. [3]. I then describe a dataset on which to evaluate the model, and lay out several experiments to test its ability to perform inference and to generalize to patterns of inference different from those seen at training time. I present results that reveal that the model is able to learn to reason using familiar patterns of inference with perfect accuracy and that it can generalize successfully to unseen patterns in many cases. I conclude with a discussion of the implications of these largely positive results, a brief error analysis, and proposals for future work.  2 Reasoning with monotonicity  Consider the statement all dogs bark. From this, one can infer quite a number of other things. One can replace the ﬁrst argument of all (the ﬁrst of the two predicates following it, here dogs) with any more speciﬁc category that contains only dogs and get a valid inference: all puppies bark; all collies bark. But one can’t replace it with anything more general: it doesn’t give one reason to say that all animals bark. The second argument of all works the opposite way—one can make it less speciﬁc, but not more—one can say all dogs make sounds, but one can’t say that all dogs bark at cars. These inferences depend at least partially on the choice of the quantiﬁer all. If we replace it with most, the ﬁrst argument doesn’t work the same way: most dogs bark doesn’t entail that most animals bark nor does it entail that most schnauzers bark.  Monotonicity is the formalization of observations of this kind. The quantiﬁer all is downward mono- tone in its ﬁrst argument because it permits the substitution of more speciﬁc terms, and upward monotone in its second argument because it permits the substitution of more general terms. For- mally, monotonicity is a property of certain positions in semantic structures that determines what kinds of substitution can be made in that position while preserving truth. Most quantiﬁers allow monotonicity reasoning in some direction for both arguments. No, for example is downward in both arguments, some and two are upward in both, and most is upward in its second argument, but (unusually) does not license any kind of monotonicity inference on its ﬁrst.  Monotonicity is a central insight from work on natural logic [8, 6], a theoretical framework for natural language inference that uses natural language strings as the logical symbols, rather than relying on conversion to and from ﬁrst order logic or a similar system. The experiments here are not a direct implementation of natural logic, or even of monotonicity, but I rely on these theories to deﬁne what types of inference I include in the data, and in turn, what types of inference I expect the model to learn.  I limit myself in this study to examples of reasoning involving quantiﬁers like some and all. They are of interest here for the plethora of monotonicity reasoning patterns that they license, but quanti- ﬁers as lexical items are of particular interest in vector space models for other reasons as well. It is easy to imagine how a vector space might encode entities as vectors, with similar entities clustering together and certain directions in the space representing certain relationships between entities, and this interpretation of the dimensions of the space representing properties of entities can be expanded only somewhat awkwardly to common nouns and single-argument verbs—Mikolov et al. [9] shows exactly this sort of behavior in VSMs learned for language modeling. This kind of similarity be- havior is much less useful for quantiﬁers which deﬁne abstract relations between sets of entities: it might be somehow informative to encode lexical items such that two and three are more similar to each other than either is to no, but this is not especially helpful for any task that involves interpreting or reasoning with these words.  2  name  entailment  reverse entailment  equivalence alteration negation  cover  independence  symbol x ⊏ y x ⊐ y x ≡ y x | y x ∧ y x ` y x # y  set-theoretic deﬁnition  x ⊂ y x ⊃ y x = y  example crow, bird Asian, Thai couch, sofa  x ∩ y = ∅ ∧ x ∪ y 6= D x ∩ y = ∅ ∧ x ∪ y = D x ∩ y 6= ∅ ∧ x ∪ y = D animal, non-ape hungry, hippo  able, unable  (else)  cat, dog  Table 1: The entailment relations in B. D is the universe of possible objects of the same type as those being compared, and the relation # applies whenever none of the other six do, including when there is insufﬁcient knowledge to choose a label.  2.1 The task: natural language inference  Natural language inference provides what I claim to be the simplest way to evaluate the ability of learned representation models to capture speciﬁc semantic behaviors. In the standard formulation of the task (and the one used in the RTE datasets [10]), the goal is to determine whether a reasonable human would infer a hypothesis from a premise. MacCartney formalizes a method of inferring entailment relations, and moves past two way entailment/non-entailment classiﬁcation, proposing the set B of seven labels meant to describe all of the possible non-trivial relations that might hold between pairs of statements, shown in Table 1.  Interest in NLI in the NLP community is ongoing, and a version of it has been included in the 2014 SemEval challenge specially targeted towards the evaluation of distributional models. Neither this dataset, nor any other existing RTE and NLI datasets are appropriate for the task at hand however. I intend for the present experiment to evaluate the ability of a class of models to learn certain types of inference behavior, and need a dataset that precisely tests these phenomena. With existing datasets that use unrestricted natural language, there is the risk that a model could successfully capture mono- tonicity inference, but fail to accurately label test data due to problems with, for example, lexical ambiguity, syntactic ambiguity, coreference resolution, or pragmatic enrichment.  In order to minimize this possibility, I deﬁne the task of strict unambiguous NLI (SU-NLI). In this task, only entailments that are licensed by a strictly literal interpretation of the provided sentences are considered valid, and several constraints are applied to the language to minimize ambiguity:  • A small, unambiguous vocabulary is used. • All strings are given an explicit unlabeled tree structure parse. • Statements involving the hard-to-formalize generic senses of nouns—i.e. dogs bark as  opposed to the non-generic all dogs bark—are excluded.  • The sentences do not contain context dependent elements. This includes any reference to  time or any tense morphology, and all pronouns.  • The morphology is dramatically simpliﬁed: the copula is not used (some puppy is French is simpliﬁed to some puppy French, to make it more directly comparable to sentences like some puppy bark), and agreement marking (they walk vs. she walks) is omitted.  The key to success at this task is to learn a set of representations and functions that can mimic the logical structure underlying the data. There is limited precedent that deterministic logical behavior can be learned in supervised deep learning models: Socher et al. [11] show in an aside that a Boolean logic with negation and conjunction can be learned in a minimal recursive neural network model with one-dimensional (scalar) representations for words. Modeling the logical behavior that underlies linguistic reasoning, though, is a substantially more difﬁcult challenge, even in modular hand-built models.  The natural logic engine at the core of MacCartney’s [6] NLI system requires a complex set of linguistic knowledge, much of which takes the form of what he calls projectivity signatures. These signatures are tables showing the entailment relation that must hold between two strings that differ in a given way (such as the substitution of the argument of some quantiﬁer), and are explicitly provided  3  to the model for dozens of different cases of insertion, deletion and substitution of different types of lexical item. For example in judging the inference no animals bark | some dogs bark it would ﬁrst used stored knowledge to compute the relations introduced by each of the two differences between the sentences. Here, the substitution of no for some yields ∧ and the substitution of dogs for animals yields ⊐. It would then use an additional store of knowledge about relations to resolve the resulting series of relations into one (|) that expresses the relation between the two sentences being compared:  1. no animals bark ∧ some animals bark 2. some animals bark ⊐ some dogs bark 3. no animals bark [∧ ⊲⊳ ⊐ = |] some dogs bark  This study is the ﬁrst that I am aware of to attempt to build an inference engine based on learned vector representations, but two recent projects have attempted to introduce vector representations into inference systems in other ways: Baroni et al. [12] have achieved limited success in building a classiﬁer to judge entailments between one- and two-word phrases (including some with quanti- ﬁers), though their vector representations were crucially based on distributional statistics and were not learned for the task. In another line of research, Garrette et al. [13] propose a way to improve standard discrete NLI with vector representations. They propose a deterministic inference engine (similar to MacCartney’s) which is augmented by a probabilistic component that evaluates indi- vidual lexical substitution steps in the derivation using vector representations, though again these representations are not learned, and no evaluations of this system have been published to date.  3 Methods  The model is centered on a recursively applied composition function, following Socher et al. [14], which is meant to mimic the recursive construction of meanings in formal models of semantics. In this scheme, pairs of words are merged into phrase representations by a function that maps from representations of length 2N to representations of length N . These phrase representations are then further merged with other words and phrases until the entire phrase or sentence being evaluated is represented in a single vector. This vector is then used as the input to a classiﬁer and used in a supervised learning task.  Borrowing a model directly from the existing literature for this task is impossible since none has been proposed to detect asymmetric relations between phrases (though it may be possible to slightly adapt the paraphrase model of Socher et al. [14] to the task in future work). Instead, I build a combination model, depicted in Figure 1. The two phrases being compared are built up separately on each side of the tree using the same composition function until they have each been reduced to single vectors. Then, the two phrase vectors are fed into a separate comparison layer that is meant to generate a feature vector capturing the relation between the two phrases. The output of this layer is then given to a softmax classiﬁer, which in turn produces a hypothesized distribution over the seven relations.  For a composition layer, I use the RNTN layer function proposed in Chen et al. [4] (itself adapted from Socher et al. [3]) and shown below. The standard tanh sigmoid nonlinearity is applied to the output, following Socher et al.  (1)  yi = fa(~x(l)T A[i]~x(r) + ~Bi,:[~x(l); ~x(r)] + ci)  The comparison layer uses the same type of function with different parameters and a different non- linearity function wrapped around it:  (2)  yi = fb(~x(l)T K[i]~x(r) + ~Li,:[~x(l); ~x(r)] + mi)  Rather than use a sigmoid here, I found a substantial improvement in performance by using a rectiﬁed linear function for fb. In particular, I use the leaky rectiﬁed linear function [15]: fb(~x) = max(~x, 0) + 0.01 min(~x, 0), applied elementwise. To run the model forward and label a pair of phrases, the structure of the lower layers of the network is assembled so as to mirror the tree structures provided for each phrase. The word vectors are then looked up from the vocabulary matrix V (one of the model parameters), and the composition and comparison functions are used to pass information up the tree and into the classiﬁer at the top.  4  SOFTMAX CLASSIFIER  COMPARISON LAYER  dog VECTOR FROM V  COMPOSITION LAYER  COMPOSITION LAYER  cat VECTOR FROM V  very VECTOR FROM V  big VECTOR FROM V  Figure 1: The model structure used to compare dog and (very big) cat.  The model is trained using backpropagation through structure [16], wherein the negative log of the probability assigned to the correct label is taken as a cost function, and the gradient of each parameter with respect to that cost function is computed at each node, with information passing down the tree and into both the function parameters and the vocabulary matrix. Gradients for different instances of the composition function or different instances of a word in the same tree are summed to produce at most a single gradient update per parameter.  Optimization: I train the model with stochastic gradient descent (SGD), with gradients pooled from randomly chosen minibatches of 32 training examples, and learning rates computed using AdaGrad [17] from a starting rate of 0.2. L-BFGS [18] was tried unsuccessfully as an alternative to SGD. An L2 regularization term is added to the gradients from each batch, with a λ coefﬁcient of 0.0002. L1 regularization was tested but showed no improvement, though it may be fruitful in future work to explore other ways to encourage the learning of sparse solutions in the hope that they might be able to better support discrete, deterministic behavior of the sort studied here. The parameters and word vectors are initialized randomly using a uniform distribution over [−0.1, 0.1]—I made some attempts to initialize the word vectors using pretraining on a corpus of relations between individual vocabulary items (e.g. dog ⊏ animal), but this offered no measurable improvement.  Model size: The dimensionalities of the model were tuned extensively. The performance of the model is approximately optimal with the dimensionality of the word vectors set to 16, and the dimensionality of the feature vector produced by the comparison layer set to 45.  I also experimented with increasing the size of the model in two other ways: I ran all of the exper- iments below with an additional one or two standard neural network layers positioned between the comparison layer and the softmax classiﬁer at the top of the network. This did not yield a consis- tent measurable improvement. I additionally reran the experiments with a variation of the network that uses different composition functions to compose different types of phrase, following [19]. This variation yielded no improvement in performance over the standard RNTN model described here. More detail is provided in Appendix A.  Source code and data are available at http://goo.gl/PSyF5u.  4 Data  The dataset is built on a small purpose-built vocabulary, which is intended to be large and diverse enough to permit a wide variety of experiments on different semantic phenomena beyond those shown here, but which is still small enough to allow the relations between the lexical items them- selves to be exhaustively manually labeled when needed. The vocabulary contains 41 predicates  5  1. (some x) mobile ⊐ (some y) mobile [s.t. x ⊐ y] (some dog) mobile ⊐ (some puppy) mobile (some animal) mobile ⊐ (some cat) mobile (some Asian) mobile ⊐ (some Thai) mobile ... 2. (all x) bark ⊏ (some x) bark (all puppy) bark ⊏ (some puppy) bark (all cat) bark ⊏ (some cat) bark (all hippo) bark ⊏ (some hippo) bark ...  3. (all x) French ⊏ (some x) European (all puppy) French ⊏ (some puppy) European (all cat) French ⊏ (some cat) European (all hippo) French ⊏ (some hippo) European ... 4. (all x) bark ≡ (no x) (not bark) (all puppy) bark ≡ (no puppy) (not bark) (all cat) bark ≡ (no cat) (not bark) (all hippo) bark ≡ (no hippo) (not bark) ...  Table 2: Sample datasets from four different classes.  (nouns, adjectives, and intransitive verbs; see Appendix B for more on the design of the wordlist), six quantiﬁers, the logical conjunctions and and or, and the unary operator not.  The data take the form of 12k pairs of short sentences, each annotated with a relation label, which I divide into about 200 smaller datasets. These datasets contain tightly constrained variants of a speciﬁc valid reasoning pattern, each differing in at most one lexical item, and all sharing the same relation label. The remainder of this section describes the composition of these 200 datasets. Exam- ples from four of the datasets are provided in Table 2.  Basic monotonicity: This set of datasets contains reasoning patterns like those seen in example 1 in Table 2, where one of the predicates on one side entails the predicate in the corresponding position on the other side. In some of the datasets (as in 1), this alternation is in the ﬁrst argument position, in some the second argument position, and in some both. For the alternating ﬁrst argument sub- classes, I have every lexical entailment pair in the data in the ﬁrst argument position, the terms bark, mobile, and European in the second argument position, and every quantiﬁer. For alternating second argument datasets, I have all predicates in the ﬁrst argument position, and the pairs bark–animate, French–European, and Parisian–French in the second argument position. The datasets in which there is an alternation in both positions fall into two categories. In some, the entailment relations between the arguments work in the same direction (some dog French ⊏ some animal European), in others, they work in opposite directions (some dog European # some animal French).  Quantiﬁer substitution: These datasets, exempliﬁed in 2 above, show the interactions between each possible pair of different quantiﬁers, with the same predicates on both sides. Datasets exist with bark, mobile, and European in the second argument position, and within each dataset every possible predicate is shown in ﬁrst argument position.  Monotonicity with quantiﬁer substitution: These datasets show the more complex monotonicity interactions that emerge when reasoning between phrases with differing arguments and differing quantiﬁers, as in 3. Exhaustively creating datasets of this kind involves too large an amount of data to readily verify manually, so I sampled from the extensive set of possible combinations of the six quantiﬁers and the ﬁxed argument ﬁllers used in the monotonicity datasets. Each possible relation but ‘≡’ (which does not apply sentences like these unless the predicates on both sides are identical) is expressed, as is every possible pair of quantiﬁers.  Negation: To show the interaction of negation and monotonicity, I included variants of many of the datasets described above in which one of the four argument positions is systematically negated, as demonstrated in example 4.  5 Experiments and results  In the simplest experimental setting, which I label ALL-SPLIT, I randomly sample 85% the data— making sure to sample 85% of each of the individual datasets—train the model on that portion, and evaluate on the remaining 15% of the data. This setting is meant to test whether the model is able to correctly generalize the individual reasoning patterns represented by each of the datasets.  Performance on this setting is perfect: the model quickly converges to 100% accuracy on the test data, showing that it is capable of accurately learning to capture all of the reasoning patterns in the data. The remaining experimental settings serve to determine whether what is learned captures  6  Dataset 1. (some x) bark ⊐ (some y) bark [y ⊏ x] 2. (all x) bark ≡ (no x) (not bark) 3. (most x) bark # (no y) bark [y ⊏ x] 4. (no x) bark ⊐ (most (not x)) bark 5. (most x) European | (no x) European 6. (most x) bark | (no x) bark  ALL-SPLIT  SET-OUT  SUBCL.-OUT  PAIR-OUT  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  Table 3: Examples of which datasets are included in the training data in each experimental setting for the target evaluation dataset (most x) bark | (no x) bark. A ‘X’ indicates that 85% of the data from that dataset is seen in training, and a blank indicates that none of the examples from the dataset are seen in training.  Target dataset (most x) bark | (no x) bark  (two x) bark # (all x) bark  (some x) bark ∧ (no x) bark  Data evaluated target dataset only all held out datasets all test data target dataset only all held out datasets all test data target dataset only all held out datasets all test data  SET-OUT 100% (100%) 99.8% 0% (0%) 97.5% 0% (0%) 97.7%  SUBCL.-OUT 100% 36.8% 95.9% 100% 100% 99.3% 0% 0% 94.0%  PAIR-OUT 93.6% 78.8% 93.8% 94.7% 62.7% 93.0% 0% 25.2% 85.5%  Table 4: Test set accuracy.  the underlying logical structure of the data in a way that allows it to accurately label unseen kinds of reasoning pattern. In each of them, I choose one of three arbitrarily chosen target datasets, all involving quantiﬁer substitution, to test on. I then then hold out that dataset and—depending on the experimental setting—other similar datasets from the training data in an attempt to discover just how different a test example can be from anything seen in training and still be classiﬁed accurately. Table 3 shows what information is included in the training data for each of the four settings for one of the three target datasets.  SET-OUT: In these experiments, I hold out the target dataset, training on none of it and testing on all of it, and additionally split each remaining dataset as in ALL-SPLIT. This setting is meant to test whether the model can generalize a broader reasoning pattern from one dataset to another—the model will still have seen other similar quantiﬁer substitution datasets that differ from the target dataset only in which ﬁller word is placed in the second argument position, as in row 5 in Table 3 above.  SUBCLASS-OUT: In these experiments, I hold out the target dataset as well as all of datasets rep- resenting the same reasoning pattern as the target dataset, and split each remaining dataset as in ALL-SPLIT. For the target dataset (most x) bark | (no x) bark in this setting, all of the datasets of the general form (most x) y | (no x) y would be held out. PAIR-OUT: In these experiments, I hold out all of the datasets that demonstrate the interaction between a particular pair of quantiﬁers. For the target dataset (most x) bark | (no x) bark in this setting, all examples with most on one side and no on the other, in either order, with or without negation or differing predicates, are held out from training. The remaining datasets are split as above.  These last three experiments have multiple sources of test data: some from the 15% test portions of the datasets that were split, some from the target datasets, and some from any other similar datasets that were held out. In Table 4, I report results for each experiment on the entire test dataset, on the single target dataset, and on all of the held out datasets (i.e., the test data excluding the 15% portions from the training datasets) in aggregate. This third ﬁgure is identical to the second for the SET-OUT experiments, since the only held out dataset is the target dataset.  Performance was perfect for at least some held out datasets in both the SET-OUT and SUBCLASS- OUT settings, and near perfect for some in PAIR-OUT. Performance for one of the three target datasets—(some x) bark ∧ (no x) bark—was consistently poor across training settings. This poor  7  performance is consistent across random initializations, and should be a target of further investiga- tion.  6 Discussion  The model learns to generalize well to novel data in most but not all of the training conﬁgurations. This inconsistent performance suggests that there is room to improve the optimization techniques used on the model, but the fact that it is able to perform well in these settings even some of the time suggests that the structure of the model is basically capable of learning meaning representations that support inference.  The results in the ALL-SPLIT condition show two important behaviors. The model is able to learn to identify the difference between two unseen sentences and consistently return the label that corre- sponds to that difference. In addition, the model can learn lexical relations from the training data, such as dog ⊏ animal from (no dog) bark ⊐ (no animal) bark, and it can then use these learned lexical relations to compute the relation for a sentence pair like (some dog) bark ⊏ (all animal) bark. The results from the other three experimental settings show that the model is able to learn general purpose representations for quantiﬁers which, at least in many cases, allow it to perform in- ference when a crucial difference between two sentences—the substitution of one speciﬁc quantiﬁer for another—has not been seen. These results serve to conﬁrm that the representations learned are capturing some of the underlying logic, rather than than just supporting the memorization of ﬁxed reasoning patterns.  MacCartney’s natural logic provides a perspective with which to view these results. That logic is not directly implemented here, and it is not meant to describe the behavior of a learned model, but it can provide some insight about what types of inference an ideal model should be able to learn to perform. For example, one can use that logic to infer the unseen relation (most x) y | (no x) y by way of seen examples like (most x) y ⊏ (some x) y and (some x) y ∧ (no x) y using a similar method to the one outlined in at the end of Section 2.1. The quantiﬁer pair some–no and the target examples involving that pair behave somewhat unusually in this logic, which could shed some light on the model’s failure to handle them well. If no examples with some and no are included in training, the logic doesn’t have the information that it needs to derive the strict negation relation (some x) y ∧ (no x) y from the remaining data in these experiments. It can only instead derive the strictly weaker (less informative) relation |, by way of pairs of seen examples like (some x) y ⊐ (most x) y and (most x) y | (no x) y. The learned model does exactly this, guessing | for ∧ in the target dataset 100% of the time in the SET- and SUBCLASS-OUT settings and 84% of the time in the PAIR-OUT setting. It is possible, then, that the model is capable of consistently learning general purpose representations for quantiﬁers in the PAIR-OUT setting, but only if the held out pair of quantiﬁers is one whose relation can be inferred from the training data under something like MacCartney’s logic.  These results leave open the question of how much information is minimally needed to learn gen- eral purpose representations for quantiﬁers in this setting. There are two lines of experimental work that could help to clarify this. Including longer sentences and constructions involving conjunctions (i.e. and, or), transitive verbs (i.e. eats, kicks) or other constructions in the training and test data could further test what kinds of behavior can be learned from a small training set, as could further experiments on this data involving even smaller training sets than those shown here or more elabo- rately structured conﬁgurations of train and test sets. In a more formal direction, a thorough study of MacCartney’s system of projectivity and its underlying logic (following Icard [20]) could lead to a clearer theoretical bound on what can be expected from learned models. Such formal work could also help to establish what tasks can be accomplished using vector representation models, and what tasks—if any—require more powerful word and phrase representations like the higher order tensors studied in Coecke et al. [21] and Grefenstette [22].  Acknowledgments  I owe thanks to Chris Manning and Chris Potts for their advice at every stage of this project, to Richard Socher and Jeffrey Pennington for extensive and helpful discussions, and to J.T. Chipman for help in designing a pilot experiment.  8  Bibliography  [1] Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing:  Deep neural networks with multitask learning. In Proc. ICML. ACM, 2008.  [2] Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Man- ning. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proc. EMNLP. ACL, 2011.  [3] Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, An- drew Y. Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proc. EMNLP. EMNLP, 2013.  [4] Danqi Chen, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. Learning new facts from knowledge bases with neural tensor networks and semantic word vectors. In Proc. ICLR, 2013.  [5] Marco Baroni, Raffaella Bernardi, and Roberto Zamparelli. Frege in space: A program Submitted, draft at http://clic. cimec. unitn.  for compositional distributional semantics. it/composes, 2013.  [6] Bill MacCartney. Natural language inference. PhD thesis, Stanford University, 2009. [7] Jack Hoeksema. Monotonicity phenomena in natural language. Linguistic Analysis, 16(1–2),  1986.  [8] V´ıctor S´anchez-Valencia. Categorial grammar and natural reasoning. ILTI Publication Series for Logic, Semantics, and Philosophy of Language LP-91-08, University of Amsterdam, 1991. [9] Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space  word representations. Proceedings of NAACL-HLT, 2013.  [10] Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL Recognising Textual Entail- ment Challenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classiﬁcation, and Recognising Tectual Entailment. Springer, 2006.  [11] Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. Semantic compo-  sitionality through recursive matrix-vector spaces. In Proc. EMNLP, 2012.  [12] Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, and Chung-chieh Shan. Entailment above  the word level in distributional semantics. In Proc. EACL. ACL, 2012.  [13] Dan Garrette, Katrin Erk, and Raymond Mooney. A formal approach to linking logical form  and vector-space lexical semantics. Computing Meaning, 4, 2013.  [14] Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Man- ning. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. NIPS, 24, 2011.  [15] Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. Rectiﬁer nonlinearities improve neural  network acoustic models. In Proc. ICML 30, 2013.  [16] Christoph Goller and Andreas Kuchler. Learning task-dependent distributed representations In Proc. IEEE International Conference on Neural  by backpropagation through structure. Networks, volume 1. IEEE, 1996.  [17] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning  and stochastic optimization. The Journal of Machine Learning Research, 2011.  [18] Jorge Nocedal. Updating quasi-Newton matrices with limited storage. Mathematics of compu-  tation, 35(151), 1980.  [19] Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. Parsing with com-  positional vector grammars. In Proc. ACL, 2013.  [20] Thomas F. Icard. Inclusion and exclusion in natural language. Studia Logica, 100(4), 2012. [21] Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. Mathematical foundations for a com-  positional distributional model of meaning. arXiv:1003.4394, 2010.  [22] Edward Grefenstette. Towards a formal distributional semantics: Simulating logical calculi  with tensors. arXiv:1304.5823, 2013.  9  Appendix A: The syntactically untied model  I additionally ran some the experiments described above using a potentially more powerful variant of the model, following Socher et al. [19], with three separately-parametrized composition functions instead of a single universal function.  All instances of the composition layer use the same basic RNTN layer structure, including the sig- moid nonlinearity, but the parameters are chosen from one of three sets as follows:  • Negation composition parameters: Used in composing not with any predicate, as in not +  European or not + dog.  • Quantiﬁer ﬁrst argument parameters: Used in composing a quantiﬁer with its ﬁrst ar- gument (a single word or a two-word phrase with not), as in all + dog or some + (not European).  • Quantiﬁer second argument parameters: Used in composing a quantiﬁer phrase (a quan- tiﬁer with its ﬁrst argument) with the quantiﬁer’s second argument (a single word or a two-word phrase with not), as in all dog + bark or most sofa + not indestructible.  Appendix B: Wordlist  hippo ape cat Thai crow snail bark (i.e. barks) mobile live Parisian inanimate woody oak a indestructible  hungry animal unable Asian sofa vertebrate puppy immobile European human seat plant bush mammal  non-ape dog able bird couch invertebrate feline dead French animate bench tree destructible  This wordlist is part of a larger ongoing experiment, and is designed such that each of the seven of the relation types holds between some pair of words (for example, animal ` non-ape). Most of these relations are not included in the data used for this experiment. Since I do not train the model on a list of pairs of single words, lexical knowledge only comes in indirectly through monotonicity reasoning examples. For example, if the model sees (some dog) bark ⊏ (some animal) bark, it could infer from that that dog ⊏ animal.  I avoid specifying most of the possible relations between words by any means to avoid giving the model so much background knowledge that it can evaluate whether a sentence it is trying to reason about is true. Though the task is deﬁned such that the ground truth of the sentences being compared is irrelevant, mixing sentences that are known to be true with those that are not known to be true may provide unnecessary added difﬁculties in learning. For example, I do not train the model on all dog bark ⊏ all hungry bark (thus teaching the model dog ⊏ hungry), since this would make it potentially problematic to use later train the model on examples like some dog hungry ⊏ some animal hungry where the left side is known to be true a priori.  10  ","Recursive neural network models and their accompanying vector representationsfor words have seen success in an array of increasingly semanticallysophisticated tasks, but almost nothing is known about their ability toaccurately capture the aspects of linguistic meaning that are necessary forinterpretation or reasoning. To evaluate this, I train a recursive model on anew corpus of constructed examples of logical reasoning in short sentences,like the inference of ""some animal walks"" from ""some dog walks"" or ""some catwalks,"" given that dogs and cats are animals. This model learns representationsthat generalize well to new types of reasoning pattern in all but a few cases,a result which is promising for the ability of learned representation models tocapture logical reasoning."
1312.6168,2014,Factorial Hidden Markov Models for Learning Representations of Natural Language  ,"['Anjan Nepal', 'Alexander Yates']",https://arxiv.org/pdf/1312.6168.pdf,"4 1 0 2     b e F 8 1         ]  G L . s c [      3 v 8 6 1 6  .  2 1 3 1 : v i X r a  Factorial Hidden Markov Models for Learning  Representations of Natural Language  Anjan Nepal  Temple University  Broad St. and Montgomery Ave.  Philadelphia, PA 19122  anjan.nepal@temple.edu  Abstract  Alexander Yates Temple University  Broad St. and Montgomery Ave.  Philadelphia, PA 19122 yates@temple.edu  Most representation learning algorithms for language and image processing are local, in that they identify features for a data point based on surrounding points. Yet in language processing, the correct meaning of a word often depends on its global context. As a step toward incorporating global context into representation learning, we develop a representation learning algorithm that incorporates joint prediction into its technique for producing features for a word. We develop ef- ﬁcient variational methods for learning Factorial Hidden Markov Models from large texts, and use variational distributions to produce features for each word that are sensitive to the entire input sequence, not just to a local context window. Experiments on part-of-speech tagging and chunking indicate that the features are competitive with or better than existing state-of-the-art representation learning methods.  1 Introduction  Most existing representation learning algorithms project and transform the local context of data to produce their representations. Yet in many applications, particularly in natural language processing (NLP), joint prediction is crucial to the success of the application, and this requires reasoning over global structure.  We investigate a representation learning approach based on Factorial Hidden Markov Models [23]. Like spectral methods [18] and neural network methods [52, 11] for learning representations in NLP, FHMMs provide a distributed representation of words, meaning that they assign a multi-dimensional vector of features to each word, although in our models each latent dimension is discrete rather than real-valued. The factored latent states in the FHMMs make it tractable to train models with exponentially larger latent state spaces than those permitted by regular Hidden Markov Models (HMMs) [27, 28] or Brown clusters [9, 49], and empirically, models with larger latent state spaces produce better language models and better estimates of latent features for information extraction [1]. More importantly, FHMMs provide the ability to assign a representation to a word that is sensitive to the full observation sequence. In contrast, both spectral methods and neural network models rely on ﬁxed-length context windows to produce word representations.  Our contributions in this paper are mainly two fold: 1) We develop novel variational approximation algorithms for FHMMs with discrete-valued variables and 2) we provide empirical evaluation of the distributed representations from FHMMs which can provide different representations per word depending on the whole sentence context. We compare the performance against the systems that produce either only global context dependent but not distributed representations, or ﬁxed represen- tations per word which are not distributed, or ﬁxed distributed representations per word which are dependent on the local context of the sequence.  1  In experiments, we investigate the ability of FHMMs to provide useful representations for part-of- speech (PoS) tagging and noun-phrase chunking. In comparison with existing HMM representa- tions, we ﬁnd that our FHMM-based representations are more scalable to large latent state spaces and provide better application performance, reducing error by 4% on words that never appear in labeled PoS-tagging training data, and by 5% on chunks whose initial word never appears in labeled chunking training data.  The next section discusses previous work in representation learning for NLP. Section 3 introduces our FHMM model for representation learning, and Section 4 presents the inference and learning algorithms. Section 5 describes our empirical evaluation of the model as a technique for learning latent features of words for subsequent classiﬁcation tasks. Section 6 concludes.  2 Previous Work  There is a long tradition of NLP research on representations, mostly falling into one of four cate- gories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics [45, 50, 44]; 2) dimensionality reduction techniques for vector space models [16, 26, 34, 43, 5, 51]; 3) using clusters that are induced from distributional similarity [9, 42, 40] as non-sparse features [38, 10, 35, 53]; 4) and recently, language models [3, 41] as representations [12, 52, 11, 2], some of which have already yielded state of the art performance on domain adaptation tasks [27, 29, 30, 49, 28] and information extraction tasks [1, 19]. Our work combines the power of distributed representations from the neural network models with the joint inference from the HMM- based clustering approaches.  To the best of our knowledge, FHMMs have not been used for learning representations in NLP or elsewhere, and they are rarely used with discrete-valued observations. This may be in part due to the perception that training with discrete-valued observations is too computationally challenging, at least for real NLP datasets. Jacobs et al. [31] use a generalized backﬁtting approach for training a discrete-observation FHMM, but they have not run experiments on a naturally-occurring dataset, and they focus on language modeling rather than representation learning. We use a different training procedure based on variational EM [33], and provide empirical results for a representation learning task on a standard dataset.  Recent work by Socher et al. [47] has annotated parse trees with latent vectors that compose hi- erarchically. Like our model, these techniques operate on distributed representations for natural language, but as with neural network models they do not perform any kind of joint prediction over the whole structure to optimize the representations for individual words. Titov and Henderson [48] and Henderson and Titov [25] also learn representations that are affected by global context and also use variational approximation for inference but apply the technique to parsing. Grave et al. [24] use mainly two types of latent variable models: one essentially an HMM and the other has the latent variables associated in a syntactic dependency tree. While they also get the latent states for the words that are both context dependent and globally affected, our method can handle exponentially large state space and the representations are distributed.  Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains [13, 32, 15, 22, 21, 20]. Learning bounds are known [6, 39]. Daum´e III et al. [14] use semi-supervised learning to incorporate labeled and unlabeled data from the target domain. In contrast, we investigate a domain adaptation setting where no labeled data is available for the target domain.  3 The Factorial Hidden Markov Model for Learning Representations  The Factorial Hidden Markov Model (FHMM) [23] is a bayesian graphical model in which a se- quence of observed variables is generated from a latent sequence of discrete-valued vectors, as shown in Figure 1a. At a given time step t ∈ {1, . . . , T }, the latent state St is factored into M dimensions, where each latent factor Sm (m ∈ {1, . . . , M }) is a vector of Km boolean variables t with the constraint that exactly one of the boolean variables has value 1 at any time. We refer to the variable Sm t,k that has value 1 as the state of layer m at time step t. In general the model could allow a different number of states Km for each layer m, but in our experiments we use a single value K  2  S 1  t−1  S 2  t−1  S 1  t  S 2  t  S 1  t+1  S 2  t+1  S 1  t−1  S 2  t−1  S 1  t  S 2  t  S 1  t+1  S 2  t+1  Sm  t−1  Sm  t  Sm  t+1  Sm  t−1  Sm  t  Sm  t+1  Yt−1  Yt  Yt+1  (a) The Factorial Hidden Markov Model.  (b) Structured variational approximation model  Figure 1: Factorial Hidden Markov Model and the variational approximation of it.  for all layers. The observed variables Yt are also discrete variables, taking on V possible values, where V is the size of the vocabulary of words in the data.  The model provides a joint distribution over these variables, which factors as follows:  T  P ({St, Yt}) = P (S1)P (Y1|S1)  P (St|St−1)P (Yt|St)  (1)  Yt=2  where we refer to P (S1) as the initial distribution, P (Yt|St) as the observation distribution, and P (St|St−1) as the transition distribution. The model assumes that states in one layer evolve independently of the other layers, and it uses log-linear models for the transition distribution of each layer. This allows the transition distribution to factor as follows:  P (St|St−1) =  =  M  M  Ym=1 Ym=1  P (Sm  t |Sm  t−1)  k=1 (exp θmjk)Sm k′=1(exp θmjk′ )Sm  t−1,j ·Sm  t,k  t−1,j  K  Yj=1QK PK  where θmjk is the transition parameter of the model indexed by the layer, previous state and current state respectively. The initial distribution is deﬁned the same way, except that we drop indicators for the previous time step and use parameters θmk rather than θmjk. There are several possible functions to choose from for the observation distribution P (Yt|St). A table approach would need a matrix with V × K M parameters, with exponential blow-up as we increase the number of layers. Two standard ways of approximating this matrix of parameters are the noisy-or model and the log-linear model, and we choose log-linear distribution in our model. The observations Yt are dependent on the states in all layers of St:  P (Yt|St) = Qm,k (exp θYtmk)Sm PY Qm,k (exp θY mk)Sm  t,k  t,k  where θY mk is the observation parameter of the model indexed by the observed value, the latent layer, and the state of the latent layer respectively.  The number of parameters for the initial distribution is M × K, for the transition distribution it is M × K 2, and for the observation distribution it is M × K × V . For ﬁnding representations using this model, we ﬁrst estimate the parameters of the model by train- ing it on unlabeled data in unsupervised fashion. Later, we use these parameters for ﬁnding the latent states for the data and use them as the representations.  3  (2)  (3)  (4)  While we borrow the model structure from Ghahramani and Jordan [23], the main between their model and our model is in the observation distribution. Crucially, their model works with real-valued observations, while our model works with discrete-valued observations like words. As a result of the change to the observation distribution, we need to change the inference and learning procedures, although we continue to follow [23] in using variational approximations, which we explain below.  4 Variational Methods for Learning and Inference  We aim to learn our representations from unlabeled text. The objective for our unsupervised param- eter estimation procedure is to maximize marginal log likelihood:  L(θ) = log P ({Yt}) = logX{St}  P ({St, Yt})  (5)  We can re-write the objective using an arbitrary new distribution Q({St}), which we call the varia- tional distribution, and apply Jensen’s inequality, as follows:  logX{St}  P ({St, Yt}) = logX{St} ≥ X{St}  Q({St})  P ({St, Yt})  Q({St})  Q({St}) log  P ({St, Yt})  Q({St})  = F (Q, θ)  (6)  (7)  F (Q, θ) is a lower bound on the objective function that becomes exact when Q is equal to the posterior distribution P ({St}|{Yt}). We can re-write F (Q, θ) in two different ways:  F (Q, θ) = X{St}  Q({St}) log P ({Yt}) + X{St}  = L(θ) − KL(Q({St})||P ({St}|{Yt}))  Q({St}) log  P ({St}|{Yt})  Q({St})  and  F (Q, θ) = E{St}∼Q[log P ({St, Yt})] − E{St}∼Q[log Q({St})]  (8)  (9)  We use the Expectation Maximization (EM) algorithm [17], a block coordinate-ascent algorithm, to learn the parameters θ. The E-step consists of maximizing F with respect to Q while keeping θ ﬁxed to the current set of parameters, which is equivalent to minimizing the KL-divergence in Equation 8. The M-step consists of maximizing F with respect to θ while keeping Q ﬁxed, which is equivalent to maximizing the ﬁrst term in Equation 9. The algorithm guarantees that these two steps converge to a (local) maximum.  E-step:  M-step:  4.1 E-step  Qt+1 = arg max  F (Q, θt) = arg min  KL(Q({St})||P ({St}|{Yt}, θt)  (10)  Q  Q  θt+1 = arg max  F (Qt+1, θ) = arg max  θ  θ  E{St}∼Qt+1 [log P ({St, Yt}|θ)]  (11)  In the E-step, we can minimize the KL-divergence to zero by setting the Q distribution equal to the posterior distribution P ({St}|{Yt}) if we can compute the posterior exactly. The posterior for the model is given by  P ({St}|{Yt}) =  1  Z Yt Qm,k (exp θYtmk)Sm PY Qm,k (exp θY mk)Sm  t,k Yt,m,j Qk (exp θmjk)Sm Pk′ (exp θmjk′ )Sm  t,k  t−1,j  t−1,j ·Sm  t,k  (12)  where, Z =P{St} P ({St, Yt}).  4  However, in an FHMM, the exact computation of this posterior is computationally intractable, even using a forward-backward algorithm. The hidden state variables at each time-step become depen- dent conditioned on the observed variable requiring the computation of K M expectations, which is infeasible to compute for large M .  For this reason, we resort to a variational approximation of the posterior P by distribution Q with its own variational parameters. The graphical model for the variational distribution Q is chosen such that most of the structure of the original model is preserved, but such that the inference becomes tractable. We borrow the structured variational approximation model from Ghahramani and Jordan [23] which is shown in 1b. The variational model is essentially M independent Markov chains, one for each layer of the FHMM. Markov chains permit efﬁcient maximum a posteriori inference using the standard Viterbi algorithm and efﬁcient computation of marginal distributions using the standard forward-backward algorithm.  The full variational distribution is given by  Q({St}|ϕ) =  exp(ϕtmk)Sm  1  ZQ Yt,m,k  t−1,j ·Sm  t,k  t,k Yt,m,j Qk (exp ϕmjk)Sm Pk′ (exp ϕmjk′ )Sm  t−1,j  where ϕ are the variational parameters of the model. ϕtmk indicate observation variational param- eters, and ϕmjk indicate transition variational parameters. The transition variational distribution matches the FHMM transition distribution. Notice that the observation parameters are indexed by time step, rather than by the actual observation at that time step. Thus the variational distribution can be thought of as having each layer a ﬁctitious observation, chosen so as to mimic the true posterior as closely as possible.  (13)  The variational parameters are optimized by minimizing the KL-divergence, which obtains its min- imum when the initial and transition variational parameters are equal to the initial and transition parameters of the original model, i.e. ∀m, j, k . ϕmjk = θmjk and ϕmk = θmk. In this subspace of the parameter space, the KL-divergence simpliﬁes to  KL(Q||P ) =E  − log ZQ + log Z + Xt,m,k + E""Xt exp Xk  logXY Ym  Sm  t,k(ϕtmk − θYtmk)   θY mkSm  t,k!#  (14)  where, the expectations are taken using the Q({St}) distribution. Unfortunately, Equation 14 still does not permit efﬁcient optimization of the ϕ parameters because of the log-sum-exp expression in the ﬁnal term. While we followed the variational approximation from the original FHMM model so far, we provide a new algorithm to handle the log-sum-exp term introduced due to the observation distribution in our model. To handle this term, we make use of a second variational bound, log x ≤ φx − log φ − 1 [33, 4], where φ is a new variational parameter which can be varied to make the bound tight. This allows us to rewrite the ﬁnal term in 14 as follows:  E""Xt  logXY Ym  exp Xk  θY mkSm  t,k!# ≤Xt =Xt  =Xt  φt(XY E""Ym exp Xk φt(XY Ym E""exp Xk φt(XY Ym ""Xk  θY mkSm  t,k!# − log φt − 1)  (15)  θY mkSm  t,k!# − log φt − 1) t,k(cid:3)# − log φt − 1)  (16)  (17)  exp(θY mk)E(cid:2)Sm  In Eqn. 16, we move the expectation inside the product over M independent layers. Eqn. 17 follows from the moment generating function for the multinomial random variable Sm t .  5  Making use of this bound in Eqn. 14, we get a new upper bound on the KL-divergence, which we denote by KL. The variational parameters φ have a closed-form solution when we set the gradient of .  KL with respect to the parameters φ to zero. We get φt =(cid:16)PY QmPk′ EhSm  To optimize the variational parameters ϕ, we ﬁnd the gradient to minimize KL.  t,k′i exp θY mk′(cid:17)−1  ∂KL ∂ϕtmk  =  ∂KL  t,ki ∂EhSm  (18)  (19)  ∂ϕtmk  ∂EhSm t,ki t,ki to zero, we get Setting the derivative with respect to EhSm  t,k′(cid:3) exp θY mk′  Yn6=mXk′ E(cid:2)Sm   ϕtmk = θYtmk − φtXY   exp θY mk   To ﬁnd the optimized value for ϕtmk, we initially set the values to random values. We then run the forward-backward algorithm algorithm in each markov chain of the variational model and ﬁnd the  expectations EhSm  t,ki using these variational parameters. Then these expectations are used to ﬁnd  the new set of variational parameters using the above equation. These two steps are iterated until convergence.  Once the variational parameters have been optimized, the variational distribution is used in the  forward-backward algorithm in each layer of Q for computing the expectations EhSm EhSm  t,ki required to compute the expected sufﬁcient statistics for the M-step.  t−1,j, Sm  t,ki and  4.2 M-step  In the M-step, the best set of parameters of the model are found by maximizing the objective function  F = E[log P ({St, Yt})]  (20)  Initial and transition parameters are normalized to proper distribution like in a standard HMM using the expected sufﬁcient statistics gathered in the E-step. For the observation parameters, no closed form solution exists and we resort to gradient descent. We again use the bound on the objective function to move the expectation inside the parameter of the log. The gradient of the lower bound F of the objective is  ∂F  ∂θY mk  =Xt   E(cid:2)Sm  t,k(cid:3) 1[Y = Yt] − φt   Yn6=m Xk′ t,k′i exp θY mk′(cid:17)−1  .  where, φt =(cid:16)PY QmPk′ EhSm  4.3 Inference  t,k′(cid:3) exp θY nk′! E(cid:2)Sn   t,k(cid:3) exp θY mk E(cid:2)Sm (21)  To compute the posterior P ({St}|{Yt}), we ﬁnd the variational distribution Q that best approxi- mates the posterior using the same procedure as in the E-step. Q is also used to ﬁnd the maximum a posteriori state sequence using Viterbi algorithm.  4.4 Implementation Details  Rather than using Equations 19 and 21 naively, we precomputed few values to serve as a lookup for reducing the runtime complexity. First, we precomputed φt and also stored for each Y the product over all m (requiring memory O(V )). To compute the sum over all Y s, we reused these values, which only vary by a factor involving the dot product involving the layer m and Y . We then used  6  these computations to ﬁnally update the parameter in time O(M KV T ). We used similar precom- putation technique in the M-step making the expensive gradient computation runtime O(M KV T ). Note that we can clear the memory after processing the result for a token making the memory use in- dependent of the number of tokens. However, the runtime is dependent on the number of tokens. We used online learning technique [37, 8] for learning our model on large corpus. We took a mini-batch of 1000 sentences per iteration and ran for 5 epochs. We found that using observation parame- ters from previous iterations of EM to initialize the variational parameters allowed the variational parameters to converge much more quickly than random initialization. The variational parameter usually converged under 10 iterations. We used an existing package of L-BFGS for optimizing the parameters in the M-step, running for a maximum of 100 iterations.  5 Experiments  We ran the experiments on two sequence labeling tasks in domain adaptation setting: part-of-speech (PoS) tagging and noun-phrase (NP) chunking. In both experiments, the source domain is from the newswire text and the target domain is from the biomedical text. However, the setting for the domain adaptation for the two experiments are different. In PoS tagging experiment, we get access to the text from the test domain and can use it to train the representation learning model. In the chunking experiment, however, we get access to a domain related to the test domain but not to the text from the test domain itself. In both experiments, latent vectors from the FHMMs are used as representations in the supervised experiments.  5.1 Experimental Setup  In the PoS tagging experiment, the labeled training data is from sections 02-21 of the Penn Treebank containing 39,832 sentences which have manually labeled PoS tags. We used a standard test set previously used by Huang et al. [28] and Blitzer et al. [7] which contains 561 MEDLINE sentences with 14554 tokens from the Penn BioIE project. In the NP chunking experiment, we used the CoNLL-2000 shared task training data by replacing all non-NP chunk tags with outside chunk tag, O. The dataset consist of sections 15-18 of the Penn Treebank containing 8932 sentences which are annotated with PoS tags and the chunk tags[46]. The test set consist of 198 manually annotated sentences [28] from the biomedical domain of Open American National Corpus (OANC).  For representation learning, we used the unlabeled data from both domains: WSJ corpus and from the MEDLINE corpus. We did not include any data from the biomedical domain of the OANC. Then we used a preprocessing step deﬁned in [36] to select a subset of good sentences for training the FHMM, resulting in 112,824 sentences. This was done to train our system in reasonable amount of time but still learning a good representation from the data from both the domains. This data size is smaller than what previous researchers have used and covers fewer number of words in the supervised text. However, we think that a good representation for such words will be found by making use of our model which predicts it depending on the whole context of the sentence in the inference time. The words appearing only once in the corpus were collapsed into a special *unk* symbol and numbers were collapsed into a special *num* symbol, resulting in a vocabulary size of 40,596. This vocabulary did not cover 2% and 0.3% of the tokens in the supervised PoS training data and testing data respectively and 3% of the tokens in both the training and test data for the chunking task.  We ran FHMMs with different state spaces and compared their representation capacity in the su- pervised experiments. We also compared the global context dependent distributed representations from FHMMs against other systems using the same unlabeled data for learning the representations. HMM provides globally context-dependent representations for the words which are not distributed representations. Brown Clusters provided a ﬁxed cluster independent of the context and are not distributed representations. Word embeddings from neural network models also provided a ﬁxed representation per word but are distributed representations. We trained 80 states HMM and used the Viterbi decoded states as representations [28] and a Brown Clustering algorithm [9] with 1000 clus- ters as representations. Following Turian et al. [49], we used the preﬁxes of length 4, 6, 10 and 20 in addition to the whole path for the Brown representation as the features. We also trained a neural network model described in Collobert and Weston [11] and used previous 10 words to predict the next word in the sequence. We learned 50-dimensional word embeddings using 100 units in the hid-  7  den layer. We make the comparison of the representation systems by using them in the supervised experiments as features. We used Conditional Random Fields (CRF) with the same features as de- ﬁned in Huang et al. [28], with addition of the context words and context representation dimension as features in a window of size 2. In the chunking dataset, the representation learning text we used is different but the test set is the same as the previous work [28] we compare against, who also used the test sentences in the representation learning system.  We use the standard evaluation metric of word error rate for PoS tagging and F1 score for NP- chunking, focusing on the words that never appeared (OOV) and that appeared at most two times (rare) in the supervised training corpus. To calculate the precision for the OOV and rare words, we deﬁne false positive as the phrases predicted by the model beginning with the OOV or rare word but actually are not phrases in the gold dataset.  5.2 Results and Discussion  We represent the baseline system trained using traditional features with no features from a repre- sentation learning system as BASELINE. FHMM-M-K is our system using Viterbi decoded states as representations from a M layer FHMM with K states in each layer. FHMM-POST-M-K is our system using posterior probability distribution over states as representations from a M layer FHMM with K states in each layer. HMM-80 is a system using Viterbi decoded states as representation from an HMM model with 80 states and BROWN-1000 is a system using Brown clustering algorithm with 1000 clusters as representations. The 50-dimensional word embeddings learned using neural network model from Collobert and Weston [11] is represented as EMBEDDINGS-50. All these sys- tems were trained on the same unlabeled representation text. In PoS tagging experiment, we report the error on all tokens (14,554). HUANGHMM-80 is a system deﬁned in [28] and SCL is a system deﬁned in [7] and report the error rate on the same test data as ours but with fewer number of tokens (9576 and approx 13,000 respectively). Also, they used unlabeled corpus of larger size to train their representation learning system. This makes the direct comparison with their system difﬁcult. Table 1 shows the error rate of the supervised classiﬁer using different representation learning systems in PoS tagging experiment. Table 2 shows the Recall(R), Precision(P) and F1 score of the supervised classiﬁer using different representation learning system. Previous system HUANGHMM-80 [28] included the text from the test domain itself for representation learning.  We found that increasing the state space by increasing the number of layers in the representation learning system generally improved the performance of the supervised classiﬁer. We also found model FHMM-POST-5-10 performs better than the same state size model FHMM-5-10. We think this is because the soft probabilities over the hidden states give more information to the supervised classiﬁer compared to the one-hot representation as in Viterbi decoded states. Compared to the baseline, our best performing system reduced the error by 3.8% on the OOV words and by 3.2% on the rare words in the PoS tagging experiment and increased the F1 score by 5% on both the OOV and the rare words in the chunking experiment.  We present analysis on why the performance of the different representation learning systems differ. BROWN-1000 suffers mainly for two reasons. First, it assigns a ﬁxed cluster to a word irrespective of its different usage as meaning in different domains. For example, the word share which appears most frequently as NN in the train domain appears only as a VBP in the test domain and the word signaling which only appears as VBG in the training domain appears only as NN in the test domain. Second, it provides no representation for the words that did not appear in the training of represen- tation learning system. Models like HMM and FHMM do not suffer from these limitations. They both can provide different representations for the same word with different meaning in two domains. They also give a representation to a word by looking at the context even if it is not seen by the rep- resentation learning system. We also experimented by giving the context clusters when using the Brown clusters as representations but it did not improve the performance. EMBEDDINGS-50 also suffers in the PoS tagging experiment because of the ﬁxed representations provided to the words irrespective of its different usage as meaning in different domains. In the chunking experiment, the performance of the HMM-80 model is worse than the baseline. We found that most of the errors were on the phrases surrounding the words like and and punctuations where the HMM-80 system got confused on whether to continue the phrase or not whereas FHMM-5-10 is performing much better in such situations. The word and in the training text of the chunking data is clustered into fairly small (5-10) number of cluster in an 80 state HMM however it is clustered into 400 different  8  Model BASELINE FHMM-1-10 FHMM-3-10 FHMM-5-10 FHMM-POST-5-10 HMM-80 BROWN-1000 EMBEDDINGS-50 HUANGHMM-80* SCL*  All % error OOV % error Rare % error  10.5 10.4 10.2 9.7 9.7 10.1 9.9 10.2 9.5 11.1  32.2 31.7 30.6 29.7 28.4 30.7 30.2 30.3 24.8 28.0  28.0 27.8 26.6 25.2 24.8 26.4 25.8 26.3  - -  Table 1: Error rate for the supervised classiﬁer for the PoS tagging experiment using different representations. The error decreases as the state space size of the FHMM increases. Our model with highest state space size FHMM-5-10 performs better than existing system trained using the representations learned on the same un- labeled text that we used. Also, the model FHMM-POST-5-10 with posterior probabilities as representation performs better than the model FHMM-5-10 with Viterbi decoded states as representations. The bottom two rows represent systems trained on larger unlabeled corpus and reporting error on the same test set but with fewer number of tokens than ours (see text for details).  Model BASELINE FHMM-1-10 FHMM-3-10 FHMM-5-10 FHMM-POST-5-10 HMM-80 BROWN-1000 EMBEDDINGS-50 HUANGHMM-80*  R .87 .87 .88 .90 .90 .87 .87 .89 .91  All P .90 .90 .90 .91 .91 .90 .90 .90 .90  F1 .88 .88 .89 .90 .90 .88 .89 .89 .91  OOV  P .89 .88 .90 .90 .91 .87 .90 .90 .89  R .78 .77 .80 .83 .85 .77 .79 .86 .80  F1 .83 .82 .85 .86 .88 .82 .84 .88 .84  Rare  P .89 .88 .90 .91 .92 .87 .90 .90 .89  R .80 .78 .82 .83 .86 .79 .80 .86 .82  F1 .84 .83 .85 .87 .89 .83 .85 .88 .85  Table 2: Recall(R), Precision(P) and F1 score for the supervised chunking experiment using different repre- sentations. Our model FHMM-POST-5-10 performs better than all other models. HUANGHMM-80 is the HMM model system from a previous work on the same test set but using a different unlabeled representation text than ours (see text for details).  clusters in FHMM model, usually varying in only few dimensions. Although we have not done fur- ther analysis, we think the representation using FHMM is able to capture larger context information into the representation which helps the supervised system to make better prediction. The reason HUANGHMM-80 performed better than the BASELINE and HMM-80 might be because it is able to see the test sentences during the representation learning phase. The best results for the chunking task are provided by the distributed representations from the FHMMs or the word embeddings. The results show that the distributed representations from the FHMMs which can provide different repre- sentations per word depending on the whole sentence context can give either superior or comparable performance to the other representations.  6 Conclusion and Future Work  We have developed a learning mechanism for discrete-valued FHMMs involving a novel two-step variational approximation that can train on datasets of nearly three million tokens in reasonable time. Using the latent vectors predicted by our best FHMM model as features, we have shown that a standard classiﬁer for two NLP tasks, chunking and tagging, and improve signiﬁcantly on new domains, especially on words that were never or rarely observed in labeled training data.  9  Signiﬁcant work remains to make the FHMM more useful as a representation learning technique. Most importantly, it needs to scale to larger datasets using a distributed implementation. More work is also needed to fully compare the relative merits of using global context against using local context in a representation learning framework. The same idea of using global context to predict features can also be extended to other tasks, including hierarchical segmentation tasks like parsing, or perhaps even to image processing tasks using different classes of models. Finally, it remains an open question whether there are efﬁcient and useful ways to combine the distinct beneﬁts of joint prediction and deep architectures for representation learning.  Acknowledgments  This research was supported in part by NSF grant IIS-1065397.  References  [1] Arun Ahuja and Doug Downey. Improved extraction assessment through better language mod- els. In Proceedings of the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (NAACL-HLT), 2010.  [2] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In International  Conference on Machine Learning (ICML), 2009.  [3] Yoshua Bengio. Neural net language models. Scholarpedia, 3(1):3881, 2008. [4] David Blei and John Lafferty. Correlated topic models. In Y. Weiss, B. Sch¨olkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18, pages 147–154. MIT Press, Cambridge, MA, 2006.  [5] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet allocation. Journal of  Machine Learning Research, 3:993–1022, January 2003.  [6] John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jenn Wortman. Learning bounds for domain adaptation. In Advances in Neural Information Processing Systems, 2007. [7] John Blitzer, Ryan McDonald, and Fernando Pereira. Domain adaptation with structural cor-  respondence learning. In EMNLP, 2006.  [8] L´eon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In IN: ADVANCES  IN NEURAL INFORMATION PROCESSING SYSTEMS 20, pages 161–168, 2008.  [9] P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. D. Pietra, and J. C. Lai. Class-based n-gram  models of natural language. Computational Linguistics, pages 467–479, 1992.  [10] M. Candito and B. Crabbe. Improving generative statistical parsing with semi-supervised word  clustering. In IWPT, pages 138–141, 2009.  [11] R. Collobert and J. Weston. A uniﬁed architecture for natural language processing: Deep neural networks with multitask learning. In International Conference on Machine Learning (ICML), 2008.  [12] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural lan- guage processing (almost) from scratch. Journal of Machine Learning Research, 12:2493– 2537, 2011.  [13] Hal Daum´e III. Frustratingly easy domain adaptation. In ACL, 2007. [14] Hal Daum´e III, Abhishek Kumar, and Avishek Saha. Frustratingly easy semi-supervised do- main adaptation. In Proceedings of the ACL Workshop on Domain Adaptation (DANLP), 2010. [15] Hal Daum´e III and Daniel Marcu. Domain adaptation for statistical classiﬁers. Journal of  Artiﬁcial Intelligence Research, 26, 2006.  [16] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman.  In- dexing by latent semantic analysis. Journal of the American Society of Information Science, 41(6):391–407, 1990.  [17] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em algorithm. JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B, 39(1):1– 38, 1977.  10  [18] Paramveer S. Dhillon, Jordan Rodu, Dean P. Foster, and Lyle H. Ungar. Two step cca: A new spectral method for estimating vector models of words. In Proceedings of the 29th Interna- tional Conference on Machine learning, ICML’12, 2012.  [19] Doug Downey, Stefan Schoenmackers, and Oren Etzioni. Sparse information extraction: Un-  supervised language models to the rescue. In ACL, 2007.  [20] Mark Dredze and Koby Crammer. Online methods for multi-domain learning and adaptation.  In Proceedings of EMNLP, pages 689–697, 2008.  [21] Mark Dredze, Alex Kulesza, and Koby Crammer. Multi-domain learning by conﬁdence  weighted parameter combination. Machine Learning, 79, 2010.  [22] Jenny Rose Finkel and Christopher D. Manning. Hierarchical bayesian domain adaptation. In  Proceedings of HLT-NAACL, pages 602–610, 2009.  [23] Zoubin Ghahramani and Michael I. Jordan. Factorial hidden markov models. Machine Learn-  ing, 29(2-3):245–273, 1997.  [24] Edouard Grave, Guillaume Obozinski, and Francis Bach. Hidden markov tree models for semantic class induction. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 94–103, Soﬁa, Bulgaria, August 2013. Association for Computational Linguistics.  [25] James Henderson and Ivan Titov. Incremental sigmoid belief networks for grammar learning.  Journal of Machine Learning Research, 2010.  [26] T. Honkela. Self-organizing maps of words for natural language processing applications. In  Proceedings of the International ICSC Symposium on Soft Computing, 1997.  [27] Fei Huang, Arun Ahuja, Doug Downey, Yi Yang, Yuhong Guo, and Alexander Yates. Learning Representations for Weakly Supervised Natural Language Processing Tasks. Computational Linguistics, 40(1), 2014.  [28] Fei Huang and Alexander Yates. Distributional representations for handling sparsity in super- vised sequence labeling. In Proceedings of the Annual Meeting of the Association for Compu- tational Linguistics (ACL), 2009.  [29] Fei Huang and Alexander Yates. Exploring representation-learning approaches to domain In Proceedings of the ACL 2010 Workshop on Domain Adaptation for Natural  adaptation. Language Processing (DANLP), 2010.  [30] Fei Huang and Alexander Yates. Open-domain semantic role labeling by modeling word spans. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2010.  [31] Robert A. Jacobs, Wenxin Jiang, and Martin A. Tanner. Factorial Hidden Markov Models and  the Generalized Backﬁtting Algorithm. Neural Computation, 14(10):2415–2437, 2002.  [32] Jing Jiang and ChengXiang Zhai. Instance weighting for domain adaptation in NLP. In ACL,  2007.  [33] Michael I. Jordan, Zoubin Ghahramani, Tommi S. Jaakkola, and Lawrence K. Saul. An in- troduction to variational methods for graphical models. Machine Learning, 37(2):183–233, 1999.  [34] S. Kaski. Dimensionality reduction by random mapping: Fast similarity computation for clus-  tering. In IJCNN, pages 413–418, 1998.  [35] T. Koo, X. Carreras, and M. Collins. Simple semi-supervised dependency parsing. In Pro- ceedings of the Annual Meeting of the Association of Computational Linguistics (ACL), pages 595–603, 2008.  [36] Percy Liang. Semi-supervised learning for natural language.  2005.  In MASTERS THESIS, MIT,  [37] Percy Liang and Dan Klein. Online em for unsupervised models. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 611–619, Stroudsburg, PA, USA, 2009. Association for Computational Linguistics.  11  [38] D. Lin and X Wu. Phrase clustering for discriminative learning. In ACL-IJCNLP, pages 1030–  1038, 2009.  [39] Y. Mansour, M. Mohri, and A. Rostamizadeh. Domain adaptation with multiple sources. In  Advances in Neural Information Processing Systems, 2009.  [40] S. Martin, J. Liermann, and H. Ney. Algorithms for bigram and trigram word clustering.  Speech Communication, 24:19–37, 1998.  [41] A. Mnih and G. E. Hinton. A scalable hierarchical distributed language model. In Neural  Information Processing Systems (NIPS), pages 1081–1088, 2009.  [42] F. Pereira, N. Tishby, and L. Lee. Distributional clustering of English words. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 183–190, 1993.  [43] M. Sahlgren. An introduction to random indexing. In Methods and Applications of Seman- tic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering (TKE), 2005.  [44] M. Sahlgren. The word-space model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces. PhD thesis, Stockholm University, 2006. [45] G. Salton and M.J. McGill.  Introduction to Modern Information Retrieval. McGraw-Hill,  1983.  [46] Erik F. Tjong Kim Sang, Sabine Buchholz, and Kim Sang.  shared task: Chunking, 2000.  Introduction to the conll-2000  [47] Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. Parsing with Com-  positional Vector Grammars. In Association for Computational Linguistics ACL), 2013.  [48] Ivan Titov and James Henderson. Constituent parsing with incremental sigmoid belief net-  works. In Association for Computational Linguistics ACL, 2007.  [49] Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 384–394, 2010.  [50] P. D. Turney and P. Pantel. From frequency to meaning: Vector space models of semantics.  Journal of Artiﬁcial Intelligence Research, 37:141–188, 2010.  [51] J. J. V¨ayrynen, T. Honkela, and L. Lindqvist. Towards explicit semantic features using inde- pendent component analysis. In Proceedings of the Workshop Semantic Content Acquisition and Representation (SCAR), 2007.  [52] Jason Weston, Frederic Ratle, and Ronan Collobert. Deep learning via semi-supervised em-  bedding. In Proceedings of the 25th International Conference on Machine Learning, 2008.  [53] Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong Zhou. Multilingual dependency learn- In CoNLL 2009  ing: A huge feature engineering method to semantic dependency parsing. Shared Task, 2009.  12  ","Most representation learning algorithms for language and image processing arelocal, in that they identify features for a data point based on surroundingpoints. Yet in language processing, the correct meaning of a word often dependson its global context. As a step toward incorporating global context intorepresentation learning, we develop a representation learning algorithm thatincorporates joint prediction into its technique for producing features for aword. We develop efficient variational methods for learning Factorial HiddenMarkov Models from large texts, and use variational distributions to producefeatures for each word that are sensitive to the entire input sequence, notjust to a local context window. Experiments on part-of-speech tagging andchunking indicate that the features are competitive with or better thanexisting state-of-the-art representation learning methods."
1312.5851,2014,Fast Training of Convolutional Networks through FFTs  ,"['Michael Mathieu', 'Mikael Henaff', 'Yann LeCun']",https://arxiv.org/pdf/1312.5851.pdf,"4 1 0 2    r a  M 6         ]  V C . s c [      5 v 1 5 8 5  .  2 1 3 1 : v i X r a  Fast Training of Convolutional Networks through  FFTs  Courant Institute of Mathematical Sciences  Courant Institute of Mathematical Sciences  Michael Mathieu  New York University  mathieu@cs.nyu.edu  Mikael Henaff  New York University mbh305@nyu.edu  Courant Institute of Mathematical Sciences  Yann LeCun  New York University yann@cs.nyu.edu  Abstract  Convolutional networks are one of the most widely employed architectures in computer vision and machine learning. In order to leverage their ability to learn complex functions, large amounts of data are required for training. Training a large convolutional network to produce state-of-the-art results can take weeks, even when using modern GPUs. Producing labels using a trained network can also be costly when dealing with web-scale datasets. In this work, we present a simple algorithm which accelerates training and inference by a signiﬁcant factor, and can yield improvements of over an order of magnitude compared to exist- ing state-of-the-art implementations. This is done by computing convolutions as pointwise products in the Fourier domain while reusing the same transformed fea- ture map many times. The algorithm is implemented on a GPU architecture and addresses a number of related challenges.  1  Introduction  As computer vision and machine learning aim to solve increasingly challenging tasks, models of greater complexity are required. This in turn requires orders of magnitude more data to take ad- vantage of these powerful models while avoiding overﬁtting. While early benchmark datasets in machine learning contained thousands or tens of thousands of samples [7, 3, 10], current datasets are of the order of millions [6, 2]. This brings about new challenges as to how to train networks in a feasible amount of time. Even using parallel computing environments, training a network on ImageNet can take weeks [8]. In addition, although inference of labels using a trained network is comparatively fast, real-world applications such as producing labels for all images on the internet can represent a signiﬁcant cost in terms of time and resources. Therefore, there is an important need to develop fast algorithms for training and inference. In this work, we present a simple algorithm which accelerates training and inference using convo- lutional networks. The idea is based on performing convolutions as products in the Fourier domain, and reusing transformed feature maps many times. The signiﬁcant operations in training convolu- tional networks can all be viewed as convolutions between pairs of 2-D matrices, which can rep- resent input and output feature maps, gradients of the loss with respect to feature maps, or weight kernels. Typically, convolutions are performed for all pairings between two sets of 2-D matrices. By computing the Fourier transforms of the matrices in each set once, we can efﬁciently perform all convolutions as pairwise products.  1  (cid:88)  Although it has long been known that convolutions can be computed as products in the Fourier do- main, until recently the number of feature maps used in convolutional networks has been too small to make a method like ours effective. Previous work in the 90’s [1] explored the possibility of using FFTs to accelerate inference at the ﬁrst layer of a trained network, where the Fourier transforms of the ﬁlters could be precomputed ofﬂine. However, this was not used during training, possibly because the number of feature maps used at the time was too small to make the overhead of com- puting FFTs at every iteration worthwhile. When the number of feature maps is large, as is the case for modern convolutional networks, using FFTs accelerates training and inference by a signiﬁcant factor and can lead to a speedup of over an order of magnitude.  2 Theory  2.1 Backpropagation  The backpropagation algorithm [9] is the standard method to compute the gradient when training a convolutional network. During training, each layer performs three tasks, which we now describe. First we ﬁx some notation: for a given layer, we have a set of input feature maps xf indexed by f, each one being a 2-D image of dimensions n× n. The output is a set of feature maps yf(cid:48) indexed by f(cid:48), which are also 2-D images whose dimension depends on the convolutional kernel and its stride. The layer’s trainable parameters consist of a set of weights wf(cid:48)f , each of which is a small kernel of dimensions k × k. 1 In the forward pass, each output feature map is computed as a sum of the input feature maps con- volved with the corresponding trainable weight kernel:  yf(cid:48) =  xf ∗ wf(cid:48)f  (1)  During the backward pass, the gradients with respect to the inputs are computed by convolving the transposed weight kernel with the gradients with respect to the outputs:  f  ∂L ∂xf  =  ∂L ∂yf(cid:48)  ∗ wT f(cid:48)f  (2)  This step is necessary for computing the gradients in (3) for the previous layer. Finally, the gradients of the loss with respect to the weight are computed by convolving each input feature map with the gradients with respect to the outputs:  ∂L wf(cid:48)f  =  ∂L ∂yf(cid:48)  ∗ xf  (3)  Note that ∂L operations consist of convolutions between various sets of 2-D matrices.  ∂yf(cid:48) is a 2-D matrix with the same dimensions as the output feature map yf(cid:48), and that all  2.2 Algorithm  The well-known Convolution Theorem states that circular convolutions in the spatial domain are equivalent to pointwise products in the Fourier domain. Letting F denote the Fourier transform and F−1 its inverse, we can compute convolutions between functions f and g as follows:  f ∗ g = F−1(F(f ) · F(g))  Typically, this method is used when the size of the convolution kernel is close to that of the input image. Note that a convolution of an image of size n× n with a kernel of size k × k using the direct 1In this paper we assume the input images and kernels are square for simplicity of notation, but the results  can be trivially extended to non-square images and kernels.  2  Figure 1: Illustration of the algorithm. Note that the matrix-multiplication involves multiplying all input feature maps by all corresponding kernels.  method requires (n − k + 1)2k2 operations. The complexity of the FFT-based method requires 6Cn2 log n + 4n2 operations: each FFT requires O(n2 log n2) = O(2n2 log n) = 2Cn2 log n, and the pointwise product in the frequency domain requires 4n2 (note that the products are between two complex numbers). Here C represents the hidden constant in the O notation. 2 Our algorithm is based on the observation that in all of the operations (1), (2) and (3), each of the matrices indexed by f is convolved with each of the matrices indexed by f(cid:48). We can therefore compute the FFT of each matrix once, and all pairwise convolutions can be performed as products in the frequency domain. Even though using the FFT-based method may be less efﬁcient for a given convolution, we can effectively reuse our FFTs many times which more than compensates for the overhead. The following analysis makes this idea precise. Assume we have f input feature maps, f(cid:48) output feature maps, images consisting of n × n pixels and kernels of k × k pixels. Also assume we are performing updates over minibatches of size S, and that C represents the hidden constant in the FFT complexity. As an example, using the direct approach (1) will take a total of S·f(cid:48)·f ·(n−k +1)2·k2 operations. Our approach requires (2C · n2 log n)(S · f + f(cid:48) · f ) operations to transform the input feature maps and kernels to the Fourier domain, a total of 4S· f(cid:48)· f · n2 additions and multiplications in the Fourier domain, and S · f(cid:48) · (2C · n2 log n) operations to transform the output feature maps back to the spatial domain. The same analysis yields similar complexity estimates for the other operations:  Direct Convolution Our Method S · f(cid:48) · f · n(cid:48)2 · k2 S · f(cid:48) · f · n2 · k2 S · f(cid:48) · f · k2 · n(cid:48)2  (cid:80) f xf ∗ wf(cid:48)f ∂yf(cid:48) ∗ wT f(cid:48)f ∂yf(cid:48) ∗ xf Here n(cid:48) = (n − k + 1) represents the size of the output feature map. Note that the high complexity of the direct method for convolution comes from the product of ﬁve terms, whereas our method has a sum of products with at most four terms. Figure 2 shows the theoretical number of operations for direct convolution and our FFT method for various input sizes.  2Cn2 log n[f(cid:48) · S + f · S + f(cid:48) · f ] + 4S · f(cid:48) · f · n2 2Cn(cid:48)2 log n(cid:48)[f(cid:48) · S + f · S + f(cid:48) · f ] + 4S · f(cid:48) · f · n(cid:48)2 2Cn log n2[f(cid:48) · S + f · S + f(cid:48) · f ] + 4S · f(cid:48) · f · n2  ∂L  ∂L  2.3  Implementation and Memory Considerations  Although conceptually straighforward, a number of challenges relating to GPU implementation needed to be addressed. First, current GPU implementations of the FFT such as cuFFT are designed to parallelize over individual transforms. This can be useful for computing a limited number of transforms on large inputs, but is not suitable for our task since we are performing many FFTs over  2Since the FFT-based method is actually computing a circular convolution, the output is cropped to discard coefﬁcients for which the kernel is not completely contained within the input image. This yields an output of the same size as the direct method, and does not require additional computation.  3  FFTsFFTsFFT s-1MatrixMultiplykernelsinputsoutputsFigure 2: Number of operations required for computing (1) with different input image sizes and S = 128, f = 96, f(cid:48) = 256, k = 7.  relatively small inputs. Therefore, we developed a custom CUDA implementation of the Cooley- Tukey FFT algorithm [5] which enabled us to parallelize over feature maps, minibatches and within each 2-D transform. Note that 2-D FFTs lend themselves naturally to parallelization since they can be decomposed into two sets of 1-D FFTs (one over rows and the other over columns), and each set can be done in parallel. Second, additional memory is required to store the feature maps in the Fourier domain. Note that by keeping the Fourier representations in memory for all layers after the forward pass, we could avoid recomputing several of the FFTs during the backward pass. However, this might become pro- hibitively expensive in terms of memory for large networks. Therefore we reuse the same memory for all the different convolutions in the network, so that the necessary amount of memory is de- termined only by the largest convolution layer. All of the analysis in the previous section and all experiments in the remainder of the paper assume we are using this memory-efﬁcient approach. For a convolution layer taking an input of size n × n, with f input features, f(cid:48) output features and a minibatch of size S, we need to store a total of S · f + S · f(cid:48) + f · f(cid:48) frequency representations of size n × n. As another means to save memory, we can use symmetry properties of FFTs of real inputs to store only half the data, i.e. n(n + 1)/2 complex numbers. Assuming ﬂoat representations, the necessary memory in bytes is:  4n(n + 1)(S · f + S · f(cid:48) + f · f(cid:48))  The following table shows the amount of RAM used for typical sizes of convolutions:  S 128 128 64 128 128 128 128 128  n 16 32 64 64 16 32 16 32  f 96 96 96 96 256 256 384 384  f(cid:48) 256 256 256 256 384 384 384 384  RAM used  76MB 294MB 784MB 1159MB 151MB 588MB 214MB 830MB  Note that this is a relatively small additional memory requirement compared to the total amount of memory used by large networks.  4  3 Experiments  To test our analysis, we ran a series of experiments comparing our method to the CudaConv GPU implementation of [8] and a custom implementation using the Torch 7 machine learning environ- ment [4]. Both of these implementations compute convolutions using the direct method in the spatial domain. All experiments were performed on the same GeForce GTX Titan GPU. We began by per- forming unit tests comparing the results of convolutions computed by our method to those computed by the Torch implementation for each of the three operations. We found that the differences in re- sults for operations (1) and (2) to be of the order of 10−5 and for operation (3) to be of the order 10−4. The differences are likely due to rounding errors in ﬂoating-point operations and are within an acceptable range. We then compared how each method performed in terms of speed with varying kernel sizes, in- put sizes and minibatch sizes. The results are shown in Figure 3. For all experiments, we chose 96 input feature maps and 256 output feature maps, which represents a typical conﬁguration of a deep network’s second layer. The functions updateOutput, updateGradInput and accGradParameters correspond to the operations in (1), (2) and (3) respectively. All times are measured in seconds. We see that our method signiﬁcantly outperforms the other two in nearly all cases. The improve- ment is especially pronounced for the accGradParameters operation, which is the most com- putationally expensive. This is likely due to the fact that the convolution we are computing has a large kernel, for which FFTs are better suited in any case. Also note that our method performs the same regardless of kernel size, since we pad the kernel to be the same size as the input image before applying the FFT. This enables the use of much larger kernels, which we intend to explore in future work.  5  Figure 3: Speed comparison with respect to size of input image (top), kernel size (middle) and minibatch size (bottom)  6  01020304050600.00.10.20.30.40.50.60.7updateOutput01020304050600.00.10.20.30.40.50.60.70.80.9updateGradInputCudaConvFFTTorch7 (custom)01020304050600.00.20.40.60.81.01.21.4accGradParametersTime (seconds) versus inputSizebatchSize=128, kernelSize=7, nInputPlanes=96, nOutputPlanes=25624681012140.000.050.100.150.200.250.300.350.400.45updateOutput24681012140.00.10.20.30.40.50.6updateGradInputCudaConvFFTTorch7 (custom)24681012140.00.10.20.30.40.50.60.70.8accGradParametersTime (seconds) versus kernelSizebatchSize=128, inputSize=32, nInputPlanes=96, nOutputPlanes=256204060801001201400.000.050.100.150.200.25updateOutput204060801001201400.050.100.150.200.250.30updateGradInputCudaConvFFTTorch7 (custom)204060801001201400.000.050.100.150.200.250.300.350.400.45accGradParametersTime (seconds) versus batchSizekernelSize=7, inputSize=32, nInputPlanes=96, nOutputPlanes=256We next ran experiments with parameter conﬁgurations typical of those used in different layers of a large convolutional network. The time taken by the different methods are given in milliseconds. The top row is a 4-tuple (k, n, f, f(cid:48)) indicating the width of the kernel, width of the input image, number of input feature maps and number of output feature maps. All kernels and input images are square, of size k × k and n × n respectively. All conﬁgurations have minibatches of size 128. The ﬁrst conﬁguration represents the ﬁrst layer, which is why we did not report times for the updateGradInput operation. For each conﬁguration, the best-performing method is highlighted in bold.  (k, n, f, f(cid:48))  (11, 32, 3, 96)  Torch7 (custom)  CudaConv  FFT  Torch7 (custom)  CudaConv  FFT  Torch7 (custom)  CudaConv  FFT  Torch7 (custom)  CudaConv  FFT  5 16 3  - - -  39 32 2  44 48 5  (7, 32, 96, 256)  (5, 16, 256, 384)  updateOutput  74 98 34 updateGradInput 76 108 76 accGradParameters 116 195 32  Total  266 401 142  178 221 34  197 261 92  285 403 33  660 885 159  (5, 16, 384, 384)  (3, 16, 384, 384)  111 146 49  116 161 116  174 280 48  401 587 213  57 86 49  62 77 116  96 178 47  215 341 212  We see that our FFT-based method performs faster in total for all conﬁgurations, sometimes to a substantial degree. The improvement is very signiﬁcant on the forward pass, which makes the method especially well suited for inference on very large datasets using a trained network. Finally, we tested times taken to perform a training iteration for a network obtained by composing the above layers, inserting max-pooling and rectiﬁed linear units between them, and adding a fully connected layer for prediction with 1000 outputs. This was to account for possible changes in performance due to implementation details such as padding, accessing memory and so on. The following table shows the results in milliseconds:  Torch7 (custom)  CudaConv  FFT  updateOutput  489 717 235  updateGradInput  577 685 471  accGradParameters Total 1756 2495 867  690 1093 161  Our FFT-based method still signiﬁcantly outperforms the other two implementations.  4 Discussion and Future Work  We have presented a simple and fast algorithm for training and inference using convolutional net- works. It outperforms known state-of-the-art implementations in terms of speed, as veriﬁed by nu- merical experiments. In the future we plan to explore the possibility of learning kernels directly in the Fourier domain. Another interesting direction would be to investigate the use of non-linearities in the Fourier domain rather than in the spatial domain, since this would remove the need for inverse transforms and accelerate training and inference further. It is worth mentioning that in our current implementation of the FFT algorithm, input images which are not a power of 2 must be padded to the next highest power. For example, using input images of size 34 × 34 will be suboptimal in terms of speed since they must be padded to be 64 × 64. This limitation is not intrinsic to the FFT and we intend to extend our implementation to accept other sizes  7  in the future. On the other hand, the fact that our method’s speed is invariant to kernel size enables us to use larger kernels at different layers of the network. In future work we intend to thoroughly explore the effect of input image and kernel sizes on performance.  8  References [1] S. Ben-Yacoub, B. Fasel, and J. Luttin. Fast face detection using mlp and fft. In Proceedings of the Second International Conference on Audio and Video-based Biometric Person Authen- tiﬁcation (AVBPA 1999), 1999.  [2] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, and Paul Lamere. The million song dataset. In Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR 2011), 2011.  [3] A. Bosch, A. Zisserman, and X. Munoz. Representing shape with a spatial pyramid kernel. In  Proceedings of the ACM International Conference on Image and Video Retrieval, 2007.  [4] Ronan Collobert, Koray Kavukcuoglu, and Clement Farabet. Torch7: A matlab-like environ-  ment for machine learning. In NIPS, 2011.  [5] James Cooley and John Tukey. An algorithm for the machine calculation of complex fourier  series. Mathematics of Computation, (19):297–301, 1965.  [6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale  hierarchical image database. 2009.  [7] L. Fei-Fei, R. Fergus, and Pietro Perona. Learning generative visual models from few training  examples: An incremental bayesian approach tested on 101 object categories. 2004.  [8] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep  convolutional neural networks. In NIPS, pages 1106–1114, 2012.  [9] Y. LeCun, L. Bottou, G. Orr, and K. Muller. Efﬁcient backprop. In G. Orr and Muller K.,  editors, Neural Networks: Tricks of the trade. Springer, 1998.  [10] G. Tzanetakis and P. Cook. Musical genre classiﬁcation of audio signals. IEEE Transactions  on Speech and Audio Processing, 10(5):293–302, July 2002.  9  ","Convolutional networks are one of the most widely employed architectures incomputer vision and machine learning. In order to leverage their ability tolearn complex functions, large amounts of data are required for training.Training a large convolutional network to produce state-of-the-art results cantake weeks, even when using modern GPUs. Producing labels using a trainednetwork can also be costly when dealing with web-scale datasets. In this work,we present a simple algorithm which accelerates training and inference by asignificant factor, and can yield improvements of over an order of magnitudecompared to existing state-of-the-art implementations. This is done bycomputing convolutions as pointwise products in the Fourier domain whilereusing the same transformed feature map many times. The algorithm isimplemented on a GPU architecture and addresses a number of related challenges."
1312.6082,2014,Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks  ,"['Julian Ibarz', 'Ian Goodfellow', 'Sacha Arnoud', 'Vinay Shet', 'Yaroslav Bulatov']",https://arxiv.org/pdf/1312.6082.pdf,"4 1 0 2    r p A 4 1         ]  V C . s c [      4 v 2 8 0 6  .  2 1 3 1 : v i X r a  Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks  Ian J. Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, Vinay Shet  Street View and reCAPTCHA Teams, Google Inc.  [goodfellow,yaroslavvb,julianibarz,sacha,vinayshet]@google.com  Abstract  Recognizing arbitrary multi-character text in unconstrained natural photographs is a hard problem. In this paper, we address an equally hard sub-problem in this domain viz. recognizing arbitrary multi-digit numbers from Street View imagery. Traditional approaches to solve this problem typically separate out the localiza- tion, segmentation, and recognition steps. In this paper we propose a uniﬁed ap- proach that integrates these three steps via the use of a deep convolutional neu- ral network that operates directly on the image pixels. We employ the DistBe- lief (Dean et al., 2012) implementation of deep neural networks in order to train large, distributed neural networks on high quality images. We ﬁnd that the per- formance of this approach increases with the depth of the convolutional network, with the best performance occurring in the deepest architecture we trained, with eleven hidden layers. We evaluate this approach on the publicly available SVHN dataset and achieve over 96% accuracy in recognizing complete street numbers. We show that on a per-digit recognition task, we improve upon the state-of-the- art, achieving 97.84% accuracy. We also evaluate this approach on an even more challenging dataset generated from Street View imagery containing several tens of millions of street number annotations and achieve over 90% accuracy. To further explore the applicability of the proposed system to broader text recognition tasks, we apply it to transcribing synthetic distorted text from a popular CAPTCHA ser- vice, reCAPTCHA. reCAPTCHA is one of the most secure reverse turing tests that uses distorted text as one of the cues to distinguish humans from bots. With the proposed approach we report a 99.8% accuracy on transcribing the hardest cat- egory of reCAPTCHA puzzles. Our evaluations on both tasks, the street number recognition as well as reCAPTCHA puzzle transcription, indicate that at speciﬁc operating thresholds, the performance of the proposed system is comparable to, and in some cases exceeds, that of human operators.  1  Introduction  Recognizing multi-digit numbers in photographs captured at street level is an important compo- nent of modern-day map making. A classic example of a corpus of such street level photographs is Google’s Street View imagery comprised of hundreds of millions of geo-located 360 degree panoramic images. The ability to automatically transcribe an address number from a geo-located patch of pixels and associate the transcribed number with a known street address helps pinpoint, with a high degree of accuracy, the location of the building it represents. More broadly, recognizing numbers in photographs is a problem of interest to the optical charac- ter recognition community. While OCR on constrained domains like document processing is well studied, arbitrary multi-character text recognition in photographs is still highly challenging. This difﬁculty arises due to the wide variability in the visual appearance of text in the wild on account of a large range of fonts, colors, styles, orientations, and character arrangements. The recognition  1  problem is further complicated by environmental factors such as lighting, shadows, specularities, and occlusions as well as by image acquisition factors such as resolution, motion, and focus blurs. In this paper, we focus on recognizing multi-digit numbers from Street View panoramas. While this reduces the space of characters that need to be recognized, the complexities listed above still apply to this sub-domain. Due to these complexities, traditional approaches to solve this problem typically separate out the localization, segmentation, and recognition steps. In this paper we propose a uniﬁed approach that integrates these three steps via the use of a deep convolutional neural network that operates directly on the image pixels. This model is conﬁgured with multiple hidden layers (our best conﬁguration had eleven layers, but our experiments suggest deeper architectures may obtain better accuracy, with diminishing returns), all with feedforward connections. We employ DistBelief to implement these large-scale deep neural networks. We have evaluated this approach on the publicly available Street View House Numbers (SVHN) dataset and achieve over 96% accuracy in recognizing street numbers. We show that on a per- digit recognition task, we improve upon the state-of-the-art and achieve 97.84% accuracy. We also evaluated this approach on an even more challenging dataset generated from Street View imagery containing several tens of millions of street number annotations and achieve over 90% accuracy. Our evaluations further indicate that at speciﬁc operating thresholds, the performance of the proposed system is comparable to that of human operators. To date, our system has helped us extract close to 100 million street numbers from Street View imagery worldwide. While the challenges listed above for numbers in Street View data can be considered to be real- world, natural variabilities in text, another class of data where text is deliberatly distorted syntheti- cally is in CAPTCHA puzzles. CAPTCHAs are reverse turing tests designed to use distorted text to distinguish humans and machines running automated text recognition software. Synthetic dis- tortions on these text based puzzles is used to increase variability in the visual appearance of the text, thus increasing transcription difﬁculty. In order to evaluate the general applicability of the peo- posed approach to the broader task of recognizing arbitrary text, we applied it to the task of solving CAPTCHA puzzles from reCAPTCHA, one of the widely used CAPTCHA service on the internet. We show that we are able to achieve a 99.8% accuracy on the hardest reCAPTCHA puzzle. The key contributions of this paper are: (a) a uniﬁed model to localize, segment, and recognize multi- digit numbers from street level photographs (b) a new kind of output layer, providing a conditional probabilistic model of sequences (c) empirical results that show this model performing best with a deep architecture (d) results of applying proposed model on the harderst category of reCAPTCHA images to achieve 99.8% transcription accuracy (e) reaching human level performance at speciﬁc operating thresholds.  2 Related work  Convolutional neural networks (Fukushima, 1980; LeCun et al., 1998) are neural networks with sets of neurons having tied parameters. Like most neural networks, they contain several ﬁltering layers with each layer applying an afﬁne transformation to the vector input followed by an elementwise non-linearity. In the case of convolutional networks, the afﬁne transformation can be implemented as a discrete convolution rather than a fully general matrix multiplication. This makes convolutional networks computationally efﬁcient, allowing them to scale to large images. It also builds equivari- ance to translation into the model (in other words, if the image is shifted by one pixel to the right, then the output of the convolution is also shifted one pixel to the right; the two representations vary equally with translation). Image-based convolutional networks typically use a pooling layer which summarizes the activations of many adjacent ﬁlters with a single response. Such pooling layers may summarize the activations of groups of units with a function such as their maximum, mean, or L2 norm. These pooling layers help the network be robust to small translations of the input. Increases in the availability of computational resources, increases in the size of available training sets, and algorithmic advances such as the use of piecewise linear units (Jarrett et al., 2009; Glorot et al., 2011; Goodfellow et al., 2013) and dropout training (Hinton et al., 2012) have resulted in many recent successes using deep convolutional neural networks. Krizhevsky et al. (2012) obtained dramatic improvements in the state of the art in object recognition. Zeiler and Fergus (2013) later improved upon these results.  2  a)  b)  Figure 1: a) An example input image to be transcribed. The correct output for this image is “700”. b) The graphical model structure of our sequence transcription model, depicted using plate nota- tion (Buntine, 1994) to represent the multiple Si. Note that the relationship between X and H is deterministic. The edges going from L to Si are optional, but help draw attention to the fact that our deﬁnition of P (S | X) does not query Si for i > L.  On huge datasets, such as those used at Google, overﬁtting is not an issue, and increasing the size of the network increases both training and testing accuracy. To this end, Dean et al. (2012) de- veloped DistBelief, a scalable implementation of deep neural networks, which includes support for convolutional networks. We use this infrastructure as the basis for the experiments in this paper. Convolutional neural networks have previously been used mostly for applications such as recog- nition of single objects in the input image. In some cases they have been used as components of systems that solve more complicated tasks. Girshick et al. (2013) use convolutional neural networks as feature extractors for a system that performs object detection and localization. However, the system as a whole is larger than the neural network portion trained with backprop, and has special code for handling much of the mechanics such as proposing candidate object regions. Szegedy et al. (2013) showed that a neural network could learn to output a heatmap that could be post-processed to solve the object localization problem. In our work, we take a similar approach, but with less post-processing and with the additional requirement that the output be an ordered sequence rather than an unordered list of detected objects. Alsharif and Pineau (2013) use convolutional maxout networks (Goodfellow et al., 2013) to provide many of the conditional probability distributions used in a larger model using HMMs to transcribe text from images. In this work, we propose to solve similar tasks involving localization and segmentation, but we propose to perform the entire task completely within the learned convolutional network. In our approach, there is no need for a sep- arate component of the system to propose candidate segmentations or provide a higher level model of the image.  3 Problem description  Street number transcription is a special kind of sequence recognition. Given an image, the task is to identify the number in the image. See an example in Fig. 1a. The number to be identiﬁed is a sequence of digits, s = s1, s2, . . . , sn. When determining the accuracy of a digit transcriber, we compute the proportion of the input images for which the length n of the sequence and every element si of the sequence is predicted correctly. There is no “partial credit” for getting individual digits of the sequence correct. This is because for the purpose of making a map, a building can only be found on the map from its address if the whole street number was transcribed correctly. For the purpose of building a map, it is extremely important to have at least human level accuracy. Users of maps ﬁnd it very time consuming and frustrating to be led to the wrong location, so it is essential to minimize the amount of incorrect transcriptions entered into the map. It is, however, acceptable not to transcribe every input image. Because each street number may have been pho- tographed many times, it is still quite likely that the proportion of buildings we can place on the map is greater than the proportion of images we can transcribe. We therefore advocate evaluating this task based on the coverage at certain levels of accuracy, rather than evaluating only the total degree of accuracy of the system. To evaluate coverage, the system must return a conﬁdence value, such as the probability of the most likely prediction being correct. Transcriptions below some conﬁdence  3  XHLSiNthreshold can then be discarded. The coverage is deﬁned to be the proportion of inputs that are not discarded. The coverage at a certain speciﬁc accuracy level is the coverage that results when the conﬁdence threshold is chosen to achieve that desired accuracy level. For map-making purposes, we are primarily interested in coverage at 98% accuracy or better, since this roughly corresponds to human accuracy. Using conﬁdence thresholding allows us to improve maps incrementally over time–if we develop a system with poor accuracy overall but good accuracy at some threshold, we can make a map with partial coverage, then improve the coverage when we get a more accurate transcription system in the future. We can also use conﬁdence thresholding to do as much of the work as possible via the automated system and do the rest using more expensive means such as hiring human operators to transcribe the remaining difﬁcult inputs. One special property of the street number transcription problem is that the sequences are of bounded length. Very few street numbers contain more than ﬁve digits, so we can use models that assume the sequence length n is at most some constant N, with N = 5 for this work. Systems that make such an assumption should be able to identify whenever this assumption is violated and refuse to return a transcription so that the few street numbers of length greater than N are not incorrectly added to the map after being transcribed as being length N. (Alternately, one can return the most likely sequence of length N, and because the probability of that transcription being correct is low, the default conﬁdence thresholding mechanism will usually reject such transcriptions without needing special code for handling the excess length case)  4 Methods  Our basic approach is to train a probabilistic model of sequences given images. Let S represent the output sequence and X represent the input image. Our goal is then to learn a model of P (S | X) by maximizing log P (S | X) on the training set. To model S, we deﬁne S as a collection of N random variables S1, . . . , SN representing the ele- ments of the sequence and an additional random variable L representing the length of the sequence. We assume that the identities of the separate digits are independent from each other, so that the probability of a speciﬁc sequence s = s1, . . . , sn is given by  P (S = s|X) = P (L = n | X)Πn  i=1P (Si = si | X).  This model can be extended to detect when our assumption that the sequence has length at most N is violated. To allow for detecting this case, we simply add an additional value of L that represents this outcome. Each of the variables above is discrete, and when applied to the street number transcription problem, each has a small number of possible values: L has only 7 values (0, . . . , 5, and “more than 5”), and each of the digit variables has 10 possible values. This means it is feasible to represent each of them with a softmax classiﬁer that receives as input features extracted from X by a convolutional neural network. We can represent these features as a random variable H whose value is deterministic given X. In this model, P (S | X) = P (S | H). See Fig. 1b for a graphical model depiction of the network structure. To train the model, one can maximize log P (S | X) on the training set using a generic method like stochastic gradient descent. Each of the softmax models (the model for L and each Si) can use exactly the same backprop learning rule as when training an isolated softmax layer, except that a digit classiﬁer softmax model backprops nothing on examples for which that digit is not present. At test time, we predict  s = (l, s1, . . . , sl) = argmaxL,S1,...,SL  log P (S | X).  This argmax can be computed in linear time. The argmax for each character can be computed independently. We then incrementally add up the log probabilities for each character. For each length l, the complete log probability is given by this running sum of character log probabilities, plus log P (l | x). The total runtime is thus O(N ). We preprocess by subtracting the mean of each image. We do not use any whitening (Hyv¨arinen et al., 2001), local contrast normalization (Sermanet et al., 2012), etc.  4  5 Experiments  In this section we present our experimental results. First, we describe our state of the art results on the public Street View House Numbers dataset in section 5.1. Next, we describe the performance of this system on our more challenging, larger but internal version of the dataset in section 5.2. We then present some experiments analyzing the performance of the system in section 5.4.  5.1 Public Street View House Numbers dataset  The Street View House Numbers (SVHN) dataset (Netzer et al., 2011) is a dataset of about 200k street numbers, along with bounding boxes for individual digits, giving about 600k digits total. To our knowledge, all previously published work cropped individual digits and tried to recognize those. We instead take original images containing multiple digits, and focus on recognizing them all simultaneously. We preprocess the dataset in the following way – ﬁrst we ﬁnd the small rectangular bounding box that will contain individual character bounding boxes. We then expand this bounding box by 30% in both the x and the y direction, crop the image to that bounding box and resize the crop to 64× 64 pixels. We then crop a 54 × 54 pixel image from a random location within the 64 × 64 pixel image. This means we generated several randomly shifted versions of each training example, in order to increase the size of the dataset. Without this data augmentation, we lose about half a percentage point of accuracy. Because of the differing number of characters in the image, this introduces considerable scale variability – for a single digit street number, the digit ﬁlls the whole box, meanwhile a 5 digit street number will have to be shrunk considerably in order to ﬁt. Our best model obtained a sequence transcription accuracy of 96.03%. This is not accurate enough to use for adding street numbers to geographic location databases for placement on maps. However, using conﬁdence thresholding we obtain 95.64% coverage at 98% accuracy. Since 98% accuracy is the performance of human operators, these transcriptions are acceptable to include in a map. We encourage researchers who work on this dataset in the future to publish coverage at 98% accuracy as well as the standard accuracy measure. Our system achieves a character-level accuracy of 97.84%. This is slightly better than the previous state of the art for a single network on the individual character task of 97.53% (Goodfellow et al., 2013). Training this model took approximately six days using 10 replicas in DistBelief. The exact training time varies for each of the performance measures reported above–we picked the best stopping point for each performance measure separately, using a validation set. Our best architecture consists of eight convolutional hidden layers, one locally connected hidden layer, and two densely connected hidden layers. All connections are feedforward and go from one layer to the next (no skip connections). The ﬁrst hidden layer contains maxout units (Goodfellow et al., 2013) (with three ﬁlters per unit) while the others contain rectiﬁer units (Jarrett et al., 2009; Glorot et al., 2011). The number of units at each spatial location in each layer is [48, 64, 128, 160] for the ﬁrst four layers and 192 for all other locally connected layers. The fully connected layers contain 3,072 units each. Each convolutional layer includes max pooling and subtractive normalization. The max pooling window size is 2× 2. The stride alternates between 2 and 1 at each layer, so that half of the layers don’t reduce the spatial size of the representation. All convolutions use zero padding on the input to preserve representation size. The subtractive normalization operates on 3x3 windows and preserves representation size. All convolution kernels were of size 5 × 5. We trained with dropout applied to all hidden layers but not the input.  5.2  Internal Street View data  Internally, we have a dataset with tens of millions of transcribed street numbers. However, on this dataset, there are no ground truth bounding boxes available. We use an automated method (beyond the scope of this paper) to estimate the centroid of each house number, then crop to a 128 × 128 pixel region surrounding the house number. We do not rescale the image because we do not know the extent of the house number. This means the network must be robust to a wider variation of scales than our public SVHN network. On this dataset, the network must also localize the house number, rather than merely localizing the digits within each house number. Also, because the training set is larger in this setting, we did not need augment the data with random translations.  5  Figure 2: Difﬁcult but correctly transcribed examples from the internal street numbers dataset. Some of the challenges in this dataset include diagonal or vertical layouts, incorrectly applied blurring from license plate detection pipelines, shadows and other occlusions.  This dataset is more difﬁcult because it comes from more countries (more than 12), has street num- bers with non-digit characters and the quality of the ground truth is lower. See Fig. 2 for some examples of difﬁcult inputs from this dataset that our system was able to transcribe correctly, and Fig. 3 for some examples of difﬁcult inputs that were considered errors. We obtained an overall sequence transcription accuracy of 91% on this more challenging dataset. Using conﬁdence thresholding, we were able to obtain a coverage of 83% with 99% accuracy, or 89% coverage at 98% accuracy. On this task, due to the larger amount of training data, we did not see signiﬁcant overﬁtting like we saw in SVHN so we did not use dropout. Dropout tends to increase training time, and our largest models are already very costly to train. We also did not use maxout units. All hidden units were rectiﬁers (Jarrett et al., 2009; Glorot et al., 2011). Our best architecture for this dataset is similar to the best architecture for the public dataset, except we use only ﬁve convolutional layers rather than eight. (We have not tried using eight convolutional layers on this dataset; eight layers may obtain slightly better results but the version of the network with ﬁve convolutional layers performed accurately enough to meet our business objectives) The locally connected layers have 128 units per spatial location, while the fully connected layers have 4096 units per layer.  5.3 CAPTCHA puzzles dataset  CAPTCHAs are reverse turing tests designed to use distorted text to distinguish humans and ma- chines running automated text recognition software. reCAPTCHA is a leading CAPTCHA provider with an installed base of several hundreds of thousands of websites. To evaluate the generality of the proposed approach to recognizing arbitrary text, we created a dataset composed of the hardest CAPTCHA puzzle examples of which are shown in Figure 4. The model we use is similar to the best one used over the SVHN dataset with the following differ- ences: we use 9 convolutional layers in this network instead of 11, with the ﬁrst layer containing normal rectiﬁer units instead of maxouts, the convolutional layers are also slightly bigger, while the fully connected ones smaller. The output of this model is case-sensitive and it can handle up to 8 character long sequences. The input is one of the two CAPTCHA words cropped to a size of 200x40 where random sub-crops of size 195x35 are taken. The performance reported was taken directly from a test set of 100K samples and a training set in the order of millions of CAPTCHA images.  6  100 vs. 676  1110 vs. 2641  23 vs. 37  1 vs. 198  4 vs. 332  2 vs 239  1879 vs. 1879-1883  228 vs. 22B  96 vs. 86  1844 vs. 184  62 vs. 62-37  1180 vs. 1780  Figure 3: Examples of incorrectly transcribed street numbers from the large internal dataset (tran- scription vs. ground truth). Note that for some of these, the “ground truth” is also incorrect. The ground truth labels in this dataset are quite noisy, as is common in real world settings. Some reasons for the ground truth errors in this dataset include: 1. The data was repurposed from an existing in- dexing pipeline where operators manually entered street numbers they saw. It was impractical to use the same size of images as the humans saw, so heuristics were used to create smaller crops. Some- times the resulting crop omits some digits. 2. Some examples are fundamentally ambiguous, for instance street numbers including non-digit characters, or having multiple street numbers in same image which humans transcribed as a single number with an arbitrary separator like “,” or “-”.  Figure 4: Examples of images from the hard CAPTCHA puzzles dataset.  7  Figure 5: Performance analysis experiments on the public SVHN dataset show that fairly deep architectures are needed to obtain good performance on the sequence transcription task.  With this model, we are able to achieve a 99.8% accuracy on transcribing the hardest reCAPTCHA puzzle. It is important to note that these results do not indicate a reduction in the anti-abuse effec- tiveness of reCAPTCHA as a whole. reCAPTCHA is designed to be a risk analysis engine taking a variety of different cues from the user to make the ﬁnal determination of human vs bot. Today distorted text in reCAPTCHA serves increasingly as a medium to capture user engagements rather than a reverse turing in and of itself. These results do however indicate that the utility of distorted text as a reverse turing test by itself is signiﬁcantly diminished.  5.4 Performance analysis  In this section we explore the reasons for the unprecedented success of our neural network architec- ture for a complicated task involving localization and segmentation rather than just recognition. We hypothesize that for such a complicated task, depth is crucial to achieve an efﬁcient representation of the task. State of the art recognition networks for images of cropped and centered digits or ob- jects may have between two to four convolutional layers followed by one or two densely connected hidden layers and the classiﬁcation layers (Goodfellow et al., 2013). In this work we used several more convolutional layers. We hypothesize that the depth was crucial to our success. This is most likely because the earlier layers can solve the localization and segmentation tasks, and prepare a representation that has already been segmented so that later layers can focus on just recognition. Moreover, we hypothesize that such deep networks have very high representational capacity, and thus need a large amount of data to train successfully. Prior to our successful demonstration of this system, it would have been reasonable to expect that factors other than just depth would be neces- sary to achieve good performance on these tasks. For example, it could have been possible that a sufﬁciently deep network would be too difﬁcult to optimize. In Fig. 5, we present the results of an experiment that conﬁrms our hypothesis that depth is necessary for good performance on this task. For control experiments showing that large shallow models cannot achieve the same performance, see Fig. 6.  5.5 Application to Geocoding  The motivation for the development of this model was to decrease the cost of geocoding as well as scale it worldwide and keep up with change in the world. The model has now reached a high enough quality level that we can automate the extraction of street numbers on Street View images. Also, even if the model can be considered quite large, it is still efﬁcient. We can for example transcribe all the views we have of street numbers in France in less than an hour using our Google infrastructure. Most of the cost actually comes from the detection stage that  8  Figure 6: Performance analysis experiments on the public SVHN dataset show that increasing the number of parameters in smaller models does not allow such models to reach the same level of performance as deep models. This is primarily due to overﬁtting.  locates the street numbers in the large Street View images. Worldwide, we automatically detected and transcribed close to 100 million physical street numbers at operator level accuracy. Having this new dataset signiﬁcantly increased the geocoding quality of Google Maps in several countries especially the ones that did not already have other sources of good geocoding. In Fig. 7, you can see some automatically extracted street numbers from Street View imagery captured in South Africa.  6 Discussion  We believe with this model we have solved OCR for short sequences for many applications. On our particular task, we believe that now the biggest gain we could easily get is to increase the quality of the training set itself as well as increasing its size for general OCR transcription. One caveat to our results with this architecture is that they rest heavily on the assumption that the sequence is of bounded length, with a reasonably small maximum length N. For unbounded N, our method is not directly applicable, and for large N our method is unlikely to scale well. Each separate digit classiﬁer requires its own separate weight matrix. For long sequences this could incur too high of a memory cost. When using DistBelief, memory is not much of an issue (just use more machines) but statistical efﬁciency is likely to become problematic. Another problem with long sequences is the cost function itself. It’s also possible that, due to longer sequences having more digit probabilities multiplied together, a model of longer sequences could have trouble with systematic underestimation of the sequence length. One possible solution could be to train a model that outputs one “word” (N character sequence) at a time and then slide it over the entire image followed by a simple decoding. Some early experiments in this direction have been promising. Perhaps our most interesting ﬁnding is that neural networks can learn to perform complicated tasks such as simultaneous localization and segmentation of ordered sequences of objects. This approach of using a single neural network as an entire end-to-end system could be applicable to other prob- lems, such as general text transcription or speech recognition.  9  Acknowledgments  We would like to thank Ilya Sutskever and Samy Bengio for helpful discussions. We would also like to thank the entire operation team in India that did the labeling effort and without whom this research would not have been possible.  References  Alsharif, O. and Pineau, J. (2013). End-to-end text recognition with hybrid hmm maxout models. Technical  report, arXiv:1310.1811.  Buntine, W. (1994). Operations for learning with graphical models. Journal of Artiﬁcial Intelligence Research,  2, 159–225.  Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Le, Q., Mao, M., Ranzato, M., Senior, A., Tucker, P.,  Yang, K., and Ng, A. Y. (2012). Large scale distributed deep networks. In NIPS’2012.  Fukushima, K. (1980). Neocognitron: A self-organizing neural network model for a mechanism of pattern  recognition unaffected by shift in position. Biological Cybernetics, 36, 193–202.  Girshick, R., Donahue, J., Darrell, T., and Malik, J. (2013). Rich feature hierarchies for accurate object detec-  tion and semantic segmentation. Technical report, arXiv:1311.2524.  Glorot, X., Bordes, A., and Bengio, Y. (2011). Deep sparse rectiﬁer neural networks.  In JMLR W&CP: Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2011).  Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. (2013). Maxout networks. In  ICML’2013.  Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinv, R. (2012). Improving neural  networks by preventing co-adaptation of feature detectors. Technical report, arXiv:1207.0580.  Hyv¨arinen, A., Karhunen, J., and Oja, E. (2001). Independent Component Analysis. Wiley-Interscience. Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2009). What is the best multi-stage architecture for object recognition? In Proc. International Conference on Computer Vision (ICCV’09), pages 2146–2153. IEEE.  Krizhevsky, A., Sutskever, I., and Hinton, G. (2012). ImageNet classiﬁcation with deep convolutional neural  networks. In NIPS’2012.  LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient based learning applied to document recog-  nition. Proc. IEEE.  Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Y. (2011). Reading digits in natural images  with unsupervised feature learning. Deep Learning and Unsupervised Feature Learning Workshop, NIPS.  Sermanet, P., Chintala, S., and LeCun, Y. (2012). Convolutional neural networks applied to house numbers  digit classiﬁcation. In International Conference on Pattern Recognition (ICPR 2012).  Szegedy, C., Toshev, A., and Erhan, D. (2013). Deep neural networks for object detection. In NIPS’2013. Zeiler, M. D. and Fergus, R. (2013). Visualizing and understanding convolutional neural networks. Technical  report, arXiv:1311.2901.  Appendix A: Example inference  In this appendix we provide a detailed example of how to run inference in a trained network to transcribe a house number. The purpose of this appendix is to remove any ambiguity from the more general description in the main text. Transcription begins by computing the distribution over the sequence S given an image X. See Fig. 8 for details of how this computation is performed. To commit to a single speciﬁc sequence transcription, we need to compute argmaxsP (S = s | H). It is easiest to do this in log scale, to avoid multiplying together many small numbers, since such multiplication can result in numerical underﬂow. i.e., in practice we actually compute argmaxs log P (S = s | H). Note that  log softmax(z) can be computed efﬁciently and with numerical stability with the formula j exp(zj). It is best to compute the log probabilities using this stable approach, rather than ﬁrst computing the probabilities and then taking their logarithm. The latter approach is unstable; it can incorrectly yield −∞ for small probabilities.  log softmax(z)i = zi −(cid:80)  10  Suppose that we have all of our output probabilities computed, and that they are the following (these are idealized example values, not actual values from the model):  P (L)  log P (L)  L = 0 .002  -6.2146  L = 1 .002  -6.2146  L = 2 .002  -6.2146  L = 3  .9  -0.10536  L = 4  .09  -2.4079  L = 5 .002  -6.2146  L > 5 .002  -6.2146  P (S1 = i)  log P (S1 = i)  P (S2 = i)  log P (S2 = i)  P (S3 = i)  log P (S3 = i)  P (S4 = i)  log P (S4 = i)  P (S5 = i)  log P (S5 = i)  i = 0 .00125 -6.6846 .00125 -6.6846 .00125 -6.6846 .08889 -2.4204  .1  i = 1  .9  -0.10536 .00125 -6.6846 .00125 -6.6846  .2  -1.6094  .1  i = 2 .00125 -6.6846 .00125 -6.6846 .00125 -6.6846 .08889 -2.4204  .1  i = 3 .00125 -6.6846 .00125 -6.6846 .00125 -6.6846 .08889 -2.4204  .1  i = 4 .00125 -6.6846 .00125 -6.6846 .00125 -6.6846 .08889 -2.4204  .1  i = 5 .00125 -6.6846 .00125 -6.6846  .9  -0.10536 .08889 -2.4204  .1  i = 6 .00125 -6.6846 .00125 -6.6846  .1  -2.4079 .08889 -2.4204  .1  i = 7  .1  -2.4079  .9  -0.10536 .00125 -6.6846 .08889 -2.4204  .1  i = 8 .00125 -6.6846 .00125 -6.6846 .00125 -6.6846 .08889 -2.4204  .1  i = 9 .00125 -6.6846  .1  -2.4079 .00125 -6.6846 .08889 -2.4204  .1  -2.3026  -2.3026  -2.3026  -2.3026  -2.3026  -2.3026  -2.3026  -2.3026  -2.3026  -2.3026  Refer to the example input image in Fig. 8 to understand these probabilities. The correct length is 3. Our distribution over L accurately reﬂects this, though we do think there is a reasonable possibility that L is 4– maybe the edge of the door looks like a fourth digit. The correct transcription is 175, and we do assign these digits the highest probability, but also assign signiﬁcant probability to the ﬁrst digit being a 7, the second being a 9, or the third being a 6. There is no fourth digit, but if we parse the edge of the door as being a digit, there is some chance of it being a 1. Our distribution over the ﬁfth digit is totally uniform since there is no ﬁfth digit. Our independence assumptions mean that when we compute the most likely sequence, the choice of which digit appears in each position doesn’t affect our choice of which digit appears in the other positions. We can thus pick the most likely digit in each position separately, leaving us with this table:  log P (Sj = sj) maxsj log P (Sj = sj)  argmaxsj  j 1 2 3 4 5  1 7 5 1 0  -0.10536 -0.10536 -0.10536 -1.6094 -2.3026  Finally, we can complete the maximization by explicitly calculating the probability of all seven possible se- quence lengths:  L 0 1 2 3 4 5 > 5  Prediction  1 17 175 1751 17510 17510. . .  log P (S1, . . . SL)  0.  -0.1054 -0.2107 -0.3161 -1.9255 -4.2281 -4.2281  log P (S) -6.2146 -7.2686 -8.3226 -0.42144 -4.3334 -10.443 -10.443  Here the third column is just a cumulative sum over log P (SL) so it can be computed in linear time. Likewise, the fourth column is just computed by adding the third column to our existing log P (L) table. It is not even necessary to keep this ﬁnal table in memory, we can just use a for loop that generates it one element at a time and remembers the maximal element. The correct transcription, 175, obtains the maximal log probability of −0.42144, and the model outputs this correct transcription.  11  Figure 7: Automatically extracted street numbers from Street View imagery captured in South Africa.  12  Figure 8: Details of the computational graph we used to transcribe house numbers. In this diagram, we show how we compute the parameters of P (S | X), where X is the input image and S is the sequence of numbers depicted by the image. We ﬁrst extract a set of features H from X using a convolutional network with a fully connected ﬁnal layer. Note that only one such feature vector is extracted for the entire image. We do not use an HMM that models features explicitly extracted at separate locations. Because the ﬁnal layer of the convolutional feature extractor is fully connected and has no weight sharing, we have not explicitly engineered any concept of spatial location into this representation. The network must learn its own means of representing spatial location in H. Six separate softmax classiﬁers are then connected to this feature vector H, i.e., each softmax classiﬁer forms a response by making an afﬁne transformation of H and normalizing this response with the softmax function. One of these classiﬁers provides the distribution over the sequence length P (L | H), while the others provide the distribution over each of the members of the sequence, P (S1 | H), . . . , P (S5 | H). .  13  X: 128x128x3  input imageDeep convolutional feature extractionH∈(cid:1)4096: Feature vector ZS1 ∈(cid:1)10 =WS1H+bS1ZL ∈(cid:1)7 =WLH+bLZS2 ∈(cid:1)10 =WS2H+bS2ZS3 ∈(cid:1)10 =WS3H+bS3ZS4 ∈(cid:1)10 =WS4H+bS4ZS5 ∈(cid:1)10 =WS5H+bS5P(L|H) =softmax(ZL)P(S1|H) =softmax(ZS1)P(S2|H) =softmax(ZS2)P(S3|H) =softmax(ZS3)P(S4|H) =softmax(ZS4)P(S5|H) =softmax(ZS5)","Recognizing arbitrary multi-character text in unconstrained naturalphotographs is a hard problem. In this paper, we address an equally hardsub-problem in this domain viz. recognizing arbitrary multi-digit numbers fromStreet View imagery. Traditional approaches to solve this problem typicallyseparate out the localization, segmentation, and recognition steps. In thispaper we propose a unified approach that integrates these three steps via theuse of a deep convolutional neural network that operates directly on the imagepixels. We employ the DistBelief implementation of deep neural networks inorder to train large, distributed neural networks on high quality images. Wefind that the performance of this approach increases with the depth of theconvolutional network, with the best performance occurring in the deepestarchitecture we trained, with eleven hidden layers. We evaluate this approachon the publicly available SVHN dataset and achieve over $96\%$ accuracy inrecognizing complete street numbers. We show that on a per-digit recognitiontask, we improve upon the state-of-the-art, achieving $97.84\%$ accuracy. Wealso evaluate this approach on an even more challenging dataset generated fromStreet View imagery containing several tens of millions of street numberannotations and achieve over $90\%$ accuracy. To further explore theapplicability of the proposed system to broader text recognition tasks, weapply it to synthetic distorted text from reCAPTCHA. reCAPTCHA is one of themost secure reverse turing tests that uses distorted text to distinguish humansfrom bots. We report a $99.8\%$ accuracy on the hardest category of reCAPTCHA.Our evaluations on both tasks indicate that at specific operating thresholds,the performance of the proposed system is comparable to, and in some casesexceeds, that of human operators."
1312.6115,2014,Neuronal Synchrony in Complex-Valued Deep Networks  ,"['David Reichert', 'Thomas Serre']",https://arxiv.org/pdf/1312.6115.pdf,"Neuronal Synchrony in Complex-Valued Deep Networks  David P. Reichert Thomas Serre Department of Cognitive, Linguistic & Psychological Sciences, Brown University  DAVID_REICHERT@BROWN.EDU THOMAS_SERRE@BROWN.EDU  Appearing in the proceedings of the 2nd International Conference on Learning Representations (ICLR2014).  4 1 0 2    r a     M 2 2      ] L M  . t a t s [      5 v 5 1 1 6  .  2 1 3 1 : v i X r a  Abstract  Deep learning has recently led to great suc- cesses in tasks such as image recognition (e.g Krizhevsky et al., 2012). However, deep net- works are still outmatched by the power and ver- satility of the brain, perhaps in part due to the richer neuronal computations available to corti- cal circuits. The challenge is to identify which neuronal mechanisms are relevant, and to ﬁnd suitable abstractions to model them. Here, we show how aspects of spike timing, long hypothe- sized to play a crucial role in cortical information processing, could be incorporated into deep net- works to build richer, versatile representations. We introduce a neural network formulation based on complex-valued neuronal units that is not only biologically meaningful but also amenable to a variety of deep learning frameworks. Here, units are attributed both a ﬁring rate and a phase, the latter indicating properties of spike timing. We show how this formulation qualitatively captures several aspects thought to be related to neuronal synchrony, including gating of information pro- cessing and dynamic binding of distributed ob- ject representations. Focusing on the latter, we demonstrate the potential of the approach in sev- eral simple experiments. Thus, neuronal syn- chrony could be a ﬂexible mechanism that fulﬁlls multiple functional roles in deep networks.  1. Introduction Deep learning approaches have proven successful in var- ious applications, from machine vision to language pro- cessing (Bengio et al., 2012). Deep networks are often taken to be inspired by the brain as idealized neural net- works that learn representations through several stages of non-linear processing, perhaps akin to how the mammalian cortex adapts to represent the sensory world. These ap- proaches are thus also relevant to computational neuro-  science (Cadieu et al., 2013): for example, convolutional networks (LeCun et al., 1989) possibly capture aspects of the organization of the visual cortex and are indeed closely related to biological models like HMAX (Serre et al., 2007), while deep Boltzmann machines (Salakhutdi- nov & Hinton, 2009) have been applied as models of gen- erative cortical processing (Reichert et al., 2013). The most impressive recent deep learning results have been achieved in classiﬁcation tasks, in a processing mode akin to rapid feed-forward recognition in humans (Serre et al., 2007), and required supervised training with large amounts of labeled data. It is perhaps less clear whether current deep networks truly support neuronal representations and processes that naturally allow for ﬂexible, rich reasoning about e.g. objects and their relations in visual scenes, and what machinery is necessary to learn such representations from data in a mostly unsupervised way. At the implemen- tational level, there is a host of cortical computations not captured by the simpliﬁed mechanisms utilized in deep net- works, from the complex laminar organization of the cor- tex to dendritic computations or neuronal spikes and their timing. Such mechanisms might be key to realizing richer representations, but the challenge is to identify which of these mechanisms are functionally relevant and which can be discarded as mere implementation details. One candidate mechanism is temporal coordination of neu- ronal output, or in particular, synchronization of neuronal ﬁring. Various theories posit that synchrony is a key ele- ment of how the cortex processes sensory information (e.g. von der Malsburg, 1981; Crick, 1984; Singer & Gray, 1995; Fries, 2005; Uhlhaas et al., 2009; Stanley, 2013), though these theories are also contested (e.g. Shadlen & Movshon, 1999; Ray & Maunsell, 2010). Because the degree of syn- chrony of neuronal spikes affects the output of downstream neurons, synchrony has been postulated to allow for gat- ing of information transmission between neurons or whole cortical areas (Fries, 2005; Benchenane et al., 2011). More- over, the relative timing of neuronal spikes may carry infor- mation about the sensory input and the dynamic network  Neuronal Synchrony in Complex-Valued Deep Networks  state (e.g. Geman, 2006; Stanley, 2013), beyond or in addi- tion to what is conveyed by ﬁring rates. In particular, neu- ronal subpopulations could dynamically form synchronous groups to bind distributed representations (Singer, 2007), to signal that perceptual content represented by each group forms a coherent entity such as a visual object in a scene. Here, we aim to demonstrate the potential functional role of neuronal synchrony in a framework that is amenable to deep learning. Rather than dealing with more realistic but elaborate spiking neuron models, we thus seek a mathe- matical idealization that naturally extends current deep net- works while still being interpretable in the context of bio- logical models. To this end, we use complex-valued units, such that each neuron’s output is described by both a ﬁring rate and a phase variable. Phase variables across neurons represent relative timing of activity. In Section 2, we brieﬂy describe the effect of synchrony on neuronal information processing. We present the frame- work based on complex-valued networks, and show what functional roles synchrony could play, within this frame- work. Thanks to the speciﬁc formulation employed, we had some success with converting deep nets trained without synchrony to incorporate synchrony. Using this approach, in Section 3 we underpin our argument with several simple experiments, focusing on binding by synchrony. Exploiting the presented approach further will require learning with synchrony. We discuss principled ways to do so and chal- lenges to overcome in Section 4. It should be noted that complex-valued neural networks are not new (e.g. Zemel et al., 1995; Kim & Adalı, 2003; Nitta, 2004; Fiori, 2005; Aizenberg & Moraga, 2007; Savitha et al., 2011; Hirose, 2011). However, they do not seem to have attracted much attention within the deep learning community—perhaps because their beneﬁts still need to be explored further.1 There are a few cases where such net- works were employed with the interpretation of neuronal synchrony, including the work of Rao et al. (2008), Rao & Cecchi (2010; 2011), which is similar to ours. These prior approaches will be discussed in Section 4.  2. Neuronal synchrony Cortical neurons communicate with electric action poten- tials (so-called spikes). There is a long-standing debate in neuroscience on whether various features of spike timing matter to neuronal information processing, rather than just average ﬁring rates (e.g. Stanley, 2013). In common deep neural networks (convolutional networks, Boltzmann ma- chines, etc.), the output of a neuronal unit is characterized by a single (real-valued) scalar; the state of a network and  1Beyond possibly applications where the data itself is natu-  rally represented in terms of complex numbers.  how it relates to an interpretation of sensory input is fully determined by the joint scalar outputs across all units. This suggests an interpretation in terms of average, static ﬁring rates, lacking any notion of relative timing. Here, we con- sider how to incorporate such notions into deep networks. Consider Figure 1a for an example of how a more dynamic code could be transmitted between neurons (simulated with the Brian simulator, Goodman & Brette, 2009). This ex- ample is based on the hypothesis that neuronal rhythms, ubiquitous throughout the brain, play a functional role in information processing (e.g. Singer & Gray, 1995; Fries, 2005; Uhlhaas et al., 2009; Benchenane et al., 2011). A neuron receives spike train inputs modulated by oscillatory ﬁring rates. This results in rhythmic output activity, with an average ﬁring rate that depends both on the amplitudes and relative phase of the inputs (Figure 1b). Such interactions are difﬁcult to represent with just static ﬁring rates.  2.1. Modeling neuronal synchrony with  complex-valued units  In deep networks, a neuronal unit receives inputs from other neurons with states vector x via synaptic weights vec- tor w. We denote the total ‘postsynaptic’ input as χ := w·x. The output is computed with an activation function f as f (χ) (or, in the case of Gibbs-sampling in Boltzmann ma- f (χ) is a conditional probability from which the chines, output state is sampled).2 We can now model aspects of spike timing by replacing the real-valued states x with com- plex states z. For unit state zi = rieφi, the magnitude ri = |zi| can be interpreted as the average ﬁring rate analogously to the real-valued case. The phase φi could correspond to the phase of a neuronal rhythm as in Figure 1a, or, more gener- ally, the timing of maximal activity in some temporal inter- val (Figure 1c). Because neuronal messages are now added in the complex plane (keeping the weights w real-valued, for now), a neuron’s total input ζ := w · z no longer de- pends only on the ﬁring rates of the input units, and the strength of the synapses, but also their relative timing. This naturally accounts for the earlier, spiking neuron example: input states that are synchronous, i.e. have similar phases, result in a stronger total input, whereas less synchronous inputs result in weaker total input (Figure 1d). A straightforward way to deﬁne a neuron’s output state zi = rieiφi from the (complex-valued) total input ζ is to apply an activation function, f : R+ (cid:55)→ R+, to the input’s magnitude |ζ| to compute the output magnitude, and to set the output phase to the phase of the total input:  φi = arg(ζ ),  ri = f (|ζ|), where ζ = w· z.  (1)  2Operations such as max-pooling require separate treatment. Also, bias parameters b can be added to the inputs to control the intrinsic excitability of the neurons. We omit them for brevity.  (a)  (c)  Neuronal Synchrony in Complex-Valued Deep Networks  (b)  (e)  (d)  Figure 1. Transmission of rhythmical activity, and corresponding model using complex-valued units. (a) A Hodgkin–Huxley model neuron receives two rhythmic spike trains as input, plus background activity. The inputs are modeled as inhomogeneous Poisson processes modulated by sinusoidal rate functions (left; shown are rates and generated spikes), with identical frequencies but differing phases. The output of the neuron is itself rhythmical (right; plotted is the membrane potential). (b) The neuron’s output rate is modulated by the phase difference between the two inputs (rate averaged over 15s runs). (c) We represent the timing of maximal activity of a neuron as the phase of a complex number, corresponding to a direction in the complex plane. The ﬁring rate is the magnitude of that complex number. Also shown is the color coding used to indicate phase throughout this paper (thus, ﬁgures should be viewed in color). (d) The outputs of the input neurons are scaled by synaptic weights (numbers next to edges) and added in the complex plane. The phase of the resulting complex input determines the phase of the output neuron. The activation function f is applied to the magnitude of the input to compute the output magnitude. Together, this models the inﬂuence of synchronous neuronal ﬁring on a postsynaptic neuron. (e) Output magnitude as function of phase difference of two inputs. With a second term added to a neuron’s input, out-of-phase excitation never cancels out completely (see main text for details; curves are for w1 = w2 > 0, |z1| = |z2|). Compare to 1b.  Again this is intuitive as a biological model, as the total strength and timing of the input determine the ‘ﬁring rate’ and timing of the output, respectively. There are, however, issues with this simple approach to modeling neuronal synchrony, which are problematic for the biological model but also, possibly, for the functional capabilities of a network. In analogy to the spiking neuron example, consider two inputs to a neuron that are excita- tory (i.e., w1,w2 > 0), and furthermore of equal magnitude, |w1z1| = |w2z2|. While it is desirable that the net total input is decreased if the two inputs are out of phase, the net input in the complex-valued formulation can actually be zero, if the difference in input phases is π, no matter how strong the individual inputs (Figure 1e, lower curve). Biologically, it seems unrealistic that strong excitatory input, even if not synchronized, would not excite a neuron.3  3Arguably, refractory periods or network motifs such as disy-  Moreover, in the above formulation, the role of inhibition (i.e., connections with w < 0) has changed: inputs with negative weights are equivalent to excitatory inputs of the opposite phase, due to −1 = eiπ. Again, this is a desir- able property that leads to desynchronization between neu- ronal groups, in line with biological models, as we will show below. However, it also means that inputs from con- nections with negative weights, on their own, can strongly drive a neuron; in that sense, there is no longer actual inhi- bition that always has a suppressive effect on neuronal out- puts. Additionally, we found that the phase shifting caused by inhibition could result in instability in networks with dominant negative weights, leading to fast switching of the phase variables.  naptic feedforward inhibition (Gabernet et al., 2005; Stanley, 2013) could indeed result in destructive interference of out-of- phase excitation.  0.5time (s)0100200rate (s-1)0.5time (s)0100200rate (s-1)0.51.0time (s)−80−40040voltage (mV)0π2π∆φ (rad)01normalized output rate0π/2π3/2π2π0π2πΔφ(rad)01norm. output magnitudeNeuronal Synchrony in Complex-Valued Deep Networks  We introduce a simple ﬁx for these issues, modifying how the output magnitude of a neuron is computed as follows:  ri = f (|ζ|) (cid:44)→ ri = f ( where ζ = w· z,  1 2  1 |ζ| + χ), 2 χ := w·|z|.  (2)  The ﬁrst term, which we refer to as synchrony term, is the same as before. The second, classic term, applies the weights to the magnitudes of the input units and thus does not depend on their phases; a network using only the clas- sic terms reduces to its real-valued counterpart (we thus reuse the variable χ, earlier denoting postsynaptic input in a real-valued network). Together, the presence of the clas- sic term implies that excitatory input always has a net ex- citatory component, even if the input neurons are out of phase such that the synchrony term is zero (thus matching the spiking neuron example,4 compare Figures 1b and 1e). Similarly, input from negative connections alone is never greater than zero. Lastly, this formulation also makes it possible to give different weightings to synchrony and clas- sic terms, thus controlling how much impact synchrony has on the network; we do not explore this possibility here.  2.2. The functional relevance of synchrony  The advantage of using complex-valued neuronal units rather than, say, spiking neuron models is that it is natural to consider how to apply deep learning techniques and ex- tend existing deep learning neural networks in this frame- work. Indeed, our experiments presented later are based on pretraining standard, real-valued nets (deep Boltzmann ma- chines in this case) and converting them to complex-valued nets after training. In this section, we brieﬂy describe how our framework lends itself to realize two functional roles of synchrony as postulated by biological theories.  GROUPING (BINDING) BY SYNCHRONY  The activation of a real-valued unit in an artiﬁcial neural network can often be understood as signaling the presence of a feature or combination of features in the data. The phase of a complex-valued unit could provide additional information about the feature. Binding by synchrony theo- ries (Singer, 2007) postulate that neurons in the brain dy- namically form synchronous assemblies to signal where distributed representations together correspond to coherent sensory entities. For example, different objects in a visual scene would correspond to different synchronous assem- blies in visual cortex. In our formulation, phases can anal- ogously signal a soft assignment to different assemblies. Importantly, communication with complex-valued mes-  4Real neuronal networks and realistic simulations have many degrees of freedom, hence we make no claim that our formulation is a general or quantitative model of neuronal interactions.  Figure 2. Gating of interactions. Out-of-phase input, when com- bined with a stronger input, is weakened. In this example, with ∆φ = π and as long as |w1 · z1| > |w2z2|, effective input from the neuron to the right is zero, for any input strength (classic and synchrony terms contributions cancel, bottom panel). Hence, neu- ronal groups with different phases (gradually) decouple.  sages also naturally leads to different synchronous groups emerging: for excitatory connections, messages that ‘agree’ (Zemel et al., 1995) in their phases prevail over those that do not; inhibitory messages, on the other hand, equate to excitatory messages of opposite phases, and thus encourage desynchronization between neurons. For com- parison, consider the more realistic spiking model of visual cortex of Miconi & VanRullen (2010), where synchronous groups arise from a similar interaction between excitation and inhibition. That these interactions can indeed lead to meaningful groupings of neuronal representations in deep networks will be shown empirically in Section 3.  DYNAMIC GATING OF INTERACTIONS AND INFORMATION FLOW  Because synchrony affects which neuronal messages are transmitted preferentially, it has also been postulated that synchrony may gate information ﬂow dynamically depend- ing on the sensory input, the current network state and top-down control (Stanley, 2013), as well as to modulate the effective interactions between cortical areas depend- ing on their level of coherence (Fries, 2005; Benchenane et al., 2011). A similar modulation of interactions can be reproduced in our framework. Let us consider an exam- ple scenario (Figure 2) where a neuron is a member of a synchronous assembly, receiving excitatory inputs w1 · z1 from neurons that all have similar phases. Now consider the effect of adding another neuron that also provides ex- citatory input, w2z2, but of a different phase, and assume that |w1·z1| < |w2z2| (i.e. the input of the ﬁrst group domi- nates). The net effect the latter additional input has depends again on the phase difference. In particular, if the phase difference is maximal (π), the net contribution from the second neuron turns out to be zero. The output magnitude is computed as in Eq. 2, taking both synchrony and classic terms into account. In the complex plane, w2z2 is antiparal-  Neuronal Synchrony in Complex-Valued Deep Networks  lel to w1 · z1, thus the synchrony term is reduced by |w2z2|. However, this reduction is exactly canceled out by the clas- sic term contribution from the second input (Figure 2 lower panel). There is also no effect on the output phase as the phase of the total input remains equal to the phase of w1·z1. Analogous reasoning applies for inhibitory connections. Thus, the effective connectivity between neuronal units is modulated by the units’ phases, which themselves are a re- sult of network interactions. In particular, if inference re- sults in neurons being segregated into different assemblies (ideally because they represent independent causes in the sensory input, or independent regions in an image), exis- tent connections between groups are weakened.  3. Experiments: the case of binding by  synchrony  In this section, we support our reasoning with several sim- ple experiments, and further elucidate on the possible roles of synchrony. We focus on the binding aspect. All experiments were based on pretraining networks as normal, real-valued deep Boltzmann machines (DBMs, Salakhutdinov & Hinton, 2009). DBMs are multi-layer net- works that are framed as probabilistic (undirected graphi- cal) models. The visible units make up the ﬁrst layer and are set according to data, e.g. images. Several hidden layers learn internal representations of the data, from which they can generate the latter by sampling the visible units. By deﬁnition, in a DBM there are only (symmetric) connec- tions between adjacent layers and no connections within a layer. Given the inputs from adjacent layers, a unit’s state is updated stochastically with a probability given by a sigmoid (logistic) activation function (implementing Gibbs sampling). Training was carried out layer-wise with stan- dard methods including contrastive divergence (for model and training details, see Appendix B). Training and exper- iments were implemented within the Pylearn2 framework of Goodfellow et al. (2013b). We emphasize however that our framework is not spe- ciﬁc to DBMs, but can in principle be adapted to various deep learning approaches (we are currently experimenting with networks trained as autoencoders or convolutional net- works). The learning and inference procedures of a DBM derive from its deﬁnition as a probabilistic model, but for our purpose here it is more appropriate to simply think of a DBM as a multi-layer recurrent neural network (cf. Good- fellow et al., 2013a) with logistic activation function;5 we can demonstrate how our framework works by taking the pretrained network, introducing complex-valued unit states and applying the activation function to magnitudes as de-  scribed in Section 2.1.6 However, developing a principled probabilistic model based on Boltzmann machines to use with our framework is possible as well (Section 4). This conversion procedure applied to real-valued networks offers a simple method of exploring aspects of synchrony, but there is no guarantee that it will work (for additional discussion, see Appendix B). We use it here to show what the functional roles of synchrony could be in principle; learning with synchrony will be required to move beyond simple experiments (Section 4). Throughout the experiments, we clamped the magnitudes of the visible units according to (binary) input images, which were not seen during training, and let the network infer the hidden states over multiple iterations. The phases of the visible layer were initialized randomly and then de- termined by the input from the hidden layer above. Hence, any synchronization observed was spontaneous.  3.1. Dynamic binding of independent components in  distributed representations  In this ﬁrst experiment, we trained a DBM with one hidden layer (a restricted Boltzmann machine, Smolensky, 1986) on a version of the classic ‘bars problem’ (Földiák, 1990), where binary images are created by randomly drawing hor- izontal and vertical bars (Figure 3a). This dataset has clas- sically been used to test whether unsupervised learning al- gorithms can ﬁnd the independent components that consti- tute the image, by learning to represent the individual bars (though simple, the bars problem is still occasionally em- ployed, e.g. Lücke & Sahani, 2008; Spratling, 2011). We chose this dataset speciﬁcally to elucidate on the role of synchrony in the context of distributed representations. We hard-coded the receptive ﬁeld sizes (regions with non- zero weights to the input) of the hidden units to be restricted to regions smaller than the entire image (but together tiling the whole image). By necessity, this implies that any in- dividual unit can never fully represent a full-length bar, in the sense that the the unit’s weights correspond to the bar, or that one can read out the presence of the full bar from this unit’s state alone. However, this does not imply that the full network cannot learn that the images are constituted by bars (as long as receptive ﬁelds overlap). For example, we found that when sampling from the model (activating hidden and visible units freely), the resulting images con- tained full-length bars most of the time (see supplementary ﬁgure S1a and supplementary videos, Appendix A); simi- larly, the network would ﬁll in the remainder of a bar when the visible units where clamped to a part of it. After conversion to a complex-valued network, the model  5The activation function is stochastic in the case of Gibbs sam-  pling, deterministic in the case of mean-ﬁeld inference.  6Our results were qualitatively similar whether we computed  the output magnitudes stochastically or deterministically.  (a)  (b)  (c)  Neuronal Synchrony in Complex-Valued Deep Networks  Figure 3. Binding by synchrony in shallow, distributed representations. (a) Each image of our version of the bars problem contained 6 vertical and 6 horizontal bars at random positions. (b) A restricted Boltzmann machine was trained on bars images and then converted to a complex-valued network. The magnitudes of the visible units were clamped according to the input image (bottom left), whereas the hidden units and phases of the visible units were activated freely. After 100 iterations, units representing the various bars were found to have synchronized (right; the phases are color-coded for units that are active; black means a unit is off). The neurons synchronized even though receptive ﬁelds of the hidden units were constrained to be smaller than the bars. Thus, binding by synchrony could make the ‘independent components’ of sensory data explicit in distributed representation, in particular when no single neuron can possibly represent a component (a full-length bar) on its own. (c) Histogram of the unit phases in the visible layer for the example shown in b.  was run on input images for 100 iterations each. Results are plotted in Figure 3b, depicting both visible and hid- den states for one input image (Further examples in Figure S1b). We found that visible neurons along a bar would often synchronize to the same phase (except where bars crossed), whereas different bars tended to have different phase values. Figure 3c shows a histogram of phase values in the visible layer for this example image, with clear peaks corresponding to the phases of the bars. Such synchroniza- tion was also found in the hidden layer units (3b). Based on these results, we make three points. First, the re- sults show that our formulation indeed allows for neurons to dynamically organize into meaningful synchronous as- semblies, even without supervision towards what neurons should synchronize to, e.g. by providing phase targets in training—here, synchrony was not used in training at all. That the conversion from a real-valued network can work suggests that an unsupervised or semi-supervised approach to learning with synchrony could be successful as long as synchrony beneﬁts the task at hand. Second, synchronization of visible and hidden units, which together represent individual bars, can occur for neurons several synapses apart. At the same time, not all bars syn- chronized to different phases. The number of distinct, sta- ble phase groups that can be formed is likely to be limited. Notably, it has been argued that this aspect of synchrony coding explains certain capacity limits in cognition (Jensen & Lisman, 2005; Fell & Axmacher, 2011). The third point relates to the nature of distributed repre- sentations. For the bars problem, whether a neural net (or probabilistic model) discovers the bars is usually evaluated by examining whether individual units correspond to indi- vidual bars, as can be seen by inspecting the weights or by  probing the response properties of individual neurons (e.g. Lücke & Sahani, 2008; Spratling, 2011). A similar ‘local- ist’ approach was taken in recent attempts to make sense of the somewhat opaque hidden representations learned by deep networks (as in the example of the neurons that dis- covered the ‘concept’ of a cat from unsupervised learning Le et al., 2011, or the work of Zeiler & Fergus, 2013 on analyzing convolutional networks). In our experiment, it is not possible to map individual neurons to the image con- stituents, by construction; bars could only be represented in a distributed fashion. Synchrony could make explicit which neurons together represent a sensory entity, e.g. for a read- out (more on that below), as well as offer a mechanism that establishes the grouping in the ﬁrst place.  3.2. Binding in deep networks  To examine the effects of synchrony in deeper networks, we trained a DBM with three hidden layers on another dataset, consisting of binary images that contained both four ‘cor- ners’ arranged in a square shape (centered at random po- sitions) and four corners independently drawn (Figure 4a). Receptive ﬁeld sizes in the ﬁrst hidden layer were chosen such that the ﬁelds would only cover individual corners, not the whole square arrangements, making it impossible for the ﬁrst hidden layer to discover the latter during training.7 Receptive ﬁeld sizes were larger in higher layers, with the topmost hidden layer being fully connected. After converting the net to complex values, we found that the four corners arranged as a square would often synchro- nize to one phase, whereas the other, independent corners  7Note that there was only layer-wise pretraining, no training of the full DBM, thus ﬁrst layer representations were not inﬂuenced by higher layers during training either.  inputNeuronal Synchrony in Complex-Valued Deep Networks  (a)  (b)  Figure 4. Binding by synchrony in a deep network. (a) Each image contained four corners arranged in a square shape, and four randomly positioned corners. (b) The four corners arranged in a square were usually found to synchronize. The synchronization of the corresponding hidden units is also clearly visible in the hidden layers. The receptive ﬁeld sizes in the ﬁrst hidden layer were too small for a hidden unit to ‘see’ more than individual corners. Hence, the synchronization of the neurons representing the square in the ﬁst hidden and visible layers was due to feedback from higher layers (the topmost hidden layer had global connectivity).  would assume one or multiple phases different from the phase of the square (Figure 4b; more examples Figure S1c). Synchronization was also clear in the hidden layers. Again we make several observations. First, because of the restricted receptive ﬁelds, the synchronization of the units representing parts of the square in the visible layer and ﬁrst hidden layer was necessarily due to top-down input from the higher layers. Whether or not a corner represented by a ﬁrst layer neuron was part of a larger whole was made ex- plicit in the synchronous state. Second, this example also demonstrates that neurons need not synchronize through connected image regions as was the case in the bars ex- periment. Lastly, note that, with or without synchrony, restricted receptive ﬁelds and topographic arrangement of hidden units in intermediate hidden layers make it possible to roughly identify which units participate in representing the same image content, by virtue of their position in the layers. This is no longer possible with the topmost, glob- ally connected layer. By identifying hidden units in the topmost layer with visible units of similar phase, however, it becomes possible to establish a connection between the hidden units and what they are activated by in image space.  3.3. Reading out object representations via phase  With a ﬁnal set of experiments, we demonstrate that indi- vidual synchrony assemblies can be selected on the basis of their phase, and their representational content be accessed  one group at a time. We trained on two additional datasets containing multiple simple objects: one with images of ge- ometric toy shapes (triangles or squares, Figure 5a), with three randomly chosen instances per image, and a dataset where we combined handwritten digits from the commonly used MNIST dataset with the geometric shapes (Figure 5c). As before, we found a tendency in the complex-valued net- work to synchronize individual objects in the image to dis- tinct phases (Figure 5b, d, Figure S1d, e). After a network was run for a ﬁxed number of steps, for each layer, units were clustered according to their activity vectors in the complex plane. For clustering we assumed for simplicity that the number of objects was known in ad- vance and used k-means, with k, the number of clusters, being set to the number of objects plus one for a general background. In this fashion, each neuron was assigned to a cluster, and the assignments could be used to deﬁne masks to read out one representational component at a time.8 For the visible layer, we thus obtained segmented images as shown in Figures 5b, d. Especially for the modiﬁed MNIST images, the segmentations are often noisy. However, it is noteworthy that segmentations can be obtained at all, given that the network training involved no notion of segmenta- tion. Moreover, binding by synchrony is more general than segmentation (in the sense of assigning labels to pixels), as it applies to all neurons in the network and, in principle, to arbitrarily abstract and non-visual forms of representation. Thus, units can also be selected in the hidden layers accord- ing to phase. The phase-masked representations could, for instance, be used for classiﬁcation, one object at a time. We can also decode what these representations corresponded to in image space. To this end, we took the masked states for each cluster (treating the other states as being zero9) and used a simple decoding procedure as described by Re- ichert et al. (2010); Reichert (2012), performing a single deterministic top-down pass in the network (with doubled weights) to obtain a reconstructed image. See Figure 6 for an example. Though the images decoded in this fashion are somewhat noisy, it is apparent that the higher layer units do indeed represent the same individual objects as the visi- ble layer units that have assumed the same phase (in cases where objects are separated well). Earlier, we discussed gating by synchrony as it arises from the effect that synchrony has directly on network interac-  8Alternatively, peaks could be selected in the phase histogram of a layer and units masked according to distance to the peaks, allowing for overlapping clusters.  9This only works in a network where units signal the presence of image content by being on and not by being off, so that setting other units to zero has the effect of removing image content. This can be achieved with inductive biases such as sparsity being ap- plied during training, see the discussion of Reichert et al. (2011).  inputNeuronal Synchrony in Complex-Valued Deep Networks  (a)  (b)  (c)  (d)  Figure 5. Simple segmentation from phases. (a) The 3-shapes set consisted of binary images each containing three simple geometric shapes (square, triangle, rotated triangle). (b) Visible states after synchronization (left), and segmented images (right). (c) For each image in the MNIST+shape dataset, a MNIST digit and a shape were drawn each with probability 0.8. (d) Analogous to (b).  Figure 6. Using phase to access and decode inter- nal object representations. By selecting subsets of neurons according to their phase (e.g. through clustering), representations of each object can be read out one by one (right-hand side). For the hid- den layers, plotted are images decoded from each of the synchronous sub-populations, using a simple decoding procedure (see main text).  tions. Selecting explicitly individual synchrony assemblies for further processing, as done here, is another potential form of gating by synchrony. In the brain, some cortical regions, such as in prefrontal cortex, are highly intercon- nected with the rest of the cortex and implement functions such as executive control and working memory that de- mand ﬂexible usage of capacity-limited resources accord- ing to context and task-demands. Coherence of cortical ac- tivity and synchrony have been suggested to possibly play a causal role in establishing dynamic routing between these areas (e.g. Benchenane et al., 2011; Miller & Buschman, 2013). Similarly, attentional processing has been hypoth- esized to emerge from a dynamically changing, globally coherent state across the cortex (e.g. Duncan et al., 1997; Miller & Buschman, 2013). It is possible that there are dedicated structures in the brain, such as the pulvinar in the thalamus, that coordinate cross-cortical processing and cortical rhythms (e.g. Shipp, 2003; Saalmann et al., 2012). In our model, one could interpret selecting synchrony as- semblies as prefrontal areas reading out subsets of neuronal populations as demanded by the task. Through binding  by synchrony, such subsets could be deﬁned dynamically across many different cortical areas (or at least several lay- ers in a feature hierarchy, in our model).  4. Discussion We argue that extending neural networks beyond real- valued units could allow for richer representations of sen- sory input. Such an extension could be motivated by the fact that the brain supports such richer coding, at least in principle. More speciﬁcally, we explored the notion of neu- ronal synchrony in deep networks. We motivated the hypo- thetical functional roles of synchrony from biological the- ories, introduced a formulation based on complex-valued units, showed how the formulation related to the biological phenomenon, and examined its potential in simple experi- ments. Neuronal synchrony could be a versatile mechanism that supports various functions, from gating or modulating neuronal interactions to establishing and signaling seman- tic grouping of neuronal representations. In the latter case, synchrony realizes a form of soft constraint on neuronal  input0π/2π3/2π2πdecodephasemaskNeuronal Synchrony in Complex-Valued Deep Networks  representations, imposing that the sensory world should be organized according to distinct perceptual entities such as objects. Unfortunately, this melding of various functional roles might make it more difﬁcult to treat the synchrony mechanism in principled theoretical terms. The formulation we introduced is in part motivated by it being interpretable in a biological model. It can be un- derstood as a description of neurons that ﬁre rhythmically, and/or in relation to a global network rhythm. Other as- pects of spike timing could be functionally relevant,10 but the complex-valued formulation can be seen as a step be- yond current, ‘static ﬁring rate’ neural networks. The for- mulation also has the advantage of making it possible to explore synchrony in converted pretrained real-valued net- works (without the addition of the classic term in Eq. 2, the qualitative change of excitation and inhibition is detri- mental to this approach). However, for the machine learn- ing application, various alternative formulations would be worthy of exploration (different weights in synchrony and classic terms, complex-valued weights, etc.). We presented our simulation results in terms of representa- tive examples. We did not provide a quantitative analysis, simply because we do not claim that the current, simple ap- proach would compete with, for example, a dedicated seg- mentation algorithm, at this point. In particular, we found that the conversion from arbitrary real-valued nets did not consistently lead to favorable results (we provide additional comments in Appendix B). Our aim with this paper is to demonstrate the synchrony concept, how it could be im- plemented and what functions it could fulﬁll, in principle, to the deep learning community. To ﬁnd out whether syn- chrony is useful in real applications, it is necessary to de- velop appropriate learning algorithms. We address learning in the context of related work in the following. We are aware of a few examples of prior work employ- ing complex-valued neural networks with the interpreta- tion of neuronal synchrony.11 Zemel et al. (1995) intro- duced the ‘directional unit Boltzmann machine’ (DUBM), an extension of Boltzmann machines to complex weights and states (on the unit circle). A related approach is used by Mozer et al. (1992) to model binding by synchrony in vision, performing phase-based segmentation of simple ge- ometric contours, and by Behrmann et al. (1998) to model aspects of object-based attention. The DUBM is a prin- cipled probabilistic framework, within which a complex- valued extension of Hebbian learning can be derived, with  10Consider for instance the tempotron neuron model (Gütig &  Sompolinsky, 2006), which learns to recognize spike patterns.  11Of interest is also the work of Cadieu & Olshausen (2011), who use a complex-valued formulation to separate out motion and form in a generative model of natural movies. They make no con- nection to neuronal synchrony however.  potentially interesting functional implications and biolog- ical interpretations in terms of Spike Timing Dependant Plasticity (Sjöström & Gerstner, 2010). For our purposes, the DUBM energy function could be extended to include synchrony and classic terms (Eq. 2), if desired, and to al- low the units to switch off (rather than being constrained to the unit circle), e.g. with a ‘spike and slab’ formulation (Courville et al., 2011; Kivinen & Williams, 2011). Per- forming mean-ﬁeld inference in the resulting model should be qualitatively similar to running the networks used in our work here. The original DUBM applications were limited to simple data, shallow architectures, and supervised training (input phases were provided). It would be worthwhile to reex- amine the approach in the context of recent deep learn- ing developments; however, training Boltzmann machines successfully is not straightforward, and it is not clear whether approximate training methods such as contrastive divergence (Hinton, 2002; 2010) can be translated to the complex-valued case with success. Weber & Wermter (2005) brieﬂy describe a complex- valued neural network for image segmentation, modeling synchrony mediated by lateral interactions in primary vi- sual cortex, though the results and analysis presented are perhaps too limited to conclude much about their approach. A model of binding by synchrony in a multi-layer network is proposed by Rao et al. (2008) and Rao & Cecchi (2010; 2011). There, both neuronal dynamics and weight learn- ing are derived from optimizing an objective function (as in sparse coding, Olshausen & Field, 1997). The resulting formulation is actually similar to ours in several respects, as is the underlying motivation and analysis. We became aware of this work after having developed our approach. Our work is complementary in several regards: our goal is to provide a broader perspective on how synchrony could be used in neural networks, rather than proposing one par- ticular model; we performed a different set of experiments and conceptual analyses (for example, Rao and colleagues do not address the gating aspect of synchrony); Rao et al.’s approach relied on the model seeing only individual objects during training, which we showed to be unnecessary; and lastly, even though they applied synchrony during learn- ing, the dataset they used for their experiments is, arguably, even simpler than our datasets. Thus, it remains to be tested whether their particular model formulation is ideal. Finally, we are currently also exploring training synchrony networks with backpropagation. Even a feed-forward net- work could potentially beneﬁt from synchrony as the latter could carry information about sensory input and network state (Geman, 2006), though complex-valued weights may be necessary for detecting synchrony patterns. Alterna- tively, to allow for dynamic binding by synchrony, a net-  Neuronal Synchrony in Complex-Valued Deep Networks  work could be trained as recurrent network with backprop- agation through time (Rumelhart et al., 1985), given appro- priate input data and cost functions. In our experiments, the number of iterations required was in the order of tens or hundreds, thus making such training challenging. Again, complex-valued weights could be beneﬁcial in establishing synchrony assemblies more rapidly. Note: ICLR has an open review format and allows for pa- pers to be updated. We address some issues raised by the reviewers in Appendix C.  Acknowledgements  We thank Nicolas Heess, Christopher K.I. Williams, Michael J. Frank, David A. Mély, and Ian J. Goodfellow for helpful feedback on earlier versions of this work. We would also like to thank Elie Bienenstock and Stuart Ge- man for insightful discussions which motived this project. This work was supported by ONR (N000141110743) and NSF early career award (IIS-1252951). DPR was sup- ported by a fellowship within the Postdoc-Programme of the German Academic Exchange Service (DAAD). Addi- tional support was provided by the Robert J. and Nancy D. Carney Fund for Scientiﬁc Innovation, the Brown Institute for Brain Sciences (BIBS), the Center for Vision Research (CVR) and the Center for Computation and Visualization (CCV).  Appendix A. Supplementary ﬁgures and videos Additional outcome examples from the various experi- ments are shown in Figure S1 (as referenced in main text). We also provide the following supplementary videos on the arXiv (http://arxiv.org/abs/1312.6115): a sample being generated from a model trained on the bars problem (bars_sample_movie.mp4), an example of the synchro- nization process in the visible and hidden layers on a bars image (bars_synch_movie.mp4), and several examples of visible layer synchronization for the 3-shapes and MNIST+shape datasets (3shapes_synch_movie.mp4 and MNIST_1_shape_synch_movie.mp4, respectively).  B. Model and simulation parameters Training was implemented within the Pylearn2 framework of Goodfellow et al. (2013b). All networks were trained as real-valued deep Boltzmann machines, using layer-wise training. Layers were trained with 60 epochs of 1-step con- trastive divergence (Hinton, 2002; learning rate 0.1, mo- mentum 0.5, weight decay 10−4; see Hinton, 2010, for ex- planation of these training aspects), with the exception of the model trained on MNIST+shape, where 5-step persis-  tent contrastive divergence (Tieleman, 2008) was used in- stead (learning rate 0.005, with exponential decay factor of 1 + 1.5 × 10−5). All datasets had 60,000 training im- ages, and were divided into mini-batches of size 100. Bi- ases were initialized to -4 to encourage sparse representa- tions (for reasons discussed by Reichert et al., 2011). Initial weights were drawn randomly from a uniform distribution with support [−0.05,0.05]. The number of hidden layers, number of hidden units, and sizes of the receptive ﬁelds were varied from experiment to experiment to demonstrate various properties of neuronal synchronization in the networks (after conversion to com- plex values). The speciﬁc numbers were chosen mostly to be in line with earlier work and not of importance. In detail, model architectures were as follows: for the bars problem (Section 3.1), input images were 20×20, and the restricted Boltzmann machine had one hidden layer with 14× 14× 3 units (14 height, 14 width, 3 units per location), and 7× 7 receptive ﬁelds. For the corners dataset (Section 3.2), input images were 28× 28, three hidden layers had 22× 22× 2, 13×13×4, and 676 units, respectively, and receptive ﬁelds were 7 × 7, 10 × 10, and 13 × 13 (i.e., global in the last layer). For the 3-shapes dataset (Section 3.3), input im- ages were 20 × 20, hidden layer dimensions 14 × 14 × 3, 8× 8× 10, and 676, and receptive ﬁelds 7× 7, 7× 7, and 8 × 8 (global). For the MNIST+shape data (also Section 3.3), input images were 28× 28, hidden layer dimensions 22×22×2, 13×13×4, and 676, and receptive ﬁelds 7×7, 10× 10, and 13× 13 (global). For the synchronization ﬁgures, the number of steps to run was chosen so that synchronization was fairly stable at that point (100 steps was generally found to be sufﬁcient for all models but the one trained on MNIST+shape images, where we chose 1000 steps). Lastly, as mentioned in the main text, we note that the con- version of pretrained real-valued DBMs did not always lead to models exhibiting successful synchronization. Here, successful refers to the ability of the model to separately synchronize different objects in the input images. Unsuc- cessful setups resulted in either all visible units synchro- nizing to a single phase, or objects not synchronizing fully, across most of the images in a dataset. We found that whether or not a setup worked depended both on the dataset and the training procedures used. The presented results are representative of well performing networks. Proper synchronization is an outcome of the right balance of excitatory and inhibitory connectivity patterns. Further analysis of how network parameters affect synchronization is the subject of ongoing work, as is incorporating synchro- nization during learning to achieve desired synchronization behavior.  Neuronal Synchrony in Complex-Valued Deep Networks  (a)  (b)  (c)  (d)  (e)  Supplementary ﬁgure S1. Additional results. (a) Samples generated from a restricted Boltzmann machine trained on the bars problem. The generated images consist mostly of full-length bars. The individual receptive ﬁelds in the hidden layer were constrained to image regions of smaller extent than the bars. Thus, bars were necessarily represented in a distributed fashion. (b) - (e) Additional examples of synchronized visible units for the various datasets. The magnitudes of the visible units were set according to the binary input images (not used in training), the phases were determined by input from the hidden units. See also supplementary videos (http: //arxiv.org/abs/1312.6115), and the main text for details.  C. Addressing issues raised by the reviewers In the following, we summarize parts of the discussion of the ICLR review period, paraphrasing the comments of the ICLR reviewers. We expand several points that were only brieﬂy covered in the main text. 1. In the bars experiment, some bars appear to share the same phase. Wouldn’t a readout be confused and judge multiple bars to be the same object?  This is a very important issue that we are still considering. It is perhaps an issue more generally with the underlying biological theories rather than just our speciﬁc approach. As we noted in the main text, some theories pose that a limit on how many discrete objects can be represented in an oscillation cycle, without interference, explains certain ca- pacity limits in cognition. The references we cited (Jensen & Lisman, 2005; Fell & Axmacher, 2011) refer to working memory as an example (often 4-7 items; note the number of peaks in Figure 3c—obviously this needs more quanti- tative analysis). We would posit that, more generally, anal- ysis of visual scenes requiring the concurrent separation of multiple objects is limited accordingly (one might call  this a prediction—or a ‘postdiction’?—of our model). The question is then, how does the brain cope with this limi- tation? As usual in the face of perceptual capacity limits, the solution likely would involve attentional mechanisms. Such mechanisms might dynamically change the grouping of sensory inputs depending on task and context, such as whether questions are asked about individual parts and ﬁne detail, or object groups and larger patterns. In the bars ex- ample, one might perceive the bars as a single group or tex- ture, or focus on individual bars as capacity allows, perhaps relegating the rest of the image to a general background. Dynamically changing phase assignments according to context, in principle, be possible within the proposed framework: this is similar to grouping according to parts or wholes with top-down input, as in the experiment of Section 3.2. 2.What about the overlaps of the bars? These areas seem to be mis- or ambiguously labeled.  through top-down attentional input, should,  This is more of a problem with the task itself being ill- deﬁned on binary images, where an overlapping pixel can- not really be meaningfully said to belong to either object  Neuronal Synchrony in Complex-Valued Deep Networks  alone (as there is no occlusion as such). We plan to use (representations of) real-valued images in the future. 3. What are the contributions of this paper compared to the work of Rao et al.?  model of lateral interactions in V1. A more rigorous math- ematical and quantitative analysis is needed in any case. 5. How does running the complex-valued network relate to inference in the DBM?  As we have acknowledged, the work of Rao et al. is similar in several points (we arrived at our framework and results independently). We make additional contributions. First of all, to clarify the issue of training on multiple objects: in Rao et al.’s work, the training data consisted of a small number of ﬁxed 8×8 pixel images (16 or less images in to- tal for a dataset), containing simple patterns (one example has 4 small images with two faces instead). To demon- strate binding by synchrony, two of these patterns are su- perimposed during test time. We believe that going beyond this extremely constrained task, in particular showing that the binding can work when trained and tested on multiple objects, on multiple datasets including MNIST containing thousands of (if simple) images, is a valid contribution from our side. Our results also provide some insights into the nature of representations in a DBM trained on multiple ob- jects. Similarly, as far as we can see, Rao et al. do not discuss the gating aspect at all (Section 2.2), nor the speciﬁc is- sues with excitation and inhibition (Section 2.1) that we pointed out as motivation for using both classic and syn- chrony terms. Lastly, the following issues are addressed in our experiments only: network behavior on more than two objects; synchronization for objects that are not contigu- ous in the input images, as well as part vs. whole effects (Section 3.2); decoding distributed hidden representations according to phase (Section 3.3). In particular, it seems to be the case that Rao et al.’s networks had a localist (single object ↔ single unit) representation in the top hidden layer in the majority of cases. 4. The introduction of phase is done in an ad-hoc way, with- out real justiﬁcation from probabilistic goals.  We agree that framing our approach as a proper proba- bilistic model would be helpful (e.g. using an extension of the DUBM of Zemel et al., 1995, as discussed). At the same time, there is value to presenting the heuris- tic as is, based on a speciﬁc neuronal activation function, to emphasize that this idea could ﬁnd application in neu- ral networks more generally, not only those with a prob- abilistic interpretation or Boltzmann machines (that our approach is divorced from any one particular model is another difference when compared to Rao et al.’s work). In particular, we have performed exploratory experiments with networks trained (pretrained as real-valued nets or trained as complex-valued nets) with backpropagation, in- cluding (convolutional) feed-forward neural networks, au- toencoders, or recurrent networks, as well as a biological  We essentially use the normal DBM training as a form of pretraining for the ﬁnal, complex-valued architecture. The resulting neural network is likely not exactly to be inter- preted as a probabilistic model. However, if such an inter- pretation is desired, our understanding is that running the network could be seen as an approximation of inference in a suitably extended DUBM (by adding an off state and a classic term; refer to Zemel et al., 1995, for comparison). For our experiments, we used two procedures (with similar outcomes) in analogy to inference in a DBM: either sam- pling a binary output magnitude from f (), or letting f () de- termine the output magnitude deterministically; the output phase was always set to the phase of the total postsynaptic input. The ﬁrst procedure is similar to inference in such an extended DUBM, but, rather than sampling from a circular normal distribution on the unit circle when the unit is on, we simply take the mode of that distribution. The second procedure should qualitatively correspond to mean-ﬁeld in- ference in an extended DUBM (see Eqs. 9 and 10 in the DUBM paper), using a slightly different output function. 6. Do phases assigned to the input change when running for more iterations than what is shown?  Phase assignments appear to be stable (see the supplemen- tary movies), though we did not analyze this in detail. It should also be noted that the overall network is invariant to absolute phase, so only the relative phases matter.  References Aizenberg, I. & Moraga, C. (2007). Multilayer feedforward neural network based on multi-valued neurons (MLMVN) and a backpropagation learning algorithm. Soft Computing, 11(2):169–183.  Behrmann, M., Zemel, R.S., & Mozer, M.C. (1998). Object- based attention and occlusion: evidence from normal partic- ipants and a computational model. Journal of experimental psychology. Human perception and performance, 24(4):1011– 1036. PMID: 9706708.  Benchenane, K., Tiesinga, P.H., & Battaglia, F.P. (2011). Os- cillations in the prefrontal cortex: a gateway to memory and attention. Current Opinion in Neurobiology, 21(3):475–485.  Bengio, Y., Courville, A., & Vincent, P. (2012). Representation learning: A review and new perspectives. arXiv:1206.5538 [cs].  Cadieu, C.F., Hong, H., Yamins, D., Pinto, N., Majaj, N.J., & DiCarlo, J.J. (2013). The neural representation benchmark and its evaluation on brain and machine. arXiv:1301.3530.  Neuronal Synchrony in Complex-Valued Deep Networks  Cadieu, C.F. & Olshausen, B.A. (2011). Learning intermediate- level representations of form and motion from natural movies. Neural Computation, 24(4):827–866.  Jensen, O. & Lisman, J.E. (2005). Hippocampal sequence- encoding driven by a cortical multi-item working memory buffer. Trends in Neurosciences, 28(2):67–72.  Courville, A.C., Bergstra, J., & Bengio, Y. (2011). A spike and In International Confer-  slab restricted Boltzmann machine. ence on Artiﬁcial Intelligence and Statistics, p. 233–241.  Crick, F. (1984). Function of the thalamic reticular complex: the searchlight hypothesis. Proceedings of the National Academy of Sciences, 81(14):4586–4590. PMID: 6589612.  Duncan, J., Humphreys, G., & Ward, R. (1997). Competitive brain activity in visual attention. Current Opinion in Neurobi- ology, 7(2):255–261.  Fell, J. & Axmacher, N. (2011). The role of phase synchro- nization in memory processes. Nature Reviews Neuroscience, 12(2):105–118.  Fiori, S. (2005). Nonlinear complex-valued extensions of Heb- bian learning: An essay. Neural Computation, 17(4):779–838.  Fries, P. (2005). A mechanism for cognitive dynamics: neuronal communication through neuronal coherence. Trends in cogni- tive sciences, 9(10):474–480. PMID: 16150631.  Földiák, P. (1990). Forming sparse representations by local anti-  Hebbian learning. Biological Cybernetics, 64(2):165–170.  Gabernet, L., Jadhav, S.P., Feldman, D.E., Carandini, M., & Scanziani, M. (2005). Somatosensory integration controlled by dynamic thalamocortical feed-forward inhibition. Neuron, 48(2):315–327.  Geman, S. (2006). Invariance and selectivity in the ventral visual  pathway. Journal of Physiology-Paris, 100(4):212–224.  Goodfellow,  I.J., Courville, A., & Bengio, Y.  (2013a). Joint training deep Boltzmann machines for classiﬁcation. arXiv:1301.3568.  Goodfellow, I.J., Warde-Farley, D., Lamblin, P., Dumoulin, V., Mirza, M., Pascanu, R., Bergstra, J., Bastien, F., & Bengio, Y. (2013b). Pylearn2: a machine learning research library. arXiv e-print 1308.4214.  Goodman, D.F.M. & Brette, R. (2009). The brian simulator. Fron- tiers in Neuroscience, 3(2):192–197. PMID: 20011141 PM- CID: PMC2751620.  Gütig, R. & Sompolinsky, H. (2006). The tempotron: a neuron that learns spike timing–based decisions. Nature Neuroscience, 9(3):420–428.  Hinton, G.E. (2002). Training products of experts by minimiz- ing contrastive divergence. Neural Computation, 14(8):1771– 1800.  Hinton, G.E. (2010). A practical guide to training restricted Boltz- mann machines. Technical report UTML TR 2010-003, Depart- ment of Computer Science, Machine Learning Group, Univer- sity of Toronto.  Kim, T. & Adalı, T. (2003). Approximation by fully complex mul-  tilayer perceptrons. Neural Computation, 15(7):1641–1666.  Kivinen, J. & Williams, C. (2011). Transformation equivariant Boltzmann machines. In T. Honkela, W. Duch, M. Girolami, & S. Kaski, eds., Artiﬁcial Neural Networks and Machine Learn- ing – ICANN 2011, vol. 6791 of Lecture Notes in Computer Science, pp. 1–9. Springer Berlin / Heidelberg.  Krizhevsky, A., Sutskever, I., & Hinton, G. (2012).  ImageNet classiﬁcation with deep convolutional neural networks. In P. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou, & K.Q. Weinberger, eds., Advances in Neural Information Processing Systems 25, pp. 1106–1114.  Le, Q.V., Ranzato, M., Monga, R., Devin, M., Chen, K., Corrado, G.S., Dean, J., & Ng, A.Y. (2011). Building high-level features using large scale unsupervised learning. arXiv:1112.6209 [cs].  LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., & Jackel, L.D. (1989). Backpropagation ap- plied to handwritten zip code recognition. Neural Computa- tion, 1(4):541–551.  Lücke, J. & Sahani, M. (2008). Maximal causes for non-linear component extraction. The Journal of Machine Learning Re- search, 9:1227–1267.  Miconi, T. & VanRullen, R. (2010). The gamma slideshow: Object-based perceptual cycles in a model of the visual cor- tex. Frontiers in Human Neuroscience, 4. PMID: 21120147 PMCID: PMC2992033.  Miller, E.K. & Buschman, T.J. (2013). Cortical circuits for the control of attention. Current Opinion in Neurobiology, 23(2):216–222.  Mozer, M.C., Zemel, R.S., Behrmann, M., & Williams, C.K. (1992). Learning to segment images using dynamic feature binding. Neural Computation, 4(5):650–665.  Nitta, T. (2004).  complex-valued neural networks. 16(1):73–97.  Orthogonality of decision boundaries in Neural Computation,  Olshausen, B.A. & Field, D.J. (1997). Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision Research, 37(23):3311–3325.  Rao, A.R. & Cecchi, G.A. (2010). An objective function utiliz- ing complex sparsity for efﬁcient segmentation in multi-layer oscillatory networks. International Journal of Intelligent Com- puting and Cybernetics, 3(2):173–206.  Rao, A. & Cecchi, G. (2011). The effects of feedback and lateral connections on perceptual processing: A study using oscilla- tory networks. In IJCNN 2011, pp. 1177–1184.  Rao, A., Cecchi, G., Peck, C., & Kozloski, J. (2008). Unsuper- vised segmentation with dynamical units. IEEE Transactions on Neural Networks, 19(1):168–182.  Hirose, A. (2011). Nature of complex number and complex- valued neural networks. Frontiers of Electrical and Electronic Engineering in China, 6(1):171–180.  Ray, S. & Maunsell, J.H.R. (2010). Differences in gamma fre- quencies across visual cortex restrict their possible use in com- putation. Neuron, 67(5):885–896.  Neuronal Synchrony in Complex-Valued Deep Networks  Reichert, D., Seriès, P., & Storkey, A. (2010). Hallucinations in Charles Bonnet syndrome induced by homeostasis: a deep Boltzmann machine model. In J. Lafferty, C.K.I. Williams, J. Shawe-Taylor, R.S. Zemel, & A. Culotta, eds., Advances in Neural Information Processing Systems 23, pp. 2020–2028.  Reichert, D.P. (2012). Deep Boltzmann machines as hierarchical generative models of perceptual inference in the cortex. PhD thesis, University of Edinburgh, Edinburgh, UK.  Reichert, D.P., Seriès, P., & Storkey, A.J. (2011). A hierar- chical generative model of recurrent object-based attention in the visual cortex. In T. Honkela, W. Duch, M. Girolami, & S. Kaski, eds., Artiﬁcial Neural Networks and Machine Learn- ing - ICANN 2011, vol. 6791, pp. 18–25. Springer Berlin Hei- delberg, Berlin, Heidelberg.  Spratling, M.W. (2011). Unsupervised learning of generative and discriminative weights encoding elementary image com- ponents in a predictive coding model of cortical function. Neu- ral Computation, 24(1):60–103.  Stanley, G.B. (2013). Reading and writing the neural code. Nature  Neuroscience, 16(3):259–263.  Tieleman, T. (2008). Training restricted Boltzmann machines using approximations to the likelihood gradient. In Proceed- ings of the 25th Annual International Conference on Machine Learning, pp. 1064–1071. Helsinki, Finland.  Uhlhaas, P.J., Pipa, G., Lima, B., Melloni, L., Neuenschwander, S., Nikoli´c, D., & Singer, W. (2009). Neural synchrony in cor- tical networks: history, concept and current status. Frontiers in Integrative Neuroscience, 3:17.  Reichert, D.P., Seriès, P., & Storkey, A.J. (2013). Charles Bon- net syndrome: Evidence for a generative model in the cortex? PLoS Comput Biol, 9(7):e1003134.  von der Malsburg, C. (1981). The correlation theory of brain func- tion. Tech. Rep. S1-2, Department of Neurobiology, MPI for Biophysical Chemistry, Goettingen, W.-Germany.  Weber, C. & Wermter, S. (2005).  Image segmentation by complex-valued units. In W. Duch, J. Kacprzyk, E. Oja, & S. Zadro˙zny, eds., Artiﬁcial Neural Networks: Biological In- spirations – ICANN 2005, no. 3696 in Lecture Notes in Com- puter Science, pp. 519–524. Springer Berlin Heidelberg.  Zeiler, M.D. & Fergus, R. (2013). Visualizing and understanding  convolutional networks. arXiv:1311.2901 [cs].  Zemel, R.S., Williams, C.K., & Mozer, M.C. (1995). Lending direction to neural networks. Neural Networks, 8(4):503–512.  Rumelhart, D.E., Hinton, G.E., & Williams, R.J. (1985). Learning  internal representations by error propagation. Tech. rep.  Saalmann, Y.B., Pinsk, M.A., Wang, L., Li, X., & Kastner, S. (2012). The pulvinar regulates information transmission be- tween cortical areas based on attention demands. Science, 337(6095):753–756.  Salakhutdinov, R. & Hinton, G. (2009). Deep Boltzmann ma- In Proceedings of the 12th International Conference chines. on Artiﬁcial Intelligence and Statistics (AISTATS), vol. 5, pp. 448–455.  Savitha, R., Suresh, S., & Sundararajan, N. (2011). Metacognitive learning in a fully complex-valued radial basis function neural network. Neural Computation, 24(5):1297–1328.  Serre, T., Oliva, A., & Poggio, T. (2007). A feedforward archi- tecture accounts for rapid categorization. Proceedings of the National Academy of Sciences, 104(15):6424–6429.  Shadlen, M.N. & Movshon, J.A. (1999). Synchrony unbound: a critical evaluation of the temporal binding hypothesis. Neuron, 24(1):67–77.  Shipp, S. (2003). The functional logic of cortico–pulvinar con- nections. Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences, 358(1438):1605–1624. PMID: 14561322.  Singer, W. & Gray, C.M. (1995). Visual feature integration and the temporal correlation hypothesis. Annual review of neuro- science, 18:555–586. PMID: 7605074.  Singer, W. (2007).  2(12):1657.  Binding by synchrony.  Scholarpedia,  Sjöström, J. & Gerstner, W. (2010). Spike-timing dependent plas-  ticity. Scholarpedia, 5(2):1362.  Smolensky, P. (1986). Information processing in dynamical sys- tems: foundations of harmony theory. In Parallel distributed processing: explorations in the microstructure of cognition. Vol. 1. Foundations, p. 194–281. MIT Press, Cambridge, MA.  ","Deep learning has recently led to great successes in tasks such as imagerecognition (e.g Krizhevsky et al., 2012). However, deep networks are stilloutmatched by the power and versatility of the brain, perhaps in part due tothe richer neuronal computations available to cortical circuits. The challengeis to identify which neuronal mechanisms are relevant, and to find suitableabstractions to model them. Here, we show how aspects of spike timing, longhypothesized to play a crucial role in cortical information processing, couldbe incorporated into deep networks to build richer, versatile representations.We introduce a neural network formulation based on complex-valued neuronalunits that is not only biologically meaningful but also amenable to a varietyof deep learning frameworks. Here, units are attributed both a firing rate anda phase, the latter indicating properties of spike timing. We show how thisformulation qualitatively captures several aspects thought to be related toneuronal synchrony, including gating of information processing and dynamicbinding of distributed object representations. Focusing on the latter, wedemonstrate the potential of the approach in several simple experiments. Thus,neuronal synchrony could be a flexible mechanism that fulfills multiplefunctional roles in deep networks."
1312.6114,2014,Auto-Encoding Variational Bayes  ,"['Diederik P. Kingma', 'Max Welling']",https://arxiv.org/pdf/1312.6114.pdf,"4 1 0 2     y a M 1         ] L M  . t a t s [      0 1 v 4 1 1 6  .  2 1 3 1 : v i X r a  Auto-Encoding Variational Bayes  Diederik P. Kingma  Machine Learning Group Universiteit van Amsterdam dpkingma@gmail.com  Max Welling  Machine Learning Group Universiteit van Amsterdam  welling.max@gmail.com  Abstract  How can we perform efﬁcient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differ- entiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using stan- dard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made espe- cially efﬁcient by ﬁtting an approximate inference model (also called a recogni- tion model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reﬂected in experimental results.  1  Introduction  How can we perform efﬁcient approximate inference and learning with directed probabilistic models whose continuous latent variables and/or parameters have intractable posterior distributions? The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior. Unfortunately, the common mean-ﬁeld approach requires analytical solutions of expecta- tions w.r.t. the approximate posterior, which are also intractable in the general case. We show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for ef- ﬁcient approximate posterior inference in almost any model with continuous latent variables and/or parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques. For the case of an i.i.d. dataset and continuous latent variables per datapoint, we propose the Auto- Encoding VB (AEVB) algorithm. In the AEVB algorithm we make inference and learning especially efﬁcient by using the SGVB estimator to optimize a recognition model that allows us to perform very efﬁcient approximate posterior inference using simple ancestral sampling, which in turn allows us to efﬁciently learn the model parameters, without the need of expensive iterative inference schemes (such as MCMC) per datapoint. The learned approximate posterior inference model can also be used for a host of tasks such as recognition, denoising, representation and visualization purposes. When a neural network is used for the recognition model, we arrive at the variational auto-encoder.  2 Method  The strategy in this section can be used to derive a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables. We will restrict ourselves here to the common case where we have an i.i.d. dataset with latent variables per datapoint, and where we like to perform maximum likelihood (ML) or maximum a posteriori (MAP) inference on the (global) parameters, and variational inference on the latent variables. It is, for example,  1  φ  z  θ  x  N  Figure 1: The type of directed graphical model under consideration. Solid lines denote the generative model pθ(z)pθ(x|z), dashed lines denote the variational approximation qφ(z|x) to the intractable posterior pθ(z|x). The variational parameters φ are learned jointly with the generative model pa- rameters θ.  straightforward to extend this scenario to the case where we also perform variational inference on the global parameters; that algorithm is put in the appendix, but experiments with that case are left to future work. Note that our method can be applied to online, non-stationary settings, e.g. streaming data, but here we assume a ﬁxed dataset for simplicity.  2.1 Problem scenario Let us consider some dataset X = {x(i)}N i=1 consisting of N i.i.d. samples of some continuous or discrete variable x. We assume that the data are generated by some random process, involving an unobserved continuous random variable z. The process consists of two steps: (1) a value z(i) is generated from some prior distribution pθ∗ (z); (2) a value x(i) is generated from some condi- tional distribution pθ∗ (x|z). We assume that the prior pθ∗ (z) and likelihood pθ∗ (x|z) come from parametric families of distributions pθ(z) and pθ(x|z), and that their PDFs are differentiable almost everywhere w.r.t. both θ and z. Unfortunately, a lot of this process is hidden from our view: the true parameters θ Very importantly, we do not make the common simplifying assumptions about the marginal or pos- terior probabilities. Conversely, we are here interested in a general algorithm that even works efﬁ- ciently in the case of:  ∗ as well as the values of the latent variables z(i) are unknown to us.  the case where the integral of the marginal  (cid:82) pθ(z)pθ(x|z) dz is intractable (so we cannot evaluate or differentiate the marginal like-  likelihood pθ(x) = lihood), where the true posterior density pθ(z|x) = pθ(x|z)pθ(z)/pθ(x) is intractable (so the EM algorithm cannot be used), and where the required integrals for any reason- able mean-ﬁeld VB algorithm are also intractable. These intractabilities are quite common and appear in cases of moderately complicated likelihood functions pθ(x|z), e.g. a neural network with a nonlinear hidden layer.  1. Intractability:  2. A large dataset: we have so much data that batch optimization is too costly; we would like to make parameter updates using small minibatches or even single datapoints. Sampling- based solutions, e.g. Monte Carlo EM, would in general be too slow, since it involves a typically expensive sampling loop per datapoint.  We are interested in, and propose a solution to, three related problems in the above scenario:  1. Efﬁcient approximate ML or MAP estimation for the parameters θ. The parameters can be of interest themselves, e.g. if we are analyzing some natural process. They also allow us to mimic the hidden random process and generate artiﬁcial data that resembles the real data. 2. Efﬁcient approximate posterior inference of the latent variable z given an observed value x  for a choice of parameters θ. This is useful for coding or data representation tasks.  3. Efﬁcient approximate marginal inference of the variable x. This allows us to perform all kinds of inference tasks where a prior over x is required. Common applications in computer vision include image denoising, inpainting and super-resolution.  2  For the purpose of solving the above problems, let us introduce a recognition model qφ(z|x): an approximation to the intractable true posterior pθ(z|x). Note that in contrast with the approximate posterior in mean-ﬁeld variational inference, it is not necessarily factorial and its parameters φ are not computed from some closed-form expectation. Instead, we’ll introduce a method for learning the recognition model parameters φ jointly with the generative model parameters θ. From a coding theory perspective, the unobserved variables z have an interpretation as a latent representation or code. In this paper we will therefore also refer to the recognition model qφ(z|x) as a probabilistic encoder, since given a datapoint x it produces a distribution (e.g. a Gaussian) over the possible values of the code z from which the datapoint x could have been generated. In a similar vein we will refer to pθ(x|z) as a probabilistic decoder, since given a code z it produces a distribution over the possible corresponding values of x.  2.2 The variational bound  log pθ(x(1),··· , x(N )) =(cid:80)N  The marginal likelihood is composed of a sum over the marginal likelihoods of individual datapoints  i=1 log pθ(x(i)), which can each be rewritten as:  log pθ(x(i)) = DKL(qφ(z|x(i))||pθ(z|x(i))) + L(θ, φ; x(i))  (1) The ﬁrst RHS term is the KL divergence of the approximate from the true posterior. Since this KL-divergence is non-negative, the second RHS term L(θ, φ; x(i)) is called the (variational) lower bound on the marginal likelihood of datapoint i, and can be written as:  log pθ(x(i)) ≥ L(θ, φ; x(i)) = Eqφ(z|x) [− log qφ(z|x) + log pθ(x, z)]  (2)  which can also be written as:  L(θ, φ; x(i)) = −DKL(qφ(z|x(i))||pθ(z)) + E  (3) We want to differentiate and optimize the lower bound L(θ, φ; x(i)) w.r.t. both the variational (cid:80)L parameters φ and generative parameters θ. However, the gradient of the lower bound w.r.t. φ is a bit problematic. The usual (na¨ıve) Monte Carlo gradient estimator for this type of problem is: ∇φEqφ(z) [f (z)] = Eqφ(z) l=1 f (z)∇qφ(z(l)) log qφ(z(l)) where z(l) ∼ qφ(z|x(i)). This gradient estimator exhibits exhibits very high variance (see e.g. [BJP12]) and is impractical for our purposes.  (cid:2)f (z)∇qφ(z) log qφ(z)(cid:3) (cid:39) 1  log pθ(x(i)|z)  qφ(z|x(i))  L  (cid:104)  (cid:105)  2.3 The SGVB estimator and AEVB algorithm  In this section we introduce a practical estimator of the lower bound and its derivatives w.r.t. the parameters. We assume an approximate posterior in the form qφ(z|x), but please note that the technique can be applied to the case qφ(z), i.e. where we do not condition on x, as well. The fully variational Bayesian method for inferring a posterior over the parameters is given in the appendix. Under certain mild conditions outlined in section 2.4 for a chosen approximate posterior qφ(z|x) we  can reparameterize the random variable(cid:101)z ∼ qφ(z|x) using a differentiable transformation gφ((cid:15), x) of an (auxiliary) noise variable (cid:15):(cid:101)z = gφ((cid:15), x) with (cid:105) (cid:39) 1 L(cid:88)  (4) See section 2.4 for general strategies for chosing such an approriate distribution p((cid:15)) and function gφ((cid:15), x). We can now form Monte Carlo estimates of expectations of some function f (z) w.r.t. qφ(z|x) as follows:  (cid:15)(l) ∼ p((cid:15)) (5)  f (gφ((cid:15)(l), x(i))) where  qφ(z|x(i)) [f (z)] = Ep((cid:15)) E  f (gφ((cid:15), x(i)))  (cid:15) ∼ p((cid:15))  We apply this technique to the variational lower bound (eq. (2)), yielding our generic Stochastic  Gradient Variational Bayes (SGVB) estimator (cid:101)LA(θ, φ; x(i)) (cid:39) L(θ, φ; x(i)):  (cid:104)  l=1  L  (cid:101)LA(θ, φ; x(i)) =  L(cid:88)  l=1  1 L  where  z(i,l) = gφ((cid:15)(i,l), x(i))  and  3  log pθ(x(i), z(i,l)) − log qφ(z(i,l)|x(i))  (cid:15)(l) ∼ p((cid:15))  (6)  L(cid:88)  l=1  1 L  (cid:15)(l) ∼ p((cid:15))  Algorithm 1 Minibatch version of the Auto-Encoding VB (AEVB) algorithm. Either of the two SGVB estimators in section 2.3 can be used. We use settings M = 100 and L = 1 in experiments.  θ, φ ← Initialize parameters repeat  g ← ∇θ,φ(cid:101)LM (θ, φ; XM , (cid:15)) (Gradients of minibatch estimator (8))  XM ← Random minibatch of M datapoints (drawn from full dataset) (cid:15) ← Random samples from noise distribution p((cid:15)) θ, φ ← Update parameters using gradients g (e.g. SGD or Adagrad [DHS10])  until convergence of parameters (θ, φ) return θ, φ  Often, the KL-divergence DKL(qφ(z|x(i))||pθ(z)) of eq. (3) can be integrated analytically (see appendix B), such that only the expected reconstruction error E estimation by sampling. The KL-divergence term can then be interpreted as regularizing φ, encour- aging the approximate posterior to be close to the prior pθ(z). This yields a second version of the  (cid:2)log pθ(x(i)|z)(cid:3) requires SGVB estimator (cid:101)LB(θ, φ; x(i)) (cid:39) L(θ, φ; x(i)), corresponding to eq. (3), which typically has less  qφ(z|x(i))  variance than the generic estimator:  (cid:101)LB(θ, φ; x(i)) = −DKL(qφ(z|x(i))||pθ(z)) +  (log pθ(x(i)|z(i,l)))  (8)  where  (7) Given multiple datapoints from a dataset X with N datapoints, we can construct an estimator of the marginal likelihood lower bound of the full dataset, based on minibatches:  z(i,l) = gφ((cid:15)(i,l), x(i))  and  L(θ, φ; X) (cid:39) (cid:101)LM (θ, φ; XM ) =  M(cid:88)  i=1  N M  (cid:101)L(θ, φ; x(i))  where the minibatch XM = {x(i)}M i=1 is a randomly drawn sample of M datapoints from the full dataset X with N datapoints. In our experiments we found that the number of samples L per datapoint can be set to 1 as long as the minibatch size M was large enough, e.g. M = 100.  Derivatives ∇θ,φ(cid:101)L(θ; XM ) can be taken, and the resulting gradients can be used in conjunction  with stochastic optimization methods such as SGD or Adagrad [DHS10]. See algorithm 1 for a basic approach to compute the stochastic gradients. A connection with auto-encoders becomes clear when looking at the objective function given at eq. (7). The ﬁrst term is (the KL divergence of the approximate posterior from the prior) acts as a regularizer, while the second term is a an expected negative reconstruction error. The function gφ(.) is chosen such that it maps a datapoint x(i) and a random noise vector (cid:15)(l) to a sample from the approximate posterior for that datapoint: z(i,l) = gφ((cid:15)(l), x(i)) where z(i,l) ∼ qφ(z|x(i)). Subse- quently, the sample z(i,l) is then input to function log pθ(x(i)|z(i,l)), which equals the probability density (or mass) of datapoint x(i) under the generative model, given z(i,l). This term is a negative reconstruction error in auto-encoder parlance.  2.4 The reparameterization trick  In order to solve our problem we invoked an alternative method for generating samples from qφ(z|x). The essential parameterization trick is quite simple. Let z be a continuous random vari- able, and z ∼ qφ(z|x) be some conditional distribution. It is then often possible to express the random variable z as a deterministic variable z = gφ((cid:15), x), where (cid:15) is an auxiliary variable with independent marginal p((cid:15)), and gφ(.) is some vector-valued function parameterized by φ. This reparameterization is useful for our case since it can be used to rewrite an expectation w.r.t qφ(z|x) such that the Monte Carlo estimate of the expectation is differentiable w.r.t. φ. A proof i dzi =  is as follows. Given the deterministic mapping z = gφ((cid:15), x) we know that qφ(z|x)(cid:81) i d(cid:15)i. Therefore1,(cid:82) qφ(z|x)f (z) dz = (cid:82) p((cid:15))f (z) d(cid:15) = (cid:82) p((cid:15))f (gφ((cid:15), x)) d(cid:15). It follows p((cid:15))(cid:81) 1Note that for inﬁnitesimals we use the notational convention dz =(cid:81)  i dzi  4  that a differentiable estimator can be constructed: (cid:82) qφ(z|x)f (z) dz (cid:39) 1  l=1 f (gφ(x, (cid:15)(l))) where (cid:15)(l) ∼ p((cid:15)). In section 2.3 we applied this trick to obtain a differentiable estimator of the variational lower bound. Take, for example, the univariate Gaussian case: let z ∼ p(z|x) = N (µ, σ2). In this case, a valid reparameterization is z = µ + σ(cid:15), where (cid:15) is an auxiliary noise variable (cid:15) ∼ N (0, 1). Therefore, EN (z;µ,σ2) [f (z)] = EN ((cid:15);0,1) [f (µ + σ(cid:15))] (cid:39) 1 For which qφ(z|x) can we choose such a differentiable transformation gφ(.) and auxiliary variable (cid:15) ∼ p((cid:15))? Three basic approaches are:  (cid:80)L l=1 f (µ + σ(cid:15)(l)) where (cid:15)(l) ∼ N (0, 1).  (cid:80)L  L  L  1. Tractable inverse CDF. In this case, let (cid:15) ∼ U(0, I), and let gφ((cid:15), x) be the inverse CDF of qφ(z|x). Examples: Exponential, Cauchy, Logistic, Rayleigh, Pareto, Weibull, Reciprocal, Gompertz, Gumbel and Erlang distributions.  2. Analogous to the Gaussian example, for any ”location-scale” family of distributions we can choose the standard distribution (with location = 0, scale = 1) as the auxiliary variable (cid:15), and let g(.) = location + scale · (cid:15). Examples: Laplace, Elliptical, Student’s t, Logistic, Uniform, Triangular and Gaussian distributions.  3. Composition: It is often possible to express random variables as different transformations of auxiliary variables. Examples: Log-Normal (exponentiation of normally distributed variable), Gamma (a sum over exponentially distributed variables), Dirichlet (weighted sum of Gamma variates), Beta, Chi-Squared, and F distributions.  When all three approaches fail, good approximations to the inverse CDF exist requiring computa- tions with time complexity comparable to the PDF (see e.g. [Dev86] for some methods).  3 Example: Variational Auto-Encoder  In this section we’ll give an example where we use a neural network for the probabilistic encoder qφ(z|x) (the approximation to the posterior of the generative model pθ(x, z)) and where the param- eters φ and θ are optimized jointly with the AEVB algorithm. Let the prior over the latent variables be the centered isotropic multivariate Gaussian pθ(z) = N (z; 0, I). Note that in this case, the prior lacks parameters. We let pθ(x|z) be a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data) whose distribution pa- rameters are computed from z with a MLP (a fully-connected neural network with a single hidden layer, see appendix C). Note the true posterior pθ(z|x) is in this case intractable. While there is much freedom in the form qφ(z|x), we’ll assume the true (but intractable) posterior takes on a ap- proximate Gaussian form with an approximately diagonal covariance. In this case, we can let the variational approximate posterior be a multivariate Gaussian with a diagonal covariance structure2: (9) where the mean and s.d. of the approximate posterior, µ(i) and σ(i), are outputs of the encoding MLP, i.e. nonlinear functions of datapoint x(i) and the variational parameters φ (see appendix C). As explained in section 2.4, we sample from the posterior z(i,l) ∼ qφ(z|x(i)) using z(i,l) = gφ(x(i), (cid:15)(l)) = µ(i) + σ(i) (cid:12) (cid:15)(l) where (cid:15)(l) ∼ N (0, I). With (cid:12) we signify an element-wise product. In this model both pθ(z) (the prior) and qφ(z|x) are Gaussian; in this case, we can use the estimator of eq. (7) where the KL divergence can be computed and differentiated without estimation (see appendix B). The resulting estimator for this model and datapoint x(i) is:  log qφ(z|x(i)) = log N (z; µ(i), σ2(i)I)  L(θ, φ; x(i)) (cid:39) 1 2  1 + log((σ(i)  j )2) − (µ(i)  log pθ(x(i)|z(i,l))  where z(i,l) = µ(i) + σ(i) (cid:12) (cid:15)(l)  (10) As explained above and in appendix C, the decoding term log pθ(x(i)|z(i,l)) is a Bernoulli or Gaus- sian MLP, depending on the type of data we are modelling.  and  j )2(cid:17)  L(cid:88)  l=1  +  1 L  j )2 − (σ(i) (cid:15)(l) ∼ N (0, I)  J(cid:88)  (cid:16)  j=1  2Note that this is just a (simplifying) choice, and not a limitation of our method.  5  4 Related work  The wake-sleep algorithm [HDFN95] is, to the best of our knowledge, the only other on-line learn- ing method in the literature that is applicable to the same general class of continuous latent variable models. Like our method, the wake-sleep algorithm employs a recognition model that approximates the true posterior. A drawback of the wake-sleep algorithm is that it requires a concurrent optimiza- tion of two objective functions, which together do not correspond to optimization of (a bound of) the marginal likelihood. An advantage of wake-sleep is that it also applies to models with discrete latent variables. Wake-Sleep has the same computational complexity as AEVB per datapoint. Stochastic variational inference [HBWP13] has recently received increasing interest. Recently, [BJP12] introduced a control variate schemes to reduce the high variance of the na¨ıve gradient estimator discussed in section 2.1, and applied to exponential family approximations of the poste- rior. In [RGB13] some general methods, i.e. a control variate scheme, were introduced for reducing the variance of the original gradient estimator. In [SK13], a similar reparameterization as in this paper was used in an efﬁcient version of a stochastic variational inference algorithm for learning the natural parameters of exponential-family approximating distributions. The AEVB algorithm exposes a connection between directed probabilistic models (trained with a variational objective) and auto-encoders. A connection between linear auto-encoders and a certain class of generative linear-Gaussian models has long been known. In [Row98] it was shown that PCA corresponds to the maximum-likelihood (ML) solution of a special case of the linear-Gaussian model with a prior p(z) = N (0, I) and a conditional distribution p(x|z) = N (x; Wz, (cid:15)I), speciﬁcally the case with inﬁnitesimally small (cid:15). In relevant recent work on autoencoders [VLL+10] it was shown that the training criterion of un- regularized autoencoders corresponds to maximization of a lower bound (see the infomax princi- ple [Lin89]) of the mutual information between input X and latent representation Z. Maximiz- ing (w.r.t. parameters) of the mutual information is equivalent to maximizing the conditional en- tropy, which is lower bounded by the expected loglikelihood of the data under the autoencoding model [VLL+10], i.e. the negative reconstrution error. However, it is well known that this recon- struction criterion is in itself not sufﬁcient for learning useful representations [BCV13]. Regular- ization techniques have been proposed to make autoencoders learn useful representations, such as denoising, contractive and sparse autoencoder variants [BCV13]. The SGVB objective contains a regularization term dictated by the variational bound (e.g. eq. (10)), lacking the usual nuisance regu- larization hyperparameter required to learn useful representations. Related are also encoder-decoder architectures such as the predictive sparse decomposition (PSD) [KRL08], from which we drew some inspiration. Also relevant are the recently introduced Generative Stochastic Networks [BTL13] where noisy auto-encoders learn the transition operator of a Markov chain that samples from the data distribution. In [SL10] a recognition model was employed for efﬁcient learning with Deep Boltz- mann Machines. These methods are targeted at either unnormalized models (i.e. undirected models like Boltzmann machines) or limited to sparse coding models, in contrast to our proposed algorithm for learning a general class of directed probabilistic models. The recently proposed DARN method [GMW13], also learns a directed probabilistic model using an auto-encoding structure, however their method applies to binary latent variables. Even more recently, [RMW14] also make the connection between auto-encoders, directed proabilistic models and stochastic variational inference using the reparameterization trick we describe in this paper. Their work was developed independently of ours and provides an additional perspective on AEVB.  5 Experiments  We trained generative models of images from the MNIST and Frey Face datasets3 and compared learning algorithms in terms of the variational lower bound, and the estimated marginal likelihood. The generative model (encoder) and variational approximation (decoder) from section 3 were used, where the described encoder and decoder have an equal number of hidden units. Since the Frey Face data are continuous, we used a decoder with Gaussian outputs, identical to the encoder, except that the means were constrained to the interval (0, 1) using a sigmoidal activation function at the  3Available at http://www.cs.nyu.edu/˜roweis/data.html  6  Figure 2: Comparison of our AEVB method to the wake-sleep algorithm, in terms of optimizing the lower bound, for different dimensionality of latent space (Nz). Our method converged considerably faster and reached a better solution in all experiments. Interestingly enough, more latent variables does not result in more overﬁtting, which is explained by the regularizing effect of the lower bound. Vertical axis: the estimated average variational lower bound per datapoint. The estimator variance was small (< 1) and omitted. Horizontal axis: amount of training points evaluated. Computa- tion took around 20-40 minutes per million training samples with a Intel Xeon CPU running at an effective 40 GFLOPS.  decoder output. Note that with hidden units we refer to the hidden layer of the neural networks of the encoder and decoder. Parameters are updated using stochastic gradient ascent where gradients are computed by differenti- ating the lower bound estimator ∇θ,φL(θ, φ; X) (see algorithm 1), plus a small weight decay term corresponding to a prior p(θ) = N (0, I). Optimization of this objective is equivalent to approxi- mate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower bound. We compared performance of AEVB to the wake-sleep algorithm [HDFN95]. We employed the same encoder (also called recognition model) for the wake-sleep algorithm and the variational auto- encoder. All parameters, both variational and generative, were initialized by random sampling from N (0, 0.01), and were jointly stochastically optimized using the MAP criterion. Stepsizes were adapted with Adagrad [DHS10]; the Adagrad global stepsize parameters were chosen from {0.01, 0.02, 0.1} based on performance on the training set in the ﬁrst few iterations. Minibatches of size M = 100 were used, with L = 1 samples per datapoint.  Likelihood lower bound We trained generative models (decoders) and corresponding encoders (a.k.a. recognition models) having 500 hidden units in case of MNIST, and 200 hidden units in case of the Frey Face dataset (to prevent overﬁtting, since it is a considerably smaller dataset). The chosen number of hidden units is based on prior literature on auto-encoders, and the relative performance of different algorithms was not very sensitive to these choices. Figure 2 shows the results when comparing the lower bounds. Interestingly, superﬂuous latent variables did not result in overﬁtting, which is explained by the regularizing nature of the variational bound.  Marginal likelihood For very low-dimensional latent space it is possible to estimate the marginal likelihood of the learned generative models using an MCMC estimator. More information about the marginal likelihood estimator is available in the appendix. For the encoder and decoder we again used neural networks, this time with 100 hidden units, and 3 latent variables; for higher dimensional latent space the estimates became unreliable. Again, the MNIST dataset was used. The AEVB and Wake-Sleep methods were compared to Monte Carlo EM (MCEM) with a Hybrid Monte Carlo (HMC) [DKPR87] sampler; details are in the appendix. We compared the convergence speed for the three algorithms, for a small and large training set size. Results are in ﬁgure 3.  7  105106107108# Training samples evaluated150140130120110100LMNIST, Nz=3105106107108150140130120110100MNIST, Nz=5105106107108150140130120110100MNIST, Nz=10105106107108150140130120110100MNIST, Nz=20105106107108150140130120110100MNIST, Nz=20010510610710802004006008001000120014001600LFrey Face, Nz=2Wake-Sleep (test)Wake-Sleep (train)AEVB (test)AEVB (train)10510610710802004006008001000120014001600Frey Face, Nz=510510610710802004006008001000120014001600Frey Face, Nz=1010510610710802004006008001000120014001600Frey Face, Nz=20Figure 3: Comparison of AEVB to the wake-sleep algorithm and Monte Carlo EM, in terms of the estimated marginal likelihood, for a different number of training points. Monte Carlo EM is not an on-line algorithm, and (unlike AEVB and the wake-sleep method) can’t be applied efﬁciently for the full MNIST dataset.  Visualisation of high-dimensional data If we choose a low-dimensional latent space (e.g. 2D), we can use the learned encoders (recognition model) to project high-dimensional data to a low- dimensional manifold. See appendix A for visualisations of the 2D latent manifolds for the MNIST and Frey Face datasets.  6 Conclusion  We have introduced a novel estimator of the variational lower bound, Stochastic Gradient VB (SGVB), for efﬁcient approximate inference with continuous latent variables. The proposed estima- tor can be straightforwardly differentiated and optimized using standard stochastic gradient meth- ods. For the case of i.i.d. datasets and continuous latent variables per datapoint we introduce an efﬁcient algorithm for efﬁcient inference and learning, Auto-Encoding VB (AEVB), that learns an approximate inference model using the SGVB estimator. The theoretical advantages are reﬂected in experimental results.  7 Future work  Since the SGVB estimator and the AEVB algorithm can be applied to almost any inference and learning problem with continuous latent variables, there are plenty of future directions: (i) learning hierarchical generative architectures with deep neural networks (e.g. convolutional networks) used for the encoders and decoders, trained jointly with AEVB; (ii) time-series models (i.e. dynamic Bayesian networks); (iii) application of SGVB to the global parameters; (iv) supervised models with latent variables, useful for learning complicated noise distributions.  8  0102030405060# Training samples evaluated (millions)160150140130120110100Marginal log-likelihoodNtrain = 10000102030405060160155150145140135130125Ntrain = 50000Wake-Sleep (train)Wake-Sleep (test)MCEM (train)MCEM (test)AEVB (train)AEVB (test)References  [BCV13] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A re-  [BJP12]  [BTL13]  [Dev86]  [DHS10]  view and new perspectives. 2013. David M Blei, Michael I Jordan, and John W Paisley. Variational bayesian inference with stochastic search. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 1367–1374, 2012. Yoshua Bengio and ´Eric Thibodeau-Laufer. Deep generative stochastic networks train- able by backprop. arXiv preprint arXiv:1306.1091, 2013. Luc Devroye. Sample-based non-uniform random variate generation. In Proceedings of the 18th conference on Winter simulation, pages 260–265. ACM, 1986. John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121– 2159, 2010.  [DKPR87] Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. Hybrid  monte carlo. Physics letters B, 195(2):216–222, 1987.  [GMW13] Karol Gregor, Andriy Mnih, and Daan Wierstra. Deep autoregressive networks. arXiv  preprint arXiv:1310.8499, 2013.  [HBWP13] Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic varia- tional inference. The Journal of Machine Learning Research, 14(1):1303–1347, 2013. [HDFN95] Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The” wake- sleep” algorithm for unsupervised neural networks. SCIENCE, pages 1158–1158, 1995. [KRL08] Koray Kavukcuoglu, Marc’Aurelio Ranzato, and Yann LeCun. Fast inference in sparse coding algorithms with applications to object recognition. Technical Report CBLL- TR-2008-12-01, Computational and Biological Learning Lab, Courant Institute, NYU, 2008. Ralph Linsker. An application of the principle of maximum information preservation to linear systems. Morgan Kaufmann Publishers Inc., 1989.  [Lin89]  [RGB13] Rajesh Ranganath, Sean Gerrish, and David M Blei. Black box variational inference.  arXiv preprint arXiv:1401.0118, 2013.  [Row98]  [RMW14] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic back- propagation and variational inference in deep latent gaussian models. arXiv preprint arXiv:1401.4082, 2014. Sam Roweis. EM algorithms for PCA and SPCA. Advances in neural information processing systems, pages 626–632, 1998. Tim Salimans and David A Knowles. Fixed-form variational posterior approximation through stochastic linear regression. Bayesian Analysis, 8(4), 2013. Ruslan Salakhutdinov and Hugo Larochelle. Efﬁcient learning of deep boltzmann ma- chines. In International Conference on Artiﬁcial Intelligence and Statistics, pages 693– 700, 2010.  [SK13]  [SL10]  [VLL+10] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. The Journal of Machine Learning Research, 9999:3371–3408, 2010.  A Visualisations  See ﬁgures 4 and 5 for visualisations of latent space and corresponding observed space of models learned with SGVB.  9  (a) Learned Frey Face manifold  (b) Learned MNIST manifold  Figure 4: Visualisations of learned data manifold for generative models with two-dimensional latent space, learned with AEVB. Since the prior of the latent space is Gaussian, linearly spaced coor- dinates on the unit square were transformed through the inverse CDF of the Gaussian to produce values of the latent variables z. For each of these values z, we plotted the corresponding generative pθ(x|z) with the learned parameters θ.  (a) 2-D latent space  (b) 5-D latent space  (c) 10-D latent space  (d) 20-D latent space  Figure 5: Random samples from learned generative models of MNIST for different dimensionalities of latent space.  B Solution of −DKL(qφ(z)||pθ(z)), Gaussian case  The variational lower bound (the objective to be maximized) contains a KL term that can often be integrated analytically. Here we give the solution when both the prior pθ(z) = N (0, I) and the posterior approximation qφ(z|x(i)) are Gaussian. Let J be the dimensionality of z. Let µ and σ denote the variational mean and s.d. evaluated at datapoint i, and let µj and σj simply denote the j-th element of these vectors. Then:  (cid:90)  (cid:90)  qθ(z) log p(z) dz =  N (z; µ, σ2) log N (z; 0, I) dz  J(cid:88)  j=1  (µ2  j + σ2 j )  = − J 2  log(2π) − 1 2  10  And:  (cid:90)  (cid:90)  qθ(z) log qθ(z) dz =  Therefore:  −DKL((qφ(z)||pθ(z)) =  (cid:90)  N (z; µ, σ2) log N (z; µ, σ2) dz  = − J 2  log(2π) − 1 2  (1 + log σ2 j )  J(cid:88)  j=1  J(cid:88)  qθ(z) (log pθ(z) − log qθ(z)) dz  (cid:0)1 + log((σj)2) − (µj)2 − (σj)2(cid:1)  =  1 2  j=1  When using a recognition model qφ(z|x) then µ and s.d. σ are simply functions of x and the variational parameters φ, as exempliﬁed in the text.  C MLP’s as probabilistic encoders and decoders  In variational auto-encoders, neural networks are used as probabilistic encoders and decoders. There are many possible choices of encoders and decoders, depending on the type of data and model. In our example we used relatively simple neural networks, namely multi-layered perceptrons (MLPs). For the encoder we used a MLP with Gaussian output, while for the decoder we used MLPs with either Gaussian or Bernoulli outputs, depending on the type of data.  C.1 Bernoulli MLP as decoder In this case let pθ(x|z) be a multivariate Bernoulli whose probabilities are computed from z with a fully-connected neural network with a single hidden layer:  D(cid:88)  log p(x|z) =  xi log yi + (1 − xi) · log(1 − yi)  (11) where fσ(.) is the elementwise sigmoid activation function, and where θ = {W1, W2, b1, b2} are the weights and biases of the MLP.  where y = fσ(W2 tanh(W1z + b1) + b2)  i=1  C.2 Gaussian MLP as encoder or decoder  In this case let encoder or decoder be a multivariate Gaussian with a diagonal covariance structure:  log p(x|z) = log N (x; µ, σ2I) where µ = W4h + b4 log σ2 = W5h + b5  (12) where {W3, W4, W5, b3, b4, b5} are the weights and biases of the MLP and part of θ when used as decoder. Note that when this network is used as an encoder qφ(z|x), then z and x are swapped, and the weights and biases are variational parameters φ.  h = tanh(W3z + b3)  D Marginal likelihood estimator  We derived the following marginal likelihood estimator that produces good estimates of the marginal likelihood as long as the dimensionality of the sampled space is low (less then 5 dimensions), and sufﬁcient samples are taken. Let pθ(x, z) = pθ(z)pθ(x|z) be the generative model we are sampling from, and for a given datapoint x(i) we would like to estimate the marginal likelihood pθ(x(i)). The estimation process consists of three stages:  11  1. Sample L values {z(l)} from the posterior using gradient-based MCMC, e.g. Hybrid Monte  Carlo, using ∇z log pθ(z|x) = ∇z log pθ(z) + ∇z log pθ(x|z).  2. Fit a density estimator q(z) to these samples {z(l)}. 3. Again, sample L new values from the posterior. Plug these samples, as well as the ﬁtted  q(z), into the following estimator:  q(z(l))  pθ(z)pθ(x(i)|z(l))  where  z(l) ∼ pθ(z|x(i))  (cid:33)−1  (cid:82) q(z) pθ (x(i),z)  pθ (x(i),z) dz pθ(x(i)) q(z)  dz  pθ(x(i), z)  q(z)  pθ(x(i), z)  dz  (cid:32)  l=1  1 L  L(cid:88) (cid:82) q(z) dz (cid:90) pθ(x(i), z) (cid:90)  pθ(x(i)) pθ(z|x(i))  pθ(x(i))  =  L(cid:88)  l=1  pθ(x(i)) (cid:39)  Derivation of the estimator:  1  pθ(x(i))  =  =  =  (cid:39) 1 L  E Monte Carlo EM  q(z(l))  pθ(z)pθ(x(i)|z(l))  where  z(l) ∼ pθ(z|x(i))  The Monte Carlo EM algorithm does not employ an encoder, instead it samples from the pos- terior of the latent variables using gradients of the posterior computed with ∇z log pθ(z|x) = ∇z log pθ(z) + ∇z log pθ(x|z). The Monte Carlo EM procedure consists of 10 HMC leapfrog steps with an automatically tuned stepsize such that the acceptance rate was 90%, followed by 5 weight updates steps using the acquired sample. For all algorithms the parameters were updated using the Adagrad stepsizes (with accompanying annealing schedule). The marginal likelihood was estimated with the ﬁrst 1000 datapoints from the train and test sets, for each datapoint sampling 50 values from the posterior of the latent variables using Hybrid Monte Carlo with 4 leapfrog steps.  F Full VB  As written in the paper, it is possible to perform variational inference on both the parameters θ and the latent variables z, as opposed to just the latent variables as we did in the paper. Here, we’ll derive our estimator for that case. Let pα(θ) be some hyperprior for the parameters introduced above, parameterized by α. The marginal likelihood can be written as:  log pα(X) = DKL(qφ(θ)||pα(θ|X)) + L(φ; X)  (13) where the ﬁrst RHS term denotes a KL divergence of the approximate from the true posterior, and where L(φ; X) denotes the variational lower bound to the marginal likelihood: qφ(θ) (log pθ(X) + log pα(θ) − log qφ(θ)) dθ  L(φ; X) =  (cid:90)  (14)  of a sum over the marginal likelihoods of individual datapoints log pθ(X) = (cid:80)N  Note that this is a lower bound since the KL divergence is non-negative; the bound equals the true marginal when the approximate and true posteriors match exactly. The term log pθ(X) is composed i=1 log pθ(x(i)),  which can each be rewritten as:  log pθ(x(i)) = DKL(qφ(z|x(i))||pθ(z|x(i))) + L(θ, φ; x(i))  (15)  12  where again the ﬁrst RHS term is the KL divergence of the approximate from the true posterior, and L(θ, φ; x) is the variational lower bound of the marginal likelihood of datapoint i: log pθ(x(i)|z) + log pθ(z) − log qφ(z|x)  L(θ, φ; x(i)) =  qφ(z|x)  (cid:16)  (cid:17)  (16)  dz  (cid:90)  The expectations on the RHS of eqs (14) and (16) can obviously be written as a sum of three separate expectations, of which the second and third component can sometimes be analytically solved, e.g. when both pθ(x) and qφ(z|x) are Gaussian. For generality we will here assume that each of these expectations is intractable. Under certain mild conditions outlined in section (see paper) for chosen approximate posteriors  where we choose a prior p((cid:15)) and a function gφ((cid:15), x) such that the following holds:  qφ(θ) and qφ(z|x) we can reparameterize conditional samples(cid:101)z ∼ qφ(z|x) as (cid:17) (cid:17)(cid:12)(cid:12)(cid:12)(cid:12)z=gφ((cid:15),x(i))  (cid:101)z = gφ((cid:15), x) with (cid:16)  log pθ(x(i)|z) + log pθ(z) − log qφ(z|x)  log pθ(x(i)|z) + log pθ(z) − log qφ(z|x)  (17)  d(cid:15)  (18)  L(θ, φ; x(i)) =  (cid:15) ∼ p((cid:15))  qφ(z|x)  (cid:90) (cid:90)  (cid:16)  p((cid:15))  dz  =  The same can be done for the approximate posterior qφ(θ):  (cid:101)θ = hφ(ζ) with  ζ ∼ p(ζ)  (19) where we, similarly as above, choose a prior p(ζ) and a function hφ(ζ) such that the following holds:  L(φ; X) =  =  qφ(θ) (log pθ(X) + log pα(θ) − log qφ(θ)) dθ p(ζ) (log pθ(X) + log pα(θ) − log qφ(θ))  (cid:12)(cid:12)(cid:12)(cid:12)θ=hφ(ζ)  dζ  (20)  (cid:90) (cid:90)  For notational conciseness we introduce a shorthand notation fφ(x, z, θ):  fφ(x, z, θ) = N · (log pθ(x|z) + log pθ(z) − log qφ(z|x)) + log pα(θ) − log qφ(θ)  (21) Using equations (20) and (18), the Monte Carlo estimate of the variational lower bound, given datapoint x(i), is:  fφ(x(l), gφ((cid:15)(l), x(l)), hφ(ζ (l)))  (22)  where (cid:15)(l) ∼ p((cid:15)) and ζ (l) ∼ p(ζ). The estimator only depends on samples from p((cid:15)) and p(ζ) which are obviously not inﬂuenced by φ, therefore the estimator can be differentiated w.r.t. φ. The resulting stochastic gradients can be used in conjunction with stochastic optimization methods such as SGD or Adagrad [DHS10]. See algorithm 1 for a basic approach to computing stochastic gradients.  F.1 Example  Let the prior over the parameters and latent variables be the centered isotropic Gaussian pα(θ) = N (z; 0, I) and pθ(z) = N (z; 0, I). Note that in this case, the prior lacks parameters. Let’s also assume that the true posteriors are approximatily Gaussian with an approximately diagonal covari- ance. In this case, we can let the variational approximate posteriors be multivariate Gaussians with a diagonal covariance structure:  L(cid:88)  l=1  L(φ; X) (cid:39) 1 L  log qφ(θ) = log N (θ; µθ, σ2 θI) log qφ(z|x) = log N (z; µz, σ2 zI)  13  (23)  Algorithm 2 Pseudocode for computing a stochastic gradient using our estimator. See text for meaning of the functions fφ, gφ and hφ. Require: φ (Current value of variational parameters)  g ← 0 for l is 1 to L do  x ← Random draw from dataset X (cid:15) ← Random draw from prior p((cid:15)) ζ ← Random draw from prior p(ζ) g ← g + 1  L∇φfφ(x, gφ((cid:15), x), hφ(ζ))  end for return g  where µz and σz are yet unspeciﬁed functions of x. Since they are Gaussian, we can parameterize the variational approximate posteriors:  (cid:101)θ = µθ + σθ (cid:12) ζ (cid:101)z = µz + σz (cid:12) (cid:15)  qφ(θ) qφ(z|x)  as as  where where  ζ ∼ N (0, I) (cid:15) ∼ N (0, I)  With (cid:12) we signify an element-wise product. These can be plugged into the lower bound deﬁned above (eqs (21) and (22)). In this case it is possible to construct an alternative estimator with a lower variance, since in this model pα(θ), pθ(z), qφ(θ) and qφ(z|x) are Gaussian, and therefore four terms of fφ can be solved analytically. The resulting estimator is:  L(φ; X) (cid:39) 1 L  N ·  1 + log((σ(l)  z,j)2) − (µ(l)  z,j)2 − (σ(l)  + log pθ(x(i)z(i))  z,j)2(cid:17)  θ,j)2(cid:17)   1  2  J(cid:88)  (cid:16)  j=1  L(cid:88) (cid:16) J(cid:88)  l=1  j=1    (24)  +  1 2  1 + log((σ(l)  θ,j)2) − (µ(l)  θ,j)2 − (σ(l)  µ(i) j and σ(i)  j  simply denote the j-th element of vectors µ(i) and σ(i).  14  ","How can we perform efficient inference and learning in directed probabilisticmodels, in the presence of continuous latent variables with intractableposterior distributions, and large datasets? We introduce a stochasticvariational inference and learning algorithm that scales to large datasets and,under some mild differentiability conditions, even works in the intractablecase. Our contributions is two-fold. First, we show that a reparameterizationof the variational lower bound yields a lower bound estimator that can bestraightforwardly optimized using standard stochastic gradient methods. Second,we show that for i.i.d. datasets with continuous latent variables perdatapoint, posterior inference can be made especially efficient by fitting anapproximate inference model (also called a recognition model) to theintractable posterior using the proposed lower bound estimator. Theoreticaladvantages are reflected in experimental results."
1312.6116,2014,Improving Deep Neural Networks with Probabilistic Maxout Units  ,"['Jost Tobias Springenberg', 'Martin Riedmiller']",https://arxiv.org/pdf/1312.6116.pdf,"4 1 0 2     b e F 9 1         ] L M  . t a t s [      2 v 6 1 1 6  .  2 1 3 1 : v i X r a  Improving Deep Neural Networks with Probabilistic  Maxout Units  Jost Tobias Springenberg and Martin Riedmiller  Department of Computer Science  University of Freiburg  79110, Freiburg im Breisgau, Germany  {springj,riedmiller}@cs.uni-freiburg.de  Abstract  We present a probabilistic variant of the recently introduced maxout unit. The suc- cess of deep neural networks utilizing maxout can partly be attributed to favorable performance under dropout, when compared to rectiﬁed linear units. It however also depends on the fact that each maxout unit performs a pooling operation over a group of linear transformations and is thus partially invariant to changes in its input. Starting from this observation we ask the question: Can the desirable prop- erties of maxout units be preserved while improving their invariance properties ? We argue that our probabilistic maxout (probout) units successfully achieve this balance. We quantitatively verify this claim and report classiﬁcation performance matching or exceeding the current state of the art on three challenging image clas- siﬁcation benchmarks (CIFAR-10, CIFAR-100 and SVHN).  1  Introduction  Regularization of large neural networks through stochastic model averaging was recently shown to be an effective tool against overﬁtting in supervised classiﬁcation tasks. Dropout [1] was the ﬁrst of these stochastic methods which led to improved performance on several benchmarks ranging from small to large scale classiﬁcation problems [2, 1]. The idea behind dropout is to randomly drop the activation of each unit within the network with a probability of 50%. This can be seen as an extreme form of bagging in which parameters are shared among models, and the number of trained models is exponential in the number of these model parameters. During testing an approximation is used to average over this large number of models without instantiating each of them. When combined with efﬁcient parallel implementations this procedure opened the possibility to train large neural networks with millions of parameters via back-propagation [2, 3] . Inspired by this success a number of other stochastic regularization techniques were recently devel- oped. This includes the work on dropconnect[4], a generalization of dropout, in which connections between units rather than their activation are dropped at random. Adaptive dropout [5] is a recently introduced variant of dropout in which the stochastic regularization is performed through a binary belief network that is learned alongside the neural network to decrease the information content of its hidden units. Stochastic pooling [6] is a technique applicable to convolutional networks in which the pooling operation is replaced with a sampling procedure. Instead of changing the regularizer the authors in [7] searched for an activation function for which dropout performs well. As a result they introduced the maxout unit, which can be seen as a gen- eralization of rectiﬁed linear units (ReLUs) [8, 9], that is especially suited for the model averaging performed by dropout. The success of maxout can partly be attributed to the fact that maxout aids the optimization procedure by partially preventing units from becoming inactive; an artifact caused by the thresholding performed by the rectiﬁed linear unit. Additionally, similar to ReLUs, they are  1  piecewise linear and – in contrast to e.g. sigmoid units – typically do not saturate, which makes networks containing maxout units easier to optimize. We argue that an equally important property of the maxout unit however is that its activation func- tion can be seen as performing a pooling operation over a subspace of k linear feature mappings (in the following referred to as subspace pooling). As a result of this subspace pooling operation each maxout unit is partially invariant to changes within its input. A natural question arising from this observation is thus whether it could be beneﬁcial to replace the maximum operation used in maxout units with other pooling operations, such as L2 pooling. The utility of different subspace pooling operations has already been explored in the context of unsupervised learning where e.g. L2-pooling is known give rise to interesting invariances [10, 11, 12]. While work on generalizing maxout by replacing the max-operation with general Lp-pooling exists [13], a deviation from the standard max- imum operation comes at the price of discarding some of the desirable properties of the maxout unit. For example abandoning piecewise linearity, restricting units to positive values and the introduction of saturation regimes, which potentially worsen the accuracy of the approximate model averaging performed by dropout. Based on these observations we propose a stochastic generalization of the maxout unit that pre- serves its desirable properties while improving the subspace pooling operation of each unit. As an additional beneﬁt when training a neural network using our proposed probabilistic maxout units the gradient of the training error is more evenly distributed among the linear feature mappings of each unit. In contrast, a maxout network helps gradient ﬂow through each of the maxout units but not through their k linear feature mappings. Compared to maxout our probabilistic units thus learn to better utilize their full k-dimensional subspace. We evaluate the classiﬁcation performance of a model consisting of these units and show that it matches the state of the art performance on three challenging classiﬁcation benchmarks.  2 Model Description  Before deﬁning the probabilistic maxout unit we brieﬂy review the notation used in the following for deﬁning deep neural network models. We adopt the standard feed-forward neural network formula- tion in which given an input x and desired output y (a class label) the network realizes a function computing a C-dimensional vector o – where C is the number of classes – predicting the desired output. The prediction is computed by ﬁrst sequentially mapping the input to a hierarchy of N hid- i within hidden layer l ∈ [1, N ] in the hierarchy realizes den layers h(1), . . . , h(N ). Each unit h(l) a function h(l) i ) mapping its inputs v (given either as the input x or the output of the previous layer h(l−1)) to an activation using weight and bias parameters w(l) . Finally the prediction is computed based on the last layer output hN . This prediction is realized using a softmax layer o = sof tmax(WN +1h(N ) + bN +1) with weights WN +1 and bias bN +1. All parameters θ = {W (1), b(1), . . . , W (N +1), b(N +1)} are then learned by minimizing the cross entropy loss be- i=1 yi log(oi) + (1 − yi)log(1 − oi).  tween output probabilities o and label y : L(o, y; x) = −(cid:80)C  i (v; w(l)  and b(l) i  , b(l)  i  i  2.1 Probabilistic Maxout Units  The maxout unit was recently introduced in [7] and can be formalized as follows: Given the units input v ∈ Rd (either the activation from the previous layer or the input vector) the activation of a maxout unit is computed by ﬁrst computing k linear feature mappings z ∈ Rk where  (1) and k is the number of linear sub-units combined by one maxout unit. Afterwards the output hmaxout of the maxout hidden unit is given as the maximum over the k feature mappings:  zi = wiv + bi,  hmaxout(v) = max[z1, . . . , zk].  (2)  When formalized like this it becomes clear that (in contrast to conventional activation functions) the maxout unit can be interpreted as performing a pooling operation over a k-dimensional subspace of linear units [z1, . . . , zk] each representing one transformation of the input v. This is similar to spatial max-pooling which is commonly employed in convolutional neural networks. However, unlike in  2  Figure 1: Schematic of different pooling operations. a) An exemplary input image taken from the ImageNet dataset together with the depiction of a spatial pooling region (cyan) as well as the input to one maxout / probout unit (marked in magenta). b) Spatial max-pooling proceeds by computing the maximum of one ﬁlter response at the four different positions from a). c) Maxout computes a pooled response of two linear ﬁlter mappings applied to one input patch. d) The activation of a probout unit is computed by sampling one of the linear responses according to their probability.  spatial pooling the maxout unit pools over a subspace of k different linear transformations applied to the same input v. In contrast to this, spatial max-pooling of linear feature maps would compute a pooling over one linear transformation applied to k different inputs. A schematic of the difference between several pooling operations is given in Fig. 1 . As such maxout is thus more similar to the subspace pooling operations used for example in topo- graphic ICA [10] which is known to result in partial invariance to changes within its input. On the basis of this observation we propose a stochastic generalization of the maxout unit that preserves its desirable properties while improving gradient propagation among the k linear feature mappings as well as the invariance properties of each unit. In the following we call these generalized units probout units since they are a direct probabilistic generalization of maxout. We derive the probout unit activation function from the maxout formulation by replacing the maxi- mum operation in Eq. (2) with a probabilistic sampling procedure. More speciﬁcally we assume a Boltzmann distribution over the k linear feature mappings and sample the activation h(v) from the activation of the corresponding subspace units. To this end we ﬁrst deﬁne a probability for each of the k linear units in the subspace as:  eλzi(cid:80)k  j=1 eλzj  pi =  ,  (3)  where λ is a hyperparameter (referred to as an inverse temperature parameter) controlling the vari- ance of the distribution. The activation hprobout(x) is then sampled as  hprobout(v) = zi, where i ∼ M ultinomial{p1, . . . , pk}.  (4)  Comparing Eq. (4) to Eq. (2) we see that both, are not bounded from above or below and their activation is always given as one of the linear feature mappings within their subspace. The probout unit hence preserves most of the properties of the maxout unit, only replacing the sub-unit selection mechanism. We can further see that Eq. (4) reduces to the maxout activation for λ → ∞. For other values of λ the probout unit will behave similarly to maxout when the activation of one linear unit in the subspace dominates. However, if the activation of multiple linear units differs only slightly they will be selected with almost equal probability. Futhermore, each active linear unit will have a chance to be selected. The sampling approach therefore ensures that gradient ﬂows through each of the k linear subspace units of a given probout unit for some examples (given that λ is sufﬁciently small). We hence argue that probout units can learn to better utilize their full k-dimensional subspace. In practice we want to combine the probout units described by Eq. (4) with dropout for regularizing the learned model. To achieve this we directly include dropout in the probabilistic sampling step by  3  1.80.90.1-0.5max1.8a) Imageb) spatial poolingc) maxout (subspace pooling)ﬁlterﬁlters0.8-0.3max0.8ﬁlters0.8-0.30.8zz0.750.25sample: i ~ e.g. i = 1 d) probout (subspace pooling)probabilitiesre-deﬁning the probabilities as:  ˆp0 = 0.5  ˆpi =  2 ·(cid:80)k  eλzi j=1 eλzj  .  Consequently, we sample the probout activation function including dropout ˆhprobout(v) as  ˆhprobout(v) =  , where i ∼ M ultinomial{ˆp0, ˆp1, . . . , ˆpk}.  (cid:26)0 if i = 0  zi else  (5)  (6)  (7)  2.2 Relation to other pooling operations  The idea of using a stochastic pooling operation has been explored in the context of spatial pooling within the machine learning literature before. Among this work the approach most similar to ours is [14]. There the authors introduced a probabilistic pooling approach in order to derive a convolutional deep believe network (DBN). They also use a Boltzmann distribution based on unit activations to calculate a sampling probability. The main difference between their work and ours is that they calculate the probability of sampling one unit at different spatial locations whereas we calculate the probability of sampling a unit among k units forming a subspace at one spatial location. Another difference is that we forward propagate the sampled activation zi whereas they use the calculated probability to activate a binary stochastic unit. Another approach closely related to our work is the stochastic pooling presented in [6]. Their stochastic pooling operation samples the activation of a pooling unit pi proportionally to the ac- tivation a of a rectiﬁed linear unit [8] computed at different spatial positions. This is similar to Eq. (4) in the sense that the activation is sampled from a set of different activations. Similar to [14] it however differs in that the sampling is performed over spatial locations rather than activations of different units. It should be noted that our work also bears some resemblance to recent work on training stochastic units, embedded in an autoencoder network, via back-propagation [15, 16]. In contrast to their work, which aims at using stochastic neurons to train a generative model, we embrace stochasticity in the subspace pooling operation as an effective means to regularize a discriminative model.  2.3  Inference  At test time we need to account for the stochastic nature of a neural network containing probout units. During a forward pass through the network the value of each probout unit is sampled from one of k values according to their probability. The output of such a forward pass thus always represents only one of kM different instantiations of the trained probout network; where M is the number of probout units in the network. When combined with dropout the number of possible instantiations increases to (k + 1)M . Evaluating all possible models at test time is therefore clearly infeasible. The Dropout formulation from [1] deals with this large amount of possible models by removing dropout at test time and halving the weights of each unit. If the network consists of only one softmax layer then this modiﬁed network performs exact model averaging [1]. For general models this computation is merely an approximation of the true model average which, however, performs well in practice for both deep ReLU networks [2] and the maxout model [7]. We adopt the same procedure of halving the weights for removing the inﬂuence of dropout at test- i=1 ˆpi = 1 and ˆp0 = 0, effectively replacing the sampling from Eq .(7) with Eq. (4). We further observe that from the kM models remaining after removing dropout only few models will be instantiated with high probability. We therefore resort to sampling a small number of outputs o from the networks softmax layer and average their values. An evaluation of the exact effect of this model averaging can be found in Section 3.1.1 .  time and rescale the probabilities such that(cid:80)k  3 Evaluation  We evaluate our method on three different image classiﬁcation datasets (CIFAR-10, CIFAR-100 and SVHN) comparing it against the basic maxout model as well as the current state of the art on  4  Figure 2: Visualization of pairs of ﬁrst layer linear ﬁlters learned by the maxout model (left) as well as the probout model (right). In contrast to the maxout ﬁlters the ﬁlter pairs learned by the probout model appear to mostly be transformed versions of each other.  all datasets. All experiments were performed using an implementation based on Theano and the pylearn2 library [17] using the fast convoltion code of [2]. We use mini-batch stochastic gradient descent with a batch size of 100. For each of the datasets we start with the same network used in [7] – retaining all of their hyperparameter choices – to ensure comparability between results. We replace the maxout units in the network with probout units and choose one λ(l) via crossvalidation for each layer l in a preliminary experiment on CIFAR-10.  3.1 Experiments on CIFAR-10  We begin our experiments with the CIFAR-10 [18] dataset. It consists of 50, 000 training images and 10, 000 test images that are grouped into 10 categories. Each of these images is of size 32 × 32 pixels and contains 3 color channels. Maxout is known to yield good performance on this dataset, making it an ideal starting point for evaluating the difference between maxout and probout units.  3.1.1 Effect of replacing maxout with probout units  We conducted a preliminary experiment to evaluate the effect of the probout parameters λ(l) on the performance and compare it to the standard maxout model. For this purpose we use a ﬁve layer model consisting of three convolutional layers with 48, 128 and 128 probout units respectively which pool over 2 linear units each. The penultimate layer then consists of 240 probout units pooling over a subspace of 5 linear units. The ﬁnal layer is a standard softmax layer mapping from the 240 units in the penultimate layer to the 10 classes of CIFAR-10. The receptive ﬁelds of units in the convolutional layers are 8, 8 and 5 respectively. Additionally, spatial max-pooling is performed after each convolutional layer with pooling size of 4 × 4, 4 × 4 and 2 × 2 using a stride of 2 in all layers. We split the CIFAR-10 training data retaining the ﬁrst 40000 samples for training and using the last 10000 samples as a validation set. We start our evaluation by using probout units everywhere in the network and cross-validate the choice of the inverse-temperature parameters λ(l) ∈ {0.1, 0.5, 1, 2, 3, 4} keeping all other hyperpa- rameters ﬁxed. We ﬁnd that annealing the λ(l) parameter during training to a lower value improved performance for all λ(l) > 0.5 and hence linearly decrease λ(l) to a value that is 0.9 lower than the initial λ in these cases. As shown in Fig. 3a the best classiﬁcation performance is achieved when λ is set to allow higher variance sampling for the ﬁrst two layers, speciﬁcally when λ(1) = 1 and λ(2) = 2. For the third as well as the fully connected layer we observe a performance increase when λ(3) is chosen as λ(3) = 3 and λ(4) = 4, meaning that the sampling procedure selects the maximum value with high probability. This indicates that the probabilistic sampling is most effective in lower layers. We veriﬁed this by replacing the probout units in the last two layers with maxout units which did not signiﬁcantly decrease classiﬁcation accuracy. We hypothesize that increasing the probability of sampling a non maximal linear unit in the subspace pulls the units in the subspace closer together and forces the network to become “more invariant” to changes within this subspace. This is a property that is desired in lower layers but might turn to be detrimental in higher layers where the model averaging effect of maxout is more important than achieving invariance. Here sampling units with non-maximal activation could result in unwanted correlation between the “submodels”. To qualitatively verify this claim we plot the ﬁrst layer linear ﬁlters learned using probout units alongside the ﬁlters learned by a model consisting only of maxout  5  w1w2w1w2) t e s  n o i t a d i l a v (  r o r r E  . s s a l C  13.4 13.3 13.2 13.1 13 12.9 12.8 12.7 12.6 12.5  l = 2 (with λ(1) = 1) l = 1 (with λ(2) = 2)  r o r r e n o i t a c ﬁ i s s a l C  15  14  13  Maxout baseline Probout Maxout + sampling  0.1 0.5 1  2 λ(l)  (a)  3  4  1 510 20 30 40 50 60 70 80 90 100  Model evaluations  (b)  Figure 3: (a) Validation of the λ(l) parameter for layers l ∈ [1, 2] on CIFAR-10. We plot the error on the validation set after training (using 50 model evaluations). When evaluating the choice of λ(1) (red curve) the second parameter ﬁxed λ(2) = 2. Likewise, for the experiments regarding λ(2) (blue curve) λ(1) = 1. (b) Evolution of the classiﬁcation error and standard deviation on the CIFAR- 10 dataset for a changing number E of model evaluations. We average the activation o ∈ RC of the softmax layer over all E evaluations and compute the predicted class label ˆy as the maximum ˆy = arg maxi∈{1,...,C} oi. The standard deviation is computed over 10 runs of E model evaluations.  units in Fig. 2. When inspecting the ﬁlters we can see that many of the ﬁlters belonging to one subspace formed by a probout unit seem to be transformed versions of each other, with some of then resembling “quadrature pairs” of ﬁlters. Among the linear ﬁlters learned by the maxout model some also appear to encode invariance to local transformations. Most of the ﬁlters contained in a subspace however are seemingly unrelated. To support this observation empirically we probed for changes in the feature vectors of different layers (extracted from both maxout and probout models) when they are applied to translated and rotated images from the validation set. Similar to [19, 3] we calculate the normalized Euclidean distance between feature vectors extracted from an unchanged image and a transformed version. We then plot these distances for several exemplary images as well as the mean over 100 randomly sampled images. The result of this experiment is given in Fig. 4, showing that introducing probout units into the network has a moderate positive effect on both invariance to translation and rotations. Finally, we evaluate the computational cost of the model averaging procedure described in Section 2.3 at test time. As depicted in Fig. 3b the classiﬁcation error for the probout model decreases with more model evaluations saturating when a moderate amount of 50 evaluations is reached. Con- versely, using sampling at test time in conjunction with the standard maxout model signiﬁcantly decreases performance. This indicates that the maxout model is highly optimized for the maximum responses and cannot deal with the noise introduced through the sampling procedure. We addition- ally also tried to replace the model averaging mechanism with cheaper approximations. Replacing the sampling in the probout units with a maximum operation at test time resulted in a decrease in performance, reaching 14.13%. We also tried to use probability weighting during testing [6] which however performed even worse, achieving 15.21%.  3.1.2 Evaluation of Classiﬁcation Performance  As the next step, we evaluate the performance of our model on the full CIFAR-10 benchmark. We follow the same protocol as in [7] to train the probout model. That is, we ﬁrst preprocess all images by applying contrast normalization followed by ZCA whitening. We then train our model using the ﬁrst 40000 examples from the training set using the last 10000 examples as a validation set. Training then proceeds until the validation error stops decreasing. We then retrain the model on the complete training set for the same amount of epochs it took to reach the best validation error. To comply with the experiments in [7] we used a larger version of the model from Section 3.1.1 in all experiments. Compared to the preliminary experiment the size of the convolutional layers was  6  Table 1: Classiﬁcation error of different models on the CIFAR-10 dataset.  METHOD CONV. NET + SPEARMINT [20] CONV. NET + MAXOUT [7] CONV. NET + PROBOUT 12 × CONV. NET + DROPCONNECT [4] CONV. NET + MAXOUT [7] CONV. NET + PROBOUT  ERROR 14.98 % 11.69 % 11.35 % 9.32 % 9.38 % 9.39 %  increased to 96, 192 and 192 units respectively. The size of the fully connected layer was increased to 500 probout units pooling over a 5 dimensional subspace. The top half of Table 1 shows the result of training this model as well as other recent results. We achieve an error of 11.35%, slightly better than – but statistically tied to – the previous state of the art given by the maxout model. We also evaluated the performance of this model when the training data is augmented with additional transformed training examples. For this purpose we train our model using the original training images as well as add randomly translated and horizontally ﬂipped versions of the images. The bottom half of Table 1 shows a comparison of different results for training on CIFAR-10 with additional data augmentation. Using this augmentation process we achieve a classiﬁcation error of 9.39%, matching, but not outperforming the maxout result.  3.2 CIFAR-100  The images contained in the CIFAR-100 dataset [18] are – just as the CIFAR-10 images – taken from a subset of the 10-million images database. The dataset contains 50, 000 training and 10, 000 test examples of size 32× 32 pixels each. The dataset is hence similar to CIFAR-10 in both size and image content. It, however, differs from CIFAR-10 in its label distribution. Concretely, CIFAR-100 contains images of 100 classes grouped into 20 “super-classes”. The training data therefore contains 500 training images per class – 10 times less examples per class than in CIFAR-10 – which are accompanied by 100 examples in the test-set. We do not make use of the 20 super-classes and train a model using a similar setup to the experiments we carried out on CIFAR-10. Speciﬁcally, we use the same preprocessing and training procedure (determining the amount of epochs using a validation set and then retraining the model on the com- plete data). The same network as in Section 3.1.1 was used for this experiment (adapted to classify 100 classes). Again, this is the same architecture used in [7] thus ensuring comparability between results. During testing we use 50 model evaluations to average over the sampled probout units. The result of this experiment is given in Table 2. In agreement with the CIFAR-10 results our model performs marginally better than the maxout model (by 0.45%1). As also shown in the table the current best method on CIFAR-100 achieves a classiﬁcation error of 36.85% [21], using a larger convolutional neural network together with a tree-based prior on the classes formed by utilizing the super-classes. A similar performance increase could potentially be achieved by combining their tree-based prior with our model.  3.3 SVHN  The street view house numbers dataset [22] is a collection of images depicting digits which were obtained from google street view images. The dataset comes in two variants of which we restrict ourselves to the one containing cropped 32 × 32 pixel images. Similar to the well known MNIST dataset [23] the task for this dataset is to classify each image as one of 10 digits in the range from 0 to 9. The task is considerably more difﬁcult than MNIST since the images are cropped out of natural image data. The images thus contain color information and show signiﬁcant contrast vari-  1While we were writing this manuscript it came to our attention that the experiments on CIFAR-100 in [7] were carried out using a different preprocessing than mentioned in the original paper. To ensure that this does not substantially effect our comparison we ran their experiment using the same preprocessing used in our experiments. This resulted in a slightly improved classiﬁcation error of 38.50%.  7  Table 2: Classiﬁcation error of different models on the CIFAR-100 dataset.  METHOD RECEPTIVE FIELD LEARNING [24] LEARNED POOLING [25] CONV. NET + STOCHASTIC POOLING [6] CONV. NET + DROPOUT + TREE [21] CONV. NET + MAXOUT [7] CONV. NET + PROBOUT  ERROR 45.17 % 43.71 % 42.51 % 36.85 % 38.57 % 38.14 %  ation. Furthermore, although centered on one digit, several images contain multiple visible digits, complicating the classiﬁcation task. The training and test set contain 73, 257 and 20, 032 labeled examples respectively. In addition to this data there is an “extra” set of 531, 131 labeled digits which are somewhat less difﬁcult to differentiate and can be used as additional training data. As in [7] we build a validation set by selecting 400 examples per class from the training and 200 examples per class from the extra dataset. We conﬂate all remaining training images to a large set of 598, 388 images which we use for training. The model trained for this task consists of three convolutional layers containing 64, 128 and 128 units respectively, pooling over a 2 dimensional subspace. These are followed by a fully connected and a softmax layer of which the fully connected layer contains 400 units pooling over a 5 dimen- sional subspace. This yields a classiﬁcation error of 2.39% (using 50 model evaluations at test- time), matching the current state of the art for a model trained on SVHN without data augmentation achieved by the maxout model (2.47%). A comparison to other results can be found in Table 3 . This includes the current best result with data augmentation which was obtained using a generalization of dropout in conjunction with a large network containing rectiﬁed linear units [4]. Table 3: Classiﬁcation error of different models on the SVHN dataset. The top half shows a compar- ison of our result with the current state of the art achieved without data augmentation. The bottom half gives the best performance achieved with data augmentation as additional reference.  METHOD CONV. NET + STOCHASTIC POOLING [6] CONV. NET + DROPOUT [26] CONV. NET + MAXOUT [7] CONV. NET + PROBOUT CONV. NET + DROPOUT [26] 5 × CONV. NET + DROPCONNECT [4]  ERROR 2.80 % 2.78 % 2.47 % 2.39 % 2.68 % 1.93 %  4 Conclusion  We presented a probabilistic version of the recently introduced maxout unit. A model built using these units was shown to yield competitive performance on three challenging datasets (CIFAR-10, CIFAR-100, SVHN). As it stands, replacing maxout units with probout units is computationally expensive at test time. This problem could be diminished by developing an approximate inference scheme similar to [2, 3] which we see as an interesting possibility for future work. We see our approach as part of a larger body of work on exploring the utility of learning “complex cell like” units which can give rise to interesting invariances in neural networks. While this paradigm has extensively been studied in unsupervised learning it is less explored in the supervised scenario. We believe that work towards building activation functions incorporating such invariance properties, while at the same time designed for use with efﬁcient model averaging techniques such as dropout, is a worthwhile endeavor for advancing the ﬁeld.  Acknowledgments  The authors want to thank Alexey Dosovistkiy for helpful discussions and comments, as well as Thomas Brox for generously providing additional computing resources.  8  References [1] Alex Krizhevsky Ilya Sutskever Ruslan R. Salakhutdinov Geoffrey E. Hinton, Nitish Srivastava. Improv-  ing neural networks by preventing co-adaptation of feature detectors. arxiv:cs/1207.0580v3.  [2] Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.  Imagenet classiﬁcation with deep convolutional  neural networks. In Advances in Neural Information Processing Systems 25. 2012.  [3] Matthew Zeiler and Rob Fergus.  arxiv:cs/1311.2901v3.  Visualizing and understanding convolutional networks.  [4] Li Wan, Matthew D. Zeiler, Sixin Zhang, Yann LeCun, and Rob Fergus. Regularization of neural networks  using dropconnect. In International Conference on Machine Learning (ICML), 2013.  [5] Jimmy Ba and Brendan Frey. Adaptive dropout for training deep neural networks. In Advances in Neural  Information Processing Systems 26. 2013.  [6] Matthew D. Zeiler and Rob Fergus. Stochastic pooling for regularization of deep convolutional neural  networks. In International Conference on Learning Representations (ICLR): Workshop track, 2013.  [7] Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout  networks. In International Conference on Machine Learning (ICML), 2013.  [8] Vinod Nair and Geoffrey E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In  International Conference on Machine Learning (ICML), 2010.  [9] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectiﬁer neural networks. In AISTATS  2011, April 2011.  [10] Jarmo Hurri Aapo Hyvrinen and Patrik O. Hoyer. Natural Image Statistics. [11] Yoshua Bengio and James S. Bergstra. Slow, decorrelated features for pretraining complex cell-like  networks. In Advances in Neural Information Processing Systems 22. 2009.  [12] Will Y. Zou, Shenghuo Zhu, Andrew Y. Ng, and Kai Yu. Deep learning of invariant features via simulated  ﬁxations in video. In Neural Information Processing Systems (NIPS 2012), 2012.  [13] Razvan Pascanu Yoshua Bengio Caglar Gulcehre, Kyunghyun Cho. Learned-norm pooling for deep neural  networks. arxiv:stat/1311.1780v3.  [14] Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y Ng. Convolutional deep belief networks  for scalable unsupervised learning of hierarchical representations. pages 1–8, 2009.  [15] Yoshua Bengio. Estimating or propagating gradients through stochastic neurons. [16] Jason Yosinski Yoshua Bengio, ric Thibodeau-Laufer. Deep generative stochastic networks trainable by  backprop.  [17] Pascal Lamblin Vincent Dumoulin Mehdi Mirza Razvan Pascanu James Bergstra Frdric Bastien Yoshua Bengio Ian J. Goodfellow, David Warde-Farley. Pylearn2: a machine learning research library. arxiv:stat/1308.4214.  [18] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. 2009. [19] Koray Kavukcuoglu, Marc’Aurelio Ranzato, Rob Fergus, and Yann LeCun. Learning invariant features In Proc. International Conference on Computer Vision and Pattern  through topographic ﬁlter maps. Recognition (CVPR), 2009.  [20] Jasper Snoek, Hugo Larochelle, and Ryan Prescott Adams. Practical bayesian optimization of machine  learning algorithms. In Advances in Neural Information Processing Systems 25, 12/2012 2012.  [21] Nitish Srivastava and Ruslan Salakhutdinov. Discriminative transfer learning with tree-based priors. In  Advances in Neural Information Processing Systems 26, pages 2094–2102. 2013.  [22] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011, 2011.  [23] Yann LeCun, Lon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to docu-  ment recognition. In Proceedings of the IEEE, number 11, 1998.  [24] Yangqing Jia, Chang Huang, and Trevor Darrell. Beyond spatial pyramids: Receptive ﬁeld learning for  pooled image features. In CVPR, 2012.  [25] Mateusz Malinowski and Mario Fritz. Learnable pooling regions for image classiﬁcation. In International  Conference on Learning Representations (ICLR): Workshop track, 2013.  [26] Nitish Srivastava. Improving neural networks with dropout. In Master’s thesis, University of Toronto,  2013.  9  1.6 1.4 1.2 1 0.8 0.6 0.4 0.2  e c n a t s i D  e c n a t s i D n a e  M  2 1.8 1.6 1.4 1.2 1 0.8 0.6 0.4 0.2  (a)  (b)  e c n a t s i D  2 1.8 1.6 1.4 1.2 1 0.8 0.6 0.4 0.2  Horse Bird Truck Cat  −15 −10 −5  10 Vertical translation (pixels)  0  5  15  (c)  Horse Bird Truck Cat  2 1.8 1.6 1.4 1.2 1 0.8 0.6 0.4 0.2  1.6 1.4 1.2 1 0.8 0.6 0.4 0.2  e c n a t s i D  e c n a t s i D  Horse Bird Truck Cat  −15 −10 −5  10 Vertical translation (pixels)  0  5  15  (d)  Horse Bird Truck Cat  0  45 90 135 180 225 270 315 360  0  45 90 135 180 225 270 315 360  Rotation Angle (Degrees)  Rotation Angle (Degrees)  (e)  Conv. Layer 1 Conv. Layer 2 Fully Connected Layer 4  −15 −10 −5  10 Vertical translation (pixels)  0  5  (f)  1.6 1.4 1.2 1 0.8 0.6 0.4 0.2  e c n a t s i D n a e  M  Conv. Layer 1 Conv. Layer 2 Fully Connected Layer 4  15  0  45 90 135 180 225 270 315 360  Rotation Angle (Degrees)  (g)  (h)  Figure 4: Analysis of the impact of vertical translation and rotation on features extracted from a maxout and probout network. We plot the distance between normalized feature vectors extracted on transformed images and the original, unchanged, image. The distances for the probout model are plotted using thick lines. The distances for the maxout model are depicted using dashed lines. (a,b) 4 exemplary images undergoing different vertical translations and rotations respectively. (c,d) Euclidean distance between feature vectors from the original 4 images depicted in (a,b) and trans- formed images for Layer 1 (convolutional) and Layer 4 (fully connected) respectively. (e,f) Eu- clidean distance between feature vectors from the original 4 images and transformed versions for Layer 2 (convolutional) and Layer 4 (fully connected) respectively. (g,h) Mean Euclidean distance between feature vectors extracted from 100 randomly selected images and their transformed versions for different layers in the network.  10  ","We present a probabilistic variant of the recently introduced maxout unit.The success of deep neural networks utilizing maxout can partly be attributedto favorable performance under dropout, when compared to rectified linearunits. It however also depends on the fact that each maxout unit performs apooling operation over a group of linear transformations and is thus partiallyinvariant to changes in its input. Starting from this observation we ask thequestion: Can the desirable properties of maxout units be preserved whileimproving their invariance properties ? We argue that our probabilistic maxout(probout) units successfully achieve this balance. We quantitatively verifythis claim and report classification performance matching or exceeding thecurrent state of the art on three challenging image classification benchmarks(CIFAR-10, CIFAR-100 and SVHN)."
1312.5985,2014,Learning Type-Driven Tensor-Based Meaning Representations  ,"['Tamara Polajnar', 'Luana Fagarasan', 'Stephen Clark']",https://arxiv.org/pdf/1312.5985.pdf,"4 1 0 2     b e F 8 1         ] L C . s c [      2 v 5 8 9 5  .  2 1 3 1 : v i X r a  Learning Type-Driven Tensor-Based Meaning  Representations  Tamara Polajnar and Luana Fˇagˇaras¸an and Stephen Clark  University of Cambridge  Computer Laboratory  first.last@cl.cam.ac.uk  Abstract  This paper investigates the learning of 3rd-order tensors representing the seman- tics of transitive verbs. The meaning representations are part of a type-driven tensor-based semantic framework, from the newly emerging ﬁeld of compositional distributional semantics. Standard techniques from the neural networks literature are used to learn the tensors, which are tested on a selectional preference-style task with a simple 2-dimensional sentence space. Promising results are obtained against a competitive corpus-based baseline. We argue that extending this work beyond transitive verbs, and to higher-dimensional sentence spaces, is an interest- ing and challenging problem for the machine learning community to consider.  1  Introduction  An emerging subﬁeld of natural language processing and computational linguistics is concerned with learning compositional distributional representations of meaning [23, 3, 10, 18, 9, 28, 7]. The advantage of such representations lies in their potential to combine the beneﬁts of distributional approachs to word meaning [26, 31] with the more traditional compositional methods from formal semantics [13]. Distributional representations have the properties of robustness, learnability from data, ease of handling ambiguity, and the ability to represent gradations of meaning; whereas com- positional models handle the unbounded nature of natural language, as well as providing established accounts of logical words, quantiﬁcation, and inference. One promising approach which attempts to combine elements of compositional and distributional semantics is by Coecke et al. [10]. The underlying idea is to take the type-driven approach from formal semantics — in particular the idea that the meanings of complex grammatical types should be represented as functions — and apply it to distributional representations. Since the mathematics of distributional semantics is provided by linear algebra, a natural set of functions to consider is the set of linear maps. Coecke et al. recognize that there is a natural correspondence from complex grammatical types to tensors (multi-linear maps), so that the meaning of an adjective, for example, is represented by a matrix (a 2nd-order tensor)1 and the meaning of a transitive verb is represented by a 3rd-order tensor. Coecke et al. use the grammar of pregroups as the syntactic machinery to construct distributional meaning representations, since both pregroups and vector spaces can be seen as examples of the same abstract structure, which leads to a particularly clean mathematical de- scription of the compositional process. However, the approach applies more generally, for example to other forms of categorial grammar, such as Combinatory Categorial Grammar [29], and also to phrase-structure grammars in a way that a formal linguist would recognize [2]. Clark [7] provides a description of the tensor-based framework aimed more at computational linguists, relying only on the mathematics of multi-linear algebra rather than the category theory used in [10]. Section 2 repeats some of this description.  1This same insight lies behind the work of Baroni and Zamparelli [3].  1  A major open question associated with the tensor-based semantic framework is how to learn the tensors representing the meanings of words with complex types, such as verbs and adjectives. The framework is essentially a compositional framework, providing a recipe for how to combine distri- butional representations, but leaving open what the underlying vector spaces are and how they can be acquired. One signiﬁcant challenge is an engineering one: in a wide-coverage grammar able to handle naturally occurring text, there will be a) a large lexicon with many word-category pairs re- quiring tensor representations; and b) many higher-order tensors with large numbers of parameters which need to be learned. In this paper we take a ﬁrst step towards learning such representations, by learning tensors for transitive verbs. One feature of the tensor-based framework is that it allows the meanings of words and phrases with different basic types, for example nouns and sentences, to live in different vector spaces; but this means that the sentence space must be speciﬁed in advance. In this paper we consider a simple sentence space: the “plausibility space” described by Clark [7], represented here as a probability distribution (and hence having only 2 dimensions). Logistic regression is used to learn a plausibility classiﬁer. We begin with this simple space since we want to see the extent to which the tensor-based representations can be learned at all. One goal of this paper is to introduce the problem of learning tensor-based semantic representations to the ML community. Current methods, for example the work of Socher [28], typically use only matrix representations, and also assume that words, phrases and sentences all live in the same vector space. The tensor-based semantic framework is more ﬂexible, in that it allows different spaces for different grammatical types, which results from it being tied more closely to a type-driven syntactic description; however, this ﬂexibility comes at a price, since there are many more paramaters to learn. Various communities are beginning to recognize the additional power that tensor representations can provide, through the capturing of interactions that are difﬁcult to represent with vectors and matrices (see e.g. [25, 30, 11]). Hierarchical recursive structures in language potentially represent a large number of such interactions (the obvious example for this paper being the interaction between a transitive verb’s subject and object), and present a signiﬁcant challenge for machine learning.  2 Syntactic Types to Tensors The syntactic type of a transitive verb in English is (S\NP )/NP (using Steedman [29] notation), meaning that a transitive verb is a function which takes an NP argument to the right, an NP argu- ment to the left, and results in a sentence S. Such categories with slashes are complex categories; S and NP are basic or atomic categories. Interpreting such categories under the Coecke et al. frame- work is straightforward. First, for each atomic category there is a corresponding vector space; in this case the sentence space S and the noun space N.2 Hence the meaning of a noun or noun phrase, for −−−→people ∈ N. In order to obtain the meaning of a example people, will be a vector in the noun space: transitive verb, each slash is replaced with a tensor product, so that the meaning of eat, for example, is a 3rd-order tensor: eat ∈ S ⊗ N ⊗ N. Just as in the syntactic case, the meaning of a transitive verb is a function (a multi-linear map) which takes two noun vectors as arguments and returns a sentence vector. Meanings combine using tensor contraction, which can be thought of as a multi-linear generalisation of matrix multiplication [17]. Consider ﬁrst the adjective-noun case, for example black cat. The syntactic type of black is N /N ; hence its meaning is a 2nd-order tensor (matrix): black ∈ N ⊗ N. In the syntax, N /N combines with N using the rule of forward application (N /N N ⇒ N ), which is an instance of function application. Function application is also used in the tensor-based semantics, which, for a matrix and vector argument, corresponds to matrix multiplication. Figure 1 shows how the syntactic types combine with a transitive verb, and the corresponding tensor- based semantic types. Note that, after the verb has combined with its object NP, the type of the verb phrase is S\NP, with a corresponding meaning tensor (matrix) in S⊗N. This matrix then combines with the subject vector, through matrix multiplication, to give a sentence vector. In practice, using for example the wide-coverage grammar from CCGbank [19], there will be many types with more than 3 slashes, with corresponding higher-order tensors. For example, a common  2In practice, for example using the CCG parser of Clark and Curran [8], there will be additional atomic  categories, such as PP, but not many more.  2  eat  people ﬁsh NP (S\NP )/NP NP N S ⊗ N ⊗ N N  S\NP S ⊗ N S S  >  <  Figure 1: Syntactic reduction and tensor-based semantic types for a transitive verb sentence  category for a preposition is the following: ((S\NP )\(S\NP ))/NP, which would be assigned to with in eat with a fork. (The way to read the syntactic type is as follows: with requires an NP argument to the right – a fork in this example – and then a verb phrase to the left – eat with type S\NP – resulting in a verb phrase.) The corresponding meaning tensor lives in the space S ⊗ N ⊗ S ⊗ N ⊗ N, i.e. a 5th-order tensor. Categories with even more slashes are not uncommon, for example ((N /N )/(N /N ))/((N /N )/(N /N )). Clearly learning parameters for such tensors is highly challenging, and it is likely that lower dimensional approximations will be required. We leave investigation of such approximations to future work.  3 Verb and Sentence Representation  As described above, in this paper we have chosen to focus on a two-dimensional “plausibility space” for the meanings of sentences. One way to think of this space is the simplest extension of truth values from the traditional truth-theoretic account to a real-valued setting.3 We also focus on the plausibility of transitive verb sentences with the simple subject verb object (SVO) grammatical structure, for example people eat ﬁsh (as in Figure 1). These sentences were generated automatically by ﬁnding speciﬁc transitive verbs in a dependency-parsed corpus and extracting the head nouns from the subject and the object (see Section 4). The nouns have atomic syntactic types and are represented by distributional semantic vectors, built using standard techniques [31], while the verb is a multi-linear map that takes in two nouns and outputs values in the plausibility space. We deﬁne the plausibility space to have two dimensions, one corresponding to plausible and one corresponding to implausible. Hence the verb tensor outputs two real values for each subject-verb- object triple. If the vectors in noun space have dimensionality K and the sentence space has di- mensionality S (two in this case), then the verb is a K × K × S tensor. We add some additional processing to the tensor network, following standard practice in neural networks and following [20], by passing the output values though a non-linear sigmoid function, and then creating a probability distribution over over two classes, plausible ((cid:62)) and implausible (⊥), using a softmax function. In Section 3.1, we propose a two-class logistic regression classiﬁer for simultaneous learning of the verb function and the plausibility space. This method was introduced in [20], but only imple- mented as a proof-of-concept with vectors of length 2 and small, manually created datasets based on propositional logic examples. In order to make the learning practical, given the large numbers of contextual features in the noun vectors, we employ a technique (described in Section 4.2) that im- proves low-dimensional singular value decomposition (SVD) [12], and thus enables us to effectively limit the number of parameters while learning from corpus-sized data. As a baseline we adapted a method from [18], where the verb is represented as the average of the Kronecker products of the subject and object vectors from the positive training data. This method does not produce a plausibil- ity space, but plausibility of the subject-verb-object triple can be calculated using cosine similarity (see Section 3.2).  3.1 Tensor learning  Following [20], we learn the tensor values as parameters (V) of a regression algorithm. To represent this space as a distribution over two classes ((cid:62),⊥) we apply a sigmoid (σ) to restrict the output to the [0,1] range and the softmax activation function (g) to balance the class probabilities. The full  3We are not too concerned with the philosophical interpretation of these plausibility values; rather we see  the plausibility sentence space as a useful inital testbed for the tensor-based semantic framework.  3  parameter set which we need to optimise for is B = {V, Θ}, where Θ = {θ(cid:62), θ⊥} are the softmax parameters for the two classes. For each verb we optimise the KL-divergence L between the training labels ti and classiﬁer predictions using:  L(cid:0)ti, g(cid:0)σ(cid:0)(ni  o)T(cid:1) , Θ(cid:1)(cid:1) +  s)V(ni  N(cid:88)  i=1  O(B) =  ||B||2  λ 2  (1)  s and ni  o are the subject and object of the training instance i ∈ N. The gold-standard where ni distribution over training labels is deﬁned as (1, 0) or (0, 1), depending on whether the training instance is a positive (plausible) or negative (implausible) example. The derivatives are obtained via the chain rule with respect to each set of parameters and gradient descent is performed using the Adagrad algorithm [14]. Tensor contraction is implemented using the Matlab Tensor Toolbox [1].  3.2 Baseline  The baseline is a simple corpus-driven approach of generating a matrix from an average of Kro- necker products of the subject and object vectors from the positively labelled subset of the training data [18], for each verb. The intuition is that the matrix for each verb represents an average of the pairwise contextual features of a typical subject and object (as extracted from instances of the verb). For example, the matrix for eat may have a high value for the contextual feature pair (is human, is food) (assuming that the features are interpretable in this way). To determine the plausibility of a new subject-object pair for a particular verb, we calculate the Kronecker product of the subject and object noun vectors for this pair, and compare the resulting matrix with the average verb matrix us- ing cosine similarity. Intuitively, the average verb matrix can be thought of as what the verb expects to see in terms of the contextual features of its subject and objects, and the cosine is determining the extent to which the particular argument pair satisﬁes those expectations. As well as being an intuitive corpus-based method for representing the meaning of a transitive verb, this method has also performed well experimentally [18], and hence we consider it to be a competitive baseline. For label prediction, the cutoff is estimated at the break-even point of the receiver operator character- istic (ROC) generated by testing the positive and negative examples of the training data against the learned average matrix.4 In practice it would be more accurate to estimate the cutoff on a validation dataset, but some of the verbs have so few training instances that this was not possible.  4 Data  To train a classiﬁer for each verb, a dataset of positive and negative examples is required. While we can consider subject-verb-object triples that naturally occur in corpus data as positive, a technique for generating pseudo-negative examples is needed, which is described below.  4.1 Training examples  In order to generate training data we made use of two large corpora: the Google Syntactic N-grams (GSN) [16] and the Wikipedia October 2013 dump. The Wikipedia corpus consists of the textual content tokenised using the Stanford NLP tools5 and parsed and lemmatised using the C&C parser and the Morpha lemmatiser [8, 22]. We ﬁrst chose transitive verbs with different concreteness scores [5] and frequencies, in order to obtain a variety of verb types. Then the positive SVO examples were extracted from the GSN corpus. More precisely, we extracted all distinct syntactic trigrams of the form nsubj ROOT dobj, where the root of the phrase was one of our target verbs. We lemmatised the words using the NLTK6 lemmatiser and ﬁltered these examples to retain only the ones that contain nouns that also occur in Wikipedia, obtaining the counts reported in Table 1. For every positive (plausible) training example, we constructed a negative (implausible) one by replacing both the subject and the object with a confounder, using a standard technique from the  4The break-even point is when the true positive rate is equal to the false positive rate. 5http://nlp.stanford.edu/software/index.shtml 6http://nltk.org/  4  Verb APPLY CENSOR COMB DEPOSE EAT IDEALIZE INCUBATE JUSTIFY REDUCE WIPE  Concreteness 2.5 3 5 2.5 4.44 1.17 3.5 1.45 2 4  # of Positive Examples 5618 26 164 118 5067 99 82 5636 26917 1090  Frequency 47361762 278525 644447 874463 26396728 485580 833621 10517616 40336784 6348595  Table 1: The 10 chosen verbs together with their concreteness scores. The number of positive SVO examples was capped at 2000. Frequency is the frequency of the verb in the GSN corpus.  Positive  court APPLY law woman COMB hair animal EAT plant  Negative plan APPLY title role COMB guitarist mountain EAT product  Table 2: Some example training instances  selection preferences literature [6]. A confounder was generated by choosing a random noun from the same frequency bucket as the original noun. Frequency buckets of size 10 were constructed by collecting noun frequency counts from the Wikipedia corpus. Table 2 presents a few pairs of positive and negative training examples.  4.2 Noun representation  Distributional semantic models [31] encode word meaning in a vector format by counting co- occurrences with other words within a speciﬁed context, which can be deﬁned in many ways, for example as a whole document, an N-word window, or a grammatical relation. In this paper we use sentence boundaries to deﬁne context windows. To generate noun context vectors, the Wikipedia corpus described above is scanned for the nouns that appear in the training data and the number of times a context word (cj) occurs within the same sentence as the target noun (wi) is recorded in the vector representing that noun. The context words are the top 10,000 most frequent lemmatised words in the whole corpus excluding stopwords. The raw co-occurrence counts are re-weighted us- ing the standard tTest weighting scheme, where fwicj is the number of times target noun wi occurs with context word cj:  (cid:80) (cid:80) (cid:80)  j  fwicj  k  l  fwk cl  where p(wi) =  tT est( (cid:126)wi, cj) =  (cid:80) (cid:80) (cid:80)  i  fwicj  k  l  fwk cl  , p(cj) =  , and p(wi, cj) =  (2)  .  fwicj(cid:80) (cid:80)  k  l  fwk cl  p(wi, cj) − p(wi)p(cj)  (cid:112)p(wi)p(cj)  Using all 10,000 context words would result in a large number of parameters for each verb tensor, and so we apply the following dimensionality reduction technique which makes training tractable. Considering tTest values as a ranking function, we choose the top N highest ranked context words for each noun. The value N is chosen by testing on the development subset of the MEN dataset (MENdev), a standard dataset for evaluating the quality of semantic vectors [4].7 The tTest weights span the range [−1, 1], but are generally tightly concentrated around zero. Hence an additional . Hence each noun technique we use is to spread the range using row normalisation: (cid:126)w := (cid:126)w|| (cid:126)w||2 vector now contains only N non-zero values, where each value is a (weighted, normalised) co- occurrence frequency.  7The MEN dataset contains 3000 word pairs that were judged for similarity by human annotators. Of that  2000 are in the development subset and the remaining 1000 are used as test data.  5  Figure 2: Vectors tuned for sparseness (dark) consistently produce equal or better dimensionality reductions (MENdev). The solid lines show improvement in lower dimensional representations of SVD when dimensionality reduction is applied after normalisation.  Finally, placing each noun vector as a row in a matrix results in a noun-context co-occurrence matrix. Singular value decomposition (SVD) is applied to this matrix, with 20 latent dimensions. Applying SVD to such a matrix is a standard technique for removing noise and uncovering the latent semantic dimensions in the data, and has been reported to improve performance on a number of semantic similarity tasks [31]. Together these two simple techniques [24] markedly improve the performance of SVD on smaller dimensions (K) on the MENdev set (see Figure 2), and enable us to train the verb tensors using 20-dimensional noun vectors. On an older, highly reported dataset of 353 word pairs [15] our vectors achieve the Spearman correlation of 0.63 without context selection and nor- malisation, and 0.60 with only 20 dimensions after these techniques have been applied.On MENtest we get 0.73 and 0.71, respectively.  5 Experiments  We conducted three experiments. The ﬁrst used all of the available training examples in 5 repetitions of a 2-fold cross-validation (5x2cv) experiment to evaluate the peak performance for each of the verbs (Table 3). The verbs with many subject-object pairs were capped at 4,000 instances (the 2,000 most frequent positive pairs and 2,000 pseudo-negative). We compared the performance of the baseline and tensor learning methods on 20 and 40 dimensional vectors.8 The performance was evaluated using the area under the ROC (AUC) and the F1 measure (based on precision and recall over the plausible class). The AUC evaluates whether a method is ranking positive examples above negative ones, regardless of the class cutoff value. F1 shows how accurately a method assigns the correct class label. Since the baseline uses ad hoc class assignment, AUC is the more fair measure. In the second experiment, we repeated the 5x2cv with datasets of 52 training points for each verb, as this is the size of the smallest dataset of the verb CENSOR (Table 4). The points were randomly sampled from the datasets used in the ﬁrst experiment. Finally, the four verbs with the largest datasets were used to examine how the performance of the methods change as the amount of training data increases. The 4,000 training samples were randomised and half was used for testing. We sampled between 10 and 1000 training triples from the other half (Figure 3).  5.1 Analysis  In general the tensor learning algorithm learns more effectively and with smaller variance than the baseline, particularly from the smaller dimensional noun vectors. The F1 scores indicate that learn- ing is necessary for accurate classiﬁcation while the baseline AUC results show that in principle only positive examples are necessary (since the baseline only sees positive examples). Analysis of errors shows that the baseline method mostly generates false negative errors (i.e. predicting implausible  8We also implemented and experimented with a matrix method, which outputs a sigmoid transformed single plausibility value instead of the overparameterised 2-value softmax vector. This method performed worse than baseline and was thus left out of this paper.  6  01002003004005006007008000.60.620.640.660.680.70.720.740.760.78Number of dimensions (K)Spearman  N=140N=10000norm N=140norm N=10000K=10000 140K=10000 N=10000Verb  Vectors K = 20  Vectors K = 40  Baseline 78.67 ± 0.81 89.79 ± 7.52 82.93 ± 1.70 92.78 ± 1.44 92.99 ± 0.43 75.18 ± 7.48 79.70 ± 4.85 87.32 ± 0.85 94.24 ± 0.38 80.53 ± 1.04 64.24 ± 13.90 56.12 ± 34.37 52.38 ± 31.35 56.84 ± 31.87 54.03 ± 33.38 49.03 ± 24.74 55.42 ± 29.80 69.70 ± 14.41 77.26 ± 6.99 55.94 ± 27.64  APPLY CENSOR COMB DEPOSE EAT IDEALIZE INCUBATE JUSTIFY REDUCE WIPE  APPLY CENSOR COMB DEPOSE EAT IDEALIZE INCUBATE JUSTIFY REDUCE WIPE  Tensor AUC  84.81 ± 0.69† 82.01 ± 6.02 87.68 ± 2.19 91.10 ± 1.85 94.01 ± 0.68 69.52 ± 2.83 84.94 ± 2.64 88.21 ± 0.61 95.03 ± 0.46 82.00 ± 1.18 77.37 ± 0.57 74.62 ± 7.22 80.19 ± 3.36 84.50 ± 1.73 87.78 ± 0.74 53.61 ± 28.43 77.81 ± 3.95 81.44 ± 0.69 89.06 ± 0.55 75.77 ± 1.21  F1  Baseline 81.46 ± 0.58 85.54 ± 9.04 85.65 ± 2.13 94.44 ± 1.16 93.81 ± 0.45 75.84 ± 6.85 85.53 ± 2.78 88.70 ± 0.62 95.48 ± 0.48 84.47 ± 0.98 64.00 ± 16.48 47.93 ± 31.08 45.02 ± 34.25 54.77 ± 38.04 52.45 ± 27.68 48.28 ± 23.41 50.84 ± 37.99 73.71 ± 8.74 71.24 ± 17.23 47.62 ± 33.63  Tensor  85.68 ± 0.72† 79.40 ± 8.23 89.41 ± 2.33 92.70 ± 1.48 94.62 ± 0.37 69.56 ± 4.52 89.33 ± 2.52 85.27 ± 0.93 96.13 ± 0.37 85.19 ± 1.16 79.27 ± 1.03 70.66 ± 10.83 81.15 ± 2.59 84.60 ± 2.66 88.91 ± 0.54 66.53 ± 4.69 80.30 ± 5.51 79.73 ± 0.94 91.24 ± 0.57 78.57 ± 0.73  Table 3: Full-size data cross-validation results with standard deviation. Bold indicates that the method performs better, and † that the result is signiﬁcant according to the 5x2cv F-test [32].  Verb  APPLY CENSOR COMB DEPOSE EAT IDEALIZE INCUBATE JUSTIFY REDUCE WIPE  Baseline 67.03 ± 8.86 83.55 ± 6.81 71.87 ± 11.48 92.74 ± 3.45 73.96 ± 3.47 66.72 ± 4.45 51.52 ± 8.14 72.36 ± 12.66 79.69 ± 5.24 76.70 ± 8.14  Tensor  83.24 ± 4.06 83.93 ± 3.96 81.26 ± 7.28 95.84 ± 2.86 90.14 ± 3.02† 52.35 ± 7.54 79.75 ± 7.01† 75.44 ± 9.25 91.25 ± 4.33 75.24 ± 5.70  Table 4: Small data (26 positive + 26 negative per verb) cross-validation results show AUC with standard deviation. The † indicates statistically signiﬁcant results.  when the gold standard label is plausible), particularly on triples that contain nouns that have not been seen with the verb in the training data, which indicates that the baseline may not adequately generalise over the latent dimensions from the SVD. In contrast, tensor learning (TL) produces al- most equal numbers of false positives and false negatives, but sometimes produces false negatives with some low frequency nouns (e.g. bourgeoisie idealize work), presumably because there is not enough information in the noun vector to decide on the correct class. TL also produces some false positive errors when either of the nouns is plausible (but the triple is implausible), which would sug- gest results may be improved by training with data where only one noun is confounded or treating negative data as possibly positive [21]. Both the full data and small data experiments indicate that IDEALIZE is the most difﬁcult verb to learn. It has the twin properties of low frequency and low concreteness. In addition, it is likely to have low selectional preference strength, not selecting strongly for the semantic types of its argu- ments. Both verb frequency and concreteness have positive Spearman correlation with the TL AUC values from Tables 3 and 4. Frequency has much stronger correlation (Table 3:0.53, Table 4:0.31) than concreteness (Table 3:0.14, Table 4:0.08), even when all datasets are reduced to the same num- ber of examples. This is probably due to the fact that the more frequent verbs occur in more frequent  7  Figure 3: Comparison of baseline (dashed) and tensor learning (full) methods as the number of training instances increases.  triples, which are likely to contain highly frequent nouns, and hence have higher quality noun vec- tors. However, if we just consider the most frequent verbs (Figure 3) we can see that EAT, which has the highest concreteness (4.44), provides a much smoother learning curve and asymptotes quicker than the less concrete verbs APPLY (2.4), REDUCE (2), and JUSTIFY (1.45). From this brief analysis, we hypothesise that noun frequency, verb concreteness, and selectional preference strength of the verb for its arguments all inﬂuence the quality of the learned representation.  6 Conclusion  In this paper we have investigated learning 3rd-order tensors to represent the semantics of transitive verbs, with a 2-dimensional “plausibility” sentence space. There are obvious connections with the large literature on selectional preference learning (see e.g. [27] for a recent paper); however, our goal is not to contribute to that literature, but rather to use a selectional preference task as a ﬁrst corpus-driven test of the type-driven tensor-based semantic framework of Coecke et al., as well as introduce this framework to the machine learning community. We have shown that standard techniques from the neural networks literature can be effectively ap- plied to learning 3rd-order tensors from corpus data, with our results showing positive trends com- pared to a competitive corpus-based baseline. There is much work to be done in extending the techniques in this paper, both in terms of a higher-dimensional, potentially more structured, sen- tence space, and in terms of incorporating the many syntactic types making up a wide-coverage grammar. Since many of these types require higher order tensors than 3rd-order, we suggest that tensor decomposition techniques are likely to be necessary for practical performance.  Acknowledgments  Tamara Polajnar is supported by ERC Starting Grant DisCoTex (306920). Stephen Clark is supported by ERC Starting Grant DisCoTex and EPSRC grant EP/I037512/1. Luana Fˇagˇaras¸an is supported by an EPSRC Doctoral Training Partnership award. Thanks to Laura Rimell and Jean Maillard for helpful discussion.  References  [1] Brett W. Bader, Tamara G. Kolda, et al. Matlab tensor toolbox version 2.5. Available online, Jan 2012.  8  1020408015030060080010000.680.70.720.740.760.780.80.82# Training ExamplesAUCApply  baselinetensor1020408015030060080010000.70.750.80.850.90.95# Training ExamplesAUCEat  baselinetensor1020408015030060080010000.650.70.750.80.850.9# Training ExamplesAUCJustify  baselinetensor1020408015030060080010000.80.820.840.860.880.90.920.940.96# Training ExamplesAUCReduce  baselinetensor[2] M. Baroni, R. Bernardi, and R. Zamparelli. Frege in space: A program for compositional distributional  semantics (to appear). Linguistic Issues in Language Technologies, 2013.  [3] M. Baroni and R. Zamparelli. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Conference on Empirical Methods in Natural Language Processing (EMNLP-10), pages 1183–1193, Cambridge, MA, 2010.  [4] E. Bruni, G. Boleda, M. Baroni, and N. Tran. 2012. Distributional semantics in technicolor. In Proceed-  ings of the 50th Annual Meeting of the ACL, pages 136–145, Jeju Island, Korea, 2012.  [5] Marc Brysbaert, Amy Beth Warriner, and Victor Kuperman. Concreteness ratings for 40 thousand gener-  ally known English word lemmas. Behavior research methods, pages 1–8, 2013.  [6] Nathanael Chambers and Dan Jurafsky. Improving the use of pseudo-words for evaluating selectional  preferences. In Proceedings of the 48th Meeting of the ACL, pages 445–453, Uppsala, Sweden, 2010.  [7] Stephen Clark. Type-driven syntax and semantics for composing meaning vectors.  In Chris Heunen, Mehrnoosh Sadrzadeh, and Edward Grefenstette, editors, Quantum Physics and Linguistics: A Composi- tional, Diagrammatic Discourse, pages 359–377. Oxford University Press, 2013.  [8] Stephen Clark and James R. Curran. Wide-coverage efﬁcient statistical parsing with CCG and log-linear  models. Computational Linguistics, 33(4):493–552, 2007.  [9] Daoud Clarke. A context-theoretic framework for compositionality in distributional semantics. Compu-  tational Linguistics, 38(1):41–71, 2012.  [10] B. Coecke, M. Sadrzadeh, and S. Clark. Mathematical foundations for a compositional distributional model of meaning. In J. van Bentham, M. Moortgat, and W. Buszkowski, editors, Linguistic Analysis (Lambek Festschrift), volume 36, pages 345–384. 2010.  [11] Tim Van de Cruys, Laura Rimell, Thierry Poibeau, and Anna Korhonen. Multi-way tensor factorization  for unsupervised lexical acquisition. In Proceedings of COLING 2012, Mumbai, India, 2012.  [12] Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391– 407, 1990.  [13] D.R. Dowty, R.E. Wall, and S. Peters. Introduction to Montague Semantics. Dordrecht, 1981. [14] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and  stochastic optimization. J. Mach. Learn. Res., 12:2121–2159, July 2011.  [15] Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20:116–131, 2002.  [16] Yoav Goldberg and Jon Orwant. A dataset of syntactic-ngrams over time from a very large corpus of English books. In Second Joint Conference on Lexical and Computational Semantics, pages 241–247, Atlanta,Georgia, 2013.  [17] Edward Grefenstette. Category-Theoretic Quantitative Compositional Distributional Models of Natural  Language Semantics. PhD thesis, University of Oxford, 2013.  [18] Edward Grefenstette and Mehrnoosh Sadrzadeh. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1394–1404, Edinburgh, Scotland, UK, July 2011.  [19] Julia Hockenmaier and Mark Steedman. CCGbank: a corpus of CCG derivations and dependency struc-  tures extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396, 2007.  [20] Jayant Krishnamurthy and Tom M Mitchell. Vector space semantic parsing: A framework for compo- sitional vector space models. In Proceedings of the 2013 ACL Workshop on Continuous Vector Space Models and their Compositionality, Soﬁa, Bulgaria, 2013.  [21] Wee Sun Lee and Bing Liu. Learning with positive and unlabeled examples using weighted logistic regression. In Proceedings of the Twentieth International Conference on Machine Learning (ICML, 2003. [22] Guido Minnen, John Carroll, and Darren Pearce. Applied morphological processing of English. Natural  Language Engineering, 7(3):207–223, 2001.  [23] Jeff Mitchell and Mirella Lapata. Vector-based models of semantic composition. In Proceedings of ACL-  08, pages 236–244, Columbus, OH, 2008.  [24] Tamara Polajnar and Stephen Clark. Improving distributional semantic vectors through context selection and normalisation. In 14th Conference of the European Chapter of the Association for Computational Linguistics, EACL’14, 2014.  [25] M. Ranzato, A. Krizhevsky, and G. E. Hinton. Factored 3-way restricted boltzmann machines for mod- eling natural images. In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), Sardinia, Italy, 2010.  9  [26] Hinrich Sch¨utze. Automatic word sense discrimination. Computational Linguistics, 24(1):97–124, 1998. [27] Diarmuid O Seaghdha. Latent variable models of selectional preference. In Proceedings of ACL 2010,  Uppsala, Sweden, 2010.  [28] Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. Semantic compositional- ity through recursive matrix-vector spaces. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1201–1211, Jeju, Korea, 2012.  [29] Mark Steedman. The Syntactic Process. The MIT Press, Cambridge, MA, 2000. [30] I. Sutskever, R. Salakhutdinov, and J. B. Tenenbaum. Modelling relational data using bayesian clustered tensor factorization. In Proceedings of Advances in Neural Information Processing Systems (NIPS 2009), Vancouver, Canada, 2009.  [31] Peter D. Turney and Patrick Pantel. From frequency to meaning: Vector space models of semantics.  Journal of Artiﬁcial Intelligence Research, 37:141–188, 2010.  [32] Aydın Ulas¸, Olcay Taner Yıldız, and Ethem Alpaydın. Cost-conscious comparison of supervised learning  algorithms over multiple data sets. Pattern Recognition, 45(4):1772–1781, April 2012.  10  ","This paper investigates the learning of 3rd-order tensors representing thesemantics of transitive verbs. The meaning representations are part of atype-driven tensor-based semantic framework, from the newly emerging field ofcompositional distributional semantics. Standard techniques from the neuralnetworks literature are used to learn the tensors, which are tested on aselectional preference-style task with a simple 2-dimensional sentence space.Promising results are obtained against a competitive corpus-based baseline. Weargue that extending this work beyond transitive verbs, and tohigher-dimensional sentence spaces, is an interesting and challenging problemfor the machine learning community to consider."
1312.6062,2014,Stopping Criteria in Contrastive Divergence: Alternatives to the Reconstruction Error  ,"['David Buchaca', 'Enrique Romero', 'Ferran Mazzanti', 'Jordi Delgado']",https://arxiv.org/pdf/1312.6062.pdf,"Stopping Criteria in Contrastive Divergence: Alternatives to the  Reconstruction Error  4 1 0 2    r p A 9         ]  G L . s c [      2 v 2 6 0 6  .  2 1 3 1 : v i X r a  David Buchaca Prats Departament de Llenguatges i Sistemes Inform`atics, Universitat Polit`ecnica de Catalunya, Barcelona, Spain Enrique Romero Merino Departament de Llenguatges i Sistemes Inform`atics, Universitat Polit`ecnica de Catalunya, Barcelona, Spain Ferran Mazzanti Castrillejo Departament de F´ısica i Enginyeria Nuclear, Universitat Polit`ecnica de Catalunya, Barcelona, Spain Jordi Delgado Pin Departament de Llenguatges i Sistemes Inform`atics, Universitat Polit`ecnica de Catalunya, Barcelona, Spain  Abstract  Restricted Boltzmann Machines (RBMs) are general unsupervised learning devices to as- certain generative models of data distributions. RBMs are often trained using the Contrastive Di- vergence learning algorithm (CD), an approxi- mation to the gradient of the data log-likelihood. A simple reconstruction error is often used to de- cide whether the approximation provided by the CD algorithm is good enough, though several au- thors (Schulz et al., 2010; Fischer & Igel, 2010) have raised doubts concerning the feasibility of this procedure. However, not many alternatives to the reconstruction error have been used in the literature. In this manuscript we investigate sim- ple alternatives to the reconstruction error in or- der to detect as soon as possible the decrease in the log-likelihood during learning.  Proceedings of the 2 nd International Conference on Learning Representations, Banff, Canada, 2014. Copyright 2014 by the au- thor(s).  DAVIDBUCHACA@GMAIL.COM  EROMERO@LSI.UPC.EDU  FERRAN.MAZZANTI@UPC.EDU  JDELGADO@LSI.UPC.EDU  1. Introduction Learning algorithms for deep multi-layer neural networks have been known for a long time (Rumelhart et al., 1986), though none of them have been widely used to solve large scale real-world problems. In 2006, Deep Belief Networks (DBNs) (Hinton et al., 2006) came out as a real break- through in this ﬁeld, since the learning algorithms pro- posed ended up being a feasible and practical method to train these networks, with spectacular results (Hinton & Salakhutdinov, 2006; Larochelle et al., 2009; Lee et al., 2009; Le et al., 2012). DBNs have Restricted Boltzmann Machines (RBMs) (Smolensky, 1986) as their building blocks. RBMs are topologically constrained Boltzmann Machines (BMs) with two layers, one of hidden and another of vis- ible neurons, and no intra-layer connections. This prop- erty makes working with RBMs simpler than with regular BMs, and in particular the stochastic computation of the log-likelihood gradient may be performed more efﬁciently by means of Gibbs sampling (Bengio, 2009). In 2002, the Contrastive Divergence learning algorithm (CD) was proposed as an efﬁcient training method for product-of-expert models, from which RBMs are a special case (Hinton, 2002). It was observed that using CD to train  Stopping Criteria in Contrastive Divergence: Alternatives to the Reconstruction Error  RBMs worked quite well in practice. This fact was im- portant for deep learning since some authors suggested that a multi-layer deep neural network is better trained when each layer is pre-trained separately as if it were a single RBM (Hinton & Salakhutdinov, 2006; Bengio et al., 2007; Larochelle et al., 2009). Thus, training RBMs with CD and stacking up RBMs seems to be a good way to go when de- signing deep learning architectures. However, the picture is not as nice as it looks. CD is not a ﬂawless training algorithm. Despite CD being an approximation of the true log-likelihood gradient (Bengio & Delalleau, 2009), it is biased and it may not converge in some cases (Carreira-Perpi˜n´an & Hinton, 2005; Yuille, 2005; MacKay, 2001). Moreover, it has been observed that CD, and variants such as Persistent CD (Tieleman, 2008) or Fast Persistent CD (Tieleman & Hinton, 2009) can lead to a steady decrease of the log-likelihood during learning (Fischer & Igel, 2010; Desjardins et al., 2010). There- fore, the risk of learning divergence imposes the require- ment of a stopping criterion. The two main methods used to decide when to stop the learning process are reconstruc- tion error and Annealed Importance Sampling (AIS) (Neal, 1998). Reconstruction error is easy to compute and it has been often used in practice, though its adequacy remains unclear (Fischer & Igel, 2010). AIS seems to work better than reconstruction error in some cases, though it my also fail (Schulz et al., 2010). In this paper we propose an alternative stopping criteria for CD and show its preliminary results. These criteria are based on the computation of two probabilities that do not require from the knowledge of the partition function of the system. The early detection of the decrease of the like- lihood allows to overcome the reconstruction error faulty observed behavior.  2. Learning in Restricted Boltzmann  Machines  2.1. Energy-based Probabilistic Models  Energy-based probabilistic models deﬁne a probability dis- tribution from an energy function, as follows:  P (x, h) =  e−Energy(x,h)  Z  ,  (1)  where x stand for visible variables and h are hidden vari- ables (typically binary) introduced to increase the expres- sive power of the model. The normalization factor Z is called partition function and reads  Z =  e−Energy(x,h) .  (2)  (cid:88)  x,h  marginal distribution  P (x) =  (cid:80) h e−Energy(x,h)  Z  ,  (3)  but the evaluation of the partition function Z is computa- tionally prohibitive since it involves an exponentially large number of terms. The energy function depends on several parameters θ, that are adjusted at the learning stage. This is done by maxi- mizing the likelihood of the data. In energy-based models, the derivative of the log-likelihood can be expressed as  − ∂ log P (x; θ)  = EP (h|x)  (cid:20)  ∂θ − E  ∼ x)  P (  E  P (h|∼ x)  (cid:21)  (cid:20) ∂Energy(x, h) (cid:21)(cid:21)  ∂θ ∼ x,h)  ∂Energy( ∂θ  ,  (cid:20)  (4)  where the ﬁrst term is called the positive phase and the sec- ond term is called the negative phase. Similar to (3), the exact computation of the derivative of the log-likelihood is usually unfeasible because of the second term in (4), which comes from the derivative of the partition function.  2.2. Restricted Boltzmann Machines  Restricted Boltzmann Machines are energy-based proba- bilistic models whose energy function is:  Energy(x, h) = −btx − cth − htW x .  (5)  RBMs are at the core of DBNs (Hinton et al., 2006) and other deep architectures that use RBMs to unsupervised pre-training previous to the supervised step (Hinton & Salakhutdinov, 2006; Bengio et al., 2007; Larochelle et al., 2009). The consequence of the particular form of the energy func- tion is that in RBMs both P (h|x) and P (x|h) factorize. In this way it is possible to compute P (h|x) and P (x|h) in one step, making possible to perform Gibbs sampling ef- ﬁciently (Geman & Geman, 1984) that can be the basis of the computation of an approximation of the derivative of the log-likelihood (4).  2.3. Contrastive Divergence  The most common learning algorithm for RBMs uses an algorithm to estimate the derivative of the log-likelihood of a Product of Experts model called CD (Hinton, 2002). The algoritmh for CDn estimates the derivative of the log- likelihood as  (cid:20) ∂Energy(x1, h)  (cid:21)  − ∂ log P (x1; θ)  ∂θ  (cid:39) EP (h|x1)  (cid:104) ∂Energy(xn+1,h)  (cid:105)  ∂θ  ∂θ  .  (6)  Since only x is observed, one is only interested in the  − EP (h|xn+1)  Stopping Criteria in Contrastive Divergence: Alternatives to the Reconstruction Error  where xn+1 is the last sample from the Gibbs chain starting from x1 obtained after n steps:  h1 ∼ P (h|x1) x2 ∼ P (x|h1)  ... hn ∼ P (h|xn) xn+1 ∼ P (x|hn) .  (cid:104) ∂Energy(x,h)  (cid:105)  For binary RBMs, EP (h|x) computed. Several alternatives to CDn are Persistent CD (PCD) (Tieleman, 2008), Fast PCD (FPCD) (Tieleman & Hinton, 2009) or Parallel Tempering (PT) (Desjardins et al., 2010).  can be easily  ∂θ  2.4. Monitoring the Learning Process in RBMs  Learning in RBMs is a delicate procedure involving a lot of data processing that one seeks to perform at a reasonable fast speed in order to be able to handle large spaces with a huge amount of states. In doing so, drastic approximations that can only be understood in a statistically averaged sense are performed (section 2.3). One of the most relevant points to consider at the learning stage is to ﬁnd a good way to determine whether a good solution has been found or not, and so to determine when should the learning process stop. One of the most widely used criteria for stopping is the reconstruction error, which is a measure of the capability of the network to produce an output that is consistent with the data at input. Since RBMs are probabilistic models, the reconstruction error of a data point x(i) is computed as the probability of x(i) given the expected value of h for x(i):  (cid:16)  (cid:104)  h|x(i)(cid:105)(cid:17)  R(x(i)) = P  x(i)|E  ,  (7)  which is the equivalent of the sum-of-squares reconstruc- tion error for deterministic networks. Some authors have shown that it may happen that learn- ing induces an undesirable decrease in likelihood that goes undetected by the reconstruction error (Schulz et al., 2010; Fischer & Igel, 2010). It has been studied (Fischer & Igel, 2010) that the reconstruction error deﬁned in (7) usually de- creases monotonically. Since no increase in the reconstruc- tion error takes place during training there is no apparent way to detect the change of behavior of the log-likelihood for CDn.  N(cid:89)  i=1  3. Proposed Stopping Criteria The proposed stopping criteria are based on the monitor- ization of the ratio of two probabilities: the probability of the data (that should be high) and the probability of points in the input space whose probability should be low. More formally, it can be deﬁned as:  ξ =  P (X) P (Y )  =  P (x(i)) P (y(i))  ,  (8)  where X stands for the complete training set of N samples and Y is a suitable artiﬁcially generated data set. The data set Y can be generated in different ways (see below). The idea behind ξ comes from the fact that the standard gradient descent update rule used during learning requires from the evaluation of two terms: the positive and negative phases. The positive phase tends to decrease the energy (hence increase the probability) of the states related to the training data, while the negative phase tends to increase the energy of the whole set of states with the corresponding decrease in probability. In this way, if Y is selected so as to have low probability, the numerator in ξ is expected to increase while the denominator is expected to decrease dur- ing the learning process, making ξ maximal when learning is achieved. Most relevant to the discussion is the fact that, being a ratio of probabilities computed at every step of the Markov chain built on-the-ﬂy, the partition functions Z involved in P (X) and P (Y ) cancel out in ξ. In other words, the computation of ξ can be equivalently deﬁned as  N(cid:89)  (cid:80) (cid:80) h e−Energy(x(i),h) h e−Energy(y(i),h)  .  (9)  i=1  (cid:80) The particular topology of RBMs allows to compute h e−Energy(x,h) efﬁciently. This fact dramatically de- creases the computational cost involved in the calculation, which would otherwise become unfeasible in most real- world problems where RBMs could been successfully ap- plied. While P (x(i)) in ξ is directly evaluated from the data in the training set, the problem of ﬁnding suitable values for Y still remains. In order to select a point y(i) with low probability, one may seek for zones of the space distant from x(i), thus representing the complementary of the fea- tures to be learnt. This point should not be difﬁcult to ﬁnd. On the one hand, in small spaces one can enumerate the states. On the other hand, in large spaces with a small train- ing set X the probability that a state picked up at random does not belong to X should be large. A second possibil- ity is, for ﬁxed x(i), to suitably change the values of the hidden units during learning in such a way that they differ  ξ =  P (X) P (Y )  =  Stopping Criteria in Contrastive Divergence: Alternatives to the Reconstruction Error  signiﬁcantly from the values they should take during data reconstruction. We expect that, once learning is done, the reconstruction vectors should be independent of the value of the hidden units. However, this may not be the case while the system is still learning, as the basins of attraction of the energy functional depend explicitly on the values of the weights and bias terms, which can change signiﬁcantly. This is in fact the main idea behind the stopping criteria proposed in this work, that we shall exploit in the follow- ing. With all that in mind, two different alternatives have been explored: i) y(i) = E[x|hs], where hs is a random vector whose components are drawn from the uniform distribution in [0,1].  ii) y(i) = E[x|hs], where hs = 1 − h(i)  1 , i.e., the com- plementary of the ﬁrst hidden vector obtained in the Gibbs chain for x(i).  Regarding the ﬁrst alternative, random hidden vectors are expected to lead to regions of low reconstruction probabil- ity, at least while the system is still learning. In the second alternative, we expect that if a good reconstruction of x(i) is achieved for a certain value of h(i) (see Eq. (7)), the op- 1 posite should happen when 1 − h(i) is used instead. Other related possibilities like monitoring the average value E[h|x(i) 1 ] and using its complementary instead of 1 − h(i) 1 have also been explored and yield similar results to the ones shown in the following. Notice that the reconstruction error only gathers informa- tion from the training set X, while the proposed estimator ξ in equation (8) samples also states from the rest of the input space.  1  4. Experiments We performed several experiments to explore the afore- mentioned alternatives deﬁned in section 3 and compare the behavior of the estimator ξ to that of the actual log- likelihood and the reconstruction error in a couple of prob- lems. The ﬁrst problem, denoted Bars and Stripes (BS), tries to identify vertical and horizontal lines in 4×4 pixel images. The training set consists in the whole set of images contain- ing all possible horizontal or vertical lines (but not both), ranging from no lines (blank image) to completely ﬁlled images (black image), thus producing 2 × 24 − 2 = 30 different images (avoiding the repetition of fully back and fully white images) out of the space of 216 possible images with black or white pixels. The second problem, named  Labeled Shifter Ensemble (LSE), consists in learning 19-bit states formed as follows: given an initial 8-bit pattern, gen- erate three new states concatenating to it the bit sequences 001, 010 or 100. The ﬁnal 8-bit pattern of the state is the original one shifting one bit to the left if the intermediate code is 001, copying it unchanged if the code is 010, or shifting it one bit to the right if the code is 100. One thus generates the training set using all possible 28 × 3 = 768 states that can be created in this form, while the system space consists of all possible 219 different states one can build with 19 bits. These two problems have already been explored in (Fischer & Igel, 2010) and are adequate in the current context since, while still large, the dimensionality of space allows for a direct monitorization of the partition function and the log-likelihood during learning. In the following we discuss the learning processes of both problems with single RBMs, employing the Contrastive Di- vergence algorithm CDn with n = 1 and n = 10 as de- scribed in section 2.3. In all cases, binary visible and hid- den units were used. In the BS case the RBM had 16 visible and 8 hidden units, while in the LSE problem these num- bers were 19 and 10, respectively. Every simulation was carried out for a total of 50000 epochs, with measures be- ing taken every 50 epochs. Moreover, every point in the subsequent plots was the average of ten different simula- tions starting from different random values of the weights and bias. Other parameters affecting the results that were changed along the analysis are the learning rate (LR) in- volved in the weight and bias update rules and a weight decay parameter (WD) that prevents weights from achiev- ing large values that would saturate the sigmoid functions present in the analytical expressions associated to binary RBMs. We present the results for the two problems at hand, show- ing for each instance analyzed three different plots corre- sponding to the actual log-likelihood of the problem, log(ξ) (ξ as deﬁned in (9)) and the logarithm of the reconstructed error (7), all three quantities monitored during the learning process. Figure 1 shows results for the BS problem using the al- ternatives i) and ii) deﬁned in section 3 using CD1 with LR=0.01 and WD=0. The left panel corresponds to alter- native i) and the right panel corresponds to alternative ii). As can be seen, the log-likelihood increases very rapidly, reaches a maximum and then starts decreasing, thus indi- cating that further learning only worsens the model. In both cases, though, the log probability of the reconstruc- tion converges towards a constant value (very near 0, indi- cating high probabilities for the reconstructed data), giving the false impression that going on with the learning process will neither improve nor worsen the predictions produced by the network. Interestingly enough, though, the middle  Stopping Criteria in Contrastive Divergence: Alternatives to the Reconstruction Error  Figure 1. Log-likelihood, log(ξ) and log-probability of the recon- struction (upper, middle and lower panels) for the BS problem. Left and right columns correspond to options i) and ii) when choosing values for the hidden units using CD1 with LR=0.01 and WD=0.  plot on the right panel indicate that ii) is able to capture the increasing and decreasing behavior of the log-likelihood, a feature that i) seems to miss. At this point it looks like ii) is a better estimator of optimal log-likelihood than the re- construction error. This same behaviour is seen in ﬁgure 2 where a weight decay value WD=0.001 is employed. The LSE problem yields somewhat similar results under the same learning and monitoring conditions. The log- likelihood, log(ξ) and log-reconstruction error are shown as before in the upper, middle and lower panels of ﬁgure 3, with options i) and ii) on the left and right, respectively. In this case the learning rate has been set to LR=0.001 (oth- erwise the log-likelihood of the problem decreases mono- tonically). In this case, however, both estimators i) and ii) are able to ﬁnd the region where the log-likelihood is max-  Figure 2. Same as in ﬁgure 1 but with WD=0.001.  imal, decreasing similarly to the later when this point is surpassed. These results seem to indicate that estimator ii) is more ro- bust than estimator i). Still, these two are better than the reconstruction error which always present a similar pattern, both for the BS and LSE problems, with a transient regime that always stabilizes to a plateau that apparently has little to do with the actual behavior of the log-likelihood. All these results have been obtained in the CD1 approxi- mation. Since it is known that CDn with increasing n can lead to better learning results because of the increased sta- tistical independence of the input and output values gener- ated, estimators i) and ii) can also be used in this case. We have checked their performance using CD10 on the same two problems at hand. Results for the LSE problem using CD10, LR=0.01 and WD=0 are shown in the left and right panels of ﬁgure 4 for estimators i) and ii), respectively. In this case, none of the estimators is able to detect the region  Stopping Criteria in Contrastive Divergence: Alternatives to the Reconstruction Error  Figure 3. Results for the LSE problem as reported in ﬁgure 1 for CD1, LR=0.001 and WD=0.001.  Figure 4. Results for the LSE problem as reported in ﬁgure 1 for CD10, LR=0.01 and WD=0.  of maximal likelihood, stressing that none of these shall be used as a test to stop the learning algorithm. However, the reconstruction error has a similar behavior, thus indicating that it is not a good testing quantity either. Similar results for the BS problem are obtained when using CD10. A pos- sible explanation can be related to the fact that the Markov chain involved in the process tends to lose memory with increasing number of steps. Therefore, ξ is computed with more independent data in CD10 than in CD1. Anyway, the behavior of the proposed criteria with CD10 should be fur- ther studied. As a ﬁnal remark, we note that for the BS problem the trained RBM stopped using the proposed criteria is able to qualitatively generate samples similar to those in the training set. We show in ﬁgure 5 the complete training set (two upper rows) and the same number of generated samples obtained from the RBM stopped after 3000 epochs in the training process using CD1 as discussed above, cor-  responding to the maximum value of the proposed crite- rion ii), which coincides with the optimal value of the log- likelihood (two lower rows in the same ﬁgure).  5. Conclusion Based on the fact that learning tries to increase the contri- bution of the relevant states while decreasing the rest, two new estimators based on the ratio of two probabilities have been proposed and discussed as an alternative to the re- construction error. It has been shown that the better one, obtained by replacing the value of the (binary) hidden units h by 1 − h, can at some point be able to monitor the ac- tual behavior of the log-likelihood of the model without additional computational cost. This estimator works well for CD1 but for CD10, which is considered to yield better learning results at the expense however of a linear increase in computational cost. We believe that the use of the esti- mator presented here in CD1 learning problems provides a  Stopping Criteria in Contrastive Divergence: Alternatives to the Reconstruction Error  Figure 5. Training data (two upper rows) and generated samples (two lower rows) for the BS problems after 3000 epochs in the training process using CD1.  faster stopping criteria for the learning algorithm that can yield results compatible in quality to those obtained in stan- dard CDn learning for moderate n. Future work along this line will be carried out in an attempt to formalize that state- ment.  Acknowledgments JD: This work was partially supported by MICINN project TIN2011-27479-C04-03 (BASMATI) and by SGR2009- 1428 (LARCA). FM: This work has been supported by grant No. FIS2011- 25275 from DGI (Spain) and Grant No. 2009-SGR1003 from the Generalitat de Catalunya (Spain). ER: This research is partially funded by Spanish research project TIN2012-31377.  References Bengio, Y. Learning deep architectures for AI. Foundations  and Trends in Machine Learning, 2(1):1–127, 2009.  Bengio, Y. and Delalleau, O.  Justifying and Generaliz- ing Contrastive Divergence. Neural Computation, 21(6): 1601–1621, 2009.  Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. Greedy Layer-wise Training of Deep Networks. In Advances in Neural Information Processing (NIPS’06), volume 19, pp. 153–160. MIT Press, 2007.  Carreira-Perpi˜n´an, M. A. and Hinton, G. E. On Contrastive Divergence Learning. In International Workshop on Ar- tiﬁcial Intelligence and Statistics, pp. 33–40, 2005.  Desjardins, G., Courville, A., Bengio, Y., Vincent, P., and Delalleau, O. Parallel Tempering for Training of Re- In 13th International stricted Boltzmann Machines.  Conference on Artiﬁcial Intelligence and Statistics (AIS- TATS), pp. 145–152, 2010.  Fischer, A. and Igel, C. Empirical Analysis of the Diver- gence of Gibbs Sampling Based Learning Algorithms In International for Restricted Boltzmann Machines. Conference on Artiﬁcial Neural Networks (ICANN), vol- ume 3, pp. 208–217, 2010.  Geman, S. and Geman, D. Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images. IEEE Trans. Pattern Analysis and Machine Intelligence, 6(6):721–741, nov 1984.  Hinton, G. E. Training Products of Experts by Minimizing Contrastive Divergence. Neural Computation, 14:1771– 1800, 2002.  Hinton, G. E. and Salakhutdinov, R. R. Reducing the Di- mensionality of Data with Neural Networks. Science, 313(5786):504–507, 2006.  Hinton, G. E., Osindero, S., and Teh, Y. A Fast Learning Algorithm for Deep Belief Nets. Neural Computation, 18(7):1527–1554, 2006.  Larochelle, H., Bengio, Y., Lourador, J., and Lamblin, P. Exploring Strategies for Training Deep Neural Net- works. Journal of Machine Learning Research, 10:1–40, 2009.  Le, Q. V., Ranzato, M. A., Monga, R., Devin, M., Chen, K., Corrado, G. S., and Ng, A. Y. Building High-level Fea- tures Using Large Scale Unsupervised Learning. In 29th International Conference on Machine Learning, 2012.  Lee, H., Grosse, R., Ranganath, R., and Ng, A. Y. Convolu- tional Deep Belief Networks for Scalable Unsupervised In Interna- Learning of Hierarchical Representations. tional Conference on Machine Learning, pp. 77, 2009.  Stopping Criteria in Contrastive Divergence: Alternatives to the Reconstruction Error  MacKay, D. J. C. Failures of the one-step learning algo-  rithm, 2001. Unpublished Technical Report.  Neal, R. M. Annealed Importance Sampling, 1998. Techni- cal Report 9805, Dept. Statistics, University of Toronto.  Rumelhart, David E., Hinton, Geoffrey E., and Williams, R. J. Learning Internal Representations by Error Prop- agation. In Rumelhart, D. E., McClelland, J. L., and the PDP research group. (eds.), Parallel distributed pro- cessing: Explorations in the microstructure of cognition, Volume 1: Foundations. MIT Press, 1986.  Schulz, H., M¨uller, A., and Behnke, S. Investigating Con- vergence of Restricted Boltzmann Machine Learning. In NIPS 2010 Workshop on Deep Learning and Unsuper- vised Feature Learning, 2010.  Smolensky, P. Information Processing in Dynamical Sys- tems: Foundations of Harmony Theory. In Rumelhart, D. E. and McClelland, J. L. (eds.), Parallel Distributed Processing: Explorations in the Microstructure of Cog- nition (vol. 1), pp. 194–281. MIT Press, 1986.  Tieleman, T. Training Restricted Boltzmann Machines using Approximations to the Likelihood Gradient. In 25th International Conference on Machine Learning, pp. 1064–1071, 2008.  Tieleman, T. and Hinton, G. E. Using Fast Weights to Im- prove Persistent Contrastive Divergence. In 26th Inter- national Conference on Machine Learning, pp. 1033– 1040, 2009.  Yuille, A. The Convergence of Contrastive Divergence. In Advances in Neural Information Processing Systems (NIPS’04), volume 17, pp. 1593–1600. MIT Press, 2005.  ","Restricted Boltzmann Machines (RBMs) are general unsupervised learningdevices to ascertain generative models of data distributions. RBMs are oftentrained using the Contrastive Divergence learning algorithm (CD), anapproximation to the gradient of the data log-likelihood. A simplereconstruction error is often used to decide whether the approximation providedby the CD algorithm is good enough, though several authors (Schulz et al.,2010; Fischer & Igel, 2010) have raised doubts concerning the feasibility ofthis procedure. However, not many alternatives to the reconstruction error havebeen used in the literature. In this manuscript we investigate simplealternatives to the reconstruction error in order to detect as soon as possiblethe decrease in the log-likelihood during learning."
1312.6055,2014,Unit Tests for Stochastic Optimization  ,"['Tom Schaul', 'Ioannis Antonoglou', 'David Silver']",https://arxiv.org/pdf/1312.6055.pdf,"4 1 0 2     b e F 5 2         ]  G L . s c [      3 v 5 5 0 6  .  2 1 3 1 : v i X r a  Unit Tests for Stochastic Optimization  Tom Schaul  Ioannis Antonoglou  David Silver  DeepMind Technologies  130 Fenchurch Street, London, UK  {tom,ioannis,david}@deepmind.com  Abstract  Optimization by stochastic gradient descent is an important component of many large-scale machine learning algorithms. A wide variety of such optimization algorithms have been devised; however, it is unclear whether these algorithms are robust and widely applicable across many different optimization landscapes. In this paper we develop a collection of unit tests for stochastic optimization. Each unit test rapidly evaluates an optimization algorithm on a small-scale, isolated, and well-understood difﬁculty, rather than in real-world scenarios where many such issues are entangled. Passing these unit tests is not sufﬁcient, but absolutely necessary for any algorithms with claims to generality or robustness. We give initial quantitative and qualitative results on numerous established algorithms. The testing framework is open-source, extensible, and easy to apply to new algorithms.  1  Introduction  Stochastic optimization [1] is among the most widely used components in large-scale machine learn- ing, thanks to its linear complexity, efﬁcient data usage, and often superior generalization [2, 3, 4]. In this context, numerous variants of stochastic gradient descent have been proposed, in order to improve performance, robustness, or reduce tuning effort [5, 6, 7, 8, 9]. These algorithms may derive from simplifying assumptions on the optimization landscape [10], but in practice, they tend to be used as general-purpose tools, often outside of the space of assumptions their designers in- tended. The troublesome conclusion is that practitioners ﬁnd it difﬁcult to discern where potential weaknesses of new (or old) algorithms may lie [11], and when they are applicable – an issue that is separate from raw performance. This results in essentially a trial-and-error procedure for ﬁnd- ing the appropriate algorithm variant and hyper-parameter settings, every time that the dataset, loss function, regularization parameters, or model architecture change [12]. The objective of this paper is to establish a collection of benchmarks to evaluate stochastic opti- mization algorithms and guide algorithm design toward robust variants. Our approach is akin to unit testing, in that it evaluates algorithms on a very broad range of small-scale, isolated, and well- understood difﬁculties, rather than in real-world scenarios where many such issues are entangled. Passing these unit tests is not sufﬁcient, but absolutely necessary for any algorithms with claims to generality or robustness. This is a similar approach to the very fruitful one taken by the black-box optimization community [13, 14]. The core assumption we make is that stochastic optimization algorithms are acting locally, that is, they aim for a short-term reduction in loss given the current noisy gradient information, and possibly some internal variables that capture local properties of the optimization landscape. These local actions include both approaching nearby optima, and navigating slopes, valleys or plateaus that are far from an optimum. The locality property stems from computational efﬁciency concerns, but it has the additional beneﬁts of minimizing initialization bias and allowing for non-stationary optimization, because properties of the obervation surface observed earlier in the process (and their  1  Figure 1: Some one-dimensional shape prototypes. The ﬁrst six example shapes are atomic pro- totypes: a quadratic bowl, an absolute value, a cliff with a non differential point after which the derivative increases by a factor ten, a rectiﬁed linear shape followed by a bend, an inverse Gaus- sian, an inverse Laplacian. The next six example shapes are concatenations of atomic prototypes: a sigmoid as a concatenation of a non convex Gaussian a line and an exponential, a quadratic bowl followed by a cliff and then by an exponential function, a quadratic bowl followed by a cliff and another quadratic bowl, a sinusoid as a concatenation of quadratic bowls, a line followed by a Gaus- sian bowl, a quadratic bowl and a cliff and ﬁnally, a Laplace bowl followed by a cliff and another Laplace bowl.  conseuences for the algorithm state) are quickly forgotten. We therefore concentrate on building local unit tests, that investigate algorithm dynamics on a broad range of local scenarios, because we expect that detecting local failure modes will ﬂag an algorithm as unlikely to be robust on more complex tasks – and as a ﬁrst approximation, optimization on such a complex task can be seen as a sequence of many smaller optimization problems (many of which will not have local optima). Our divide-and-conquer approach consists of disentangling potential difﬁculties and testing them in isolation or in simple couplings. Given that our unit tests are small and quick to evaluate, we can have a much larger collection of them, testing hundreds of qualitatively different aspects in less time than it would take to optimize a single traditional benchmark to convergence, thus allowing us to spot and address potential weaknesses early. Our main contribution is a testing framework, with unit tests designed to test aspects such as: dis- continuous or non-differentiable surfaces, curvature scales, various noise conditions and outliers, saddle-points and plateaus, cliffs and asymmetry, and curl and bootstrapping. It also allows test cases to be concatenated by chaining them in a temporal series, or by combining them into multi- dimensional unit tests (with or without variable coupling). We give initial quantitative and qualitative results on a number of established algorithms. We do not expect this to replace traditional benchmark domains that are closer to the real-world, but to complement it in terms of breadth and robustness. We have tried to keep the framework general and extendable, in the hope it will further grow in diversity, and help others in doing robust algorithm design.  2  2 Unit test Construction  Our testing framework is an open-source library containing a collection of unit tests and visualization tools. Each unit test is deﬁned by a prototype function to be optimized, a prototypical scale, a noise prototype, and optionally a non-stationarity prototype. A prototype function is the concatenation of one or more local shape prototypes. A multi-dimensional unit test is a composition of one- dimensional unit tests, optionally with a rotation prototype or curl prototype.  2.1 Shape Prototypes  Shape prototypes are functions deﬁned on an interval, and our collection includes linear slopes (zero curvature), quadratic curves (ﬁxed curvature), convex or concave curves (varying curvature), and curves with exponentially increasing or decreasing slope. Further, there are a number of non- differentiable local shape prototypes (absolute value, rectiﬁed-linear, cliff). All of these occur in realistic learning scenarios, for example in logistic regression the loss surface is part concave and part convex, an MSE loss is the prototypical quadratic bowl, but then regularization such as L1 introduces non-differentiable bends (as do rectiﬁed-linear or maxout units in deep learning [15, 16]). Steep cliffs in the loss surface are a common occurrence when training recurrent neural networks, as discussed in [11]. See the top rows of Figure 1 for some examples of shape prototypes.  2.2 One-dimensional Concatenation  In our framework, we can chain together a number of shape prototypes, in such a way that the result- ing function is continuous and differentiable at all junction points. We can thus produce many pro- totype functions that closely mimic existing functions, e.g., the Laplace function, sinusoids, saddle- points, step-functions, etc. See the bottom rows of Figure 1 for some examples. A single scale parameter determines the scaling of a concatenated function across all its shapes using the junction constraints. Varying the scales is an important aspect of testing robustness because it is not possible to guarantee well-scaled gradients without substantial overhead. In many learning prob- lems, effort is put into proper normalization [17], but that is insufﬁcient to guarantee homogeneous scaling, for example throughout all the layers of a deep neural network.  2.3 Noise Prototypes  The distinguishing feature of stochastic gradient optimization (compared to batch methods) is that it relies on sample gradients (coming from a subset of even a single element of the dataset) which are inherently noisy. In out unit tests, we model this by four types of stochasticity:  • Scale-independent additive Gaussian noise on the gradients, which is equivalent to random translations of inputs in a linear model with MSE loss. Note that this type of noise ﬂips the sign of the gradient near the optimum and makes it difﬁcult to approach precisely.  • Multiplicative (scale-dependent) Gaussian noise on the gradients, which multiplies the gra- dients by a positive random number (signs are preserved). This corresponds to a learning scenario where the loss curvature is different for different samples near the current point.  • Additive zero-median Cauchy noise, mimicking the presence of outliers in the dataset. • Mask-out noise, which zeros the gradient (independently for each dimension) with a certain probability. This mimics both training with drop-out [18], and scenarios with rectiﬁed linear units where a unit will be inactive for some input samples, but not for others.  For the ﬁrst three, we can vary the noise scale, while for mask-out we pick a drop-out frequency. This noise is not necessarily unbiased (as in the Cauchy case), breaking common assumptions made in algorithm design (but the modiﬁcations in section 2.5 are even worse). See Figure 2 for an illus- tration of the ﬁrst two noise prototypes. Noise prototypes and prototype functions can be combined independently into one-dimensional unit tests.  3  Figure 2: Examples of noise applied on prototype functions, green dashed are typical sample gradients, and the standard deviation range is the blue area. The upper two subplots depict Gaussian additive noise, while the lower two show Gaussian multiplicative noise. In the left column, the noise is applied to the gradients of a quadratic bowl prototype (note how the multiplicative noise goes to zero around the optimum in the middle), and on the right it is applied to a concatenation of prototypes.  2.4 Multi-dimensional Composition  A whole range of difﬁculties for optimization only exist in higher dimensional parameter spaces (e.g., saddle points, conditoning, correlation). Therefore, we build high-dimensional unit tests by composing together one-dimensional unit tests. For example for two one-dimensional prototype shapes La and Lb combined with a p-norm, the composition is L(a,b)(θ) = (La(θ1)p + Lb(θ2)p) p . Noise prototypes are composed independently of shape prototypes. While they may be composed of concatenated one-dimensional prototypes, higher-dimensional prototypes are not concatenated themselves. Various levels of conditioning can be achieved by having dramatically different scales in different component dimensions. In addition to the choice of prototypes to be combined, and their scale, we permit a rotation in input space, which couples the dimensions together and avoids axis-alignment. These rotations are particularly important for testing diagonal/element-wise optimization algorithms.  1  2.5 Curl  In reinforcement learning a value function (the expected discounted reward for each state) can be learned using temporal-difference learning (TD), an update procedure that uses bootstrapping: i.e. it pulls the value of the current state towards the value of its successor state [19]. These stochastic update directions are not proper gradients of any scalar energy ﬁeld [20], but they still form a (more general) vector ﬁeld with non-zero curl, where the objective for the optimization algorithm is to converge to its ﬁxed-point(s). See Figure 4 for a detailed example. We implemented this aspect by allowing different amounts of curl to be added on top of a multi-dimensional vector ﬁeld in our unit tests, which is done by rotating the produced gradient vectors using a ﬁxed rotation matrix. This is reasonably realistic; in fact, for the TD example in Figure 4, the resulting vector ﬁeld is exactly the gradient ﬁeld of a quadratic combined with a (small-angle) rotation.  2.6 Non-stationarity  In many settings it is necessary to optimize a non-stationary objective function. This may typically occur in a non-stationary task where the problem to be solved changes over time. However, non-  4  Figure 3: Examples of multivariate prototypes. The ﬁrst subplot depicts an asymmetric quadratic bowl with correlated dimensions, the second a surface with a saddle point, the third a sharp valley surface, the fourth a half-pipe surface where the ﬁrst dimension is a line and the second one a quadratic bowl. The ﬁfth subplot depicts a surface with an ill conditioned minimum in the point where the two canyons overlap. The surface in the last subplot is the composition of a quadratic bowl in the ﬁrst dimension and of a cliff in the second.  Figure 4: Here, we consider a very simple Markov process, with two states and stochastic transitions between them, and a reward of 0 in the ﬁrst and of 1 in the second state. Consider the parameters of our optimization θ to be the two state values. Each TD update changes one of them, depending on the stochastic transition observed. In this ﬁgure, we plot the vector ﬁeld of expected update directions (blue arrows) as a function of θ, as well as one sampled trajectory of the TD algorithm. Note how this vector ﬁeld is not actually a gradient ﬁeld, but instead has substantial curl, making it a challenging stochastic optimization task.  stationary optimization can even be important in large stationary tasks (with temporal structure in the samples), when the algorithm chooses to track a particular dynamic aspect of the problem, rather than attempting to converge to a global but static solution of the problem [21]. In addition, reinforcement learning (RL) tasks often involve non-stationary optimization. For example, many RL algorithms proceed by evaluating the value function using the TD algorithm described in the  5  previous section. This results in two sources of non-stationarity: the target value changes at every step (resulting in the previously described curl); and also the state distribution changes as the value function improves and better actions are selected. These scenarios can be therefore be viewed as non-stationary loss functions, but whose optimum moves as a function of the current parameter values. We test non-stationarity in three different ways. We let the location of the optimum move smoothly, via random translations of the parameter space, or we let the the scale of the shape prototype vary randomly (on average by 10% in each direction), or, on noisy unit tests, we let the scale of the noise vary randomly. Currently, these changes happen once every 10 steps. A type of non-stationarity that involves more abrupt switching is discussed in section 4.1.  3 Experiments  3.1 Setup and Algorithms  For our experiments, we test the candidate algorithms on over 3000 unit tests, with up to 10 param- eter dimensions. Each algorithm-unit test pairing is repeated 10 times, but with reusing the same 10 random seeds across all algorithms and setups. For each run k, we compute the true expected loss  at the parameter value reached after 100 update steps L(k) = E(cid:104)L(cid:16)  (cid:17)(cid:105)  .  θ(k) 100  The algorithms evaluated are SGD with ﬁxed learning rate η0 ∈ [10−6, 10], SGD with annealing with decay factor in [10−2, 1] and initial rates η0, SGD with momentum (regular or Nesterov’s variant [22]) [0.1, 0.999] and initial rates η0, SGD with parameter averaging [] with decay term in [10−4, 0.5] and exponent in { 1 4 , 1}, ADAGRAD [10] with initial rates η0, ADADELTA [23] with decay parameter (1 − γ) ∈ [10−4, 0.5] and regularizer in [10−6, 10−2, the incremental delta-bar- delta algorithm (IDBD [24]), RPROP [25] with initial stepsizes η0, RMSprop [26] with minimal learning rates η0, maximal learning rates in [10, 103] and decay parameter γ, as well as conjugate gradients. For the hyper-parameters ranges, we always consider one value per order of magnitude, and exhaustively sweep all combinations.  2 , 3  3.2 Reference performance Each unit test is associated with a reference performance Lsgd, and a corresponding reference learn- ing rate ηbest that is determined by doing a parameter sweep over all ﬁxed learning rates for SGD (34 values log-uniform between 10−10 and 10) and retaining the best-performing one. In our aggregate plots, unit tests are sorted (per group) by their reference learning rate, i.e., those that require small steps on the left, and those where large steps are best on the right. Algorithm setups are sorted as well, on the vertical axis, by their median performance on a reference unit test (quadratic, additive noise).  3.3 Qualitative Evaluation The algorithm performance L(k) is converted to a normalized value L(k) where Linit = E[L(θ0)] is the expected loss value at the initial point, similar to the approach taken in [27], but even more condensed. In other words, a normalized value near zero corresponds to no progress, negative denotes divergence, and a value near one is equivalent to the best SGD. Based on these results, we assign a qualitative color value to the performance of each algorithm setup on each unit test, to able to represent it in a single pixel in the resulting ﬁgures:  norm = L(k)−Linit Lsgd−Linit  • Red: Divergence or numerical instability in all run. • Violet: Divergence or numerical instability in at least one run. • Orange: Insufﬁcient progress: median(Lnorm) < 0.1 • Yellow: Good progress: median(Lnorm) > 0.1 and high variability: Lnorm < 0.1 for at  least 1  4 of the runs.  6  • Green: Good progress: median(Lnorm) > 0.1 and low variability: Lnorm < 0.1 for at • Blue: Excellent progress: median(Lnorm) > 2.  4 of the runs.  most 1  3.4 Results  Figures 5 and 6 shows the qualitative results of all algorithm variants on all the unit tests. There is a wealth of information in these visualizations. For example the relatively scarce amount of blue indicate that it is difﬁcult to substantially beat well-tuned SGD in performance on most unit tests. Another unsurprising conclusion is that hyper-parameter tuning matters much less for the adaptive algorithms (ADAGRAD, ADADELTA, RPROP, RMSprop) than for the non-adaptive SGD variants. Also, while some unit tests are more tricky than others on average, there is quite some diversity in the sense that some algorithms may outdo SGD on a unit test where other algorithms fail (especially on the non-differentiable functions).  4 Realism and Future Work  We do not expect to replace real-world benchmark domains, but rather to complement them with our suite of unit tests. Still, it is important to have sufﬁcient coverage of the types of potential difﬁculties encountered in realistic settings. To a much lesser degree, we may not want to clutter the test suite with unit tests that measure issues which never occur in realistic problems. It is not straightforward to map very high-dimensional real-world loss functions down to low- dimensional prototype shapes, but it is not impossible. For example, in Figure 8 we show some random projections in parameter space of the loss function in an MNIST classiﬁcation task with an MLP [28]. We defer a fuller investigation of this type, namely obtaining statistics on how commonly different prototypes are occurring, to future work. However, the unit tests capture the properties of some examples that can be analyzed. One of them was discussed in section 2.5, another one is the simple loss function of a one-dimensional auto- encoder:  Lθ(x) = (x + θ2 · σ(x · θ1))2  where σ is the sigmoid function. Even in the absence of noise, this minimal scenario has a saddle- point near θ = (0, 0), a plateau shape away from the axes, a cliff shape near the vertical axis, and a correlated valley near θ = (1, 1), as illustrated in Figure 7. All of these prototypical shapes are included in our set of unit tests. An alternative approach is predictive: if the performance on the unit tests is highly predictive of an algorithm’s performance on a some real-world task, then those unit tests must be capturing the essential aspects of the task. Again, building such a predictor is an objective for future work.  4.1 Algorithm Dynamics  Our long-term objective is to be able to do systematic testing and a full investigation of the opti- mization dynamics for a given algorithm. Of course, it is not possible to test it exhaustively on all possible loss functions (because there are inﬁnitely many), but a divide-and-conquer approach may be the next best thing. For this, we introduce the notion of algorithm state, which is changing during optimization (e.g., the current stepsize or momentum). Now, a long optimization process can be seen as the chaining of a number of unit tests, while preserving the algorithm state in-between them. Our hypothesis is that the set of all possible chains of unit tests in our collection covers most of the qualitatively different (stationary or non-stationary) loss functions an optimization algorithm may encounter. To evaluate an algorithm’s robustness (rather than its expected performance), we can assume that an adversary picks the worst-case unit tests at each step in the sequence. An algorithm is only truly robust if it does not diverge under any sequence of unit tests. Besides the worst-case, we may also want to study typical expected behavior, namely whether the dynamics have an attractor in the algorithm’s state space. If an attractor exists where the algorithm is stable, then it becomes useful to look at the secondary criterion for the algorithm, namely its expected (normalized) performance.  7  , e l b a t p e c c a = n e e r g  ,  y t i l i b a i r a v = w o l l e y  ,  w o l s = e g n a r o  , e c n e g r e v i d = t e l o i v / d e r  : s i  e d o c  r o l o c  e h T  .  w o r  r e p  s r e t e m a r a p - r e p y h  f o  t e s  e n o  h t i  w  d e r a h s  y b  d e p u o r g  , t s e t  t i n u  e n o  s i  n m u l o c  f o  p u o r g  h c a E  .  g n i p p a l r e v o  y l l a i t r a p  e b  n a c  h c a E  s t s e t  t i n u  f o  s p u o r g  e r e h w  , s t s e t  t i n u D 1  e e r f - e s i o n  l l a  s e d u l c n i  p u o r g  t s r ﬁ  e h t  e l p m a x e  r o f  . s t s e t  t i n u  l a n o i s n e m i d - e n o  , y r a n n o i t a t s  l l a  n o  ) 0 5 3 (  s t n a i r a v m h t i r o g l a  l l a  r o f  s t l u s e r  e v i t a t i l a u Q  : 5  e r u g i  F  , ) n o i t p a c  e e s (  s e i t r e p o r p  ,  m h t i r o g l a  e n o  s i  s w o r  . ) s l i a t e d r o f  t x e t  n i a m e e s (  t n e l l e c x e = e u l b  8  , e l b a t p e c c a = n e e r g  ,  y t i l i b a i r a v = w o l l e y  ,  w o l s = e g n a r o  , e c n e g r e v i d = t e l o i v / d e r  : s i  e d o c  r o l o c  e h T  . ) s p u o r g  e e r h t  t s a l (  s e n o  l a n o i s n e m i d - n e t  l l a  n o  d n a  t x e n (  s e n o  l a n o i s n e m i d - o w  t  l l a  n o  , ) s p u o r g  e e r h t  s r ﬁ (  s t s e t  t i n u  l a n o i s n e m i d - e n o  y r a n o i t a t s - n o n  l l a  n o  ) 0 5 3 (  s t n a i r a v m h t i r o g l a  l l a  r o f  s t l u s e r  e v i t a t i l a u Q  . s l i a t e d r o f  t x e t n i a m d n a  5 e r u g i  F e e S  . t n e l l e c x e = e u l b  , ) s p u o r g  e e r h t  : 6  e r u g i  F  9  Figure 7: Illustration of the loss surface of a one-dimensional auto-encoder, as deﬁned in the text, where the darkest blue corresponds to the lowest loss. Left: from the zoomed-out perspective if ap- pears to be roughly a vertical valley, leading an optimizer toward the y-axis from almost anywhere in the space. Center: the zoomed-in perspective around the origin, which is looking like a prototypical saddle point. Right: the shape of the valley in the lower left quadrant, the walls of which become steeper the more the search progresses.  Figure 8: Left: collection of 64 random projections into two dimensions of the MNIST loss sur- face (based on one randomly sampled digit for each column). The projections are centered around the weights learned after one epoch of training, and different projections are plotted on scales be- tween 0.05 (top row) and 0.5 (bottom row). Right: the same as on the left, but with axis-aligned projections.  We conjecture that this analysis may lead to novel insights into how to design robust and adaptive optimization algorithms.  5 Conclusion  This paper established a large collection of simple comparative benchmarks to evaluate stochastic optimization algorithms, on a broad range of small-scale, isolated, and well-understood difﬁculties. This approach helps disentangle issues that tend to be confounded in real-world scenarios, while retaining realistic properties. Our initial results on a dozen established algorithms (under a variety of different hyperparameter settings) show that robustness is non-trivial, and that different algorithms struggle on different unit tests. The testing framework is open-source, extensible to new function classes, and easy to use for evaluating the robustness of new algorithms. The full source code (see also Appendix A) is available under BSD license at:  https://github.com/IoannisAntonoglou/optimBench  10  Acknowledgements  We thank the anonymous ICLR reviewers for their many constructive comments.  References [1] H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical  Statistics, 22:400–407, 1951.  [2] L´eon Bottou. Online Algorithms and Stochastic Approximations. In David Saad, editor, Online  Learning and Neural Networks. Cambridge University Press, Cambridge, UK, 1998.  [3] L´eon Bottou and Yann LeCun. Large Scale Online Learning. In Sebastian Thrun, Lawrence Saul, and Bernhard Sch¨olkopf, editors, Advances in Neural Information Processing Systems 16. MIT Press, Cambridge, MA, 2004.  [4] L´eon Bottou and Olivier Bousquet. The Tradeoffs of Large Scale Learning.  In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Sys- tems, volume 20, pages 161–168. NIPS Foundation (http://books.nips.cc), 2008.  [5] A. Benveniste, M. Metivier, and P. Priouret. Adaptive Algorithms and Stochastic Approxima-  tions. Springer Verlag, Berlin, New York, 1990.  [6] N. Le Roux, P.A. Manzagol, and Y. Bengio. Topmoumoute online natural gradient algorithm,  2008.  [7] Antoine Bordes, L´eon Bottou, and Patrick Gallinari.  SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent. Journal of Machine Learning Research, 10:1737–1754, July 2009.  [8] Wei Xu. Towards Optimal One Pass Large Scale Learning with Averaged Stochastic Gradient  Descent. ArXiv-CoRR, abs/1107.2490, 2011.  [9] Tom Schaul, Sixin Zhang, and Yann LeCun. No More Pesky Learning Rates. In International  Conference on Machine Learning (ICML), 2013.  [10] John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive Subgradient Methods for Online  Learning and Stochastic Optimization. 2010.  [11] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. Understanding the exploding gradient  problem. arXiv preprint arXiv:1211.5063, 2012.  [12] X. Glorot and Y. Bengio. Understanding the difﬁculty of training deep feedforward neural networks. In G. Orr and Muller K., editors, Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), pages 249–256. Society for Artiﬁcial Intelli- gence and Statistics, 2010.  [13] Nikolaus Hansen, Anne Auger, Steffen Finck, Raymond Ros, et al. Real-parameter black-box  optimization benchmarking 2010: Experimental setup. 2010.  [14] Nikolaus Hansen, Anne Auger, Raymond Ros, Steffen Finck, and Petr Poˇs´ık. Comparing results of 31 algorithms from the black-box optimization benchmarking BBOB-2009. In Pro- ceedings of the 12th annual conference companion on Genetic and evolutionary computation, pages 1689–1696. ACM, 2010.  [15] Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classiﬁcation with deep con- volutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1106–1114, 2012.  [16] Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio.  Maxout networks. arXiv preprint arXiv:1302.4389, 2013.  [17] Y. LeCun, L. Bottou, G. Orr, and K. Muller. Efﬁcient BackProp. In G. Orr and Muller K.,  editors, Neural Networks: Tricks of the trade. Springer, 1998.  [18] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhut- Improving neural networks by preventing co-adaptation of feature detectors. arXiv  dinov. preprint arXiv:1207.0580, 2012.  [19] R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. IEEE Transactions on  Neural Networks, 9(5):1054–1054, Sep 1998.  11  [20] Etienne Barnard. Temporal-difference methods and Markov models. IEEE Transactions on  Systems, Man, and Cybernetics, 23(2):357–365, 1993.  [21] Richard S. Sutton, Anna Koop, and David Silver. On the role of tracking in stationary environ- ments. In Proceedings of the Twenty-Fourth International Conference on Machine Learning (ICML 2007, pages 871–878. ACM Press, 2007.  [22] Yurii Nesterov and Arkadii Semenovich Nemirovskii. Interior-point polynomial algorithms in  convex programming, volume 13. SIAM, 1994.  [23] Matthew D Zeiler. ADADELTA: An Adaptive Learning Rate Method.  arXiv:1212.5701, 2012.  arXiv preprint  [24] Richard S Sutton. Adapting bias by gradient descent: An incremental version of delta-bar-  delta. In AAAI, pages 171–176, 1992.  [25] Martin Riedmiller and Heinrich Braun. A direct adaptive method for faster backpropagation learning: The RPROP algorithm. In Neural Networks, 1993., IEEE International Conference on, pages 586–591. IEEE, 1993.  [26] T Tieleman and G Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of  its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.  [27] Tom Schaul and Yann LeCun. Adaptive learning rates and parallelization for stochastic, sparse, non-smooth gradients. In International Conference on Learning Representations, Scottsdale, AZ, 2013.  [28] Yann LeCun and Corinna Cortes. http://yann.lecun.com/exdb/mnist/.  The MNIST dataset of handwritten digits.  1998.  A Appendix: Framework Software  As part of this work a software framework was developed for the computing and managing all the results obtained for all the different conﬁgurations of function prototypes and algorithms. The main component of the system is a database where all the results are stored and can be easily retrieved by querying the database accordingly. The building blocks of this database are the individual exper- iments, where each experiment is associated to a unit test and an algorithm with ﬁxed parameters. An instance of an experiment database can either be loaded from the disk, or it can be created on the ﬂy by running the associated experiments as needed. The code below creates a database and runs all the experiments for all the readily available algorithms and default unit tests, and then saves them to disk:  require ’experiment’ local db = experimentsDB() db:runExperiments() db:save(’experimentsDB’)  This database now can be loaded from the disk, and the user can query it in order to retrieve speciﬁc experiments, using ﬁlters. An example is shown below:  local db = experimentsDB() db:load(’experimentsDB’) local experiments = db:filter({fun={’quad’, ’line’},  algo={’sgd’}, learningRate=1e-4})  The code above loads an experiment database from the disk and it retrieves all the experiments for all the quadratic and line prototype shapes, for all different types of noise and all scales, further selecting the subset of experiments to those optimized using SGD with learningRate equal to 1e- 4. The user can rerun the extracted experiments or have access to the associated results, i.e., the expected value of the function in different optimization steps, along with the associated parameters values. In order to qualitatively assess the results the following code can be used:  db:ComputeReferenceValues() db:cleanDB()  12  db:assessPerformance() db:plotExperiments()  The code above computes the reference expected values for each prototype function, it removes the experiments for which no reference value is available, then it qualitatively assesses the performance of all the available experiments and ﬁnally it plots the results given the color conﬁguration described in section 3.3. It is really easy to add a new algorithm in the database in order to evaluate its robustness. The code below illustrates a simple example:  db:addAlgorithm(algoname, algofun, opt) db:testAlgorithm(algoname) db:plotExperiments({}, {algoname})  Here a new algorithm with name algoname, function instance algo (which should satisfy the optim interface), and a table of different parameter conﬁgurations opt is added to the database and it is tested under all available functions prototypes. Finally, the last line plots a graph with all the results for this algorithm. It is also possible to add a set of new unit tests to the database, and subsequently run a set of experiments associated with them. There are different parameters to be deﬁned for the creation of a set of unit tests (that allow wildcard speciﬁcation too):  1. the concatenated shape prototypes for each dimension, 2. the noise prototype to be applied to each dimension, 3. the scale of each dimension of the function, 4. in case of multivariate unit tests, a parameter speciﬁes which p-norm is used for the com-  bination,  5. a rotation parameter that induces correlation of the different parameter dimensions, and 6. a curl parameter that changes the vector ﬁeld of a multivariate function.  13  ","Optimization by stochastic gradient descent is an important component of manylarge-scale machine learning algorithms. A wide variety of such optimizationalgorithms have been devised; however, it is unclear whether these algorithmsare robust and widely applicable across many different optimization landscapes.In this paper we develop a collection of unit tests for stochasticoptimization. Each unit test rapidly evaluates an optimization algorithm on asmall-scale, isolated, and well-understood difficulty, rather than inreal-world scenarios where many such issues are entangled. Passing these unittests is not sufficient, but absolutely necessary for any algorithms withclaims to generality or robustness. We give initial quantitative andqualitative results on numerous established algorithms. The testing frameworkis open-source, extensible, and easy to apply to new algorithms."
1312.6095,2014,Multi-View Priors for Learning Detectors from Sparse Viewpoint Data  ,"['Bojan Pepik', 'Michael Stark', 'Peter Gehler', 'Bernt Schiele']",https://arxiv.org/pdf/1312.6095.pdf,"4 1 0 2     b e F 6 1         ]  V C . s c [      2 v 5 9 0 6  .  2 1 3 1 : v i X r a  Multi-View Priors for Learning Detectors From  Sparse Viewpoint Data  Bojan Pepik1  Michael Stark1,2  Peter Gehler3  Bernt Schiele1  1Max Planck Institute for Informatics, 2Stanford University,  3Max Planck Institute for Intelligent Systems  Abstract  While the majority of today’s object class models provide only 2D bounding boxes, far richer output hypotheses are desirable including viewpoint, ﬁne-grained category, and 3D geometry estimate. However, models trained to provide richer output require larger amounts of training data, preferably well covering the rel- evant aspects such as viewpoint and ﬁne-grained categories. In this paper, we address this issue from the perspective of transfer learning, and design an object class model that explicitly leverages correlations between visual features. Specif- ically, our model represents prior distributions over permissible multi-view de- tectors in a parametric way – the priors are learned once from training data of a source object class, and can later be used to facilitate the learning of a detector for a target class. As we show in our experiments, this transfer is not only bene- ﬁcial for detectors based on basic-level category representations, but also enables the robust learning of detectors that represent classes at ﬁner levels of granularity, where training data is typically even scarcer and more unbalanced. As a result, we report largely improved performance in simultaneous 2D object localization and viewpoint estimation on a recent dataset of challenging street scenes.  1  Introduction  Motivated by higher-level tasks such as scene understanding and object tracking it has been argued that object class models should not only provide ﬂat, 2D bounding box detections but rather provide more expressive output, such as a viewpoint estimate [39, 43, 25, 40, 32, 45, 18] or an estimate of the 3D geometry of the object of interest [34, 47, 33, 14, 29, 49]. Similarly, there has been in increased interest in object representations that allow more ﬁne-grained distinctions than basic-level categories [24, 21, 42], for two reasons. First, these representations potentially perform better in recognition, as they explicitly address the modes of intra-class variation. And second, they, again, can provide further inputs to higher-level reasoning (e.g., in the form of ﬁne-grained category labels). However, today’s methods for 3D and ﬁne-grained object representations suffer from a major weak- ness: for robust parameter estimation, they tend to require an abundance of annotated training data that covers all relevant aspects (viewpoints, sub-categories) with sufﬁcient density. Unfortunately, this abundance of training data cannot be expected in general. Even in the case of dedicated multi- view datasets [39, 31, 1, 42] or when resorting to artiﬁcially rendered CAD data [25, 40, 50], the distribution of the number of available training images over object categories is known to be highly unbalanced and heavy-tailed [46, 38, 27]. This is particularly pronounced for categories at ﬁner levels of granularity, such as individual types or brands of cars1. Transfer learning has been acknowledged as a promising way to leverage scarce training data, by reusing once acquired knowledge as a regularizer in novel learning tasks [11, 41, 16]. While it has  1Fig. 5 and 6 in Sect. 6 show the viewpoint data statistics for car-types and car-models on KITTI [17].  1  been shown that transfer learning can be beneﬁcial for performance, its use in computer vision has, so far, mostly been limited to either classiﬁcation tasks [11, 51, 37, 5] or ﬂat, 2D bounding box detection [2, 16], neglecting both the 3D nature of the recognition problem and more ﬁne-grained object class representations. The starting point and major contribution of this paper is therefore to design a transfer learning technique that is particularly tailored towards multi-view recognition (encompassing simultaneous bounding box localization and viewpoint estimation). It boosts detector performance for scarce and unbalanced training data, lending itself to ﬁne-grained object representations. To that end, we represent transferable knowledge as prior distributions over permissible mod- els [11, 16], in two different ﬂavors. The ﬁrst ﬂavor (Sect. 3.1) captures sparse correlations be- tween HOG cells in a multi-view deformable part model (DPM [12]), across viewpoints. While this is similar in spirit to [16] in terms of statistical modeling, we explicitly leverage 3D object ge- ometry [34, 33] in order to establish meaningful correspondences between HOG cells, in a fully automatic way. As we show in our experiments (Sect. 4), this already leads to some improvements in performance in comparison to [16]. The second ﬂavor (Sect. 3.2) extends the sparse correlations to a full, dense covariance matrix that potentially relates each HOG cell to every other HOG cell, again across viewpoints – this can be seen as directly learning transformations between different views, where the particular choice of source and target cells can function as a regularizer on the learned transformation, and leads to substantial improvements in simultaneous bounding box local- ization and viewpoint estimation. Both ﬂavors are simple to implement (covariance computation for prior learning and feature transformation for prior application) and hence widely applicable, yet lead to substantial performance improvements for realistic training data with unbalanced viewpoint distributions. Our paper makes the following speciﬁc contributions: First, to our knowledge, our work is the ﬁrst attempt to explicitly design a transfer learning technique for multi-view recognition and ﬁne- grained object representations. Second, we propose two ﬂavors of learning prior distributions over permissible multi-view detectors, one based on sparse correlations between cells and one based on the full covariance. Both are conveniently expressed as instantiations of a class of structured priors that are easy to implement and can be readily applied to current state-of-the-art multi-view detectors [12, 34, 33]. And third, we provide an in-depth experimental study of our models, ﬁrst investigating multi-view transfer learning under the controlled conditions of a standard multi-view benchmark [39] (Sect. 4.1), and ﬁnally demonstrating improved performance for simultaneous ob- ject localization and viewpoint estimation on realistic street scenes [17] (Sect. 4.2).  2 Related work  The problem of scarce training data has mainly been addressed in two different ways in the literature. Generating training data. The ﬁrst way is to explicitly generate more training data for the task at hand, e.g., by rendering CAD models of the object class of interest. Rendered data has successfully been applied in the context of multi-view recognition [26, 25, 40, 50, 34, 33], people detection [36, 35], and indoor scene understanding [7]. The success of these approaches is due to the fact that unlimited amounts of training data can be generated with lots of variation in viewpoint, shape and articulation, from just a few models. Existing approaches differ mostly in the acquisition of appropriate models (3D scanning [36, 35] vs. manual design [26, 25, 40, 50, 34, 33]) and the rendering output, ranging from close to photo-realistic images [35, 26] to directly rendering edge- based features [40, 36, 34, 33]. A special case of data “generation” is the borrowing of training examples from other object classes [27], adapting feature representations to exploit data from dif- ferent domains [22] or using decorrelated features [20]. While our model based on local correlation structures (Sect. 3.1) can leverage CAD models of the object class, this is done for the sole purpose of deriving correspondences based on 3D geometry, and does not include object appearance. Transfer learning. The second way is to ﬁrst condense available training data into a model, and then reusing that model in the context of another learning task. While there is a vast amount of literature on this kind of transfer learning, most approaches focus on object or image classiﬁcation rather than detection [30, 15, 4, 3, 44, 51, 23, 37, 5]. In terms of detection, approaches range from shape-based object class representations relying on manually-annotated parts in a single view [41]  2  Nt(cid:88)  i=1  over HOG- ([6]) templates with ﬁxed layout [2] to full-ﬂedged deformable part models [16] that share low-level feature statistics. Common to all these approaches is that they are agnostic about the inherent 3D nature of the object class detection problem; in contrast, our models explicitly leverage 3D (Sect. 3.1) or viewpoint (Sect. 3.2) information. This is a key difference in particular to [16], which focuses entirely on low-level features that can be universally transferred. Since part of our evaluation is performed on object categories of a ﬁne granularity (individual car-models), we acknowledge that this is of course connected to work in ﬁne-grained categorization [48, 10, 42]. In contrast to these works, however, the focus of ours is on learning multi-view detectors from scarce training data, rather than classifying cropped images according to a ﬁne-grained class taxonomy.  3 Multi-view transfer learning  We consider the scenario of transfer learning for object models. The goal is to train an object detection model for a target class for which only very few labeled examples are available. However we have access to an existing (or several) object model for a similar (or the same) object class, the source models. The main intuition that guides our approach is that if we extract common regularities shared by both object classes, then this in turn can be used to devise better target models. In the case of object detection on HOG [6] we reason that although the actual feature distribution may differ, there are similarities in how the features deform under transformations such as viewpoint changes. Preliminaries. More formally, let us denote by ws the parameters of a source model. Speciﬁcally in the case of multi component detectors we have ws = [ws C], where the individual ws i denote different components of the models. As we are interested in the multi-view setting, the components represent speciﬁc viewpoints in our case. Given ws and a few Nt labeled examples of a target class {xi, yi; i ∈ {1, . . . , Nt}}, the goal is to derive a detection model wt. This is implemented via the regularized risk functional, which has been used for multi-task learning in [9]  1, . . . , ws  wt = argmin  w  J(w) +  l(w, xi, yi),  (1)  consisting of a regularization J(w) and a data ﬁt term, here the empirical loss l on the training data points. The data term is standard and we may use loss functions such as structured losses or simpler losses like the Hinge loss for classiﬁcation. In addition to the data term we regularize the model parameters with J, that is derived using information from the source model. We use a transfer learning objective based on [9] where the same regularizer is proposed in the context of multi-task learning. The regularizer is quadratic and of the form  J(w) = w(cid:62)Ksw.  We distinguish between different choices of Ks, implementing different possibilities to transfer knowledge from the source model. When Ks = I, the objective (1) reduces to the standard SVM case. In the following, we will explore three different variants for the knowledge transfer matrix Ks.  3.1 Learning sparse correlation structures  Let us denote with w an appearance ﬁlter of one viewpoint component in the entire set of parameters w. For simplicity we will simply refer to this as w without using sub- or superscripts. This ﬁlter is of size n × m × L. It has spatial extent of n × m cells, and L are ﬁlter values computed in each cell (L = 32 in [12]). We denote each cell j as a vector wj ∈ RL. We implement different versions of the transfer learning objective (1) using a graph Laplacian regularization approach ([9], Sect 3.1.3) by choosing where Σs encodes correlations between different cells in the model. The matrix Σs is of size P × P , with P being the total number of model parameters. To distinguish between different choices for the structure of Σs we denote with ∼n a relationship of type n between two cells in w. With “type”, for example we can refer to a spatial relation among cells, such as horizontal neighbors, vertical neighbors, etc. This deﬁnes a set of cell pairs Pn = {(wj, wk)|j ∼n k} in the model that satisfy relation ∼n. From the set Pn one can compute cross covariances for different types  J(w) = w(cid:62)Ksw = w(cid:62)  (I − λΣs)w,  Σn =  (wj − ¯w)(wk − ¯w)  (cid:62),  (2)  (cid:88)  j∼nk  3  (cid:80) j wj is the mean of the set of cells. The full P × P matrix Σs is then constructed where ¯w = 1|Pn| from the smaller L× L block matrices Σn (details below). This results in a sparse Σs, as the number of cell pairs satisfying a relation is small compared to the total number of possible cell pairs. Single view correlation structures (SVM-SV). Originally proposed in [16], SVM-SV aims at capturing generic neighborhood relationships between HOG cells within a single template (i.e., a single view). This implements a speciﬁc choice for ∼n. SVM-SV focuses on 5 speciﬁc relation types: horizontal (∼h), vertical (∼v), upper-left diagonal (∼d1) and upper-right diagonal (∼d2) cell relations. In addition, SVM-SV captures self-correlations of the same cell ∼cell. For a given relation type ∼n, SVM-SV takes into account all cell pairs in the template which satisfy the relation ∼n, to compute each of the different cross-covariances Σh, Σv, Σd1, Σd2 and Σcell. Multi-view correlation structures (SVM-MV). We extend SVM-SV to encompass multi-view knowledge transfer. In our model different components w of w correspond to different viewpoints of the object class. Different components are very related since they have a common cause in the geometric structure of the three dimensional object. Therefore, the goal of SVM-MV is to capture the across-view cell relations in addition to the single view cell relation introduced by SVM-SV. For that purpose, we learn a new, across-view relation type ∼mv, capturing cell relationships across different views. In order to establish cell relationships across viewpoints, we use a 3D CAD model of the object class of interest (or a generic 3D ellipsoid with proper aspect ratio in case we do not have a CAD model available for a class), which provides a unique 3D reference frame across views. The alignment between learned and 3D CAD models is achieved by back-projecting 2D cell positions onto the 3D surface, assuming known viewpoints for the learned models and ﬁxed perspective projection. We then establish cell relationships between cells that back-project onto the same 3D surface patch in neighboring views, and learn Σmv. After computing the different cross-covariances Σn for both SVM-SV and SVM-MV from the source models, we construct the Σs matrix, which is further on used as a regularizer in the target model training (Eq. 1). Σs uses the learned cell-cell correlations of different types from the source models, to guide the training of the target model. In order to construct Σs, we ﬁrst establish pairs of cells (wi, wj) in the target model which satisfy a certain relation type ∼n (e.g. neighbors across s with Σn. We apply this procedure views) and then we populate the corresponding entries in Σs, Σi,j for all cell relation types deﬁned in SVM-SV and SVM-MV.  3.2 Learning dense multi-view correlation structures (SVM-Σ)  SVM-MV and SVM-SV capture correlation structures among model cells that satisfy certain cell relations (2D neighboring cells, 3D object surface) resulting in a sparse graph encoded by Σs. In the following, we extend this limited structure to a dense graph, that potentially captures relationships among all cells in the model. We will refer to this model as SVM-Σ. Let’s assume we are given a set of N source models {ws models using bootstrapping. Then, we compute the un-normalized covariance matrix Σemp  N}, for example by training several  1, . . . , ws  Σemp =  1 N  (cid:62)  ws i ws  i  (3)  which is a rank N matrix. We set Σs = Σemp. This variant of Σs (SVM-Σ) is dense and captures correlations of all types among all cells across all viewpoints in the model. Unlike SVM-MV and SVM-SV, which are rather generic in nature (e.g., all pairs of horizontal pairs in the template are considered when learning Σh), SVM-Σ can capture very speciﬁc and local cell correlations, within a single template (view) and across views. Fig. 1 (left) visualizes a heatmap of the strength of the learned correlations for SVM-Σ between given reference cells (red squares, black cubes) and all other cells, back-projected onto the 3D surface of a car CAD model. Note that the heatmaps indeed reﬂect meaningful relationships (e.g., the front wheel surface patch shows high correlation with back wheel patches). While SVM-Σ is a symmetric prior (as the correlations are computed across all views in the model), we also consider the case where the target training data distribution is sparse over viewpoints. We  4  N(cid:88)  i=1  Figure 1: (Left) Learned priors visualized in 3D (for a reference cell). Red indicates the reference cell. The black cube indicates the reference cell back-projected into 3D. (Right) SVM-Σ versions.  address this by sparser, asymmetric variants of SVM-Σ that connect only certain views with each other, by zeroing out parts of the Σs using an element-wise multiplication with a sparse matrix S as S ◦ Σs. Several choices of S are depicted in Figure 1 (right). We distinguish between the following asymmetric priors: SVM-Σ-TD2ND, where we transfer knowledge from views for which we have target data to views with no target training data, SVM-Σ-TD2ALL with transfer from views with target data to all views, and SVM-Σ-NB2ALL where we transfer from every viewpoint to its neighboring viewpoints.  3.3 Learning a target model using the learned Ks matrix We perform model learning (Eq. 1) by ﬁrst doing a Cholesky decomposition of Ks = U(cid:62)U. This allows us to deﬁne feature and model transformations: ˜x = U−(cid:62)x and ˜w = Uw. Using these transformations, one can show that w(cid:62)Ksw = ˜w(cid:62) ˜x = w(cid:62)x, which means we can learn a target model by ﬁrst, transforming the features and the models using U, then training a model via a standard SVM solver in the transformed space, and in the end transforming back the trained model. In the SVM-SV case, to be compatible with [16], we perform eigen decomposition instead of Cholesky.  ˜w and ˜w(cid:62)  4 Experiments  In this section, we carefully evaluate the performance of our multi-view priors. First (Sect. 4.1), we provide an in-depth analysis of different variants of the SVM-MV and SVM-Σ priors in a controlled training data setting, by varying the viewpoint distribution of the training set. We learn target models using a few target training examples plus our priors and compare them to using the SVM-SV prior proposed in [16] and using standard SVM. We perform the analysis on two tasks, 2D bounding box localization and viewpoint estimation on the 3D Object Classes dataset [39], demonstrating successful knowledge transfer even for cases in which there is no training data for 3/4 of the viewing circle. Second (Sect. 4.2), we highlight the potential of our SVM-Σ priors to greatly improve the performance of simultaneous 2D bounding box localization and viewpoint estimation in a realistic, uncontrolled data set of challenging street scenes (the tracking benchmark of KITTI [17]). For computational reasons, we restrict ourselves to the root-template-only version of the DPM [13] as the basis for all our models in Sect. 4.1, but consider the full, part-based version for the more challenging and realistic experiments in Sect. 4.2. In all cases, the C parameter is ﬁxed to 0.002 [12] for all tested methods. We set λ = 0.9/emax, where emax is the biggest eigenvalue of Σs. We empirically veriﬁed that this always resulted in a positive deﬁnite matrix Ks, which makes Eq. 1 a convex optimization problem.  4.1 Comparison of multi-view priors  We start by comparing the different multi-view priors on the 3D Object Classes dataset [39] (a widely accepted multiview-benchmark with balanced training and test data from 8 viewpoint bins, for 9 object classes), in two sets of experiments. In the ﬁrst set, we use the same number k of target training examples per view (multi-view k-shot learning). In the second set, we exclude certain viewpoints completely from the training data (k = 0), keeping only a single example from each of the other viewpoints (sparse multi-view k-shot learning). In both cases, the test set requires detecting  5  All viewsAll viewsSVM−ΣAll viewsSVM−Σ−TD2AllTarget data viewsNo target data viewsTarget data views  No target data viewsSVM−Σ−TD2NDTarget data viewsNo target data viewsNeighboring viewsAll viewsSVM−Σ−NB2All  10Figure 2: 2D BB localization (left) and viewpoint estimation (right) on 3D Object Classes [39].  car  [28]  SVM  SVM-SV SVM-MV SVM-Σ 99.6 / 92.9 99.8 / 95.0 99.8 / 92.5 99.8 / 95.0 96.0 / 89.0 bicycle 88.8 / 87.6 89.9 / 87.6 96.7 / 92.2 90.1 / 87.9 91.0 / 88.0  [19] DPM-hinge+VP 99.6 / 92.5 -/- 98.6 / 93.0 -/- 93.3 / 86.3 -/- iron 94.9 / 94.7 96.4 / 96.1 90.6 / 88.8 97.0 / 95.5 62.9 / 65.4 -/- cell. 51.0 / 82.2 51.2 / 82.3 53.7 / 80.9 51.1 / 81.5 73.1 / 62.2 -/- mouse 61.3 / 74.7 61.2 / 70.8 61.4 / 69.8 62.5 / 72.5 -/- 97.9 / 71.0 shoe 93.9 / 81.4 93.4 / 87.3 94.7 / 86.2 94.8 / 86.5 84.4 / 62.8 -/- stapler 71.5 / 69.2 72.6 / 70.2 74.2 / 70.2 74.2 / 70.2 toaster 94.4 / 70.6 95.3 / 72.8 97.2 / 66.7 95.2 / 75.6 -/- 96.0 / 50.0 mAP 81.9 / 81.7 82.5 / 82.8 83.5 / 80.9 83.1 / 83.1 61.0 / 79.2 - / 74.2 88.2 / 72.9  53.0 / - 43.0 / - 41.0 / - 78.0 / - 32.0 / - 54.0 / -  [34]  DPM-VOC+VP  [34]  99.8 / 97.5 98.8 / 97.5 96.0 / 89.7 62.4 / 83.0 72.7 / 76.3 96.9 / 89.8 83.7 / 81.2 97.8 / 79.7 88.5 / 86.8  [25]  [50]  [32]  [18]  76.7/70.0 90.4/84.0 -/86.1 99.2/85.3 69.8 / 75.5  -/80.8  -/- -/- -/- -/- -/- -/- -/-  -/- -/- -/- -/- -/- -/- -/- -/-  -/- -/- -/- -/- -/- -/- -/- -/-  -/- -/- -/- -/- -/- -/- -/-  Table 1: Comparison to state-of-the-art on 3D Object Classes [39].  objects seen from the entire viewing circle. For each class, our priors are trained using bootstrapping, from 5 source models (each trained from 15 randomly sampled examples per view). The ﬁnal target model for a class is obtained by using k training examples from that class plus the respective prior. Multi-view k-shot learning. Fig. 2 plots 2D BB localization (left) and viewpoint estima- tion (right) performance for SVM, SVM-SV, SVM-MV, and SVM-Σ, varying the number k ∈ {1, 5, 10, all} of target training examples per view, averaged over 5 randomized runs. We make the following observations. First, we see that SVM-Σ outperforms all other methods by signiﬁcant margins for restricted training data (k ∈ {1, 5, 10}), for both 2D BB localization (by at least 20.1%, 8.0% and 3.8%, respectively) and viewpoint estimation (by 15.6%, 9.2% and 4.3% ). Second, the beneﬁt of SVM-Σ increases with decreasing number of training examples, saturating for k = all. And third, SVM-SV [16] and SVM-MV apparently fail to convey viewpoint-related information beyond what can be learned from the k target examples alone, performing on par with SVM. As a sanity check, Tab. 1 relates the complete per-class results for all methods and k = all (rightmost curve points in Fig. 2) to the state-of-the-art. Despite not using parts, our models in fact outperform previously reported results [19, 25, 50, 28, 32, 18] with and without priors, except for [34] based on DPM [12] with parts. As parts obviously improve performance, we add them in Sect. 4.2. Sparse multi-view k-shot learning. We move on to a more challenging setting in which (single) training examples are only available for selected views, but not for others. Successful localization and viewpoint estimation thus depends on prior information that can be “ﬁlled in” for the missing viewpoints. Fig. 3 plots precision-recall curves for the car class and six different settings of increas- ing difﬁculty, not having training data for just one view (front) (a), not for two views (front and left) (b), not for four views (diagonal) (c), off-diagonal (d), not for six views (diagonal, back and right) (e), and not for all views except front-left (f). Average precision and viewpoint estimation results are given in plot legends. We compare the performance of SVM, SVM-MV, SVM-Σ, and three further variations of SVM-Σ that restrict the structure of the prior covariance matrix (Sect. 3.2), namely, SVM-Σ-TD2ALL, SVM-Σ-TD2ND, and SVM-Σ-NB2ALL. In Fig. 3 (a) to (f), we observe that two methods succeed in transferring information to up to 6 unseen viewpoints (SVM-Σ, dark blue, and SVM-Σ-TD2ALL, green), with APs ranging from an impressive 99.7 to 88.1% and VPs ranging from 92.1 to 49.8% for SVM-Σ). This observation is conﬁrmed by the confusion matrices in Fig. 3 (g): both SVM-Σ and SVM-Σ-TD2ALL exhibit a much stronger diagonal structure than SVM. Understandably, performance deteriorates for just one observable viewpoint (Fig. 3 (f); AP drops to 30.7%, VP to 15.3%). SVM-MV (light blue) provides  6  1510all405060708090# train examples per viewmAPObject localization results  SVM−ΣSVM−MVSVMSVM−SV1510all405060708090# train examples per viewmVPViewpoint classification results  SVM−ΣSVM−MVSVMSVM−SVFigure 3: 3D Object Classes [39]. Unbalanced multi-view 0-shot experiments (on cars) with no training data for (a) Front, (b) Front and Left, (c) Off-diagonal views, (d) Diagonal views, (e) 1 training example for Left and Front views, (f) 1 example for Front-left view. (g) VP confusion matrices for the 0-shot Diagonal case. Bars on top indicate (with black) which viewpoints are used in training for each experiment.  an advantage over SVM (cyan) only for extremely little data (Fig. 3 (e), (f)), where it improves AP by 0.9% and 2.7%. Summary. We conclude that different kinds of priors (SVM-SV, SVM-MV, and variations of SVM-Σ) vary drastically in their ability to convey viewpoint-related information. Notably, we ob- serve only minor differences between SVM-SV, SVM-MV, and SVM, but large gains in both 2D bounding box localization and viewpoint estimation for SVM-Σ.  4.2 Leveraging multi-view priors for object detection  Having veriﬁed the ability of our SVM-Σ priors to transfer viewpoint information for scarce and unbalanced training data in Sect. 4.1, we now move on to an actual, realistic setting, which naturally exhibits the dataset statistics that we simulated earlier (see Fig. 4, 5 and 6). Speciﬁcally, we focus on the car object class on the KITTI street scene dataset [17] (and the tracking benchmark subset), consisting of 21 sequences (8,008 images, corresponding to 579 car tracks and 27, 300 annotated car bounding boxes) taken from a driving vehicle. We use 5 sequences for training and the rest for testing. Due to the car-mounted camera setup, the distribution of viewpoints for car objects is already heavily skewed towards back and diagonal views (cars driving in front of the camera car or being parked on the side of the road, see Fig. 4). This becomes even more severe when considering more ﬁne-grained categories, such as individual car-types (we distinguish and annotate 7: stat. wagon, convertible, coupe, hatchback, minibus, sedan, suv) and car-models (23 in total) 2. Evaluation criteria. Average precision (AP) computed using the Pascal VOC [8] overlap crite- rion, based solely on bounding boxes (BB) has been widely used as an evaluation measure. Since the ultimate goal of our approach is to enable simultaneous object localization and viewpoint esti- mation (both are equally important in an autonomous driving scenario), and in line with [17], we report performance for two combined measures (jointly addressing both tasks) in addition to AP and VP. Speciﬁcally, AP+VP-D allows a detection ˆy to be a true positive detection if and only if the viewpoint estimate ˆyv is the same as the ground truth yv. The second measure, AP+VP-C assigns a weight ˆw = (180◦ − |∠(ˆyv, yv)|)/180◦ to the true positive detection based on how well it aligns with the ground truth viewpoint. Also in line with [17], we report results for non-occluded objects.  2The annotations will be made publicly available upon publication.  7  00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.911 − precisionrecall  SVM−Σ  (VP=87.4) (AP = 99.3)SVM−MV (VP=59.1) (AP = 90.4)SVM (VP=56.4) (AP = 90.4)SVM−Σ−TD2ALL (VP=84.5) (AP = 99.3)SVM−Σ−TD2ND  (VP=58.3) (AP = 90.9)SVM−Σ−NB2ALL (VP=60.4) (AP = 91.6)(b)  Front and Left 0-shot00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.911 − precisionrecall  SVM−Σ (VP=83.8) (AP = 95.9)SVM−MV (VP=41.2) (AP = 53.5)SVM (VP=41.9) (AP = 54.1)SVM−Σ−TD2ALL(VP=80.0) (AP = 98.3)SVM−Σ−TD2ND (VP=41.3) (AP = 53.4)SVM−Σ−NB2ALL (VP=42.0) (AP = 54.7)(c)  Diagonal views 0-shot00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.911 − precisionrecall  SVM−Σ (VP=65.0) (AP = 96.2)SVM−MV (VP=34.7) (AP = 57.1)SVM (VP=35.0) (AP = 60.1)SVM−Σ−TD2ALL (VP=64.6) (AP = 94.2)SVM−Σ−TD2ND (VP=36.1) (AP = 57.5)SVM−Σ−NB2ALL (VP=33.3) (AP = 60.3)(d)  Off-diagonal views 0-shot00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.911 − precisionrecall  SVM−Σ (VP=92.1) (AP = 99.7)SVM−MV  (VP=66.8) (AP = 81.6)SVM (VP=68.8) (AP = 81.6)SVM−Σ−TD2ALL(VP=92.1) (AP = 99.7)SVM−Σ−TD2ND (VP=67.8) (AP = 80.6)SVM−Σ−NB2ALL  (VP=68.1) (AP = 85.9)(a)  Front 0-shot00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.911 − precisionrecall  SVM−Σ (VP=49.8) (AP = 88.1)SVM−MV  (VP=27.3) (AP = 51.2)SVM  (VP=28.4) (AP = 50.3)SVM−Σ−TD2ALL (VP=52.5) (AP = 89.8)SVM−Σ−TD2ND (VP=28.0) (AP = 50.0)SVM−Σ−NB2ALL (VP=27.9) (AP = 51.7)(e)  Diagonal, Back and Right 0-shot00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.911 − precisionrecall  SVM−Σ  (VP=15.3) (AP = 30.7)SVM−MV (VP=15.8) (AP = 27.5)SVM  (VP=16.3) (AP = 24.8)SVM−Σ−TD2ALL  (VP=15.0) (AP = 31.2)SVM−Σ−TD2ND(VP=15.8) (AP = 27.2)SVM−Σ−NB2ALL (VP=16.2) (AP = 26.8)(f)  All views except FrontLeft 0-shotaccuracy = 83.8 (240 TPs)3012730193014130129211114ground truthhypothesis0459013518022527031504590135180225270315SVM-⌃(g) Viewpoint confusion matricesSVM-⌃-TD2ALLaccuracy = 80.0 (240 TPs)302301828230263012230ground truthhypothesis0459013518022527031504590135180225270315SVMaccuracy = 41.9 (210 TPs)30726155289230361530574114ground truthhypothesis0459013518022527031504590135180225270315BackBackLeftLeftFrontLeftFrontFrontRightRightBackRightBackBackLeftLeftFrontLeftFrontFrontRightRightBackRightBackBackLeftLeftFrontLeftFrontFrontRightRightBackRightBackBackLeftLeftFrontLeftFrontFrontRightRightBackRightBackBackLeftLeftFrontLeftFrontFrontRightRightBackRightBackBackLeftLeftFrontLeftFrontFrontRightRightBackRightprior  method SVM (KITTI+ 3D obj.) 87.1 / 69.3  dataset  base  AP / VP car-type  car-model  - / -  - / -  KITTI  SVM (KITTI)  86.6 / 68.7 88.7 / 70.9 83.3 / 62.0 SVM-Σ 3D objects 90.7 / 71.9 91.6 / 75.1 87.5 / 73.9 SVM-Σ 90.7 / 71.9 90.1 / 75.1 89.4 / 75.6 SVM-MV 3D objects 90.2 / 72.6 90.3 / 71.9 82.9 / 63.2 SVM-MV 89.2 / 73.1 88.5 / 71.1 76.5 / 66.5 SVM-SV 3D objects 90.7 / 71.9 86.5 / 70.4 76.6 / 65.8 SVM-SV 86.9 / 71.4 85.8 / 70.8 76.5 / 66.5  KITTI  KITTI  AP+VP-D / AP+VP-C  base  car-type  car-model  - / -  - / -  53.6 / 67.0 53.3 / 65.8 58.1 / 67.9 40.3 / 55.1 61.5 / 70.1 65.2 / 74.1 60.9 / 70.7 61.6 / 70.2 66.1 / 73.5 65.2 / 73.4 60.9 / 69.9 60.8 / 70.0 41.5 / 55.8 62.1 / 69.8 58.8 / 67.6 44.8 / 53.5 61.5 / 70.1 55.9 / 65.8 44.3 / 53.1 59.6 / 67.0 56.5 / 65.1 44.8 / 53.5  Table 2: Multi-view detection results on KITTI [17].  Basic-level category transfer. We commence by applying our priors to a standard object class detector setup, in which a detector is trained such that positive examples are annotated on the level of basic-level categories (i.e., car), denoted base in the following. Tab. 2 (left) gives the corresponding 2D bounding box localization and viewpoint estimation results, comparing our priors SVM-MV and SVM-Σ to SVM-SV and a baseline not using any prior (SVM). For each, we consider two variants depending from which data the prior (or the detector itself for SVM) has been trained (KITTI, 3D Object Classes, or both). Note that the respective prior and SVM variants use the exact same training data (but in different ways) and are hence directly comparable in terms of performance. In Tab. 2 (left, col. base), we observe that our priors SVM-MV and SVM-Σ consistently outperform SVM, for both 2D BB localization and viewpoint estimation, for both choices of training data (e.g., SVM-Σ-KITTI with 90.7% AP and 71.9% VP vs. SVM-KITTI with 86.6% AP and 68.7% VP). The performance difference is even more pronounced when considering the combined performance measures (Tab. 2 (right, col. base)). SVM-Σ-KITTI achieves 61.6% AP+VP-D and 70.2% AP+VP- C, outperforming SVM-KITTI (53.3%, 65.8%) by a signiﬁcant margin. Similarly, SVM-Σ-3D Object Classes outperforms SVM-KITTI+3D Object Classes in all measures (90.7% vs. 87.1% AP, 71.9% vs. 69.3% VP, 61.5% vs. 53.6% AP+VP-D and 70.1 vs. 67.0% AP+VP-C). SVM-MV and SVM-SV priors also show promising detection performance, outper- forming the SVM models in all metrics. Fine-grained category transfer. Recently, is has been shown that ﬁne-grained object class rep- resentations on the level of sub-categories can improve performance [24, 21, 42], since they better capture the different modes of intra-class variation than representations that equalize training exam- ples on the level of basic-level categories. Further, these representations lend themselves to generate additional output in the form of ﬁne-grained category labels that can be useful for higher-level tasks, such as scene understanding. In the following, we hence consider two ﬁne-grained object class rep- resentations that decompose cars into distinct car-types or even individual car-models. Both are implemented as a bank of multi-view detectors (one per ﬁne-grained category) that are trained inde- pendently, but combined at test time by a joint non-maxima suppression to yield basic-level category detections. Note that the individual ﬁne-grained detectors suffer even more severely from scarce and unbalanced training data (see Fig. 5 and 6 in Sect. 6) than on the basic-level (see Fig. 4) – this is where our priors come into play: we train the priors, as before, on the base level, and use them to facilitate the learning of each individual ﬁne-grained detector, effectively transferring knowledge from base to ﬁne-grained categories. Tab. 2 gives the corresponding results in columns car-type and car-model, respectively. We observe: ﬁrst, performance can in fact improve as a result of the more ﬁne-grained representation, for both SVM-MV, SVM-Σ and even SVM (SVM-KITTI-car-type improves AP from 86.6% to 88.7%, and VP from 68.7% to 70.9%, and AP+VP-D from 53.3% to 58.1% and AP+VP-C from 65.8% to 67.9% compared to SVM-KITTI-base). A similar boost in performance in viewpoint estimation and combined can be seen for SVM-Σ (SVM-Σ-KITTI-car-type improves VP from 71.9% to 75.1%, and AP+VP-D from 61.6% to 66.1% and AP+VP-C from 70.2% to 73.5% compared to SVM-Σ- KITTI-base; the AP stays consistently high with 90.7% vs. 90.1%). Second, the level of granularity can be too ﬁne: for almost all methods, the performance of the ﬁne-grained car-model drops below the performance of the corresponding base detector – there is just so little training data for each of the car models that reliable ﬁne-grained detectors can hardly be learned. Curiously, SVM-Σ-KITTI- car-model can still keep up in terms of localization (89.4% AP) and even obtains the overall best  8  @50 iou  AP / VP  prior dataset SVM (KITTI)  AP+VP-D / AP+VP-C car-type 90.9 / 74.3 93.2 / 75.9 65.2 / 72.1 66.8 / 74.9 SVM-Σ 3D obj. 94.8 / 78.6 93.4 / 81.7 72.1 / 78.7 73.0 / 80.6 SVM-Σ KITTI 94.8 / 77.2 94.3 / 78.3 70.4 / 77.3 70.4 / 79.6  car-type  base  base  @70 iou  base  car-type  AP / VP  AP+VP-D / AP+VP-C car-type 49.9 / 74.2 60.0 / 76.8 37.5 / 40.4 44.6 / 48.3 51.5 / 81.2 64.7 / 83.9 41.9 / 44.3 53.0 / 56.7 49.7 / 79.0 61.2 / 80.4 39.5 / 41.8 47.9 / 53.3  base  Table 3: Multi-view detection results on KITTI [17]. Models have root and 4 parts per view.  without parts  with parts  prior  station wagon convertible  coupe  hatchback minibus sedan suv mAP  3D obj.  3D obj.  3D obj.  SVM-Σ SVM-Σ SVM-MV SVM-MV SVM-SV SVM-SV SVM SVM-Σ SVM-Σ SVM KITTI 71.2 24.4 67.5 89.8 31.3 69.4 19.7 53.3  KITTI 82.7 50.7 79.9 95.5 59.7 83.8 34.5 69.5  KITTI 62.6 13.8 60.5 58.9 16.3 37.8 5.2 36.4  KITTI 64.5 12.9 63.7 66.4 20.0 46.7 8.1 40.3  70.2 24.0 67.1 85.7 16.8 53.8 14.7 47.5  63.6 10.8 67.0 78.2 18.7 49.4 7.3 42.1  61.9 11.7 57.7 65.0 18.0 41.8 8.0 37.7  61.9 12.7 67.1 71.0 18.6 48.7 8.6 41.2  81.9 36.8 76.6 88.0 42.0 79.8 35.1 62.9  79.0 12.0 76.5 87.2 41.4 66.2 16.4 54.1  3D obj.  -  -  Table 4: Car-type detection results on the KITTI [17] dataset.  VP accuracy of 75.6%, which is also reﬂected in the combined measures (65.2% AP+VP-D, 73.4% AP+VP-C). Third, SVM-Σ-KITTI-car-type is the overall best method, outperforming the original baseline SVM-KITTI-base by impressive margins, in particular for the combined measures (90.1% vs. 86.6% AP, 75.1% vs. 68.7% VP, 66.1% vs. 53.3% AP+VP-D, 73.5% vs. 65.8% AP+VP-C). Tab. 3 (left) gives the results for the best performing priors of Tab. 2 (SVM-Σ-KITTI, SVM-Σ- 3D Object Classes) in comparison to SVM-KITTI, now using parts. As expected, parts result in a general performance boost for all methods (around 5% for all measures). The beneﬁt of our priors remains, for both granularity levels base and car-type, in particular for the combined measures: SVM-Σ-KITTI-base outperforms SVM-KITTI-base by similarly large margins as for the no-parts case (70.4% vs. 65.2% AP+VP-D, 77.3% vs. 72.1% AP+VP-C), and SVM-Σ-KITTI-car-type outperforms SVM-KITTI-car-types by (70.4% vs. 66.8% AP+VP-D, 79.6% vs. 74.9% AP+VP-C). Tab. 3 (right) applies a tighter overlap criterion for true positive detections (0.7 intersection over union) [17]. Interestingly, this leads to a larger separation in performance between base and car-type models, in particular in AP: e.g., SVM-Σ-3D Object Classes improves from 51.5% to 64.7%, SVM- Σ-KITTI from 49.7% to 61.2% and SVM-KITTI improves from 49.9% to 60.0%, highlighting the beneﬁt of the ﬁne-grained object class representation in particular for highly precise detection. Lastly, we evaluate the performance of our ﬁne-grained detectors on the level of the respective ﬁne- grained categories (car-types), as independent detection tasks. Tab. 4 gives the results without (left) and with parts (right). Again, our priors SVM-Σ consistently outperform the baseline SVM for all individual categories as well as on average by large margins (53.3% vs. 41.2% mAP for SVM-Σ- KITTI without parts, and 69.5% vs. 54.1% with parts).  Summary. We conclude that our priors (in particular SVM-Σ) in fact improve performance for simultaneous 2D bounding box localization and viewpoint estimation, for different levels of granu- larity of the underlying object representation (base, car-type, car-model). Notably, our priors allow for robust learning even on the most ﬁne-grained level of car-models, where training data is scarce and unbalanced and SVM fails. The combination of ﬁne-grained representation and prior results in a pronounced performance gain compared to SVM on the base level.  5 Conclusion  In this paper, we have approached the problem of scarce and unbalanced training data for training multi-view and ﬁne-grained detectors from a transfer learning perspective, introducing two ﬂavors of learning prior distributions over permissible detectors, one based on sparse feature correlations, and one based on the full covariance matrix between all features. In both cases, we have demonstrated improved simultaneous 2D bounding box localization and viewpoint estimation performance when applying these priors to detectors based on basic-level category representations. In addition, the sec-  9  ond ﬂavor allowed us to learn reliable detectors even for ﬁner-grained object class representations, resulting in an additional boost in performance on a realistic dataset of street scenes [17].  References [1] M. Arie-Nachimson and R. Basri. Constructing implicit 3D shape models for pose estimation. In ICCV’09. [2] Y. Aytar and A. Zisserman. Tabula rasa: Model transfer for object category detection. In ICCV’11. [3] E. Bart and S. Ullman. Cross-generalization: Learning novel classes from a single example by feature replacement. In CVPR’05. [4] E. Bart and S. Ullman. Single-example learning of novel classes using representation by similarity. In BMVC’05. [5] T. L. Berg, A. C. Berg, and J. Shih. Automatic attribute discovery and characterization from noisy web data. In ECCV’10. [6] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR’05. [7] L. Del Pero, J. Bowdish, B. Kermgard, E. Hartley, and K. Barnard. Understanding Bayesian rooms using composite 3d object models.  In CVPR’13.  [8] M. Everingham, A. Zisserman, C. K. I. Williams, and L. Van Gool. The PASCAL Visual Object Classes Challenge 2006 (VOC2006)  Results, 2006.  [9] T. Evgeniou, C. A. Micchelli, and M. Pontil. Learning multiple tasks with kernel methods. JMLR’05. [10] R. Farrell, O. Oza, N. Zhang, V. I. Morariu, T. Darrell, and L. S. Davis. Birdlets: Subordinate categorization using volumetric primitives  and pose-normalized appearance. In ICCV’11.  [11] L. Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object categories. PAMI’06. [12] P. F. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part based models.  PAMI’10.  [13] P. F. Felzenszwalb, R. B. Girshick, and D. McAllester.  http://people.cs.uchicago.edu/ pff/latent-release4/.  Discriminatively trained deformable part models,  release 4.  [14] S. Fidler, S. Dickinson, and R. Urtasun. 3d object detection and viewpoint estimation with a deformable 3d cuboid model. In NIPS’12. [15] M. Fink. Object classiﬁcation from a single example utilizing class relevance pseudo-metrics. In NIPS’04. [16] T. Gao, M. Stark, and D. Koller. What makes a good detector? - structured priors for learning from few examples. In ECCV’12. [17] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for autonomous driving? The KITTI vision benchmark suite. In CVPR’12. [18] D. Glasner, M. Galun, S. Alpert, R. Basri, and G. Shakhnarovich. Viewpoint-aware object detection and pose estimation. In ICCV’11. [19] C. Gu and X. Ren. Discriminative mixture-of-templates for viewpoint classiﬁcation. In ECCV’10. [20] B. Hariharan, J. Malik, and D. Ramanan. Discriminative decorrelation for clustering and classiﬁcation. In ECCV, 2012. [21] M. Hoai and A. Zisserman. Discriminative sub-categorization. In CVPR’13. [22] B. Kulis, K. Saenko, and T. Darrell. What you saw is not what you get: Domain adaptation using asymmetric kernel transforms. In  CVPR’11.  [23] C. H. Lampert, H. Nickisch, and S. Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In CVPR’09. [24] T. Lan, M. Raptis, L. Sigal, and G. Mori. From subcategories to visual composites: A multi-level framework for object detection. In  ICCV’13.  [25] J. Liebelt and C. Schmid. Multi-view object class detection with a 3D geometric model. In CVPR’10. [26] J. Liebelt, C. Schmid, and K. Schertler. Viewpoint-independent object class detection using 3D feature maps. In CVPR’08. [27] J. J. Lim, R. Salakhutdinov, and A. Torralba. Transfer learning by borrowing examples for multiclass object detection. In NIPS’11. [28] R. J. Lopez-Sastre, T. Tuytelaars, and S. Savarese. Deformable part models revisited: A performance evaluation for object category pose  estimation. In ICCV-WS CORP’11.  [29] M.Hejrati and D.Ramanan. Analyzing 3d objects in cluttered images. In NIPS’12. [30] E. Miller, N. Matsakis, and P. Viola. Learning from One Example Through Shared Densities on Transforms. In CVPR’00. [31] M. Ozuysal, V. Lepetit, and P. Fua. Pose estimation for category speciﬁc multiview object localization. In CVPR’09. [32] N. Payet and S. Todorovic. From contours to 3d object detection and pose estimation. In ICCV’11. [33] B. Pepik, P. Gehler, M. Stark, and B. Schiele. 3DDPM - 3d deformable part models. In ECCV’12. [34] B. Pepik, M. Stark, P. Gehler, and B. Schiele. Teaching 3d geometry to deformable part models. In CVPR’12. [35] L. Pishchulin, A. Jain, M. Andriluka, T. Thormaehlen, and B. Schiele. Articulated people detection and pose estimation: Reshaping the  future. In CVPR’12.  [36] L. Pishchulin, A. Jain, C. Wojek, M. Andriluka, T. Thormaehlen, and B. Schiele. Learning people detection models from few training  samples. In CVPR’11.  [37] M. Rohrbach, M. Stark, G. Szarvas, I. Gurevych, and B. Schiele. What helps where – and why? semantic relatedness for knowledge  transfer. In CVPR’10.  [38] R. Salakhutdinov, A. Torralba, and J. B. Tennenbaum. Learning to share visual appearance for multiclass object detection. In CVPR’11. [39] S. Savarese and L. Fei-Fei. 3D generic object categorization, localization and pose estimation. In ICCV’07. [40] M. Stark, M. Goesele, and B. Schiele. Back to the future: Learning shape models from 3d cad data. In BMVC’10. [41] M. Stark, M. Goesele, and B. Schiele. A shape-based object class model for knowledge transfer. In ICCV’09. [42] M. Stark, J. Krause, B. Pepik, D. Meger, J. Little, B. Schiele, and D. Koller. Fine-grained categorization for 3d scene understanding. In  BMVC’12.  [43] H. Su, M. Sun, L. Fei-Fei, and S. Savarese. Learning a dense multi-view representation for detection, viewpoint classiﬁcation and  synthesis of object categories. In ICCV’09.  [44] S. Thrun. Is learning the n-th thing any easier than learning the ﬁrst. In NIPS’06. [45] M. Villamizar, H. Grabner, J. Andrade-Cetto, A. Sanfeliu, L. V. Gool, and F. Moreno-Noguer. Efﬁcient 3d object detection using multiple  pose-speciﬁc classiﬁers. In BMVC’11.  [46] J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and Y. Gong. Locality-constrained linear coding for image classiﬁcation. In CVPR’10. [47] Y. Xiang and S. Savarese. Estimating the aspect layout of object categories. In CVPR’12. [48] B. Yao, A. Khosla, and L. Fei-Fei. Combining randomization and discrimination for ﬁne-grained image categorization. In CVPR’11. [49] M. Z. Zia, M. Stark, and K. Schindler. Explicit occlusion modeling for 3d object class representations. In CVPR’13. [50] M. Z. Zia, M. Stark, K. Schindler, and B. Schiele. Revisiting 3d geometric models for accurate object shape and pose. In 3dRR-11. [51] A. Zweig and D. Weinshall. Exploiting object hierarchy: Combining models from different category levels. In ICCV’07.  10  6 Supplemental material  6.1 Training and test data distributions for car, car-type and car-model category levels  In this section, we visualize the viewpoint distributions of the realistic street scene dataset (the tracking benchmark of KITTI [17]) that is the basis of our experiments in Sect. 4.2.To that end, we plot histograms of the number of car instances for each of 8 viewpoint (azimuth angle) bins, separately for training (blue) and test (red) data, and distinguishing between three different levels of granularity (car, car-type and car-model).  Figure 4: Car train and test statistics over 8 viewpoint bins.  Fig. 4 shows the data distribution for the car class. Notice the unbalanced data distribution across views. There are two main modes in the viewpoint distribution, at 90◦ and 270◦, which represent back and front views. This behavior is expected, as in the KITTI dataset [17] the images have been taken from a driving vehicle. On the other hand, the left (0◦) and right (180◦) facing views are poorly represented. With only 16 right facing training examples, training a robust right-view car template is extremely challenging.  Figure 5: Car-types train and test statistics over 8 viewpoint bins.  Fig. 5 illustrates the train and test distributions for each of the 7 car types (station wagon, convertible, coupe, hatchback, minibus, sedan, suv). We observe: i) the amount of training data available per car- type varies dramatically; classes like station wagon, coupe, hatchback have much more examples than convertible, minibus, sedan, ii) the training data distributions are skewed. There is not a single car-type for which all viewpoints are represented in the training data, thus learning a full multi-view representation for a car-type is impossible with a standard SVM framework. And iii), the train and test distributions differ a lot for each car-type. For example, for sedan and convertible there are viewpoints in the test set which are not represented in the training set. Lastly, Fig. 6 illustrates the train and test distribution for each of the 23 different car-models we have annotated. For most of the car-models, the viewpoint train and test distributions differ drastically. For car-models like VW Touran, VW Passat, Opel Corsa, Opel Tigra, Opel Astra, Ford Ka, Ford Fiesta, Fiat Punto, Fiat Panda, BMW 1 there is either none or at most 1 viewpoint that has both training and test data. This poses a major challenge for learning a robust multi-view object detector. In addition, the absolute number of available training examples for most of the car-models is rather small (12 for Fiat Panda, 19 for Mini Cooper, 12 for Opel Astra etc.). In summary, the training and  11  0459013518022527031502004006008001000car: train (2332), test(7505)0459013518022527031502004006008001000station wagon: train (768), test(1619)0459013518022527031502004006008001000convertible: train (46), test(323)0459013518022527031502004006008001000coupe: train (503), test(948)0459013518022527031502004006008001000hatchback: train (543), test(2814)0459013518022527031502004006008001000minibus: train (58), test(128)0459013518022527031502004006008001000sedan: train (140), test(357)0459013518022527031502004006008001000suv: train (230), test(365)Figure 6: Car-models train and test statistics over 8 viewpoint bins.  test viewpoint data distributions tend to be skewed and sparse, especially on the ﬁne-grained cate- gory levels, which, in addition to the low numbers of training examples, represent serious challenges when learning models for the ﬁne-grained categories.  7 Prior visualization in 3D  In this section we visualize in 3D the SVM-Σ, SVM-MV and the SVM-Σ-NB2ALL priors learned for the car class on the 3D object classes dataset [39]. For that purpose, we sample a cell (c, r) from viewpoint v from the target model, which we call reference cell, and back-project on a 3D CAD model the learned weights (dependencies) for that particular cell in the Σs (see Eq. (3) in the paper) correlation matrix (Ks = I − λΣs). Figure 7 visualizes the dependencies for an example reference cell. On the top of the ﬁgure, an example target model of the car class, trained on the 3D object classes dataset [39] is shown, along with the cell for which the weights in the prior are visualized (denoted as red cell). As each cell has L = 32 dimensions, we average the dependencies across all of them. Each cell is back-  12  04590135180225270315050100150200audia4: train (20), test(18)04590135180225270315050100150200bmw1: train (39), test(36)04590135180225270315050100150200bmw3: train (57), test(237)04590135180225270315050100150200chevroletcaptiva: train (5), test(39)04590135180225270315050100150200fiatpanda: train (12), test(19)04590135180225270315050100150200fiatpto: train (60), test(84)04590135180225270315050100150200fordfiesta: train (32), test(48)04590135180225270315050100150200fordfocus: train (66), test(411)04590135180225270315050100150200fordka: train (37), test(79)04590135180225270315050100150200mercedesaclass: train (18), test(404)04590135180225270315050100150200mercedescclass: train (97), test(118)04590135180225270315050100150200mercedeseclass: train (89), test(113)04590135180225270315050100150200minicooper: train (19), test(45)04590135180225270315050100150200opelastra: train (12), test(136)04590135180225270315050100150200opelcorsa: train (39), test(404)04590135180225270315050100150200opeltigra: train (11), test(145)04590135180225270315050100150200opelzafira: train (19), test(35)04590135180225270315050100150200peugeot206: train (56), test(344)04590135180225270315050100150200renaulttwingo: train (79), test(37)04590135180225270315050100150200vwgolf: train (78), test(430)04590135180225270315050100150200vwpassat: train (26), test(156)04590135180225270315050100150200vwpolo: train (18), test(179)04590135180225270315050100150200vwtouran: train (75), test(46)l e d o m  t e g r a T  -  Σ M V S  L L A 2 B N Σ M V S  -  -  -  V M M V S  Figure 7: Prior visualization in 3D and target model (row 1). Red indicates the reference cell. Prior visualizations in 3D for the red cell: SVM-Σ (row 2), SVM-Σ-NB2ALL (row 3) and SVM-MV (row 4). The black cube indicates the reference cell back-projected into 3D.  projected to 3D by aligning the model template with the 3D CAD model. The alignment is two stage procedure involving viewpoint alignment in the ﬁrst stage and template to rendered object alignment in the second stage. In Figure 7 rows 2, 3 and 4 visualize the cell dependencies for the SVM-Σ, SVM-Σ-NB2ALL and the SVM-MV priors, respectively (see Section 3 in the paper for details on the different priors). The dependencies on the 3D CAD model are visualized from two different views (left and right column). Red colors signify positive correlations, blue colors signify negative correlations. If the cells are not correlated (all dependencies are 0), gray color is used. Observations. The SVM-Σ model (row 2) reveals symmetric structures in the object itself, which can be seen from the positive dependencies (red color) on the 4 wheels of the car for the reference cell. SVM-Σ-NB2ALL (row 3) and SVM-MV (row 4) establish dependencies with only a few cells in the model (lots of gray points) as they are both restricted to learning dependencies among neighboring views (SVM-Σ-NB2ALL) or neighboring cells (SVM-MV) only.  13  ","While the majority of today's object class models provide only 2D boundingboxes, far richer output hypotheses are desirable including viewpoint,fine-grained category, and 3D geometry estimate. However, models trained toprovide richer output require larger amounts of training data, preferably wellcovering the relevant aspects such as viewpoint and fine-grained categories. Inthis paper, we address this issue from the perspective of transfer learning,and design an object class model that explicitly leverages correlations betweenvisual features. Specifically, our model represents prior distributions overpermissible multi-view detectors in a parametric way -- the priors are learnedonce from training data of a source object class, and can later be used tofacilitate the learning of a detector for a target class. As we show in ourexperiments, this transfer is not only beneficial for detectors based onbasic-level category representations, but also enables the robust learning ofdetectors that represent classes at finer levels of granularity, where trainingdata is typically even scarcer and more unbalanced. As a result, we reportlargely improved performance in simultaneous 2D object localization andviewpoint estimation on a recent dataset of challenging street scenes."
1312.5847,2014,Deep learning for neuroimaging: a validation study  ,"['Sergey M. Plis', 'Devon R. Hjelm', 'Ruslan Salakhutdinov', 'Vince D. Calhoun']",https://arxiv.org/pdf/1312.5847.pdf,"4 1 0 2     b e F 9 1         ] E N . s c [      3 v 7 4 8 5  .  2 1 3 1 : v i X r a  Deep learning for neuroimaging: a validation study  Sergey M. Plis  The Mind Research Network  Albuquerque, NM 87106 s.m.plis@gmail.com  Ruslan Salakhutdinov University of Toronto  Toronto, Ontario M5S 2E4  rsalakhu@cs.toronto.edu  Devon R. Hjelm  University of New Mexico Albuquerque, NM 87131  dhjelm@mrn.org  Vince D. Calhoun  The Mind Research Network  Albuquerque, NM 87106 vcalhoun@mrn.org  Abstract  Deep learning methods have recently made notable advances in the tasks of clas- siﬁcation and representation learning. These tasks are important for brain imag- ing and neuroscience discovery, making the methods attractive for porting to a neuroimager’s toolbox. Success of these methods is, in part, explained by the ﬂexibility of deep learning models. However, this ﬂexibility makes the process of porting to new areas a difﬁcult parameter optimization problem. In this work we demonstrate our results (and feasible parameter ranges) in application of deep learning methods to structural and functional brain imaging data. We also describe a novel constraint-based approach to visualizing high dimensional data. We use it to ana- lyze the effect of parameter choices on data transformations. Our re- sults show that deep learning methods are able to learn physiologically important representations and detect latent relations in neuroimaging data.  1  Introduction  One of the main goals of brain imaging and neuroscience—and, possibly, of most natural sciences— is to improve understanding of the investigated system based on data. In our case, this amounts to inference of descriptive features of brain structure and function from non-invasive measurements. Brain imaging ﬁeld has come a long way from anatomical maps and atlases towards data driven fea- ture learning methods, such as seed-based correlation [2], canonical correlation analysis [33], and independent component analysis (ICA) [1, 24]. These methods are highly successful in revealing known brain features with new details [3] (supporting their credibility), in recovering features that differentiate patients and controls [28] (assisting diagnosis and disease understanding), and starting a “resting state” revolution after revealing consistent patters in data from uncontrolled resting ex- periments [29, 35]. Classiﬁcation is often used merely as a correctness checking tool, as the main emphasis is on learning about the brain. A perfect oracle that does not explain its conclusions would be useful, but mainly to facilitate the inference of the ways the oracle draws these conclusions. As an oracle, deep learning methods are breaking records taken over the areas of speech, signal, image, video and text mining and recognition by improving state of the art classiﬁcation accuracy by, sometimes, more than 30% where the prior decade struggled to obtain a 1-2% improvements [19, 21]. What differentiates them from other classiﬁers, however, is the automatic feature learning from data which largely contributes to improvements in accuracy. Presently, this seems to be the closest solution to an oracle that reveals its methods — a desirable tool for brain imaging. Another distinguishing feature of deep learning is the depth of the models. Based on already ac- ceptable feature learning results obtained by shallow models—currently dominating neuroimaging  1  ﬁeld—it is not immediately clear what beneﬁts would depth have. Considering the state of multi- modal learning, where models are either assumed to be the same for analyzed modalities [26] or cross-modal relations are sought at the (shallow) level of mixture coefﬁcients [23], deeper models better ﬁt the intuitive notion of cross-modality relations, as, for example, relations between genetics and phenotypes should be indirect, happening at a deeper conceptual level. In this work we present our recent advances in application of deep learning methods to functional and structural magnetic resonance imaging (fMRI and sMRI). Each consists of brain volumes but for sMRI these are static volumes—one per subject/session,—while for fMRI a single subject dataset is comprised of multiple volumes capturing the changes during an experimental session. Our goal is to validate feasibility of this application by a) investigating if a building block of deep generative models—a restricted Boltzmann machine (RBM) [17]—is competitive with ICA (a representative model of its class) (Section 2); b) examining the effect of the depth in deep learning analysis of structural MRI data (Section 3.3); and c) determining the value of the methods for discovery of latent structure of a large-scale (by neuroimaging standards) dataset (Section 3.4). The measure of feature learning performance in a shallow model (a) is comparable with existing methods and known brain physiology. However, this measure cannot be used when deeper models are investigated. As we further demonstrate, classiﬁcation accuracy does not provide the complete picture either. To be able to visualize the effect of depth and gain an insight into the learning process, we introduce a ﬂexible constraint satisfaction embedding method that allows us to control the complexity of the constraints (Section 3.2). Deliberately choosing local constraints we are able to reﬂect the transformations that the deep belief network (DBN) [15] learns and applies to the data and gain additional insight.  2 A shallow belief network for feature learning  Prior to investigating the beneﬁts of depth of a DBN in learning representations from fMRI and sMRI data, we would like to ﬁnd out if a shallow (single hidden layer) model–which is the RBM— from this family meets the ﬁeld’s expectations. As mentioned in the introduction, a number of methods are used for feature learning from neuroimaging data: most of them belong to the single matrix factorization (SMF) class. We do a quick comparison to a small subset of SMF methods on simulated data; and continue with a more extensive comparison against ICA as an approach trusted in the neuroimaging ﬁeld. Similarly to RBM, ICA relies on the bipartite graph structure, or even is an artiﬁcial neural network with sigmoid hidden units as is in the case of Infomax ICA [1] that we compare against. Note the difference with RBM: ICA applies its weight matrix to the (shorter) temporal dimension of the data imposing independence on the spatial dimension while RBM applies its weight matrix (hidden units “receptive ﬁelds”) to the high dimensional spatial dimension instead (Figure 2).  2.1 A restricted Boltzmann machine  h p(v, h) =(cid:80)  variables h [10]: p(v) =(cid:80)  A restricted Boltzmann machine (RBM) is a Markov random ﬁeld that models data distribution parameterizing it with the Gibbs distribution over a bipartite graph between visible v and hidden h e−E(v,h) is the normalization term (the partition function) and E(v, h) is the energy of the system. Each visible variable in the case of fMRI data represents a voxel of an fMRI scan with a real-valued and approximately Gaussian distribution. In this case, the energy is deﬁned as:  h 1/Z exp(−E(v, h)), where Z =(cid:80) Wjihi −(cid:88)  E(v, h) = −(cid:88)  −(cid:88)  (aj − vj)2  (cid:80)  bihi,  (1)  v  vj σj  ij  σ2 j  j  i  where aj and bj are biases and σj is the standard deviation of a parabolic containment function for each visible variable vj centered on the bias aj. In general, the parameters σi need to be learned along with the other parameters. However, in practice normalizing the distribution of each voxel to have zero mean and unit variance is faster and yet effective [27]. A number of choices affect the quality of interpretation of the representations learned from fMRI by an RBM. Encouraging sparse features via the L1-regularization: λ(cid:107)W(cid:107)1 (λ = 0.1 gave best results) and using hyperbolic tangent for hidden units non-linearity are essential settings that respectively facilitate spatial and temporal interpretation of the result. The weights were updated using the truncated Gibbs sampling method  2  called contrastive divergence (CD) with a single sampling step (CD-1). Further information on RBM model can be found in [16, 17].  2.2 Synthetic data  a: Average spatial map (SM) and time course (TC) correlations to ground truth for RBM and SMF models (gray box).  b: Ground truth (GT) SMs and estimates obtained by RBM and ICA (thresholded at 0.4 height). Colors are consistent across the methods. Grey indicates back- ground or areas without SMs above threshold.  c: Spatial, temporal, and cross correla- tion (FNC) accuracy for ICA (red) and RBM (blue), as a function of spatial overlap of the true sources from 1b. Lines indicate the average correlation to GT, and the color-ﬁll indicates ±2 standard errors around the mean.  Figure 1: Comparison of RBM estimation accuracy of features and their time courses with SMFs.  In this section we summarize our comparisons of RBM with SMF models—including Infomax ICA [1], PCA [14], sparse PCA (sPCA) [37], and sparse NMF (sNMF) [18]—on synthetic data with known spatial maps generated to simulate fMRI. Figure 1a shows the correlation of spatial maps (SM) and time course (TC) estimates to the ground truth for RBM, ICA, PCA, sPCA, and sNMF. Correlations are averaged across all sources and datasets. RBM and ICA showed the best overall performance. While sNMF also estimated SMs well, it showed inferior performance on TC estimation, likely due to the non-negativity constraint. Based on these results and the broad adoption of ICA in the ﬁeld, we focus on comparing Infomax ICA and RBM. Figure 1b shows the full set of ground truth sources along with RBM and ICA estimates for a single representative dataset. SMs are thresholded and represented as contours for visualization. Results over all synthetic datasets showed similar performance for RBM and ICA (Figure 1c), with a slight advantage for ICA with regard to SM estimation, and a slight advantage for RBM with regards to TC estimation. RBM and ICA also showed comparable performance estimating cross correlations also called functional network connectivity (FNC).  2.3 An fMRI data application  Figure 2: The processes of feature learning and time course computation from fMRI data by an RBM. The visible units are voxels and a hidden unit receptive ﬁeld covers an fMRI volume.  Data used in this work comprised of task-related scans from 28 (ﬁve females) healthy participants, all of whom gave written, informed, IRB-approved consent at Hartford Hospital and were compensated for participation1. All participants were scanned during an auditory oddball task (AOD) involving the detection of an infrequent target sound within a series of standard and novel sounds2.  1More detailed information regarding participant demographics is provided in [9] 2The task is described in more detail in [4] and [9].  3  CorrelationSpatial MapsTime Courses00.9GTRBMICAHigher is betterHigher is betterLower is betterICARBMfMRI dataRBM trainingTime coursesTarget time courseRBM FeaturesCross correlationsTask-related featuresFeaturesSpacexx=TRAININGfMRI dataANALYSISTimeSpaceScans were acquired at the Olin Neuropsychiatry Research Center at the Institute of Living/Hartford Hospital on a Siemens Allegra 3T dedicated head scanner equipped with 40 mT/m gradients and a standard quadrature head coil [4, 9]. The AOD consisted of two 8-min runs, and 249 scans (volumes) at 2 second TR (0.5 Hz sampling rate) were used for the ﬁnal dataset. Data were post-processed using the SPM5 software package [12], motion corrected using INRIalign [11], and subsampled to 53 × 63 × 46 voxels. The complete fMRI dataset was masked below mean and the mean image across the dataset was removed, giving a complete dataset of size 70969 voxels by 6972 volumes. Each voxel was then normalized to have zero mean and unit variance. The RBM was constructed using 70969 Gaussian visible units and 64 hyperbolic tangent hidden units. The hyper parameters (cid:15) (0.08 from the searched [1 × 10−4, 1 × 10−1] range) for learning rate and λ (0.1 from the searched range [1 × 10−2,1 × 10−1]) for L1 weight decay were selected as those that showed a reduction of reconstruction error over training and a signiﬁcant reduction in span of the receptive ﬁelds respectively. Parameter value outside the ranges either resulted in unstable or slow learning ((cid:15)) or uninterpretable fea- tures (λ). The RBM was then trained with a batch size of 5 for approximately 100 epochs to allow for full convergence of the parameters. After ﬂipping the sign of negative receptive ﬁelds, we then identiﬁed and labeled spatially distinct features as corresponding to brain regions with the aid of AFNI [5] excluding features which had a high probability of corresponding to white matter, ventricles, or artifacts (eg. motion, edges). We normalized the fMRI volume time series to mean zero and used the trained RBM in feed-forward mode to compute time series for each fMRI feature. This was done to better compare to ICA, where the mean is removed in PCA preprocessing. The work-ﬂow is outlined in Figure 2, while Figure 3 shows comparison of resulting features with those obtained by Infomax ICA. In general, RBM performs competitively with ICA, while providing–perhaps, not surprisingly due to the used L1 regularization—sharper and more local- ized features. While we recognize that this is a subjective measure we list more features in Fig- ure S2 of Section 5 and note that RBM features lack negative parts for corresponding features. Note, that in the case of L1 regularized weights RBM algorithms starts to resemble some of the ICA approaches (such as the recent RICA by Le at al. [20]), which may explain the sim- ilar performance. However, the differences and possible advantages are the generative nature of the RBM and no enforcement of component orthogonality (not explicit at the least). More- over, the block structure of the correlation matrix (see below the Supplementary material section) of feature time courses provide a grouping that is more physiologically supported than that pro- vided by ICA. For example, see Figure S1 in the supplementary material section below. Perhaps, because ICA working hard to enforce spatial independence subtly affects the time courses and their cross-correlations in turn. We have observed comparable running times of the (non GPU) ICA (http://www.nitrc.org/projects/gift) and a GPU implementation of the RBM (https://github.com/nitishsrivastava/deepnet). 3 Validating the depth effect  Figure 3: Intrinsic brain networks estimated by ICA and RBM.  Since the RBM results demonstrate a feature-learning performance competitive with the state of the art (or better), we proceed to investigating the effects of the model depth. To do that we turn from fMRI to sMRI data. As it is commonly assumed in the deep learning literature [22] the depth is often improving classiﬁcation accuracy. We investigate if that is indeed true in the sMRI case. Structural data is convenient for the purpose as each subject/session is represented only by a single volume that has a label: control or patient in our case. Compare to 4D data where hundreds of volumes belong to the same subject with the same disease state.  3.1 A deep belief network  A DBN is a sigmoidal belief network (although other activation functions may be used) with an RBM as the top level prior. The joint probability distribution of its visible and hidden units is  4  parametrized as follows:  ni(cid:89)  P (v, h1, h2, . . . , hl) = P (v|h1)P (h1|h2)··· P (hl−2, hl−1)P (hl−1, hl),  (2) where l is the number of hidden layers, P (hl−1, hl) is an RBM, and P (hi|hi+1) factor into indi- vidual conditionals:  P (hi|hi+1) =  P (hi  j|hi+1)  (3)  j=1  The important property of DBN for our goals of feature learning to facilitate discovery is its ability to operate in generative mode with ﬁxed values on chosen hidden units thus allowing one to investigate the features that the model have learned and/or weighs as important in discriminative decisions. We, however, not going to use this property in this section, focusing instead on validating the claim that a network’s depth provides beneﬁts for neuroimaging data analysis. And we will do this using discriminative mode of DBN’s operation as it provides an objective measure of the depth effect. DBN training splits into two stages: pre-training and discriminative ﬁne tuning. A DBN can be pre-trained by treating each of its layers as an RBM—trained in an unsupervised way on inputs from the previous layer—and later ﬁne-tuned by treating it as a feed-forward neural network. The latter allows supervised training via the error back propagation algorithm. We use this schema in the following by augmenting each DBN with a soft-max layer at the ﬁne-tuning stage.  3.2 Nonlinear embedding as a constraint satisfaction problem  A DBN and an RBM operate on data samples, which are brain volumes in the fMRI and sMRI case. A ﬁve-minute fMRI experiment with 2 seconds sampling rate yields 150 of these volumes per subject. For sMRI studies number of participating subjects varies but in this paper we operate with a 300 and a 3500 subject-volumes datasets. Transformations learned by deep learning methods do not look intuitive in the hidden node space and generative sampling of the trained model does not provide a sense if a model have learned anything useful in the case of MRI data: in contrast to natural images, fMRI and sMRI images do not look very intuitive. Instead, we use a nonlinear embedding method to control whether a model learned useful information and to assist in investigation of what have it, in fact, learned. One of the purposes of an embedding is to display a complex high dimensional dataset in a way that is i) intuitive, and ii) representative of the data sample. The ﬁrst requirement usually leads to displaying data samples as points in a 2-dimensional map, while the second is more elusive and each approach addresses it differently. Embedding approaches include relatively simple random linear projections—provably preserving some neighbor relations [6]—and a more complex class of nonlinear embedding approaches [30, 32, 34, 36]. In an attempt to organize the properties of this diverse family we have aimed at representing nonlinear embedding methods under a single constraint satisfaction problem (CSP) framework (see below). We hypothesize that each method places the samples in a map to satisfy a speciﬁc set of constraints. Although this work is not yet complete, it proven useful in our current study. We brieﬂy outline the ideas in this section to provide enough intuition of the method that we further use in Section 3. Since we can control the constraints in the CSP framework, to study the effect of deep learning we choose them to do the least amount of work—while still being useful—letting the DBN do (or not) the hard part. A more complicated method such as t-SNE [36] already does complex processing to preserve the structure of a dataset in a 2D map – it is hard to infer if the quality of the map is determined by a deep learning method or the embedding. While some of the existing method may have provided the “least amount of work” solutions as well we chose to go with the CSP framework. It explicitly states the constraints that are being satisﬁed and thus lets us reason about deep learning effects within the constraints, while with other methods—where the constraints are implicit—this would have been harder. A constraint satisfaction problem (CSP) is one requiring a solution that satisﬁes a set of constraints. One of the well known examples is the boolean satisﬁability problem (SAT). There are multiple other important CSPs such as the packing, molecular conformations, and, recently, error correcting codes [7]. Freedom to setup per point constraints without controlling for their global interactions makes a CSP formulation an attractive representation of the nonlinear embedding problem. Pursuing  5  this property we use the iterative “divide and concur” (DC) algorithm [13] as the solver for our representation. In DC algorithm we treat each point on the solution map as a variable and assign a set of constraints that this variable needs to satisfy (more on these later). Then each points gets a “replica” for each constraint it is involved into. Then DC algorithm alternates the divide and concur projections. The divide projection moves each “replica” points to the nearest locations in the 2D map that satisfy the constraint they participate in. The concur projection concurs locations of all “replicas” of a point by placing them at the average location on the map. The key idea is to avoid local traps by combining the divide and concur steps within the difference map [8]. A single location update is represented by:  xc = Pc((1 + 1/β) ∗ Pd(x) − 1/β ∗ x) xd = Pd((1 − 1/β) ∗ Pc(x) + 1/β ∗ x) x = x + β ∗ (xc − xd),  (4) where Pd(·) and Pc(·) denote the divide and concur projections and β is a user-deﬁned parameter. While the concur projection will only differ by subsets of “replicas” across different methods rep- resentable in DC framework, the divide projection is unique and deﬁnes the algorithm behavior. In this paper, we choose a divide projection that keeps k nearest neighbors of each point in the higher dimensional space also its neighbors in the 2D map. This is a simple local neighborhood constraint that allows us to assess effects of deep learning transformation leaving most of the mapping deci- sions to the deep learning. Note, that for a general dataset we may not be able to satisfy this constraint: each point has ex- actly the same neighbors in 2D as in the original space (and this is what we indeed observe). The DC algorithm, however, is only guaranteed to ﬁnd the solution if it exists and oscillates otherwise. Oscillating behavior is detectable and may be used to stop the algorithm. We found informative watching the 2D map in dynamics, as the points that keep oscillating provide additional information into the structure of the data. Another practically important feature of the algorithm: it is determin- istic. Given the same parameters (β and the parameters of Pd(·)) it converges to the same solution regardless of the initial point. If each of the points participates in each constraint then complex- ity of the algorithm is quadratic. With our simple k neighborhood constraints it is O(kn), for n samples/points.  3.3 A schizophrenia structural MRI dataset  We use a combined data from four separate schizophrenia studies conducted at Johns Hopkins University (JHU), the Maryland Psychiatric Research Center (MPRC), the Insti- tute of Psychiatry, London, UK (IOP), and the Western Psychiatric Institute and Clinic at the University of Pitts- burgh (WPIC) (the data used in Meda et al. [25]). The combined sample comprised 198 schizophrenia patients and 191 matched healthy controls and contained both ﬁrst episode and chronic patients [25]. At all sites, whole brain MRIs were obtained on a 1.5T Signa GE scanner using identical parameters and software. Original struc- tural MRI images were segmented in native space and the resulting gray and white matter images then spatially nor- malized to gray and white matter templates respectively to derive the optimized normalization parameters. These parameters were then applied to the whole brain structural images in native space prior to a new segmentation. The obtained 60465 voxel gray matter images were used in this study. Figure 4 shows example orthogonal slice views of the gray matter data samples of a patient and a healthy control. The main question of this Section is to evaluate the effect of the depth of a DBN on sMRI. To answer this question, we investigate if classiﬁcation rates improve with the depth. For that we sequentially investigate DBNs of 3 depth. From RBM experiments we have learned that even with a larger number of hidden units (72, 128 and 512) RBM tends to only keep around 50 features driving the rest to zero. Classiﬁcation rate and reconstruction error still slightly improves, however, when the number of hidden units increases. These observations affected our choice of 50 hidden  Figure 4: A smoothed gray matter seg- mentation of a patient and a healthy control: each is a training sample.  6  units of the ﬁrst two layers and 100 for the third. Each hidden unit is connected to all units in the previous layer which results in an all to all connectivity structure between the layers, which is a more common and conventional approach to constructing these models. Note, larger networks (up to double the umber of units) lead to similar results. We pre-train each layer via an unsupervised RBM and discriminatively ﬁne-tune models of depth 1 (50 hidden units in the top layer), 2 (50-50 hidden units in the ﬁrst and the top layer respectively), and 3 (50-50-100 hidden units in the ﬁrst, second and the top layer respectively) by adding a softmax layer on top of each of these models and training via the back propagation. We estimate the accuracy of classiﬁcation via 10-fold cross validation on ﬁne-tuned models splitting the 389 subject dataset into 10 approximately class-balanced folds. We train the rbf-kernel SVM, logistic regression and a k-nearest neighbors (knn) classiﬁer using activations of the top-most hidden layers in ﬁne-tuned models to the training data of each fold as their input. The testing is performed likewise but on the test data. We also perform the same 10-fold cross validation on the raw data. Table 1 summarizes the precision and recall values in the F-scores and their standard deviations. All models demonstrate a similar trend when the accu- racy only slightly increases from depth-1 to depth-2 DBN and then improves signiﬁ- cantly. Table 1 supports the general claim of deep learning community about improvement of clas- siﬁcation rate with the depth even for sMRI data. Improvement in classiﬁcation even for the simple knn classiﬁer indicates the character of the transformation that the DBN learns and applies to the data: it may be changing the data manifold to organize classes by neighborhoods. Ideally, to make general conclusion about this transformation we need to analyze several representative datasets. However, even working with the same data we can have a closer view of the depth effect using the method introduced in Section 3.2. Although it may seem that the DBN does not provide signiﬁcant  Table 1: Classiﬁcation on ﬁne-tuned models (test data)  0.66 ± 0.09 0.65 ± 0.11 0.55 ± 0.15  0.62 ± 0.12 0.61 ± 0.12 0.58 ± 0.16  0.68 ± 0.01 0.63 ± 0.09 0.61 ± 0.11  0.90 ± 0.14 0.91 ± 0.14 0.90 ± 0.16  depth SVM F-score LR F-score KNN F-score  raw  1  2  3  Figure 5: Effect of a DBN’s depth on neighborhood relations. Each map is shown at the same iteration of the algorithm with the same k = 50. The color differentiates the classes (patients and controls) and the training (335 subjects) from validation (54 subjects) data. Although the data becomes separable at depth 1 and more so at depth 2, the DBN continues distilling details that pull the classes further apart.  improvements in sMRI classiﬁcation from depth-1 to depth-2 in this model, it keeps on learning potentially useful transformaions of the data. We can see that using our simple local neighborhood- based embedding. Figure 5 displays 2D maps of the raw data, as well as the depth 1, 2, and 3 activations (of a network trained on 335 subjects): the deeper networks place patients and control groups further apart. Additionally, Figure 5 displays the 54 subjects that the DBN was not train on. These hold out subjects are also getting increased separation with depth. This DBN’s behavior is potentially useful for generalization, when larger and more diverse data become available. Our new mapping method has two essential properties to facilitate the conclusion and provide con- ﬁdence in the result: its already mentioned local properties and the deterministic nature of the algo- rithm. The latter leads to independence of the resulting maps from the starting point. The map only depends on the models parameter k—the size of the neighborhood—and the data.  3.4 A large-scale Huntington disease data  7  3  2  1  raw 0.75 0.79  1.00 ± 0.00 1.00 ± 0.00  0.65 ± 0.01 0.65 ± 0.01  0.65 ± 0.01 0.65 ± 0.01  Figure 6: A gray matter of MRI scans of an HD patient and a healthy control.  depth SVM F-score LR F-score Table 2: Classiﬁcation on ﬁne-tuned models (HD data)  In this section we focus on sMRI data collected from healthy controls and Huntington disease (HD) patients as part of the PREDICT-HD project (www.predict-hd. net). Huntington disease is a genetic neurodegenerative disease that results in degeneration of neurons in certain areas of the brain. The project is focused on identifying the earliest detectable changes in thinking skills, emotions and brain structure as a person begins the transition from health to being diagnosed with Huntington disease. We would like to know if deep learning methods can assist in answering that question. For this study T1-weighted scans were collected at mul- tiple sites (32 international sites), representing multiple ﬁeld strengths (1.5T and 3.0T) and multiple manufactures (Siemens, Phillips, and GE). The 1.5T T1 weighted scans were an axial 3D volumetric spoiled-gradient echo series (≈ 1 × 1 × 1.5 mm voxels), and the 3.0T T1 weighted scans were a 3D Volumetric MPRAGE series (≈ 1 × 1 × 1 mm voxels). The images were segmented in the native space and the normalized to a common template. After correlating the normalized gray matter segmentation with the template and eliminating poorly correlating scans we obtain a dataset of 3500 scans, where 2641 were from patients and 859 from healthy controls. We have used all of the scans in this imbalanced sample to pre-train and ﬁne tune the same model architecture (50-50-100) as in Section 3.3 for all three depths3. Table 2 lists the average F-score values for both classes at the raw data and all depth levels. Note the drop from the raw data and then a recovery at depth 3. The lim- ited capacity of levels 1 and 2 has reduced the network ability to differentiate the groups but representational capacity of depth 3 network compensates for the ini- tial bottleneck. This, conﬁrms our previous observa- tion on the depth effect, however, does not yet help the main question of the PREDICT-HD study. Note, however, while Table 1 in the previous section eval- uates generalization ability of the DBN, Table 2 here only demonstrates changes in DBN’s representational capacity with the depth as we use no testing data. To further investigate utility of the deep learning approach for scientiﬁc discovery we again augment it with the embedding method of Section 3.2. Figure 7 shows the map of 3500 scans of HD patients and healthy controls. Each point on the map is an sMRI volume, shown in Figures 6 and 7. Although we have used the complete data to train the DBN, discriminative ﬁne-tuning had access only to binary label: control or patient. In ad- dition to that, we have information about severity of the disease from low to high. We have color coded this information in Figure 7 from bright yellow (low) through orange (medium) to red (high). The network4 discriminates the patients by disease severity which results in a spectrum on the map. Note, that neither t-SNE (not shown), nor our new embedding see the spectrum or even the patient groups in the raw data. This is a important property of the method that may help support its future use in discovery of new information about the disease.  Figure 7: Patients and controls group sep- aration map with additional unsupervised spectral decomposition of sMRI scans by disease severity. The map represents 3500 scans.  3Note, in both cases we have experimented with larger layer sizes but the results were not signiﬁcantly  different to warrant increase in computation and parameters needed to be estimated. 4Note, the embedding algorithm does not have access to any label information.  8  4 Conclusions Our investigations show that deep learning has a high potential in neuroimaging applications. Even the shallow RBM is already competitive with the model routinely used in the ﬁeld: it produces physiologically meaningful features which are (desirably) highly focal and have time course cross correlations that connect them into meaningful functional groups (Section 5). The depth of the DBN does indeed help classiﬁcation and increases group separation. This is apparent on two sMRI datasets collected under varying conditions, at multiple sites each, from different disease groups, and pre-processed differently. This is a strong evidence of DBNs robustness. Furthermore, our study shows a high potential of DBNs for exploratory analysis. As Figure 7 demonstrates, DBN in conjunction with our new mapping method can reveal hidden relations in data. We did ﬁnd it difﬁcult initially to ﬁnd workable parameter regions, but we hope that other researchers won’t have this difﬁculty starting from the baseline that we provide in this paper.  References [1] A. J. Bell and T. J. Sejnowski. An information-maximization approach to blind separation and  blind deconvolution. Neural Computation, 7(6):1129–1159, November 1995.  [2] Bharat Biswal, F Zerrin Yetkin, Victor M Haughton, and James S Hyde. Functional connec- tivity in the motor cortex of resting human brain using echo-planar mri. Magnetic resonance in medicine, 34(4):537–541, 1995.  [3] M.J. Brookes, M. Woolrich, H. Luckhoo, D. Price, J.R. Hale, M.C. Stephenson, G.R. Barnes, S.M. Smith, and P.G. Morris. Investigating the electrophysiological basis of resting state net- works using magnetoencephalography. Proceedings of the National Academy of Sciences, 108(40):16783–16788, 2011.  [4] V. D. Calhoun, K. A. Kiehl, and G. D. Pearlson. Modulation of temporally coherent brain networks estimated using ICA at rest and during cognitive tasks. Human Brain Mapping, 29(7):828–838, 2008.  [5] R. W. Cox et al. AFNI: software for analysis and visualization of functional magnetic reso-  nance neuroimages. Computers and Biomedical Research, 29(3):162–173, 1996.  [6] T. de Vries, S. Chawla, and M. E. Houle. Finding local anomalies in very high dimensional In Proceedings of the 10th {IEEE} international conference on data mining, pages  space. 128–137. IEEE, IEEE Computer Society, 2010.  [7] Nate Derbinsky, Jos´e Bento, Veit Elser, and Jonathan S Yedidia. An improved three-weight  message-passing algorithm. arXiv preprint arXiv:1305.1961, 2013.  [8] V. Elser, I. Rankenburg, and P. Thibault. Searching with iterated maps. Proceedings of the  National Academy of Sciences, 104(2):418, 2007.  [9] Nathan Swanson et. al. Lateral differences in the default mode network in healthy controls and  patients with schizophrenia. Human Brain Mapping, 32:654–664, 2011.  [10] Asja Fischer and Christian Igel. An introduction to restricted boltzmann machines. In Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications, pages 14–36. Springer, 2012.  [11] L. Freire, A. Roche, and J. F. Mangin. What is the best similarity measure for motion correction  in fMRI. IEEE Transactions in Medical Imaging, 21:470–484, 2002.  [12] Karl J Friston, Andrew P Holmes, Keith J Worsley, J-P Poline, Chris D Frith, and Richard SJ Frackowiak. Statistical parametric maps in functional imaging: a general linear approach. Human brain mapping, 2(4):189–210, 1994.  [13] S. Gravel and V. Elser. Divide and concur: A general approach to constraint satisfaction.  Physical Review E, 78(3):36706, 2008.  [14] Trevor. Hastie, Robert. Tibshirani, and JH (Jerome H.) Friedman. The elements of statistical  learning. Springer, 2009.  [15] G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks.  Science, 313(5786):504–507, 2006.  [16] G. E. Hinton, S. Osindero, and Y. W. Teh. A fast learning algorithm for deep belief nets. Neural  computation, 18(7):1527–1554, 2006.  9  [17] Geoffrey Hinton. Training products of experts by minimizing contrastive divergence. Neural  Computation, 14:2002, 2000.  [18] P. O. Hoyer. Non-negative sparse coding. In Neural Networks for Signal Processing, 2002.  Proceedings of the 2002 12th IEEE Workshop on, pages 557–565, 2002.  [19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep  convolutional neural networks. In Neural Information Processing Systems, 2012.  [20] Quoc V Le, Alexandre Karpenko, Jiquan Ngiam, and Andrew Y Ng. Ica with reconstruction  cost for efﬁcient overcomplete feature learning. In NIPS, pages 1017–1025, 2011.  [21] Quoc V. Le, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeff Dean, and An- drew Y. Ng. Building high-level features using large scale unsupervised learning. In Interna- tional Conference on Machine Learning. 103, 2012.  [22] N. Le Roux and Y. Bengio. Deep belief networks are compact universal approximators. Neural  computation, 22(8):2192–2207, 2010.  [23] Jingyu Liu and Vince Calhoun. Parallel independent component analysis for multimodal anal- ysis: Application to fmri and eeg data. In Biomedical Imaging: From Nano to Macro, 2007. ISBI 2007. 4th IEEE International Symposium on, pages 1028–1031. IEEE, 2007.  [24] M. J. McKeown, S. Makeig, G. G. Brown, T. P. Jung, S. S. Kindermann, A. J. Bell, and T. J. Sejnowski. Analysis of fMRI data by blind separation into independent spatial components. Human Brain Mapping, 6(3):160–188, 1998.  [25] Shashwath A Meda, Nicole R Giuliani, Vince D Calhoun, Kanchana Jagannathan, David J Schretlen, Anne Pulver, Nicola Cascella, Matcheri Keshavan, Wendy Kates, Robert Buchanan, et al. A large scale (n= 400) investigation of gray matter differences in schizophrenia using optimized voxel-based morphometry. Schizophrenia research, 101(1):95–105, 2008.  [26] M. Moosmann, T. Eichele, H. Nordby, K. Hugdahl, and V. D. Calhoun. Joint independent com- ponent analysis for simultaneous EEG-fMRI: principle and simulation. International Journal of Psychophysiology, 67(3):212–221, 2008.  [27] Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann ma- chines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807–814, 2010.  [28] V. K. Potluru and V. D. Calhoun. Group learning using contrast NMF : Application to func- tional and structural MRI of schizophrenia. Circuits and Systems, 2008. ISCAS 2008. IEEE International Symposium on, pages 1336–1339, May 2008.  [29] Marcus E. Raichle, Ann Mary MacLeod, Abraham Z. Snyder, William J. Powers, Debra A. Gusnard, and Gordon L. Shulman. A default mode of brain function. Proceedings of the National Academy of Sciences, 98(2):676–682, 2001.  [30] Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear  embedding. Science, (5500):2323–2326, 2000.  [31] Mikail Rubinov and Olaf Sporns. Weight-conserving characterization of complex functional  brain networks. Neuroimage, 56(4):2068–2079, 2011.  [32] John W Sammon Jr. A nonlinear mapping for data structure analysis. Computers, IEEE  Transactions on, 100(5):401–409, 1969.  [33] Jing Sui, Tulay Adali, Qingbao Yu, Jiayu Chen, and Vince D. Calhoun. A review of multivari- ate methods for multimodal fusion of brain imaging data. Journal of Neuroscience Methods, 204(1):68–81, 2012.  [34] Joshua B Tenenbaum, Vin De Silva, and John C Langford. A global geometric framework for  nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000.  [35] M.P. van den Heuvel and H.E. Hulshoff Pol. Exploring the brain network: A review on resting-  state fMRI functional connectivity. European Neuropsychopharmacology, 2010.  [36] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine  Learning Research, 9(2579-2605):85, 2008.  [37] Hui Zou, Trevor Hastie, and Robert Tibshirani. Sparse principal component analysis. Journal  of computational and graphical statistics, 15(2):265–286, 2006.  10  5 Supplementary material  The correlation matrices for both RBM and ICA results on the fMRI dataset of Section 2.3 are pro- vided in Figure S1, where the ordering of components is performed separately for each method. Each network is named by their physiological function but we do not go in depth explaining these in the current paper. For RBM, modularity is more apparent, both visually and quantitatively. Modu- larity, as deﬁned in [31], averages 0.40± 0.060 across subjects for RBM, and 0.35± 0.056 for ICA. These values are signiﬁcantly greater for RBM (t = 7.15, p < 1e−6 per the paired t-test). Also note that the scale of correlation values for RBM and ICA is different, which highlights that RBM overestimated strong FNC values.  Figure S1: Correlation matrices determined from RBM (left) and ICA (right), averaged over sub- jects. Note that the color scales for RBM and ICA are different (RBM shows a larger range in correlations). The correlation matrix for ICA on the same scale as RBM is also provided as an in- set (upper right). Feature groupings for RBM and ICA were determined separately using the FNC matrices and known anatomical and functional properties.  Figure S2: Sample pairs consisting of RBM (top) and ICA (bottom) SMs thresholded at 2 standard deviations. Pairing was done with the aid of spatial correlations, temporal properties, and visual inspection. Values indicate the spatial correlation between RBM and ICA SMs.  11  ","Deep learning methods have recently made notable advances in the tasks ofclassification and representation learning. These tasks are important for brainimaging and neuroscience discovery, making the methods attractive for portingto a neuroimager's toolbox. Success of these methods is, in part, explained bythe flexibility of deep learning models. However, this flexibility makes theprocess of porting to new areas a difficult parameter optimization problem. Inthis work we demonstrate our results (and feasible parameter ranges) inapplication of deep learning methods to structural and functional brain imagingdata. We also describe a novel constraint-based approach to visualizing highdimensional data. We use it to analyze the effect of parameter choices on datatransformations. Our results show that deep learning methods are able to learnphysiologically important representations and detect latent relations inneuroimaging data."
1312.5419,2014,Large-scale Multi-label Text Classification - Revisiting Neural Networks  ,"['Jinseok Nam', 'Jungi Kim', 'Iryna Gurevych', 'Johannes Fürnkranz']",https://arxiv.org/pdf/1312.5419.pdf,"4 1 0 2     y a M 5 1         ]  G L . s c [      3 v 9 1 4 5  .  2 1 3 1 : v i X r a  Large-scale Multi-label Text Classiﬁcation —  Revisiting Neural Networks  Jinseok Nam1,2, Jungi Kim1, Eneldo Loza Menc´ıa1,  Iryna Gurevych1,2, and Johannes F¨urnkranz1  1 Department of Computer Science, Technische Universit¨at Darmstadt, Germany  2 Information Center for Education, German Institute for Educational Research, Germany  Abstract. Neural networks have recently been proposed for multi-label classi- ﬁcation because they are able to capture and model label dependencies in the output layer. In this work, we investigate limitations of BP-MLL, a neural net- work (NN) architecture that aims at minimizing pairwise ranking error. Instead, we propose to use a comparably simple NN approach with recently proposed learning techniques for large-scale multi-label text classiﬁcation tasks. In partic- ular, we show that BP-MLL’s ranking loss minimization can be efﬁciently and effectively replaced with the commonly used cross entropy error function, and demonstrate that several advances in neural network training that have been de- veloped in the realm of deep learning can be effectively employed in this setting. Our experimental results show that simple NN models equipped with advanced techniques such as rectiﬁed linear units, dropout, and AdaGrad perform as well as or even outperform state-of-the-art approaches on six large-scale textual datasets with diverse characteristics.  1  Introduction  As the amount of textual data on the web and in digital libraries is increasing rapidly, the need for augmenting unstructured data with metadata is also increasing. System- atically maintaining a high quality digital library requires extracting a variety types of information from unstructured text, from trivial information such as title and author, to non-trivial information such as descriptive keywords and categories. Time- and cost- wise, a manual extraction of such information from ever-growing document collections is impractical.  Multi-label classiﬁcation is an automatic approach for addressing such problems by learning to assign a suitable subset of categories from an established classiﬁcation sys- tem to a given text. In the literature, one can ﬁnd a number of multi-label classiﬁcation approaches for a variety of tasks in different domains such as bioinformatics [1], music [27], and text [8]. In the simplest case, multi-label classiﬁcation may be viewed as a set of binary classiﬁcation tasks that decides for each label independently whether it should be assigned to the document or not. However, this so-called binary relevance approach ignores dependencies between the labels, so that current research in multi-label classiﬁ- cation concentrates on the question of how such dependencies can be exploited [22, 3]. One such approach is BP-MLL [31], which formulates multi-label classiﬁcation prob- lems as a neural network with multiple output nodes, one for each label. The output layer is able to model dependencies between the individual labels.  2  In this work, we directly build upon BP-MLL and show how a simple, single hidden layer NN may achieve a state-of-the-art performance in large-scale multi-label text clas- siﬁcation tasks. The key modiﬁcations that we suggest are (i) more efﬁcient and more effective training by replacing BP-MLL’s pairwise ranking loss with cross entropy and (ii) the use of recent developments in the area of deep learning such as rectiﬁed linear units (ReLUs), Dropout, and AdaGrad.  Even though we employ techniques that have been developed in the realm of deep learning, we nevertheless stick to single-layer NNs. The motivation behind this is two- fold: ﬁrst, a simple network conﬁguration allows better scalability of the model and is more suitable for large-scale tasks. Second, as it has been shown in the literature [14], popular feature representation schemes for textual data such as variants of tf-idf term weighting already incorporate a certain degree of higher dimensional features, and we speculate that even a single-layer NN model can work well with text data. This paper provides an empirical evidence to support that a simple NN model equipped with recent advanced techniques for training NN performs as well as or even outperforms state-of- the-art approaches on large-scale datasets with diverse characteristics.  2 Multi-label Classiﬁcation Formally, multi-label classiﬁcation may be deﬁned as follows: X ⊂ RD is a set of M instances, each being a D-dimensional feature vector, and L is a set of labels. Each instance x is associated with a subset of the L labels, the so-called relevant labels; all other labels are irrelevant for this example. The task of the learner is to learn a mapping function f : RD → 2L that assigns a subset of labels to a given instance. An alternative view is that we have to predict an L-dimensional target vector y ∈ {0, 1}L, where yi = 1 indicates that the i-th label is relevant, whereas yi = 0 indicates that it is irrelevant for the given instance.  Many algorithms have been developed for tackling this type of problem. The most straightforward way is binary relevance (BR) learning; it constructs L binary classiﬁers, which are trained on the L labels independently. Thus, the prediction of the label set is composed of independent predictions for individual labels. However, labels often occur together, that is, the presence of a speciﬁc label may suppress or exhibit the likelihood of other labels.  To address this limitation of BR, pairwise decomposition (PW) and label power- set (LP) approaches consider label dependencies during the transformation by either generating pairwise subproblems [8, 19] or the powerset of possible label combinations [28]. Classiﬁer chains [22, 3] are another popular approach that extend BR by including previous predictions into the predictions of subsequent labels.  [6] present a large-margin classiﬁer, RankSVM, that minimizes a ranking loss by penalizing incorrectly ordered pairs of labels. This setting can be used for multi-label classiﬁcation by assuming that the ranking algorithm has to rank each relevant label before each irrelevant label. In order to make a prediction, the ranking has to be cal- ibrated [8], i.e., a threshold has to be found that splits the ranking into relevant and irrelevant labels. Similarly, Zhang and Zhou [31] introduced a framework that learns ranking errors in neural networks via backpropagation (BP-MLL).  3  (a) A neural network  (b) Threshold decision  Fig. 1: (a) a neural network with a single hidden layer of two units and multiple output units, one for each possible label. (b) shows how threshold for a training example is estimated based on prediction output o of the network. Consider nine possible labels, of which o1, o4 and o6 are relevant labels (blue) and the rest are irrelevant (red). The ﬁgure shows three exemplary threshold candidates (dashed lines), of which the middle one is the best choice because it gives the highest F1 score. See Section 3.3 for more details.  2.1 State-of-the-art multi-label classiﬁers and limitations  The most prominent learning method for multi-label text classiﬁcation is to use a BR approach with strong binary classiﬁers such as SVMs [23, 29] despite its simplicity. It is well known that characteristics of high-dimensional and sparse data, such as text data, make decision problems linearly separable [14], and this characteristic suits the strengths of SVM classiﬁers well.  Unlike benchmark datasets, real-world text collections consist of a large number of training examples represented in a high-dimensional space with a large amount of labels. To handle such datasets, researchers have derived efﬁcient linear SVMs [15, 7] that can handle large-scale problems. The training time of these solvers scales linearly with the number of instances, so that they show good performance on standard bench- marks. However, their performance decreases as the number of labels grows and the la- bel frequency distribution becomes skewed [18, 23]. In such cases, it is also intractable to employ methods that minimize ranking errors among labels [6, 31] or that learn joint probability distributions of labels [10, 3].  3 Neural Networks for Multi-label Classiﬁcation  In this section, we propose a neural network-based multi-label classiﬁcation framework that is composed of a single hidden layer and operates with recent developments in neu- ral network and optimization techniques, which allow the model to converge into good regions of the error surface in a few steps of parameter updates. Our approach consists of two modules (Figure 1): a neural network that produces label scores (Sections 3.2– 3.5), and a label predictor that converts label scores into binary using a thresholding technique (Section 3.3).  x1x5x2x3x4h2h1o3o2o1W(1)25W(2)3201ThisthreholdyieldsP=1,R=13,F1=12P=23,R=23,F1=23←tP=39,R=1,F1=12o1o7o6o5o4o3o2o9o84  3.1 Rank Loss  The most intuitive objective for multi-label learning is to minimize the number of mis- ordering between a pair of relevant label and irrelevant label, which is called rank loss:  L(y, f (x)) = w(y)  I (fi(x) > fj(x)) +  I (fi = fj)  1 2  (1)  (cid:88)  yi<yj  where w(y) is a normalization factor, I (·) is the indicator function, and fi (·) is a prediction score for a label i. Unfortunately, it is hard to minimize due to non-convex property of the loss function. Therefore, convex surrogate losses have been proposed as alternatives to rank loss [25, 6, 31].  3.2 Pairwise Ranking Loss Minimization in Neural Networks  Let us assume that we would like to make a prediction on L labels from D dimensional input features. Consider the neural network model with a single hidden layer in which F hidden units are deﬁned and input units x ∈ RD×1 are connected to hidden units h ∈ RF×1 with weights W(1) ∈ RF×D and biases b(1) ∈ RF×1. The hidden units are connected to output units o ∈ RL×1 through weights W(2) ∈ RL×F and biases b(2) ∈ RL×1. The network, then, can be written in a matrix-vector form, and we can construct a feed-forward network fΘ : x → o as a composite of non-linear functions in the range [0, 1]:  (cid:16)  W(1)x + b(1)(cid:17)  + b(2)(cid:17)  fΘ(x) = fo  (2) where Θ = {W(1), b(1), W(2), b(2)}, and fo and fh are element-wise activation func- tions in the output layer and the hidden layer, respectively. Speciﬁcally, the function fΘ (x) can be re-written as follows:  W(2)fh  (cid:16)  z(1) = W(1)x + b(1), h = fh  z(2) = W(2)h + b(2), o = fo  (cid:16) z(1)(cid:17) z(2)(cid:17) (cid:16)  where z(1) and z(2) denote the weighted sum of inputs and hidden activations, respec- tively.  Our aim is to ﬁnd a parameter vector Θ that minimizes a cost function J(Θ; x, y). The cost function measures discrepancy between predictions of the network and given targets y.  BP-MLL [31] minimizes errors induced by incorrectly ordered pairs of labels, in order to exploit dependencies among labels. To this end, it introduces a pairwise error function (PWE), which is deﬁned as follows:  JP W E(Θ; x, y) =  1  |y||¯y|  exp(−(op − on))  (3)  (cid:88)  (p,n)∈y×¯y  where p and n are positive and negative label index associated with training example x. ¯y represents a set of negative labels and | · | stands for the cardinality. The PWE is relaxation of the loss function in Equation 1 that we want to minimize.  5  As no closed-form solution exists to minimize the cost function, we use a gradient-  based optimization method.  Θ(τ +1) = Θ(τ ) − η∇Θ(τ ) J(Θ(τ ); x, y)  (4)  The parameter Θ is updated by adding a small step of negative gradients of the cost function J(Θ(τ ); x, y) with respect to the parameter Θ at step τ. The parameter η, called the learning rate, determines the step size of updates.  3.3 Thresholding Once training of the neural network is ﬁnished, its output may be interpreted as a prob- ability distribution p (o|x) over the labels for a given document x. The probability dis- tribution can be used to rank labels, but additional measures are needed in order to split the ranking into relevant and irrelevant labels. For transforming the ranked list of labels into a set of binary predictions, we train a multi-label threshold predictor from training data. This sort of thresholding methods are also used in [6, 31]  For each document xm, labels are sorted by the probabilities in decreasing order. Ideally, if NNs successfully learn a mapping function fΘ, all correct (positive) labels will be placed on top of the sorted list and there should be large margin between the set of positive labels and the set of negative labels. Using F1 score as a reference measure, we calculate classiﬁcation performances at every pair of successive positive labels and choose a threshold value tm that produces the best performance (Figure 1 (b)).  Afterwards, we can train a multi-label thresholding predictor ˆt = T (x; θ) to learn t as target values from input pattern x. We use linear regression with (cid:96)2-regularization to learn θ  J (θ) =  1 2M  (T (xm; θ) − ti)2 +  (cid:107)θ(cid:107)2  2  λ 2  (5)  M(cid:88)  m=1  where T (xm; θ) = θT xm and λ is a parameter which controls the magnitude of the (cid:96)2 penalty.  At test time, these learned thresholds are used to predict a binary output ˆykl for label l of a test document xk given label probabilities okl; ˆykl = 1 if okl > T (xk; θ), otherwise 0.  3.4 Ranking Loss vs. Cross Entropy BP-MLL is supposed to perform better in multi-label problems since it takes label cor- relations into consideration than the standard form of NN that does not. However, we have found that BP-MLL does not perform as expected in our preliminary experiments, particularly, on datasets in textual domain.  Consistency w.r.t Rank Loss Recently, it has been claimed that none of convex loss functions including BP-MLL’s loss function (Equation 3) is consistent with respect to rank loss which is non-convex and has discontinuity [2, 9]. Furthermore, univariate surrogate loss functions such as log loss are rather consistent with rank loss [4].  Jlog(Θ; x, y) = w (y)  log  1 + e  (cid:16)  (cid:17)  − ˙ylzl  (cid:88)  l  6  in which ˙y ∈ {−1, 1} a target and zl is output of a linear function zl =(cid:80) where w (y) is a weighting function that normalizes loss in terms of y and zl indicates prediction for label l. Please note that the log loss is often used for logistic regression k Wlkxk + bl where Wlk is a weight from input xk to output zl and bl is bias for label l. A typical choice is, for instance, w(y) = (|y||¯y|)−1 as in BP-MLL. In this work, we set w(y) = 1, then the log loss above is equivalent to cross entropy (CE), which is commonly used to train neural networks for classiﬁcation tasks if we use sigmoid transfer function in the output layer, i.e. fo(z) = 1/ (1 + exp(−z)), or simply fo(z) = σ (z): (yl log ol) + (1 − yl) log(1 − ol))  JCE(Θ; x, y) = −(cid:88)  (6)  l  where ol and yl are the prediction and the target for label l, respectively. Let us verify the equivalence between the log loss and the CE. Consider the log loss function for only label l.  Jlog(Θ; x, yl) = log(1 + e− ˙ylzl ) = − log  1  1 + e− ˙ylzl  (7)  (cid:18)  (cid:19)  As noted, ˙y in the log loss takes either −1 or 1, which allows us to split the above equation as follows:  (cid:18)  (cid:19)  (cid:26)  − log  1  1 + e− ˙ylzl  =  − log (σ (zl)) − log (σ (−zl))  if ˙y = 1 if ˙y = −1  (8)  Then, we have the corresponding CE by using a property of the sigmoid function σ (−z) = 1 − σ (z)  JCE (Θ; x, yl) = − (yl log ol + (1 − yl) log (1 − ol))  (9)  l  where y ∈ {0, 1} and ol = σ (zl). Computational Expenses In addition to consistency with rank loss, CE has an advan- tage in terms of computational efﬁciency; computational cost for computing gradients of parameters with respect to PWE is getting more expensive as the number of labels (cid:88) grows. The error term δ(2) for label l which is propagated to the hidden layer is deﬁned as (cid:88) − 1|y||¯y| p∈y l = −yl/ol + (1 − yl)/(1 − ol)f(cid:48) ) for the CE can be Whereas the computation of δ(2) performed efﬁciently, obtaining error terms δ(2) for the PWE is L times more expensive than one in ordinary NN utilizing the cross entropy error function. This also shows that BP-MLL scales poorly w.r.t. the number of unique labels.  exp(−(ol − on))f(cid:48) n∈¯y exp(−(op − ol))f(cid:48)    if l ∈ ¯y  if l ∈ y  δ(2) l =  o(z(2)  o(z(2)  o(z(2)  1|y||¯y|  (10)  ),  ),  l  l  l  l  Plateaus To get an idea of how differently both objective functions behave as a function of parameters to be optimized, let us draw graphs containing cost function values. Note that it has been pointed out that the slope of the cost function as a function of the  7  (a) Comparison of CE and PWE  (b) Comparison of tanh and ReLU, both for CE  Fig. 2: Landscape of cost functions and a type of hidden units. W (1) represents a denotes a weight weight connecting an input unit to a hidden unit. Likewise, W (2) from the hidden unit to output unit 1. The z-axis stands for a value for the cost func- ) where instances x, targets y and weights tion J(W (1), W (2) 1 , W (2) W (2)  ; x, y, W (2) are ﬁxed.  , W (2)  , W (2)  , W (2)  1  3  4  2  2  3  4  parameters plays an important role in learning parameters of neural networks [26, 11] which we follow. Consider two-layer neural networks consisting of W (1) ∈ R for the ﬁrst layer, W(2) ∈ R4×1 for the second, output layer. Since we are interested in function val- {2,3,4} is set ues with respect to two parameters W (1) and W (2) to a ﬁxed value c. In this paper we use c = 0.3 Figure 2 (a) shows different shapes of the functions and slope steepness. In ﬁgure 2 (a) both curves have similar shapes, but the curve for PWE has plateaus in which gradient descent can be very slow in compar- ison with the CE. Figure 2 (b) shows that CE with ReLUs, which is explained the next Section, has a very steep slope compared to CE with tanh. Such a slope can accelerate convergence speed in learning parameters using gradient descent. We conjecture that these properties might explain why our set-up converges faster than the other conﬁgu- rations, and BP-MLL performs poorly in most cases in our experiments.  out of 5 parameters, W(2)  1  3.5 Recent Advances in Deep Learning  In recent neural network and deep learning literature, a number of techniques were proposed to overcome the difﬁculty of learning neural networks efﬁciently. In particular, we make use of ReLUs, AdaGrad, and Dropout training, which are brieﬂy discussed in the following.  3 The shape of the functions is not changed even if we set c to arbitrary value since it is drawn by function values in z-axis with respect to only W (1) and W (2)  .  1  −4−2024−4−202401234567 W(1)W(2)1 J(W(1),W(2)1;x,y,W(2)2,W(2)3,W(2)4)CE w/ tanhPWE w/ tanh−4−2024−4−20242468101214161820 W(1)W(2)1 J(W(1),W(2)1;x,y,W(2)2,W(2)3,W(2)4)CE w/ ReLUCE w/ tanh8  Rectiﬁed Linear Units Rectiﬁed linear units (ReLUs) have been proposed as activation units on the hidden layer and shown to yield better generalization performance [21, 12, 30]. A ReLU disables negative activation (ReLU(x) = max(0, x)) so that the number of parameters to be learned decreases during the training. This sparsity characteristic makes ReLUs advantageous over the traditional activation units such as sigmoid and tanh in terms of the generalization performance.  Learning Rate Adaptation with AdaGrad Stochastic gradient descent (SGD) is a simple but effective technique for minimizing the objective functions of NNs (Equation 4). When SGD is considered as an optimization tool, one of the problems is the choice of the learning rate. A common approach is to estimate the learning rate which gives lower training errors on subsamples of training examples [16] and then decrease it over time. Furthermore, to accelerate learning speed of SGD, one can utilize momentum [24].  (cid:113)(cid:80)τ  Instead of a ﬁxed or scheduled learning rate, an adaptive learning rate method, namely AdaGrad, was proposed [5]. The method determines the learning rate at it- eration τ by keeping previous gradients ∆1:τ to compute the learning rate for each i,t where i stands for an index of each di- dimension of parameters ηi,τ = η0/ mension of parameters and η0 is the initial learning rate and shared by all parameters. For multi-label learning, it is often the case that a few labels occur frequently, whereas the majority only occurs rarely, so that the rare ones need to be updated with larger steps in the direction of the gradient. If we use AdaGrad, the learning rates for the fre- quent labels decreases because the gradient of the parameter for the frequent labels will get smaller as the updates proceed. On the other hand, the learning rates for rare labels remain comparatively large.  t=1 ∆2  Regularization using Dropout Training In principle, as the number of hidden layers and hidden units in a network increases, its expressive power also increases. If one is given a large number of training examples, training a larger networks will result in better performance than using a smaller one. The problem when training such a large network is that the model is more prone to getting stuck in local minima due to the huge number of parameters to learn. Dropout training [13] is a technique for preventing overﬁtting in a huge parameter space. Its key idea is to decouple hidden units that activate the same output together, by randomly dropping some hidden units’ activations. Essentially, this corresponds to training an ensemble of networks with smaller hidden layers, and combining their predictions. However, the individual predictions of all possible hidden layers need not be computed and combined explicitly, but the output of the ensemble can be approximately reconstructed from the full network. Thus, dropout training has a similar regularization effect as ensemble techniques.  4 Experimental Setup  We have shown that why the structure of NNs needs to be reconsidered in the previous Sections. In this Section, we describe evaluation measures to show how effectively NNs perform by combining recent development in learning neural networks based on the  fact that the univariate loss is consistent with respect to rank loss on large-scale textual datasets.  9  Evaluation Measures Multi-label classiﬁers can be evaluated in two groups of mea- sures: bipartition and ranking. Bipartition measures operate on classiﬁcation results, i.e. a set of labels assigned by classiﬁers to each document, while ranking measures operate on the ranked list of labels. In order to evaluate the quality of a ranked list, we consider several ranking measures [25]. Given a document x and associated label information y, consider a multi-label learner fθ(x) that is able to produce scores for each label. These scores, then, can be sorted in descending order. Let r(l) be the rank ranked label with the highest score is is a positive label or not: I(cid:0)r−1(1)(fθ(x)) /∈ y(cid:1) of a label l in the sorted list of labels. We already introduced Rank loss, which is con- cerned primarily in this work, in Section 3.1. One-Error evaluates whether the top most where r−1(1) indicates the index of a label positioning on the ﬁrst place in the sorted list. Coverage measures on average how far one needs to go down the ranked list of (cid:80) labels to achieve recall of 100%: maxli∈y r(li) − 1. Average Precision or AP mea- sures the average fraction of labels preceding relevant labels in the ranked list of labels: 1|y|  |{lj∈y|r(lj )≤r(li)}|  For bipartition measures, Precision, Recall, and F1 score are conventional methods to evaluate effectiveness of information retrieval systems. There are two ways of com- puting such performance measures: Micro-averaged measures and Macro-averaged measures4[20].  li∈y  r(li)  .  (cid:80)L (cid:80)L L(cid:88)  1 L  l=1 tpl  l=1 tpl + f pl  tpl  tpl + f pl  l=1  Pmicro =  Pmacro =  , Rmicro =  , Rmacro =  (cid:80)L (cid:80)L L(cid:88)  1 L  l=1 tpl  l=1 tpl + f nl  tpl  tpl + f nl  l=1  , F1−micro =  , F1−macro =  (cid:80)L L(cid:88)  1 L  (cid:80)L  l=1 2tpl  l=1 2tpl + f pl + f nl  2tpl  2tpl + f pl + f nl  l=1  Datasets Our main interest is in large-scale text classiﬁcation, for which we selected six representative domains, whose characteristics are summarized Table 1. For Reuters21578 we used the same training/test split as previous works [29]. Training and test data were switched for RCV1-v2 [17] which originally consists of 23,149 train and 781,265 test documents. The EUR-Lex, Delicious and Bookmarks datasets were taken from the MU- LAN repository.5 Except for Delicious and Bookmarks, all documents are represented with tf-idf features with cosine normalization such that length of the document vector is 1 in order to account for the different document lengths.  In addition to these standard benchmark datasets, we prepared a large-scale dataset from documents of the German Education Index (GEI).6 The GEI is a database of links  4 Note that scores computed by micro-averaged measures might be much higher than that by macro-averaged measures if there are many rarely-occurring labels for which the classiﬁcation system does not perform well. This is because macro-averaging weighs each label equally, whereas micro-averaged measures are dominated by the results of frequent labels. 5 http://mulan.sourceforge.net/datasets.html 6 http://www.dipf.de/en/portals/portals-educational-information/german-education-index  10  Table 1: Number of documents (D), size of vocabulary (D), total number of labels (L) and average number of labels per instance (C) for the six datasets used in our study.  Dataset  Reuters-21578  RCV1-v2 EUR-Lex Delicious Bookmarks  German Education Index  M  C D 1.13 10789 18637 3.24 804414 47236 5.31 19348 983 19.02 16105 2.03 87856 208 316061 20000 1000 7.16  L 90 103 5000 3993 500 2150  to more than 800,000 scientiﬁc articles with metadata, e.g. title, authorship, language of an article and index terms. We consider a subset of the dataset consisting of approxi- mately 300,000 documents which have abstract as well as the metadata. Each document has multiple index terms which are carefully hand-labeled by human experts with re- spect to the content of articles. We processed plain text by removing stopwords and stemming each token. To avoid the computational bottleneck from a large number of labels, we chose the 1,000 most common labels out of about 50,000. We then randomly split the dataset into 90% for training and 10% for test.  Algorithms Our main goal is to compare our NN-based approach to BP-MLL. NNA stands for the single hidden layer neural networks which have ReLUs for its hidden layer and which are trained with SGD where each parameter of the neural networks has their own learning rate using AdaGrad. NNAD additionally employs Dropout based on the same settings as NNA. T and R following BP-MLL indicate tanh and ReLU as a transfer function in the hidden layer. For both NN and BP-MLL, we used 1000 units in the hidden layer over all datasets. 7 As Dropout works well as a regularizer, no additional regularization to prevent overﬁtting was incorporated. The base learning rate η0 was also determined among [0.001, 0.01, 0.1] using validation data.  We also compared the NN-based algorithms to binary relevance (BR) using SVMs (Liblinear) as a base learner, as a representative of the state-of-the-art. The penalty parameter C was optimized in the range of [10−3, 10−2, . . . , 102, 103] based on either average of micro- and macro-average F1 or rankloss on validation set. BRB refers to linear SVMs where C is optimized with bipartition measures on the validation dataset. BR models whose penalty parameter is optimized on ranking measures are indicated as BRR. In addition, we apply the same thresholding technique which we utilize in our NN approach (Section 3.3) on a ranked list produced by BR models (BRR).  5 Results  We evaluate our proposed models and other baseline systems on datasets with varying statistics and characteristics. We ﬁrst show experimental results that conﬁrm that the  7 The optimal number of hidden units of BP-MLL and NN was tested among 20, 50, 100, 500, 1000 and 4000 on validation datasets. Usually, the more units are in the hidden layer, the better performance of networks is. We chose it in terms of computational efﬁciency.  11  Fig. 3: (left) effects of AdaGrad and momentum on three types of transfer functions in the hidden layers in terms of rank loss on Reuters-21578. The number of parameter updates in x-axis corresponds to the number of evaluations of Eq. (4). (right) effects of dropout with two different numbers of hidden units in terms of rank loss on EUR-Lex.  techniques discussed in Section 3.5 actually contribute to an increased performance of NN-based multi-label classiﬁcation, and then compare all algorithms on the six above- mentioned datasets in order to get an overall impression of their performance.  Better Local Minima and Acceleration of Convergence Speed First we intend to show the effect of ReLUs and AdaGrad in terms of convergence speed and rank loss. The left part of Figure 3 shows that all three results of AdaGrad (red lines) show a lower rank loss than all three versions of momentum. Moreover, within each group, ReLUs outperform the versions using tanh or sigmoid activation functions. That NNs with ReLUs at the hidden layer converge faster into a better weight space has been previously observed for the speech domain [30].8 This faster convergence is a major advantage of combining recently proposed learning components such as ReLUs and AdaGrad, which facilitates a quicker learning of the parameters of NNs. This is particularly important for the large-scale text classiﬁcation problems that are the main focus of this work.  Decorrelating Hidden Units While Output Units Remain Correlated One major goal of multi-label learners is to minimize rank loss by leveraging inherent correlations in a label space. However, we conjecture that these correlations also may cause overﬁt- ting because if groups of hidden units specialize in predicting particular label subsets that occur frequently in the training data, it will become harder to predict novel label combinations that only occur in the test set. Dropout effectively ﬁghts this by randomly dropping individual hidden units, so that it becomes harder for groups of hidden units to specialize in the prediction of particular output combinations, i.e., they decorrelate the hidden units, whereas the correlation of output units still remains. Particularly, a subset of output activations o and hidden activations h would be correlated through W(2).  8 However, unlike the results of [30], in our preliminary experiments adding more hidden layers did not further improve generalization performance.  110001000000.010.020.030.040.050.060.070.08The number of parameter updatesRank lossReuters−21578  ReLU w/ AdaGradtanh w/ AdaGradsigm w/ AdaGradReLU w/ momentumtanh w/ momentumsigm w/ momentum11000100000.010.0150.020.0250.030.0350.040.0450.05The number of parameter updatesRank lossEUR−Lex  w/  Dropout (1000)w/  Dropout (4000)w/o Dropout (1000)w/o Dropout (4000)12  Fig. 4: Rankloss (left) and mean average precision (right) on the German Education Index test data for the different cost functions. η denotes the base learning rate and D indicates that Dropout is applied. Note that x-axis is in log scale.  We observed overﬁtting across all datasets except for Reuters-21578 and RCV1-v2 under our experimental settings. The right part of Figure 3 shows how well Dropout pre- vents NNs from overﬁtting on the test data of EUR-Lex. In particular, we can see that with increasing numbers of parameter updates, the performance of regular NNs even- tually got worse in terms of rank loss. On the other hand, when dropout is employed, convergence is initially slower, but eventually effectively prevents overﬁtting.  Limiting Small Learning Rates in BP-MLL The learning rate strongly inﬂuences convergence and learning speed [16]. As we have already seen in the Figure 2, the slope of PWE is less steep than CE, which implies that smaller learning rates should be used. Speciﬁcally, we observed PWE allows only smaller learning rate 0.01 (blue markers) in contrast with CE that works well a relatively larger learning rate 0.1 (red markers) in Figure 4. In the case of PWE with the larger learning rate (green markers), interestingly, dropout (rectangle markers in green) makes it converge towards much better local minima, yet it is still worse than the other conﬁgurations. It seems that the weights of BP-MLL oscillates in the vicinity of local minima and, indeed, converges worse local minima. However, it makes learning procedure of BP-MLL slow compared to NNs with CE making bigger steps for parameter updates.  With respect to Dropout, Figure 4 also shows that for the same learning rates, net- works without Dropout converge much faster than ones working with Dropout in terms of both rank loss and MAP. Regardless of the cost functions, overﬁtting arises over the networks without Dropout and it is likely that overﬁtting is avoided effectively as discussed earlier.  Comparison of Algorithms Table 3 shows detailed results of all experiments with all algorithms on all six datasets, except that we could not obtain results of BP-MLL  8 A trajectory for PWE η = 0.1 is missing in the ﬁgure because it got 0.2 on the rankloss measure which is much worse than the other conﬁgurations.  1100001000000.030.0350.040.0450.050.0550.06The number of parameter updatesRank loss  1100001000000.10.150.20.250.30.350.40.450.5The number of parameter updatesMean average precision  PWE (cid:100)=0.01PWE (cid:100)=0.01 DPWE (cid:100)=0.1PWE (cid:100)=0.1  DCE  (cid:100)=0.01CE  (cid:100)=0.01 DCE  (cid:100)=0.1CE  (cid:100)=0.1  DTable 2: Average ranks of the algorithms on ranking and bipartition measures.  13  Eval. measures  Ranking  Bipartition  rankloss oneError Coverage MAP miP miR miF maP maR maF  2.2 NNA 1.2 NNAD BP-MLLTA 5.9 BP-MLLTAD 5 BP-MLLRA 5.2 BP-MLLRAD 3.1 7.4 BRB 6 BRR  Average Ranks 2.2 1.6 6.2 5.7 6.6 5.8 4.3 3.6  2.6 1.2 6.2 5 5.4 3 6.9 5.7  2.4 1.4 6.9 5.7 7 6.3 3.3 3  6 5.8 1.8 2  2.4 1.8 5.6 5.6  2 2 6.6 3.6 6.6 6.8 3 4 7 3.6 7 2.8 5.6 2.8 5 2.8 5.8 5.6 3.6 6 3.2 6.8 4.6 4.4 6.8 3.6 4.6 2.8 3.4 4.6  7 5  2 2.2 6.2 7.2 4.2 5.6 5.6 3  on EUR-Lex within a reasonable time frame. In an attempt to summarize the results, Table 2 shows the average rank of each algorithm in these six datasets according to all ranking an bipartition measures discussed in Section 4.  We can see that although BP-MLL focuses on minimizing pairwise ranking errors, thereby capturing label dependencies, the single hidden layer NNs with cross-entropy minimization (i.e., NNA and NNAD) work much better not only on rank loss but also on other ranking measures. The binary relevance (BR) approaches show acceptable performance on ranking measures even though label dependency was ignored during the training phase. In addition, NNA and NNAD perform as good as or better than other methods on bipartition measures as well as on ranking measures.  We did not observe signiﬁcant improvements by replacing hidden units of BP-MLL from tanh to ReLU. However, if we change the cost function in the previous setup from PWE to CE, signiﬁcant improvements were obtained. Because BP-MLLRAD is the same architecture as NNAD except for its cost function,9 we can say that the differences in the effectiveness of NNs and BP-MLL are due to the use of different cost functions. This also implies that the main source of improvements for NNs against BP-MLL is replacement of the cost function. Again, Figure 4 shows the difference between two cost functions more explicitly.  6 Conclusion  This paper presents a multi-label classiﬁcation framework based on a neural network and a simple threshold label predictor. We found that our approach outperforms BP- MLL, both in predictive performance as well as in computational complexity and con- vergence speed. We have explored why BP-MLL as a multi-label text classiﬁer does not perform well. Our experimental results showed the proposed framework is an effective method for the multi-label text classiﬁcation task. Also, we have conducted extensive analysis to characterize the effectiveness of combining ReLUs with AdaGrad for fast convergence rate, and utilizing Dropout to prevent overﬁtting which results in better generalization.  9 For PWE we use tanh in the output layer, but sigmoid is used for CE because predictions o for computing CE with targets y needs to be between 0 and 1.  14  Table 3: Results on ranking and bipartition measures. Results for BP-MLL on EUR-Lex are missing because the runs could not be completed in a reasonably short time.  Eval. measures  Ranking  Bipartition  rankloss oneError Coverage MAP miP miR miF maP maR maF  0.0037 0.0706 NNA 0.0031 0.0689 NNAD BP-MLLTA 0.0039 0.0868 BP-MLLTAD 0.0039 0.0808 BP-MLLRA 0.0054 0.0808 BP-MLLRAD 0.0063 0.0719 0.0040 0.0613 BRB 0.0040 0.0613 BRR  0.0040 0.0218 NNA 0.0038 0.0212 NNAD BP-MLLTA 0.0058 0.0349 BP-MLLTAD 0.0057 0.0332 BP-MLLRA 0.0058 0.0393 BP-MLLRAD 0.0056 0.0378 0.0061 0.0301 BRB 0.0051 0.0287 BRR  NNA NNAD BRB BRR  0.0195 0.2016 0.0164 0.1681 0.0642 0.1918 0.0204 0.2088  0.0350 0.2968 NNA 0.0352 0.2963 NNAD BP-MLLTA 0.0386 0.8309 BP-MLLTAD 0.0371 0.7591 BP-MLLRA 0.0369 0.4221 BP-MLLRAD 0.0353 0.4522 0.0572 0.3052 BRB 0.0434 0.3021 BRR  0.0860 0.3149 NNA 0.0836 0.3127 NNAD BP-MLLTA 0.0953 0.4967 BP-MLLTAD 0.0898 0.4358 BP-MLLRA 0.0964 0.6157 BP-MLLRAD 0.0894 0.6060 0.1184 0.4355 BRB 0.1184 0.4358 BRR  Reuters-21578  0.7473 0.6611 0.8238 0.8119 1.0987 1.2037 0.8092 0.8092  3.1564 3.1108 3.7570 3.6917 3.6730 3.6032 3.8073 3.4998  0.9484 0.8986 0.8357 0.8660 0.6439 0.4424 0.4996 0.9499 0.9042 0.8344 0.8679 0.6150 0.4420 0.4956 0.9400 0.7876 0.8616 0.8230 0.5609 0.4761 0.4939 0.9434 0.7945 0.8654 0.8284 0.5459 0.4685 0.4831 0.9431 0.8205 0.8582 0.8389 0.5303 0.4364 0.4624 0.9476 0.8421 0.8416 0.8418 0.5510 0.4292 0.4629 0.9550 0.9300 0.8096 0.8656 0.6050 0.3806 0.4455 0.9550 0.8982 0.8603 0.8789 0.6396 0.4744 0.5213 RCV1-v2 0.9491 0.9017 0.7836 0.8385 0.7671 0.5760 0.6457 0.9500 0.9075 0.7813 0.8397 0.7842 0.5626 0.6404 0.9373 0.6685 0.7695 0.7154 0.4385 0.5803 0.4855 0.9375 0.6347 0.7497 0.6874 0.3961 0.5676 0.4483 0.9330 0.7712 0.8074 0.7889 0.5741 0.6007 0.5823 0.9345 0.7612 0.8016 0.7809 0.5755 0.5748 0.5694 0.9375 0.8857 0.8232 0.8533 0.7654 0.6342 0.6842 0.9420 0.8156 0.8822 0.8476 0.6961 0.7112 0.6923 EUR-Lex  310.6202 0.5975 0.6346 0.4722 0.5415 0.3847 0.3115 0.3256 269.4534 0.6433 0.7124 0.4823 0.5752 0.4470 0.3427 0.3687 976.2550 0.6114 0.6124 0.4945 0.5471 0.4260 0.3643 0.3752 334.6172 0.5922 0.0329 0.5134 0.0619 0.2323 0.3063 0.2331 German Education Index 138.5423 0.4828 0.4499 0.4200 0.4345 0.4110 0.3132 0.3427 138.3590 0.4797 0.4155 0.4472 0.4308 0.3822 0.3216 0.3305 150.8065 0.3432 0.1502 0.6758 0.2458 0.1507 0.5562 0.2229 139.1062 0.3281 0.1192 0.5056 0.1930 0.1079 0.4276 0.1632 143.4541 0.4133 0.2618 0.4909 0.3415 0.3032 0.3425 0.2878 135.1398 0.3953 0.2400 0.5026 0.3248 0.2793 0.3520 0.2767 221.0968 0.4533 0.5141 0.2318 0.3195 0.3913 0.1716 0.2319 176.6349 0.4755 0.4421 0.3997 0.4199 0.4361 0.2706 0.3097  Delicious  396.4659 0.4015 0.3637 0.4099 0.3854 0.2488 0.1721 0.1772 389.9422 0.4075 0.3617 0.4399 0.3970 0.2821 0.1777 0.1824 434.8601 0.3288 0.1829 0.5857 0.2787 0.1220 0.2728 0.1572 418.3618 0.3359 0.1874 0.5884 0.2806 0.1315 0.2427 0.1518 427.0468 0.2793 0.2070 0.5894 0.3064 0.1479 0.2609 0.1699 411.5633 0.2854 0.2113 0.5495 0.3052 0.1650 0.2245 0.1567 496.7444 0.3371 0.1752 0.2692 0.2123 0.0749 0.1336 0.0901 496.8180 0.3371 0.2559 0.3561 0.2978 0.1000 0.1485 0.1152  Bookmarks  0.0663 0.4924 NNA 0.0629 0.4828 NNAD BP-MLLTA 0.0684 0.5598 BP-MLLTAD 0.0647 0.5574 BP-MLLRA 0.0707 0.5428 BP-MLLRAD 0.0638 0.5322 0.0913 0.5318 BRB 0.0895 0.5305 BRR  22.1183 0.5323 0.3919 0.3907 0.3913 0.3564 0.3069 0.3149 20.9938 0.5423 0.3929 0.3996 0.3962 0.3664 0.3149 0.3222 23.0362 0.4922 0.0943 0.5682 0.1617 0.1115 0.4743 0.1677 21.7949 0.4911 0.0775 0.6096 0.1375 0.0874 0.5144 0.1414 23.6088 0.5049 0.1153 0.5389 0.1899 0.1235 0.4373 0.1808 21.5108 0.5131 0.0938 0.5779 0.1615 0.1061 0.4785 0.1631 29.6537 0.4868 0.2821 0.2546 0.2676 0.1950 0.1880 0.1877 28.7233 0.4889 0.2525 0.4049 0.3110 0.2259 0.3126 0.2569  Acknowledgments This work has been supported by the Information Center for Education of the German Institute for Educational Research (DIPF) under the Knowledge Discovery in Scientiﬁc Literature (KDSL) program.  15  References  [1] Bi, W., Kwok, J.T.: Multi-label classiﬁcation on tree-and dag-structured hierar- chies. In: Proceedings of the 28th International Conference on Machine Learning. pp. 17–24 (2011)  [2] Calauz`enes, C., Usunier, N., Gallinari, P.: On the (non-)existence of convex, cali- brated surrogate losses for ranking. In: Advances in Neural Information Process- ing Systems 25. pp. 197–205 (2012)  [3] Dembczy´nski, K., Cheng, W., H¨ullermeier, E.: Bayes optimal multilabel classiﬁ- cation via probabilistic classiﬁer chains. In: Proceedings of the 27th International Conference on Machine Learning. pp. 279–286 (2010)  [4] Dembczy´nski, K., Kotłowski, W., H¨ullermeier, E.: Consistent multilabel ranking through univariate losses. In: Proceedings of the 29th International Conference on Machine Learning. pp. 1319–1326 (2012)  [5] Duchi, J., Hazan, E., Singer, Y.: Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research 12, 2121– 2159 (2011)  [6] Elisseeff, A., Weston, J.: A kernel method for multi-labelled classiﬁcation. Advances in Neural Information Processing Systems 14. pp. 681–687 (2001)  In:  [7] Fan, R.E., Chang, K.W., Hsieh, C.J., Wang, X.R., Lin, C.J.: Liblinear: A library for large linear classiﬁcation. Journal of Machine Learning Research 9, 1871–1874 (2008)  [8] F¨urnkranz, J., H¨ullermeier, E., Loza Menc´ıa, E., Brinker, K.: Multilabel classiﬁ- cation via calibrated label ranking. Machine Learning 73(2), 133–153 (Jun 2008) [9] Gao, W., Zhou, Z.H.: On the consistency of multi-label learning. Artiﬁcial Intel-  ligence 199–200, 22–44 (2013)  [10] Ghamrawi, N., McCallum, A.: Collective multi-label classiﬁcation. In: Proceed- ings of the 14th ACM International Conference on Information and Knowledge Management. pp. 195–200 (2005)  [11] Glorot, X., Bengio, Y.: Understanding the difﬁculty of training deep feedforward neural networks. In: Proceedings of the 13th International Conference on Artiﬁcial Intelligence and Statistics, JMLR W&CP. pp. 249–256 (2010)  [12] Glorot, X., Bordes, A., Bengio, Y.: Deep sparse rectiﬁer neural networks.  In: Proceedings of the 14th International Conference on Artiﬁcial Intelligence and Statistics, JMLR W&CP. pp. 315–323 (2011)  [13] Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 (2012)  [14] Joachims, T.: Text categorization with support vector machines: Learning with many relevant features. In: Proceedings of the 10th European Conference on Ma- chine Learning (ECML-98). pp. 137–142 (1998)  16  [15] Joachims, T.: Training linear svms in linear time. In: Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pp. 217–226 (2006)  [16] LeCun, Y., Bottou, L., Orr, G.B., M¨uller, K.R.: Efﬁcient backprop.  In: Neural  Networks: tricks of the trade, pp. 9–48 (2012)  [17] Lewis, D.D., Yang, Y., Rose, T.G., Li, F.: Rcv1: A new benchmark collection for text categorization research. Journal of Machine Learning Research 5, 361–397 (2004)  [18] Liu, T.Y., Yang, Y., Wan, H., Zeng, H.J., Chen, Z., Ma, W.Y.: Support vector ma- chines classiﬁcation with a very large-scale taxonomy. SIGKDD Explorations 7(1), 36–43 (2005)  [19] Loza Menc´ıa, E., Park, S.H., F¨urnkranz, J.: Efﬁcient voting prediction for pairwise  multilabel classiﬁcation. Neurocomputing 73(7-9), 1164–1176 (2010)  [20] Manning, C.D., Raghavan, P., Sch¨utze, H.: Introduction to Information Retrieval.  Cambridge University Press (2008)  [21] Nair, V., Hinton, G.E.: Rectiﬁed linear units improve restricted boltzmann ma- chines. In: Proceedings of the 27th International Conference on Machine Learn- ing. pp. 807–814 (2010)  [22] Read, J., Pfahringer, B., Holmes, G., Frank, E.: Classiﬁer chains for multi-label  classiﬁcation. Machine Learning 85(3), 333–359 (2011)  [23] Rubin, T.N., Chambers, A., Smyth, P., Steyvers, M.: Statistical topic models for multi-label document classiﬁcation. Machine Learning 88(1-2), 157–208 (2012) [24] Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning representations by back-  propagating errors. Nature 323(6088), 533–536 (1986)  [25] Schapire, R.E., Singer, Y.: Boostexter: A boosting-based system for text catego-  rization. Machine Learning 39(2/3), 135–168 (2000)  [26] Solla, S.A., Levin, E., Fleisher, M.: Accelerated learning in layered neural net-  works. Complex Systems 2(6), 625–640 (1988)  [27] Trohidis, K., Tsoumakas, G., Kalliris, G., Vlahavas, I.: Multi-label classiﬁcation of music into emotions. In: Proceedings of the 9th International Conference on Music Information Retrieval. pp. 325–330 (2008)  [28] Tsoumakas, G., Katakis, I., Vlahavas, I.P.: Random k-labelsets for multilabel clas- siﬁcation. IEEE Transactions on Knowledge and Data Engineering 23(7), 1079– 1089 (2011)  [29] Yang, Y., Gopal, S.: Multilabel classiﬁcation with meta-level features in a  learning-to-rank framework. Machine Learning 88(1-2), 47–68 (2012)  [30] Zeiler, M.D., Ranzato, M., Monga, R., Mao, M.Z., Yang, K., Le, Q.V., Nguyen, P., Senior, A., Vanhoucke, V., Dean, J., Hinton, G.E.: On rectiﬁed linear units for speech processing. In: Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. pp. 3517–3521 (2013)  [31] Zhang, M.L., Zhou, Z.H.: Multilabel neural networks with applications to func- tional genomics and text categorization. IEEE Transactions on Knowledge and Data Engineering 18, 1338–1351 (2006)  ","Neural networks have recently been proposed for multi-label classificationbecause they are able to capture and model label dependencies in the outputlayer. In this work, we investigate limitations of BP-MLL, a neural network(NN) architecture that aims at minimizing pairwise ranking error. Instead, wepropose to use a comparably simple NN approach with recently proposed learningtechniques for large-scale multi-label text classification tasks. Inparticular, we show that BP-MLL's ranking loss minimization can be efficientlyand effectively replaced with the commonly used cross entropy error function,and demonstrate that several advances in neural network training that have beendeveloped in the realm of deep learning can be effectively employed in thissetting. Our experimental results show that simple NN models equipped withadvanced techniques such as rectified linear units, dropout, and AdaGradperform as well as or even outperform state-of-the-art approaches on sixlarge-scale textual datasets with diverse characteristics."
1312.5869,2014,"Learning Non-Linear Feature Maps, With An Application To Representation Learning  ","['Dimitrios Athanasakis', 'John Shawe-Taylor', 'Delmiro Fernandez-Reyes']",https://arxiv.org/pdf/1312.5869.pdf,"4 1 0 2     b e F 8 1         ]  G L . s c [      2 v 9 6 8 5  .  2 1 3 1 : v i X r a  Principled Non-Linear Feature Selection  Dimitrios Athanasakis∗  dathanasakis@cs.ucl.ac.uk  John Shawe-Taylor* jst@cs.ucl.ac.uk  Delmiro Fernandez-Reyes† dfernan@nimr.mrc.ac.uk  Abstract  Recent non-linear feature selection approaches employing greedy optimisation of Centred Kernel Target Alignment (KTA) exhibit strong results in terms of gen- eralisation accuracy and sparsity. However, they are computationally prohibitive for large datasets. We propose randSel, a randomised feature selection algorithm, with attractive scaling properties. Our theoretical analysis of randSel provides probabilistic guarantees for correct identiﬁcation of relevant features under rea- sonable assumptions. RandSel’s characteristics make it an ideal candidate for identifying informative learned representations. We’ve conducted experimenta- tion to establish the performance of this approach, and present encouraging results, including a 3rd position result in the recent ICML black box learning challenge as well as competitive results for signal peptide prediction, an important problem in bioinformatics.  1  Introduction  Feature selection is an important aspect in the implementation of machine learning methods. The selection of informative features can reduce generalisation error as well as storage and processing requirements for large datasets. In addition, parsimonious models provide valuable insight into the relations underlying elements of the process under examination. There is a wealth of literature on the subject of feature selection when the relationship between variables is linear. Unfortunately when the relation is non-linear feature selection becomes substantially more nuanced. Kernel methods excel in modelling non-linear relations. Unsurprisingly, a number of kernel-based feature selection algorithms have been proposed. Early propositions, such as Recursive Feature Elimination (RFE) [1] can be computationally prohibitive, while attempts to learn a convex combi- nation of low-rank kernels may fail to encapsulate nonlinearities in the underlying relation. Recent approaches using explicit kernel approximations can capture non-linear relations, but increase the storage and computational requirements.  1.1 Related Work  Our approach makes extensive use of Kernel Target Alignment (KTA) [2,3], as the empirical es- timator for the Hilbert-Schmidt Independence Criterion (HSIC). Work on HSIC [4] provides the foundation of using the alignment of centred kernel matrices as the basis for measuring statistical dependence. The Hilbert-Schmidt Independence criterion is the basis for further work in [5], where greedy optimisation of centred alignment is employed for feature selection. Additionally, [5] identi-  ∗Department of Computer Science, University College London, London, UK †National Institute For Medical Research, London, UK  1  ﬁes numerous connections with other existing feature selection algorithms which can be considered as instances of the framework. Stability selection [6] is a general framework for variable selection and structure estimation of high dimensional data. The core principle of stability selection is to combine subsampling with a sparse variable selection algorithm. By repeated estimation over a number of different subsamples, the framework keeps track of the number of times each variable was used, thus maintaining an estimate for the importance of each feature. In this work, we propose a synthesis of the two aforementioned approaches through a randomised feature selection algorithm based on estimating the statistical dependence between bootstrapped random subspaces of the dataset in RKHS. The dependence esti- mation of random subsets of variables is similar to the approach of [11], which is extended through bootstrapping and carefully controlled feature set sizes. Our proposal is simple to implement and compares favourably with other methods in terms of scal- ability. The rest of the paper is structured as follows: Section 2 presents the necessary background on feature selection for kernel-based learning. Section 3 introduces a basic randomised algorithm for nonlinear feature selection, along with some simple examples, while Section 4 provides some analysis.Section 5 provides examples of how randSel can be effectively utilised as an important con- stituent of representation learning. Section 6 provides experimental results, and the brief discussion of Section 7 concludes this paper.  2 Preliminaries We consider the supervised learning problem of modelling the relationship between a m × n input matrix X and a corresponding m × n(cid:48) output matrix Y . The simplest instance of such a problem is binary classiﬁcation where the objective is the learning problem is to learn a function f : x → y mapping input vectors x to the desired outputs y. In the binary case we are presented with a m × n matrix X and a vector of outputs y, yi ∈ {+1,−1} Limiting the class of discrimination functions to linear classiﬁers we wish to ﬁnd a classiﬁer f (x) =  wixi = (cid:104)w, x(cid:105)  (cid:88)  i  The linear learning formulation can be generalised to the nonlinear setting through the use of a nonlinear feature map φ(x), leading to the kernelized formulation:  f (x) = (cid:104)w, φ(x)(cid:105) = (cid:104)(cid:88)  aiyiφ(xi), φ(x)(cid:105) =  aiyik(xi, x)  i  i  The key quantit? of interest in our approach is the centred kernel target alignment which is the empirical estimator of the HSIC[4], which measures statistical dependence in RKHS:  a(Cx, Cy) =  (cid:104)Cx, Cy(cid:105)F (cid:107)Cx(cid:107)F(cid:107)Cy(cid:107)F  =  i,j cxij cyij  i,j (cid:107)kyij(cid:107)  The matrices Cx and Cy correspond to centred kernels on the features X and outputs Y and are computed as:  (cid:88)  (cid:80) (cid:80) i,j (cid:107)cxij(cid:107)(cid:80) (cid:21)  (cid:20)  I − 11T m  (cid:20)  (cid:21)  C =  I − 11T m  K  where 1, in the above equation denotes the m-dimensional vector with all entries set equal to one.  3 Development of key ideas  The approach we will take will be based on the following well-known observation that links kernel target alignment with the degree to which an input space contains a linear projection that correlates with the target.  2  Proposition 3.1 Let P be a probability distribution on the product space X × R, where X has a projection φ into a Hilbert space F deﬁned by a kernel κ. We have that  (cid:113)E(x,y)∼P,(x(cid:48),y(cid:48))∼P [yy(cid:48)κ(x, x(cid:48))] =  = sup  w:(cid:107)w(cid:107)≤1  E(x,y)∼P [y(cid:104)w, φ(x)(cid:105)]  Proof:  sup  w:(cid:107)w(cid:107)≤1  = sup  w:(cid:107)w(cid:107)≤1  E(x,y)∼P [y(cid:104)w, φ(x)(cid:105)] =  (cid:10)w, E(x,y)∼P [φ(x)y](cid:11)  =(cid:13)(cid:13)E(x,y)∼P [φ(x)y](cid:13)(cid:13) (cid:115)(cid:90) (cid:90) (cid:113)E(x,y)∼P,(x(cid:48),y(cid:48))∼P [yy(cid:48)κ(x, x(cid:48))]  =  =  dP (x, y)dP (x(cid:48), y(cid:48))(cid:104)φ(x), φ(x(cid:48))(cid:105)yy(cid:48)  The proposition suggests that we can detect useful representations by measuring kernel target align- ment. For non-linear functions the difﬁculty is to identify which combination of features creates a useful representation. We tackle this problem by sampling subsets S of features and assessing whether on average the presence of a particular feature i contributes to an increase ci in the average kernel target alignment. In this way we derive an empirical estimate of a quantity we will term the contribution.  (cid:2)E(x,y)∼P,(x(cid:48),y(cid:48))∼P [yy(cid:48)κS(cid:48)(x, x(cid:48))](cid:3) ,  (cid:2)E(x,y)∼P,(x(cid:48),y(cid:48))∼P [yy(cid:48)κS(x, x(cid:48))](cid:3) − ES(cid:48)∼S\i  Deﬁnition 3.2 The contribution ci of feature i is deﬁned as ci = ES∼Si where κS denotes the (non-linear) kernel using features in the set S (in our case this will be a Gaussian kernel with equal width), Si the uniform distribution over sets of features of size (cid:98)n/2(cid:99) + 1 that include the feature i, S\i the uniform distribution over sets of features of size (cid:98)n/2(cid:99) that do not contain the feature i, and n is the number of features. Note that the two distributions over features Si and S\i are matched in the sense that for each S with non-zero probability in S\i, S ∪ {i} has equal probability in Si. This approach is a straightforward extension of the idea of BaHsic [5]. We will show that for variables that are independent of the target this contribution will be negative. On the other hand, provided there are combinations of variables including the given variable that can generate signiﬁcant correlations then the contribution of the variable will be positive.  Deﬁnition 3.3 We will deﬁne an irrelevant feature to be one whose value is statistically independent of the label and of the other features.  We would like an assurance that irrelevant features do not increase alignment. This is guaranteed for the Gaussian kernel by the following result. Proposition 3.4 Let P be a probability distribution on the product space X × R, where X has a projection φSi into a Hilbert space F deﬁned by the Gaussian kernel κS on a set of features S. Suppose a feature i (cid:54)∈ S is irrelevant. We have that  E(x,y)∼P,(x(cid:48),y(cid:48))∼P [yy(cid:48)κS∪{i}(x, x(cid:48))] ≤ E(x,y)∼P,(x(cid:48),y(cid:48))∼P [yy(cid:48)κS(x, x(cid:48))]  Proof (sketch): Since the feature is independent of the target and the other features, functions of these features are also independent. Hence, E(x,y)∼P,(x(cid:48),y(cid:48))∼P [yy(cid:48)κS∪{i}(x, x(cid:48))]  = E(x,y)∼P,(x(cid:48),y(cid:48))∼P [yy(cid:48)κS(x, x(cid:48)) exp(−γ(xi − x(cid:48) i)2)] = E(x,y)∼P,(x(cid:48),y(cid:48))∼P [yy(cid:48)κS(x, x(cid:48))]E(x,y)∼P,(x(cid:48),y(cid:48))∼P [exp(−γ(xi − x(cid:48) = E(x,y)∼P,(x(cid:48),y(cid:48))∼P [yy(cid:48)κS(x, x(cid:48))]α  i)2)]  3  i)2)] ≤ 1.  for α = E(x,y)∼P,(x(cid:48),y(cid:48))∼P [exp(−γ(xi − x(cid:48) In fact the quantity α is typically less than 1 so that adding irrelevant features decreases the align- ment. Our approach will be to progressively remove sets of features that are deemed to be irrelevant, hence increasing the alignment together with the signal to noise ratio for the relevant features. Fig- ure 2 shows how progressively removing features from a learning problem whose output is the XOR function of the ﬁrst two features both increases the alignment contributions and helps to highlight the two relevant features.  Figure 1: 200-dimensional XOR classiﬁcation problem, with a subsample size of 1,000 and repeated over 10,000 random partitions of the features. The expected contribution of the η-inﬂuential features, shown in red, are clearly separated from that of all irrelevant variables.  Figure 2: 200-dimensional XOR classiﬁcation problem using a subsample size of 100 and repeating over 1,000 random partitions of the variables. The expected contribution of the two relevant features is in red. Owing to the subsample size and the number of iterations, it is necessary to iteratively reject low contributing features. It can also be seen that as more of the irrelevant features are re- moved in later iterations of the method, the expected contribution of the two relevant variables rises substantially.  We now introduce our deﬁnition of a relevant feature. Deﬁnition 3.5 A feature i will be termed η-inﬂuential when its contribution ci ≥ η > 0.  4  050100150200250−202x 10−8contributionSo far we have only considered expected alignment. In practice we must estimate this expectation from a ﬁnite sample. We will omit this part of the analysis as it is a straigthforward application of U-statistics that ensures that with high probability for a sufﬁciently large sample from Si and S\i and of samples from P (whose sizes depend on η, δ, the number k of η-inﬂuential variables and the number T of iterations) an empirical estimate of the contribution of an η-inﬂuential variable will with probability at least 1 − δ be greater than 0 for all of the ﬁxed number T of iterations of the algorithm. Our ﬁnal ingredient is a method of removing irrelevant features that we will term culling. At each iteration of the algorithm the contributions of all of the features are estimated using the required sample size and the contributions are sorted. We then remove the bottom 12.5% of the features in this ordering. Our main result assures us that culling will work under the assumption that the irrelevant variables are independent.  Theorem 3.6 Fix η > 0. Suppose that there are k η-inﬂuential variables and all other variables are irrelevant. Fix δ > 0 and number T of iterations. Given sufﬁciently many samples as described above the culling algorithm will with probability at least 1 − δ remove only irrelevant variables.  Proof (sketch): Through use of Hoeffding’s inequality for U-statistics we can bound the deviation from the true expectation for all irrelevant variables i, ˆci ≤ η 2 with probability at least 1 − δ. 2 , with probability at least 1−δ. Therefore, Conversely, for all relevant variables will be within ˆcj ≥ η provided sufﬁciently many samples, removing all variables with contribution estimated contribution ˆci < η/2 will only remove irrelevant variables, and preserve all relevant variables. Figure 1 illustrates how for a large enough sample size and number of random partitions, the algo- rithm can identify the relevant variables with a high relative margin in the expected contribution. However, the sample size required to achieve the probabilistic guarantees of Theorem 3.6, is too large for most practical settings. For this reason in our experimentation, we proceed to iteratively cull a smaller percentage of the bottom-contributing features at the end of each iteration. For exam- ple, the experiments here were performed with culling 12.5% of the features after the end of each iteration, a process illustrated in ﬁgure 2.  4 Properties of the algorithm We now deﬁne our algorithm for randomised selection (randSel). Given a m × n input matrix X and corresponding output matrix Y , randSel proceeds by estimating the individual contribution of features by estimating the alignment of a number of random subsamples that include n 2 + 1 randomly selected features. This leads to an estimate for the expected alignment contribution of including a feature. The algorithm is parametrized by the number of subsamples N, a subsample sizenb and a percentage z% of features that are dropped after N subsamples. The algorithm proceeds iteratively until only two features remain. There are a number of beneﬁts to this approach, aside from the tangible probabilistic guarantees. RandSel scales gracefully. Considering the computation of a kernel k(x, x(cid:48)) for samples x, x(cid:48) atomic, the number of kernel computations for a single iteration are n2 bN, which for a sensible choice of N can be substantially smaller than the m2n complexity of HSIC variants. For example m and N = n an iteration would require mn kernel element computations, and in setting nb = addition this process is trivial to parallelize.  2 and n  √  5 Feature Selection for learned representations  Unsupervised feature learning algorithms such as sparse ﬁltering [9] are often used to learn over- complete representations of data. The depth of a learning architecture refers to the composition of different levels of non-linear operations in the learned function. This suggests that employing feature selection to reﬁne a set of learned representations, would substantially beneﬁt from capturing non- linear interactions between the learned features. Utilising randSel for feature selection in this setting is predicated on a number of properties. RandSel is readily applicable to a large sample size, a key property for the large sample sizes typically involved in representation learning. In addition, the algorithm is readily applicable to domains that have some structure. The multi-class structure of the  5  Algorithm 1 randSel  Input: input data X, labels Y , number of iterations r subsample size s, number of features n, drop percentile proportion z, top percentile proportion a, number of occasions t repeat  2 randomly selected variables  2 + 1 randomly selected variables  for i = 1 to r do  , Y (+)  (Xi, Yi) = Random subsample of size s over n ai= alignment(Xi, Yi) (X (+) i i = alignment(X (+) a(+) end for for j = 1 to n do  , Y (+)  ) = Random subsample of size s over n  )  i  i  i  mean contribution cj = meani:j∈X (+)  end for drop the z% bottom-contributing features if ﬁxing features then  i  if j top-contributor for t consecutive times then  ﬁx feature j  end if  end if  until no features left to ﬁx, or only 2 features remain Return Sequence of estimated contributions and Fixed Variables  (a(+)  i  ) − meani:j /∈Xi(ai),  black box challenge is an example where this property is important. This is a shared property of all the HSIC-variants. Finally, randSel is granular. Here, granularity refers to the fact that at the end of each iteration the algorithm returns a list of the remaining features and their expected contributions, which leads to a series of kernels of increased granularity. This can be a highly attractive property when using MKL for the ﬁnal prediction.  5.1 Prediction  RandSel produces progressively ﬁne-grained combinations of features. A prediction mechanism effectively utilising the increasingly granular combinations of features comprises the last step of our approach, where we take a boosting approach based on LPBOOST-MKL [10]. The architecture proceeds by building a number of Gaussian kernels, parametrised by the different sets of features they are deﬁned on, and a kernel bandwidth parameter σ as  κ(si,σ)(x, x(cid:48)) = exp(−σ(x(si) − x(cid:48)(si))2  , Where, x(si), x(cid:48)(si) are vectors containing only variables included in the set si. For simplicity assume there are nK distinct combinations of feature sets si and corresponding bandwidths σ. We deﬁne a kernel on each such combination of features and bandwidth. Kernel ridge regression was used to generate the individual weak predictors in our architecture. Thus, each weak predictor has the form  (cid:88)  h(x, x(cid:48)) =  aiκsj ,σ(xi, x(cid:48))  The algorithm then computes the classiﬁcation rule, which is a convex combination of the weak learners, through the following linear program:  i  minimize β  s.t.  uiyiHij ≤ β  m(cid:88) m(cid:88)  i=1  ui = 1 0 ≤ ui,≤ D  i=1  6  Where D is a regularization parameter. Provided a sensible range of kernel bandwidths σ is speci- ﬁed, the ﬁnal LPBoost classiﬁer only requires tuning the regularization parameter D. In our search for simplicity, this is a tangible beneﬁt, substantially reducing the search space of parameter combi- nations, to tuning this single regularization parameter.  6 Results  6.1 Results on the ICML Black Box Learning Challenge  We used our proposal in the recent ICML 2013 Challenges in Representation Learning Black Box Learning challenge [8][12]. The dataset used in the challenge was an obfuscated subset of the Street View House Numbers dataset [15]. The original data were projected down to 1875 dimensions by multiplication with a random matrix, and the organizers did not reveal the source of the dataset was SVHN until the competition was over. The training set comprises only 1,000 labelled samples, while an additional 130,000 samples were provided for the purposes of unsupervised pre-training. For our submissions, cross validation was used to select the number of features to learn with sparse ﬁltering, with our best solution using a set of 625 learned features. Randsel was then used to select combinations of the 625 learned features dropping 12.5% of the least contributing features at the end of each iteration. The resulting set of 34 different sets of features was combined with 75 different σ parameters to result in 2550 weak learners. The regularisation parameter D was also set through cross validation. This approach led to a generalisation accuracy of 68.44% on the public and 68.48% on the private leaderboards, ranking third in both cases out of a total of 218 teams.  6.2 Application to cleavage site prediction  Signal peptides are amino-acid sequences found in transported proteins that selectively guide the distribution of the proteins to speciﬁc cellular compartments. Often referred to as the zip-code sequences, owing to their role in sub-cellular localization, a substantial body of work is devoted to predicting the cleavage site of signal sequences. Current literature establishes the importance of a number of physicochemical properties of the signal sequence in determining the cleavage site location. The experimental pipeline presented in this section further supplements this approach, by learning a feature representation of multiple physicochemical property encodings. The Predisi dataset [13] of eukaryotic signal sequences was used for experimentation. Initial ﬁltering produced a dataset of 2,705 unique signal peptide sequences, with a sequence length of 50 amino- acids. The approach used for cleavage site prediction breaks each individual sequence into smaller windows. Cross validation was used to estimate the parameters relating to the window size. The resulting convention was to use windows that contain 9 aminoacids prior to what we deem the target of the window and 2 aminoacids following that position. For an individual prediction to be considered accurate, the window predicted as most likely to contain the cleavage site in its target position, must coincide with the actual window containing the cleavage target site for the sequence. With the window parameters chosen through cross validation, this results in each individual se- quence of 50 aminoacids producing 39 windows with a length of 12 aminoacids each. The resulting process generates a dataset comprising of 105,495 windows. The entirety of 54 distinct physico- chemical encodings offered by the Matlab bioinformatics toolbox was used for numerical represen- tation of each sequence window, which is then represented by a 648-dimensional vector of physico- chemical properties.At this point, sparse ﬁltering learns an overcomplete representation comprising of 1500 learned features. This process generates a dataset comprising of 105,495 1500-dimensional samples. There are three interesting questions which the experiments where designed to address. Concretely, the experimental comparisons are designed to establish the performance gains from using a learned representation using sparse ﬁltering, over learning in the original feature space, using randSel for feature selection as opposed to other possible feature selection methods, and ﬁnally establishing the importance of multiple kernel learning, used for prediction. To this end, a number of competing solutions were implemented. The shallow approach uses the raw physichochemical properties for prediction. For comparing the performance of different feature se-  7  lection algorithms on learned representations, (cid:96)1-logistic regression stability selection and nonlinear SVM-RFE are used in addition to randSel. Finally we compare the performance of a prediction rule relying on a single gaussian kernel SVM, to the performance of MKL. The experiments employed libSVM [17] as an svm solver and SPAMS [16] for solving sparse logistic regression in the stability selection framework. The large size of the dataset, as well as the fact that it is highly imbalanced make for some challenges. Stability selection can readily be applied to a problem of this dimensionality. While deterministic HSIC-variants are ill-equipped to deal with the size of the resulting kernel in most current commod- ity hardware, the use of sampling in randSel largely alleviates the problems related with size. In order to address the issue of the imbalance, subsamples where both classes are equally represented were used. In terms of producing the actual prediction the experiments examine two options. The ﬁrst is using a chunking, Gaussian kernel SVM, which is the same approach that enables the use of RFE, which is used in combination with the various feature selection methods. Using vanilla LPBoost-MKL for prediction is prohibitive, owing to the memory requirements of storing the kernel matrices. The approach to rectify this problem is to use subsampling from the negative class, combined with kernel ridge regression as a weak predictor in our boosting framework. This results in a mixed-norm MKL formulation which effectively addresses the limitation of not being able to store the kernel matrices. For the l2 regularization parameter λ of individual weak predictors, a very small range of parameters was used. The l1 regularization parameter D for the LPBoost prediction rule was set through cross- validation.  6.3 Experimental Results  Table 1 summarizes the results for the different attempted approaches. Using the original feature representation with a non-linear SVM leads to a generalization accuracy of 67.2%. This is substan- tially smaller than all the approaches that rely on the learned feature representation. This suggests that there are performance gains to be had in using a learned representation.  Table 1: Accuracy for the different approaches discussed in section 6.2 when applied to the signal peptide problem.  Method  Accuracy (%) 67.20 ± 4.71 Original Representation 71.46 ± 2.93 Sparse Filtering + Stab. Sel. 71.81 ± 2.79 Sparse Filtering + RFE 72.75 ± 2.85 Sparse Filtering + RandSel Sparse Filtering + RandSel & MKL 75.28 ± 1.91  In terms of using feature selection on the learned representation, the results indicate that randSel has an edge over the two other methods, with RFE also performing slightly better than l1-regularized logistic regression-based stability selection. The learned representation offers a case where it is reasonable to suspect benign non-linear collusion between features, something that both RFE and randSel are designed to take advantage of, and the large sample size allows for increased conﬁdence when inferring such relationships. The fact that randSel outperforms RFE suggests that RFE’s re- liance on the support vectors for feature selection can negatively bias the feature selection procedure. Finally, the use of MKL for prediction further improves the results. Direct comparisons to state-of- the art methods for cleavage prediction is difﬁcult as the reported accuracy highly depends on the dataset and modelling assumptions, such as the original sequence length. Our proposed approach ap- pears to outperform SignalP’s [14] reported accuracy of 72.9% for eukaryotic sequences, but it must be noted that SignalP operates under different modelling assumptions and more extensive testing is necessary to account for that and to ascertain the signiﬁcance of this ﬁnding.  8  7 Conclusions  In this paper we propose randSel, a new algorithm for non-linear feature selection based on ran- domised estimates of HSIC. RandSel, stochastically estimates the expected importance of features at each iteration, proceeding to cull uninformative features at the end of each iteration. Our theoreti- cal analysis gives strong guarantees for the expected performance of this procedure which is further demonstrated by testing on a number of real and artiﬁcial datasets. Additionally, we presented a simple system that produces a classiﬁcation rule based on non-linear learned feature combinations of increasing granularity. The architecture of the system comprises a fast, unsupervised feature learning mechanism, randomised non-linear feature selection and a mul- tiple kernel learning based classiﬁer. The guiding principle of this approach is to use simple compo- nents that require minimal parameter tuning, with components further down the pipeline making up for the potential shortcomings upstream. Indeed, the three different constituents of this architecture, require minimal parameter tuning and scale gracefully, and the experimental results on both datasets we employed appear to validate the approach.  References [1] Guyon, Isabelle, Jason Weston, Stephen Barnhill, and Vladimir Vapnik (2002) Gene selection for cancer  classiﬁcation using support vector machines. Machine learning 46, no. 1-3 : 389-422.  [2] Shawe-Taylor, N., and A. Kandola (2002) On kernel target alignment. Advances in neural information  processing systems 14: 367.  [3] Cortes, Corinna, Mehryar Mohri, and Afshin Rostamizadeh (2012) Algorithms for learning kernels based  on centered alignment. The Journal of Machine Learning Research 13: 795-828.  [4] Gretton, Arthur, Olivier Bousquet, Alex Smola, and Bernhard Schlkopf (2005) Measuring statistical dependence with Hilbert-Schmidt norms. In Algorithmic learning theory, pp. 63-77. Springer Berlin Hei- delberg.  [5] Song, Le, Alex Smola, Arthur Gretton, Justin Bedo, and Karsten Borgwardt (2012) Feature selection via  dependence maximization. The Journal of Machine Learning Research 98888: 1393-1434.  [6] Meinshausen, Nicolai, and Peter Bhlmann (2012) Stability selection. Journal of the Royal Statistical  Society: Series B (Statistical Methodology) 72, no. 4: 417-473.  [7] Weston, Jason, Sayan Mukherjee, Olivier Chapelle, Massimiliano Pontil, Tomaso Poggio, and Vladimir Vapnik (2001) Feature selection for SVMs. Advances in neural information processing systems: 668-674. [8] Challenges in Representation Learning: The Black Box Learning Challenge: https://www.  kaggle.com/c/challenges-in-representation-learning-the-black-box-learning-challenge  [9] Ngiam, Jiquan, Pang Wei Koh, Zhenghao Chen, Sonia Bhaskar, and Andrew Y. Ng.(2011) Sparse ﬁlter-  ing. Advances in Neural Information Processing Systems 24: 1125-1133.  [10] Tristan Fletcher, Zakria Hussain, John Shawe-Taylor (2010) Multiple Kernel Learning on the Limit Order  Book, Proceedings of the First Workshop on Applications of Pattern Analysis  [11] Somol, Petr, Jiri Grim, and Pavel Pudil. (2011) Fast dependency-aware feature selection in very-high- dimensional pattern recognition. In Systems, Man, and Cybernetics (SMC) 2011 IEEE International Conference on, pp. 502-509. IEEE  [12] Goodfellow, I. J., Erhan, D., Carrier, P. L., Courville, A., Mirza, M., Hamner, B., ... & Bengio, Y. (2013) Challenges in Representation Learning: A report on three machine learning contests. Proceedings of the 20th International Conference on Neural Information Processing  [13] Hiller, Karsten, Andreas Grote, Maurice Scheer, Richard Mnch, and Dieter Jahn. ”PrediSi: prediction of signal peptides and their cleavage positions.” Nucleic acids research 32, no. suppl 2 (2004): W375-W379. [14] Petersen, Thomas Nordahl, Sren Brunak, Gunnar von Heijne, and Henrik Nielsen. ”SignalP 4.0: discrim-  inating signal peptides from transmembrane regions.” Nature methods 8, no. 10 (2011): 785-786.  [15] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y. Ng Reading Digits in Natural Images with Unsupervised Feature Learning NIPS Workshop on Deep Learning and Unsuper- vised Feature Learning 2011  [16] Jenatton, Rodolphe, Julien Mairal, Francis R. Bach, and Guillaume R. Obozinski. ”Proximal methods for sparse hierarchical dictionary learning.” In Proceedings of the 27th International Conference on Machine Learning. 2010.  [17] Chang, Chih-Chung, and Chih-Jen Lin. ”LIBSVM: a library for support vector machines.” ACM Trans-  actions on Intelligent Systems and Technology (TIST) 2, no. 3 (2011): 27.  9  Supplementary Material  Figure 3: The architecture employed in our experiments; randSel is applied on the features learned by sparse ﬁltering, producing a number of nonlinear combinations of learned features of increasing granularity. A number of kernels is deﬁned on these nonlinear combinations of features, and multiple kernel learning is used for the overall prediction.  10  SPARSE FILTERINGOVER-COMPLETE LEARNED REPRESENTATIONINCOMING DATARANDSELKERNEL ON FEATURESET 1OUTPUT LAYER MACHINERYPREDICTIONKERNEL ON FEATURESET KKERNEL ON FEATURESET  ...Figure 4: How the learned representation is for the signal peptide problem is generated. The amino- acid sequence is broken into smaller windows. Each amino-acid in the window is represented by its 54 distinct physicochemical properties. Sparse ﬁltering is used to learn a representation for this encoding.  11  ","Recent non-linear feature selection approaches employing greedy optimisationof Centred Kernel Target Alignment(KTA) exhibit strong results in terms ofgeneralisation accuracy and sparsity. However, they are computationallyprohibitive for large datasets. We propose randSel, a randomised featureselection algorithm, with attractive scaling properties. Our theoreticalanalysis of randSel provides strong probabilistic guarantees for correctidentification of relevant features. RandSel's characteristics make it an idealcandidate for identifying informative learned representations. We've conductedexperimentation to establish the performance of this approach, and presentencouraging results, including a 3rd position result in the recent ICML blackbox learning challenge as well as competitive results for signal peptideprediction, an important problem in bioinformatics."
1312.5785,2014,EXMOVES: Classifier-based Features for Scalable Action Recognition  ,"['Du Tran', 'Lorenzo Torresani']",https://arxiv.org/pdf/1312.5785.pdf,"EXMOVES: Classiﬁer-based Features for Scalable Action Recognition  Du Tran, Lorenzo Torresani Computer Science Department, Dartmouth College, NH 03755 USA  {DUTRAN,LORENZO}@CS.DARTMOUTH.EDU  Abstract  introduces EXMOVES,  This paper learned exemplar-based features for efﬁcient recognition of actions in videos. The entries in our descrip- tor are produced by evaluating a set of movement classiﬁers over spatial-temporal volumes of the input sequence. Each movement classiﬁer is a simple exemplar-SVM trained on low-level fea- tures, i.e., an SVM learned using a single an- notated positive space-time volume and a large number of unannotated videos. Our representation offers two main advantages. First, since our mid-level features are learned from individual video exemplars, they require minimal amount of supervision. Second, we show that simple linear classiﬁcation models trained on our global video descriptor yield ac- tion recognition accuracy approaching the state- of-the-art but at orders of magnitude lower cost, since at test-time no sliding window is neces- sary and linear models are efﬁcient to train and test. This enables scalable action recognition, i.e., efﬁcient classiﬁcation of a large number of actions even in massive video databases. We show the generality of our approach by build- ing our mid-level descriptors from two different low-level feature vectors. The accuracy and efﬁ- ciency of the approach are demonstrated on sev- eral large-scale action recognition benchmarks.  4 1 0 2    r a     M 8 2      ]  V C . s c [      3 v 5 8 7 5  .  2 1 3 1 : v i X r a  1. Introduction Human action recognition is an important but still largely unsolved problem in computer vision with many poten- tial useful applications, including content-based video re- trieval, automatic surveillance, and human-computer inter- action. The difﬁculty of the task stems from the large intra- class variations in terms of subject and scene appearance, motion, viewing positions, as well as action duration.  Proceedings of the 2 nd International Conference on Learning Representation, Banff, Canada, 2014. Copyright 2014 by the au- thor(s).  We argue that most of the existing action recognition methods are not designed to handle such heterogeneity. Typically, these approaches are evaluated only on sim- ple datasets involving a small number of action classes and videos recorded in lab-controlled environments (Blank et al., 2005; Veeraraghavan et al., 2006). Furthermore, in the design of the action recognizer very little consideration is usually given to the computational cost which, as a result, is often very high. We believe that modern applications of action recognition demand scalable systems that can operate efﬁciently on large databases of unconstrained image sequences, such as YouTube videos. For this purpose, we identify three key- requirements to address: 1) the action recognition system must be able to handle the substantial variations of motion and appearance exhibited by realistic videos; 2) the train- ing of each action classiﬁer must have low-computational complexity and require little human intervention in order to be able to learn models for a large number of human ac- tions; 3) the testing of the action classiﬁer must be efﬁcient so as to enable recognition in large repositories, such as video-sharing websites. This work addresses these requirements by proposing a global video descriptor that yields state-of-the-art action recognition accuracy even with simple linear classiﬁcation models. The feature entries of our descriptor are obtained by evaluating a set of movement classiﬁers over the video. Each of these classiﬁers is an exemplar-SVM (Malisiewicz et al., 2011) trained on low-level features (Laptev, 2005; Wang et al., 2013) and optimized to separate a single pos- itive video exemplar from an army of “background” neg- ative videos. Because only one annotated video is needed to train an exemplar-SVM, our features can be learned with very little human supervision. The intuition behind our pro- posed descriptor is that it provides a semantically-rich de- scription of a video by measuring the presence/absence of movements similar to those in the exemplars. Thus, a linear classiﬁer trained on this representation will express a new action-class as a linear combination of the exemplar move- ments (which we abbreviate as EXMOVES). We demon- strate that these simple linear classiﬁcation models produce surprisingly good results on challenging action datasets. In  EXMOVES: Classiﬁer-based Features for Scalable Action Recognition  addition to yielding high-accuracy, these linear models are obviously very efﬁcient to train and test, thus enabling scal- able action recognition, i.e., efﬁcient recognition of many actions in large databases. Our approach can be viewed as extending to videos the idea of classiﬁer-based image descriptors (Wang et al., 2009; Torresani et al., 2010; Li et al., 2010; Deng et al., 2011) which describe a photo in terms of its relation to a set of predeﬁned object classes. To represent videos, instead of using object classes, we adopt a set of movement ex- emplars. In the domain of action recognition, our ap- proach is most closely related to the work of Sadanand and Corso (Sadanand & Corso, 2012), who have been the ﬁrst to describe videos in terms of a set of actions, which they call the Action Bank. The individual features in Action Bank are computed by convolving the video with a set of predeﬁned action templates. This representa- tion achieves high accuracy on several benchmarks. How- ever, the template-matching step to extract these mid-level features is very computationally expensive. As reported in (Sadanand & Corso, 2012), extracting mid-level features from a single video of UCF50 (Soomro et al.) takes a mini- mum of 0.4 hours up to a maximum of 34 hours. This com- putational bottleneck effectively limits the number of basis templates that can be used for the representation and con- strains the applicability of the approach to small datasets. Our ﬁrst contribution is to replace this prohibitively expen- sive procedure with a technique that is almost two orders of magnitude faster. This makes our descriptor applicable to action recognition in large video databases, where the Action Bank framework is simply too costly to be used. The second advantage of our approach is that our mid-level representation can be built on top of any arbitrary spatial- temporal low-level features, such as appearance-based de- scriptors computed at interest points or over temporal tra- jectories. This allows us to leverage the recent advances in design of low-level features: for example, we show that when we use dense trajectories (Wang et al., 2013) as low-level features, a simple linear classiﬁer trained on the HMDB51 dataset using our mid-level representation yields a 41.6% relative improvement in accuracy over the Action Bank built from the same set of video exemplars. Furthermore, we demonstrate that a linear classiﬁer applied to our mid-level representation produces consistently much higher accuracy than the same linear model directly trained on the low-level features used by our descriptor. Our EXMOVES are also related to Discriminative Patches (Jain et al., 2013), which are spatial-temporal vol- umes selected from a large collection of random video patches by optimizing a discriminative criterion. The se- lected patches are then used as a mid-level vocabulary for action recognition. Our approach differs from this prior  work in several ways. As discussed in 3.4, each EXMOVE feature can be computed from simple summations over in- dividual voxels. This model enables the use of Integral Videos (Ke et al., 2010), which reduce dramatically the time needed to extract our features. Discriminative Patches can- not take advantage of the Integral Video speedup and thus they are much more computationally expensive to compute. This prevents their application in large-scale scenarios. On the other hand, Discriminative Patches offer the advantage that they are automatically mined, without any human in- tervention. EXMOVES require some amount of human su- pervision, although minimal (just one hand-selected vol- ume per exemplar). In practice such annotations are inex- pensive to obtain. In our experiments we show that EX- MOVES learned from only 188 volumes greatly outper- form Discriminative Patches using 4000 volumes.  1.1. Related Work  Many approaches to human action recognition have been proposed over the last decade. Most of these techniques differ in terms of the representation used to describe the video. An important family of methods is the class of ac- tion recognition systems using space-time interest points, such as Haris3D (Laptev, 2005), Cuboids (Dollar et al., 2005), and SIFT3D (Scovanner et al., 2007). Efros et al. used optical ﬂows to represent and classify actions (Efros et al., 2003). Klaser et al. extended HOG (Dalal et al., 2006) to HOG3D by making use of the temporal dimension of videos (Klaser et al., 2008). Ke et al. learned volumetric features for action detection (Ke et al., 2010). Wang and Suter proposed the use of silhouettes to describe human ac- tivities (Wang & Suter, 2007). Recently, accurate action recognition has been demonstrated using dense trajectories and motion boundary descriptors (Wang et al., 2013). On all these representations, a variety of classiﬁcation mod- els have been applied to recognize human actions: bag- of-word model (Niebles & Fei-Fei, 2007), Metric Learn- ing (Tran & Sorokin, 2008), Deep Learning (Le et al., 2011), Boosting-based approaches (Laptev et al., 2008; Laptev & Prez, 2007). Although many of these approaches have been shown to yield good accuracy on standard human action bench- marks, they are difﬁcult to scale to recognition in large repositories as they involve complex feature representa- tions or learning models, which are too costly to compute on vast datasets.  2. Approach Overview We explain the approach at a high level using the schematic illustration in Figure 1. During an ofﬂine stage, our method learns Na exemplar-movement SVMs (EX-  EXMOVES: Classiﬁer-based Features for Scalable Action Recognition  Figure 1. Overview of our approach. During an ofﬂine stage, a collection of exemplar-movement SVMs (EXMOVES) is learned. Each EXMOVE is trained using a single positive video exemplar and a large number of negative sequences. These classiﬁers are then used as mid-level feature extractors to produce a semantically-rich representation of videos.  MOVES), shown on the left side of the ﬁgure. Each EX- MOVE is a binary classiﬁer optimized to recognize a spe- ciﬁc action exemplar (e.g., an instance of “biking”) and it uses histograms of quantized space-time low-level features for the classiﬁcation. Note that in order to capture different forms of each activity, we use multiple exemplars per ac- tivity (e.g., multiple instances of “biking”), each contribut- ing a separate EXMOVE. The set of learned EXMOVES are then used as mid-level feature extractors to produce an intermediate representation for any new input video: we evaluate each EXMOVE on subvolumes of the input video in order to compute the probability of the action at differ- ent space-time positions in the sequence. Speciﬁcally, we slide the subvolume of each EXMOVE exemplar at Ns dif- ferent scales over the input video. As discussed in sec- tion 3.4, this evaluation can be performed efﬁciently by us- ing Integral Videos (Ke et al., 2010). Finally, for each EX- MOVE, we perform max-pooling of the classiﬁer scores within Np spatial-temporal pyramid volumes. Thus, for any input video this procedure produces a feature vector with Na × Ns × Np dimensions. Because the EXMOVE features provide a semantically-rich representation of the video, even simple linear classiﬁcation models trained on our descriptor achieve good action categorization accuracy.  3. Exemplar-Movement SVMs (EXMOVES) Our EXMOVE classiﬁers are linear SVMs applied to his- tograms of quantized space-time low-level features calcu-  lated from subvolumes of the video. In section 3.1 we de- scribe the two space-time low-level descriptors used in our experiments, but any quantize-able appearance or motion features can be employed in our approach. In principle, to train each SVM classiﬁer we need a rea- sonable number of both positive and negative examples so as to produce good generalization. Unfortunately, we do not have many positive examples due to the high human cost of annotating videos. Thus, we resort to training each SVM using only one positive example, by extending to videos the exemplar-SVM model ﬁrst introduced by Mal- isiewicz et al. for the case of still images (Malisiewicz et al., 2011). Speciﬁcally, for each positive exemplar, we manually specify a space-time volume enclosing the ac- tion of interest and excluding the irrelevant portions of the video. The histogram of quantized low-level space-time features contained in this volume becomes the representa- tion used to describe the positive exemplar. Then, our ob- jective is to learn a linear SVM that separates the positive exemplar from the histograms computed from all possible subvolumes of the same size in negative videos. It may appear that training a movement classiﬁer from a single example will lead to severe overﬁtting. However, as already noted in (Malisiewicz et al., 2011), exemplar- SVMs actually have good generalization as their decision boundary is tightly constrained by the millions of negative examples that the classiﬁer must distinguish from the pos- itive one. In a sense, the classiﬁer is given access to an in-  Input VideoEXMOVE 1EXMOVE 2EXMOVE Na...Max-pooling onresponse volumes...EXMOVEfeatures...EXMOVES: Classiﬁer-based Features for Scalable Action Recognition  credible amount of training examples to learn what the pos- itive class is not. Furthermore, we use the exemplar-SVMs simply as mid-level feature extractors to ﬁnd movements similar to the positive exemplar. Thus, their individual cat- egorization accuracy is secondary. In other words, rather than applying the individual exemplar-SVMs as action rec- ognizers, we use them collectively as building blocks to deﬁne our action categorization model, in a role similar to the weak-learners of boosting techniques (Viola & Jones, 2001).  3.1. Low-level features used in EXMOVES  Although any arbitrary low-level description of space-time points or trajectories can be used in our framework, here we experiment with the two following representations: • HOG-HOF-STIPs. Given the input video, we ﬁrst ex- tract spatial-temporal interest points (STIPs) (Laptev, 2005). At each STIP we compute a Histogram of Oriented Gradients (HOG) and a Histogram of Flows (HOF) (Dalal et al., 2006) using the implementation in (Laptev et al., 2008). We concatenate the HOG and the HOF descriptor to form a 162-dimensional vector representing the STIP. Finally, we run k-means on these vectors to learn a codebook of D = 5, 000 cluster cen- troids. Given the codebook, any space-time volume in a video is represented in terms of the histogram of code- words occurring within that volume. We normalize the ﬁnal histogram using the L1 norm. • Dense Trajectories. These are the low-level motion and appearance descriptors obtained from dense trajectories according to the algorithm described in (Wang et al., 2013). The trajectories are computed for non-stationary points using a median-ﬁltered optical ﬂow method and are truncated every 15 frames. Each trajectory is then de- scribed in terms of its shape (point coordinate features, 30 dimensions), appearance (HOG features, 96 dimen- sions), optical ﬂow (HOF features, 108 dimensions) and boundary motion (MBHx and MBHy features, 96 di- mensions each). As in (Wang et al., 2013), we learn a separate dictionary for each of these 5 descriptors. We use a codebook of d = 5, 000 cluster centroids for each descriptor. Thus, each space-time volume in a video is then represented as a vector of D = 25, 000 dimensions obtained by concatenating the 5 histograms of trajecto- ries occurring within that volume. We L1-normalize the ﬁnal histogram.  3.2. Learning EXMOVES  The input for learning an EXMOVE consists of a positive video V + containing a manually-annotated space-time 3D box bounding the action of interest xE, and thousands of negative videos V− 1..N without action volume annotations.  The only requirement on the negative videos is that they must represent action classes different from the category of the positive exemplar (e.g., if the exemplar contains the ac- tion dancing, we exclude dancing videos from the negative set). But this constraint can be simply enforced given ac- tion class labels for the videos, without the need to know the space-time volumes of these negative actions. For ex- ample, tagged Internet videos (e.g., YouTube sequences) could be used as negative videos, by choosing action tags different from the activity of the positive exemplar. It is worth noting that different movement exemplars will have different 3D box shapes. For example, we expect a walking action to require a tall volume while swimming may have a volume more horizontally elongated. As fur- ther discussed below, we maintain the original shape-ratio of the exemplar volume in both training and testing. This means that we look for only tall volumes when detecting walking, and short-and-wide volumes when searching for the swimming action. Let xE be the manually-speciﬁed volume in the posi- tive sequence V +. Let us denote with φ(x) the L1- normalized histogram of codewords (computed from either HOG-HOF-STIPs or Dense Trajectories) within a video c(x) [c1(x), . . . , cD(x)]T , where volume x, i.e., φ(x) = 1 ci(x) is the number of codeword i occurring in volume x, and c(x) is the total number of codewords in x. Note that in the case of Dense Trajectories, each trajectory contributes 5 codewords into the histogram since it is quantized accord- ing to the 5 separate dictionaries. Adopting the exemplar-SVM method in (Malisiewicz et al., 2011), our exemplar-SVM training procedure learns a lin- ear classiﬁer f (x) = wT φ(x) + b, by minimizing the fol- lowing objective function:  (cid:88) (cid:88) N(cid:88)  i=1  x∈V−  i  x∈V + s.t.  |x∩xE| |xE| ≥0.5  + C2  h(cid:0)wT φ(x) + b(cid:1) h(cid:0)−wT φ(x) − b(cid:1)  (1)  (cid:107)w(cid:107)2 + C1  min w,b  where h(s) = max(0, 1−s) is the hinge loss, while C1 and C2 are pre-deﬁned parameters that we set so as to equalize the unbalanced proportion of positive and negative exam- ples. Note that the ﬁrst summation in the objective involves subvolumes whose spatial overlap with xE is greater than 50% and thus are expected to yield a positive score, while the second summation is over all negative subvolumes. Un- fortunately, direct minimization of the objective in Eq. 1 is not feasible since it requires optimizing the SVM parame- ters on a gigantic number of subvolumes. Thus, we resort to an alternation scheme similar to that used in (Malisiewicz et al., 2011) and (Felzenszwalb et al., 2010): we iterate be-  EXMOVES: Classiﬁer-based Features for Scalable Action Recognition  N} and a manually-  Algorithm 1 EXMOVE training Input: A set of negative videos {V−  S ← S ∪ {(xi,−1)} with xi randomly chosen from V− (w, b) ← svm training(S) Sold ← S for all x in V + s.t. wT x + b < 1 & |x∩xE|  1 , . . . ,V− selected volume xE in exemplar video V +. Output: Parameters (w, b) of exemplar-SVM. 1: S ← {(xE, +1)} 2: for i = 1 to N do 3: 4: for iter = 1 to M do 5: 6: 7: 8: 9: 10: 11: 12: 13:  S ← S ∪ {(x, +1)} //false negative for i = 1 to N do for all x in V−  i s.t. wT x + b > −1 do S ← S ∪ {(x,−1)} //false positive  |xE| > 0.5 do  if Sold = S then  break  i  tween 1) learning the parameters (w, b) given an active set S of negative volumes and 2) mining new negative volumes with the current SVM parameters. We ﬁrst initialize the parameters of the classiﬁer by tradi- tional SVM training using the manually-selected volume xE as positive example and a randomly selected subvol- umes from each of the other videos as negative example. At each iteration the current SVM is evaluated exhaustively on every negative video to ﬁnd violating subvolumes, i.e., subvolumes yielding an SVM score below exceeding −1. These subvolumes are added as negative examples to the active set S to be used in the successive iterations of SVM learning. Furthermore, our training procedure adds as posi- tive examples the subvolumes of V + that have spatial over- lap with xE greater than 50% and SVM score below 1. We stop the iterative alternation between these two steps when either no new subvolumes are added to the active set or a maximum number of iterations M is reached. In our imple- mentation we use M = 10, but we ﬁnd that in more than 85% of the cases, the learning procedure converges before reaching this maximum number of iterations. The pseudocode of our learning procedure is given in Algo- rithm 1. Lines 1 − 3 initialize the active set. The function svm training in line 5 learns a traditional binary linear SVM using the labelled examples in the active set. Note that we found that at each iteration we typically have mil- lions of subvolumes violating the constraints (lines 7-11). In order to maintain the learning of the SVM feasible, in practice we add to the active set only the volumes that yield the largest violations in each video, for a maximum of k− = 3 per negative video and k+ = 10 for the positive video.  3.3. Calibrating the ensemble of EXMOVES  The learning procedure described above is applied to each positive exemplar independently to produce a collection of EXMOVES. However, because the exemplar classiﬁers are trained disjointly, their score ranges and distributions may vary considerably. A standard solution to this problem is to calibrate the outputs by learning for each classiﬁer a func- tion that converts the raw SVM score into a proper pos- terior probability compatible across different classes. To achieve this goal we use the procedure proposed by Platt in (Platt, 1999): for each exemplar-SVM (wE, bE) we learn parameters (αE, βE) to produce calibrated probabili- ties through the sigmoid function g(x; wE, bE, αE, βE) = Ex + bE) + βE)]. The ﬁtting of param- 1/[1 + exp(αE(wT eters (αE, βE) is performed according to the iterative opti- mization described in (Platt, 1999) using as labeled exam- ples the positive/negative volumes that are in the active set at the completion of the EXMOVE training procedure. As already noted in (Malisiewicz et al., 2011), we also found that this calibration procedure yields a signiﬁcant improve- ment in accuracy since it makes the range of scores more homogeneous and diminishes the effect of outlier values.  3.4. Efﬁcient computation of EXMOVE scores  Although replacing the template matching procedure of Action Bank with linear SVMs applied to histograms of space-time features yields a good computational saving, this by itself is still not fast enough to be used in large-scale datasets due to the exhaustive sliding volume scheme. In fact, the sliding volume scheme is used in both training and testing. In training, we need to slide the current SVM over negative videos to ﬁnd volumes violating the classiﬁcation constraint. In testing, we need to slide the entire set of EX- MOVE classiﬁers over the input video in order to extract the mid-level features for the subsequent recognition. Be- low, we describe a solution to speed up the sliding volume evaluation of the SVMs. Let V be an input video of size R × C × T . Given an EX- MOVE with parameters (wE, bE), we need to efﬁciently evaluate it over all subvolumes of V having size equal to the positive exemplar subvolume xE (in practice, we slide the subvolume at Ns different scales but for simplicity we illus- trate the procedure assuming we use the original scale). It is worth noting that the branch-and-bound method of Lampert et al. (Lampert et al., 2009) cannot be applied to our prob- lem because it can only ﬁnd the subwindow maximizing the classiﬁcation score while we need the scores of all sub- volumes; moreover it requires unnormalized histograms. Instead, we use integral videos (Ke et al., 2010) to ef- ﬁciently compute the EXMOVE score for each subvol- ume. An integral video is a volumetric data-structure hav- ing size equal to the input sequence (in this case R ×  EXMOVES: Classiﬁer-based Features for Scalable Action Recognition  c(cid:48)≤c  r(cid:48)≤r  (cid:80)  (cid:80)  B(r, c, t) = (cid:80)  C × T ). It is useful to speed up the computation of func- tions deﬁned over subvolumes and expressed as cumula- tive sums over voxels, i.e, functions of the form H(x) = (r,c,t)∈x h(r, c, t), where (r, c, t) denotes a space-time point in volume x and h is a function over individual space- time voxels. The integral video for h at point (r, c, t) is (cid:80) simply an accumulation buffer B storing the sum of h over all voxels at locations less than or equal to (r, c, t), i.e., t(cid:48)≤t h(r(cid:48), c(cid:48), t(cid:48)). This buffer can be built with complexity linear in the video size. Once built, it can be used to compute H(x) for any subvolume x via a handful of additions/subtractions of the values in B. In our case, the use of integral video is enabled by the fact that the classiﬁer score can be expressed in terms of cumu- lative sums of individual point contributions, as we illus- trate next. For simplicity we describe the procedure assum- ing that φ(x) consists of a single histogram (as is the case for HOG-HOF-STIPs) but the method is straightforward to adapt for the scenario where φ(x) is the concatenation of multiple histograms (e.g., the 5 histograms of Dense Tra- jectories). Let us indicate with P (x) the set of quantized low-level features (either STIPs or Dense Trajectories) in- cluded in subvolume x of video V and let ip be the code- word index of a point p ∈ P (x). Then we can rewrite the classiﬁcation score of exemplar-SVM (w, b) on a sub- volume x as follows (we omit the constant bias term b for brevity):  wT φ(x) =  1  c(x)  wici(x) =  p∈P (x) wip p∈P (x) 1  .  (2)  Equation 2 shows that the classiﬁer score is expressed as a ratio where both the numerator and the denominator are computed as sums over individual voxels. Thus, the clas- siﬁer score for any x can be efﬁciently calculated using two integral videos (one for the numerator, one for the denominator), without ever explicitly computing the his- togram φ(x) or the inner product between w and φ(x). In the case where φ(x) contains the concatenation of multi- ple histograms, then we would need an integral video for each of the histograms (thus 5 for Dense Trajectories), in addition to the common integral video for the denominator.  4. Experiments 4.1. Experimental setup Implementation details of EXMOVE training. Since our approach shares many similarities with Action Bank, we adopt training and design settings similar to those used in (Sadanand & Corso, 2012) so as to facilitate the com- parison between these two methods. Speciﬁcally, our EX- MOVES are learned from the same set of UCF50 (Soomro et al.) videos used to build the Action Bank templates. This set consists of 188 sequences spanning a total of 50 ac-  (cid:80) (cid:80)  D(cid:88)  i=1  tions. Since the Action Bank volume annotations are not publicly available, we manually selected the action vol- ume xE on each of these exemplar sequences to obtain Na = 188 exemplars. As negative set of videos we use the remaining 6492 sequences in the UCF50 dataset: for these videos no manual labeling of the action volume is available nor it is needed by our method. Action Bank also includes 6 templates taken from other sources but these videos have not been made publicly available; it also uses 10 templates taken from the KTH dataset. However, as the KTH videos are lower-resolution and contain much sim- pler actions compared to those in UCF50, we have not used them to build our EXMOVES. In the experiments we show that, while our descriptor is deﬁned by a smaller number of movement classiﬁers (188 instead of 205), the recognition performance obtained with our mid-level features is con- sistently on par with or better than Action Bank. Parameters of EXMOVE features. In order to compute the EXMOVE features from a new video, we perform max- pooling of the EXMOVE scores using a space-time pyra- mid based on the same settings as those of Action Bank, i.e., Ns = 3 scaled versions of the exemplar volume xE (the scales are 1, 0.75, 0.5), and Np = 73 space-time volumes obtained by recursive octree subdivision of the entire video using 3 levels (this yields 1 volume at level 1, 8 subvolumes at level 2, 64 subvolumes at level 3). Thus, the ﬁnal dimensionality of our EXMOVE descriptor is Na × Ns × Np = 41, 172. Action classiﬁcation model. All our action recognition experiments are performed by training a one-vs-the-rest linear SVM on the EXMOVES extracted from a set of training videos. We opted for this classiﬁer as it is very efﬁcient to train and test, and thus it is a suitable choice for the scenario of large-scale action recognition that we are interested in addressing. The hyperparameter C of the SVM is tuned via cross-validation for all baselines, Action Bank, and our EXMOVES. Test datasets. We test our approach on the following large- scale action recognition datasets:  1. HMDB51 (Kuehne et al., 2011): It consists of 6849 image sequences collected from movies as well as YouTube and Google videos. They represent 51 action categories. The results for this dataset are presented us- ing 3-fold cross validation on the 3 publicly available training/testing splits.  2. Hollywood-2 (Marszalek et al., 2009): This dataset in- cludes over 20 hours of video, subdivided in 3669 se- quences, spanning 12 action classes. We use the pub- licly available split of training and testing examples.  3. UCF50: This dataset contains 6676 videos taken from YouTube for a total of 50 action categories. This dataset was used in (Sadanand & Corso, 2012) and (Jain et al.,  EXMOVES: Classiﬁer-based Features for Scalable Action Recognition  Low-level features  3D Gaussians  HOG3D  HOG-HOF-STIPs  Dense Trajectories  Mid-level descriptor Action Bank  Discriminative Patches  BOW  EXMOVES  BOW  EXMOVES  Descriptor  dimensionality HMDB51 Hollywood-2 UCF50 UCF101(part 2)  Datasets  44,895 9,360 5,000 41,172 25,000 41,172  26.9 n/a 20.0 27.7 34.4 41.9  n/a n/a 32.6 44.7 43.7 56.6  57.9 61.2 52.8 63.4 81.8 82.8  n/a n/a 49.1 57.2 60.9 71.6  Table 1. Comparison of recognition accuracies on four datasets. The classiﬁcation model is an efﬁcient linear SVM applied to 4 distinct global mid-level descriptors: Action Bank (Sadanand & Corso, 2012), Discriminative Patches (Jain et al., 2013), Histogram of Space- Time Visual Words (BOW) and our EXMOVES. We consider two different low-level features to build BOW and EXMOVES: HOG- HOF-STIPs and Dense Trajectories. Our EXMOVES achieve the best recognition accuracy on all four datasets using Dense Trajectories, and greatly outperform the BOW descriptor for both our choices of low-level features, HOG-HOF-STIPs and Dense Trajectories.  2013) to train and evaluate Action Bank and Discrimi- native Patches.  4. UCF101 (Soomro et al.) (part 2): UCF101 is a superset of UCF50. For this test we only use videos from ac- tion classes 51 to 101 (from now on denoted as part 2), thus omitting the above-mentioned classes and videos of UCF50. This leaves a total of 6851 videos and 51 action classes. We report the accuracy of 25-fold cross valida- tion using the publicly available training/testing splits.  4.2. Action recognition Comparison of recognition accuracies. We now present the classiﬁcation performance obtained with our features on the four benchmarks described above. We consider in our comparison three other mid-level video descriptors that can be used for action recognition with linear SVMs: Action Bank (Sadanand & Corso, 2012), Discriminative Patches (Jain et al., 2013) as well as histograms of vi- sual words (BOW) built for the two types of low-level fea- tures that we use in EXMOVES, i.e., HOG-HOF-STIPs and Dense Trajectories. As in (Wang et al., 2013), we use a dic- tionary of 25,000 visual words for Dense Trajectories and 5,000 visual words for HOG-HOF-STIPs. Due to the high computational complexity of the extraction of Action Bank features, however, we were unable to test this descriptor on the large-scale datasets of Hollywood-2 and UCF101. For Discriminative Patches, we can only report accuracy on UCF50 since this is the only large-scale dataset on which they were tested in (Jain et al., 2013) and no software to compute these features is available. The accuracies achieved by the different descriptors are summarized in Table 1. From these results we see that our EXMOVE descriptor built from Dense Trajectories yields consistently the best results across all four datasets. Fur- thermore, EXMOVES gives always higher accuracy than BOW built from the same low-level features, for both HOG-HOF-STIPs and Dense Trajectories. The gap is par- ticularly large on challenging datasets such as Hollywood-  2 and HMDB51. This underscores the advantageous effect of the movement exemplars to which we compare the input video in order to produce the EXMOVE features. Table 2 lists the individual action recognition accuracies for the same subset of 13 classes analyzed in (Jain et al., 2013). We see that EXMOVES give the highest accuracy on 10 out of these 13 action categories. Computational cost of mid-level feature extraction. We want to emphasize that although our EXMOVES are based on a subset of the exemplars used to build Action Bank, they always generate equal or higher accuracy. Further- more, our approach does so with a speedup of almost two- orders of magnitude in feature extraction: Table 3 reports the statistics of the runtime needed to extract EXMOVES and Action Bank. We used the software provided by the authors of (Sadanand & Corso, 2012) to extract Action Bank features from input videos. Due to large cost of Ac- tion Bank extraction, we collected our runtime statistics on the smaller-scale UT-I (Ryoo & Aggarwal, 2010) dataset, involving only 120 videos. Runtimes were measured on a single-core Linux machine with a CPU @ 2.66GHz. The table reports the complete time from the input of the video to the output of the descriptor, inclusive of the time needed to compute low-level features. The extraction of EXMOVES is on average over 70 times faster than for Action Bank when using HOG-HOF-STIPs and 11 times faster when using Dense Trajectories. We can process the entire UT-Interaction dataset with HOG-HOF-STIPs using a single CPU in 14 hours; extracting the Action Bank fea- tures on the same dataset would take 41 days. We were unable to collect runtime statistics for Discrimina- tive Patches due to the unavailability of the software. How- ever, we want to point out that this descriptor uses many more patches than EXMOVES (1040 instead of 188) and it cannot use the Integral Video speedup. Computational cost of action recognition. Finally, we would like to point out that as shown in Table 1, the  EXMOVES: Classiﬁer-based Features for Scalable Action Recognition  Action Class Basketball Clean and Jerk Diving Golf Swing High Jump Javeline Throw Mixing PoleVault Pull Up Push Ups Tennis Swing Throw Discus Volleyball Spiking Mean Classiﬁcation  Action Bank 53.84 85.00 78.79 90.32 38.46 45.83 42.85 60.60 91.67 85.00 44.12 75.00 43.48 64.23  Discriminative  Patches 50.00 95.65 61.29 75.86 55.56 50.00 55.56 84.37 75.00 86.36 48.48 87.10 90.90 70.47  EXMOVES  56.93 91.07 96.08 90.14 81.30 73.50 97.16 94.38 96.00 91.18 85.03 93.13 89.66 87.35  Table 2. Recognition accuracies of our EXMOVES (applied to Dense Trajectories) compared with those of Action Bank and Dis- criminative Patches using the same subset of 13 action classes from UCF50 considered in (Jain et al., 2013).  Figure 2. Accuracy on HMDB51 as a function of the number of EXMOVES. We use Recursive Feature Elimination to reduce the number of EXMOVES. The accuracy remains near the state-of- the-art even when using only 100 exemplars.  Descriptor  Action Bank EXMOVES  w/ HOG-HOF-STIPs  EXMOVES  w/ Dense Trajectory  Extraction time  per video (minutes) mean max min 495 132  1199  # frames per second  mean 0.012  7  43  16  70  3  29  0.82  0.13  Table 3. Statistics of time needed to extract the mid-level descrip- tors Action Bank and EXMOVES. The time needed to extract EX- MOVES features for the entire UT-I dataset using a single CPU is only 14 hours; instead, it would take more than 41 days to com- pute Action Bank descriptors for this dataset.  accuracies achieved by an efﬁcient linear SVM trained on EXMOVES are very close to the best published re- sults of (Wang et al., 2013), which instead were obtained with a much more computationally expensive model, not suitable for scalable action recognition: they report a top-performance of 46.6% and 58.2% on HMDB51 and Hollywood-2, respectively, using an expensive non-linear SVM with an RBF-χ2 kernel applied to BOW of Dense Trajectories. In our experiments we found that training a linear SVM on EXMOVES for one of the HMDB51 classes takes only 6.2 seconds but learning a kernel-SVM on BOW of Dense Trajectories requires 25 minutes (thus overhead is 250X); the testing of our linear SVM on a video takes only 7 milliseconds, while the nonlinear SVM is on average more than two orders of magnitude slower. Its cost depends on the on the number of support vectors, which varies from a few hundreds to several thousands. Nonlinear SVMs also need more memory to store the support vectors. Varying the number of exemplars. In this experiment we study how the accuracy of our method changes as a  function of the number of EXMOVES used in the de- scriptor. Starting from our complete feature vector de- ﬁned by Na = 188 exemplars and having dimensionality Na×Ns×Np = 41, 172, we recursively apply a feature se- lection procedure that eliminates at each iteration one of the EXMOVE exemplars and removes its associated Ns × Np features from the descriptor. We apply a variant of multi- class Recursive Feature Elimination (Chapelle & Keerthi, 2008) to determine the EXMOVE to eliminate at each iter- ation. This procedure operates as follows: given a labeled training set of video examples for K classes, at each iter- ation we retrain the one-vs-the-rest linear SVMs for all K classes using the current version of our feature vector and then we remove from the descriptor the EXMOVE that is overall “least used” by the K linear classiﬁers by looking at the average magnitude of the SVM parameter vector w for the different EXMOVE sub-blocks. We perform this analysis on the HDMB51 dataset using both HOG-HOF-STIPs and Dense Trajectories as low-level features for EXMOVES. Figure 2 reports the 3-fold cross- validation error as a function of the number of EXMOVES used in our descriptor. Interestingly, we see that the accu- racy remains close to the top-performance even when we reduce the number of exemplars to only 100. This sug- gests a certain redundancy in the set of movement exem- plars. The accuracy begins to drop much more rapidly when fewer than 50 exemplars are used.  5. Conclusions We have presented an approach for efﬁcient large-scale hu- man action recognition. It centers around the learning of a mid-level video representation that enables state-of-the-art accuracy with efﬁcient linear classiﬁcation models. Exper-  050100150200051015202530354045# EXMOVESaccuracy (%)  EXMOVES w/ HOG−HOF−STIPsEXMOVES w/ Dense TrajectoriesEXMOVES: Classiﬁer-based Features for Scalable Action Recognition  iments on large-scale action recognition benchmarks show the accuracy and efﬁciency of our approach. Our mid-level features are produced by evaluating a prede- ﬁned set of movement classiﬁers over the input video. An important question we plan to address in future work is: how many mid-level classiﬁers do we need to train before accuracy levels off? Also, what kind of movement classes are particularly useful as mid-level features? Currently, we are restricted in the ability to answer these questions by the scarceness of labeled data available, in terms of both num- ber of video examples but also number of action classes. An exciting avenue to resolve these issues is the design of methods that can learn robust mid-level classiﬁers from weakly-labeled data, such as YouTube videos. Additional material including software to extract EX- MOVES from videos is available at http://vlg.cs. dartmouth.edu/exmoves.  Acknowledgments Thanks to Alessandro Bergamo for assistance with the ex- periments. This research was funded in part by NSF CA- REER award IIS-0952943 and NSF award CNS-1205521.  References Blank, M., Gorelick, L., Shechtman, E., Irani, M., and Basri, R.  Actions as space-time shapes. ICCV, 2005.  Chapelle, O. and Keerthi, S. Multi-class feature selection with  support vector machines. Proc. Am. Stat. Ass., 2008.  Dalal, N., Triggs, B., and Schmid, C. Human detection using  oriented histograms of ﬂow and appearance. ECCV, 2006.  Deng, J., Berg, A., and Fei-Fei, L. Hierarchical semantic indexing  for large scale image retrieval. In CVPR, 2011.  Dollar, P., Rabaud, V., Cottrell, G., and Belongie, S. Behav- ior recognition via sparse spatio-temporal features. ICCV VS- PETS, pp. 65–72, 2005.  Efros, A., Berg, A., Mori, G., and Malik, J. Recognizing action at  a distance. ICCV, pp. 726–733, 2003.  Felzenszwalb, P., Girshick, R., McAllester, D., and Ramanan, D. Object detection with discriminatively trained part-based mod- els. IEEE TPAMI, 32(9):1627–1645, 2010.  Jain, A., Gupta, A., Rodriguez, M., and Davis, L. Representing videos using mid-level discriminative patches. In CVPR, pp. 2571–2578, 2013.  Ke, Y., Sukthankar, R., and Hebert, M. Volumetric features for  video event detection. IJCV, 2010.  Klaser, A., Marszalek, M., and Schmid, C. A spatio-temporal  descriptor based on 3D-gradients. BMVC, 2008.  Lampert, C., Blaschko, M., and Hofmann, T. Efﬁcient subwindow search: A branch and bound framework for object localization. IEEE TPAMI, 2009.  Laptev, I. On space-time interest points. International Journal of  Computer Vision, 64(2-3):107–123, 2005.  Laptev, I. and Prez, P. Retrieving actions in movies. ICCV, 2007.  Laptev, I., M. Marszalek, C. Schmid, and Rozenfeld, B. Learning  realistic human actions from movies. CVPR, 2008.  Le, Q., Zou, W., Yeung, S., and Ng, A. Learning hierarchical spatio-temporal features for action recognition with indepen- dent subspace analysis. CVPR, 2011.  Li, L., Su, H., Xing, E., and Fei-Fei, L. Object Bank: A high- level image representation for scene classiﬁcation & semantic feature sparsiﬁcation. In NIPS, 2010.  Malisiewicz, T., Gupta, A., and Efros, A. Ensemble of exemplar-  SVMs for object detection and beyond. ICCV, 2011.  Marszalek, M., Laptev, I., and Schmid, C. Action in context.  CVPR, 2009.  Niebles, J. and Fei-Fei, L. A hierarchical model of shape and  appearance for human action classiﬁcation. CVPR, 2007.  Platt, J. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In Advances in Large Margin Classiﬁers. MIT Press, 1999.  Ryoo, M. and Aggarwal, J. UT-Interaction Dataset, ICPR contest  on Semantic Description of Human Activities, 2010.  Sadanand, S. and Corso, J. Action bank: A high-level representa-  tion of activity in video. CVPR, 2012.  Scovanner, P., Ali, S., and Shah, M. A 3-Dimensional SIFT de- scriptor and its application to action recognition. ACM Multi- media, pp. 357–360, 2007.  Soomro, K., Roshan Zamir, A., and Shah, M. UCF101: A dataset of 101 human action classes from videos in the wild. Technical Report CRCV-TR-12-01, University of Central Florida.  Torresani, L., Szummer, M., and Fitzgibbon, A. Efﬁcient object  category recognition using classemes. In ECCV, 2010.  Tran, D. and Sorokin, A. Human activity recognition with metric  learning. ECCV, 2008.  Veeraraghavan, A., Chellappa, R, and Roy-Chowdhury, A. The  function space of an activity. CVPR, 2006.  Viola, P. and Jones, M. Rapid object detection using a boosted  cascade of simple features. In CVPR, 2001.  Wang, G., Hoiem, D., and Forsyth, D. Learning image similarity In  from ﬂickr using stochastic intersection kernel machines. ICCV, 2009.  Wang, H., Kl¨aser, A., Schmid, C., and Liu, C. Dense trajectories and motion boundary descriptors for action recognition. Inter- national Journal of Computer Vision, March 2013.  Kuehne, H., Jhuang, H., Garrote, E., Poggio, T., , and Serre, T. Hmdb: A large video database for human motion recognition. ICCV, 2011.  Wang, L. and Suter, D. Recognizing human activities from silhou- ettes: Motion subspace and factorial discriminative graphical model. CVPR, 2007.  ","This paper introduces EXMOVES, learned exemplar-based features for efficientrecognition of actions in videos. The entries in our descriptor are produced byevaluating a set of movement classifiers over spatial-temporal volumes of theinput sequence. Each movement classifier is a simple exemplar-SVM trained onlow-level features, i.e., an SVM learned using a single annotated positivespace-time volume and a large number of unannotated videos.Our representation offers two main advantages. First, since our mid-levelfeatures are learned from individual video exemplars, they require minimalamount of supervision. Second, we show that simple linear classification modelstrained on our global video descriptor yield action recognition accuracyapproaching the state-of-the-art but at orders of magnitude lower cost, sinceat test-time no sliding window is necessary and linear models are efficient totrain and test. This enables scalable action recognition, i.e., efficientclassification of a large number of different actions even in large videodatabases. We show the generality of our approach by building our mid-leveldescriptors from two different low-level feature representations. The accuracyand efficiency of the approach are demonstrated on several large-scale actionrecognition benchmarks."
1312.6026,2014,How to Construct Deep Recurrent Neural Networks  ,"['Razvan Pascanu', 'Caglar Gulcehre', 'KyungHyun Cho', 'Yoshua Bengio']",https://arxiv.org/pdf/1312.6026.pdf,"4 1 0 2    r p A 4 2         ] E N . s c [      5 v 6 2 0 6  .  2 1 3 1 : v i X r a  How to Construct Deep Recurrent Neural Networks  Razvan Pascanu1, Caglar Gulcehre1, Kyunghyun Cho2, and Yoshua Bengio1  1D´epartement d’Informatique et de Recherche Op´erationelle, Universit´e de Montr´eal,  {pascanur, gulcehrc}@iro.umontreal.ca, yoshua.bengio@umontreal.ca  2Department of Information and Computer Science, Aalto University School of Science,  kyunghyun.cho@aalto.fi  Abstract  In this paper, we explore different ways to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, however, we ﬁnd three points of an RNN which may be made deeper; (1) input-to-hidden function, (2) hidden-to- hidden transition and (3) hidden-to-output function. Based on this observation, we propose two novel architectures of a deep RNN which are orthogonal to an earlier attempt of stacking multiple recurrent layers to build a deep RNN (Schmidhu- ber, 1992; El Hihi and Bengio, 1996). We provide an alternative interpretation of these deep RNNs using a novel framework based on neural operators. The proposed deep RNNs are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNNs beneﬁt from the depth and outperform the conven- tional, shallow RNNs.  1  Introduction  Recurrent neural networks (RNN, see, e.g., Rumelhart et al., 1986) have recently become a popular choice for modeling variable-length sequences. RNNs have been successfully used for various task such as language modeling (see, e.g., Graves, 2013; Pascanu et al., 2013a; Mikolov, 2012; Sutskever et al., 2011), learning word embeddings (see, e.g., Mikolov et al., 2013a), online handwritten recog- nition (Graves et al., 2009) and speech recognition (Graves et al., 2013). In this work, we explore deep extensions of the basic RNN. Depth for feedforward models can lead to more expressive models (Pascanu et al., 2013b), and we believe the same should hold for recurrent models. We claim that, unlike in the case of feedforward neural networks, the depth of an RNN is ambiguous. In one sense, if we consider the existence of a composition of several nonlinear computational layers in a neural network being deep, RNNs are already deep, since any RNN can be expressed as a composition of multiple nonlinear layers when unfolded in time. Schmidhuber (1992); El Hihi and Bengio (1996) earlier proposed another way of building a deep RNN by stacking multiple recurrent hidden states on top of each other. This approach poten- tially allows the hidden state at each level to operate at different timescale (see, e.g., Hermans and Schrauwen, 2013). Nonetheless, we notice that there are some other aspects of the model that may still be considered shallow. For instance, the transition between two consecutive hidden states at a single level is shallow, when viewed separately.This has implications on what kind of transitions this model can represent as discussed in Section 3.2.3. Based on this observation, in this paper, we investigate possible approaches to extending an RNN into a deep RNN. We begin by studying which parts of an RNN may be considered shallow. Then,  1  (cid:110)(cid:16) Tn(cid:88) N(cid:88)  n=1  t=1  for each shallow part, we propose an alternative deeper design, which leads to a number of deeper variants of an RNN. The proposed deeper variants are then empirically evaluated on two sequence modeling tasks. The layout of the paper is as follows. In Section 2 we brieﬂy introduce the concept of an RNN. In Section 3 we explore different concepts of depth in RNNs. In particular, in Section 3.3.1–3.3.2 we propose two novel variants of deep RNNs and evaluate them empirically in Section 5 on two tasks: polyphonic music prediction (Boulanger-Lewandowski et al., 2012) and language modeling. Finally we discuss the shortcomings and advantages of the proposed models in Section 6.  2 Recurrent Neural Networks  A recurrent neural network (RNN) is a neural network that simulates a discrete-time dynamical system that has an input xt, an output yt and a hidden state ht. In our notation the subscript t represents time. The dynamical system is deﬁned by  (1) (2) where fh and fo are a state transition function and an output function, respectively. Each function is parameterized by a set of parameters; θh and θo.  ht = fh(xt, ht−1) yt = fo(ht),  Given a set of N training sequences D = of an RNN can be estimated by minimizing the following cost function:  1 ), . . . , (x(n) Tn  1 , y(n)  (x(n)  , y(n) Tn  )  , the parameters  n=1  (cid:17)(cid:111)N  J(θ) =  1 N  d(y(n)  t  , fo(h(n)  t  )),  (3)  where h(n) and b, such as Euclidean distance or cross-entropy.  t = fh(x(n)  t−1) and h(n)  , h(n)  t  0 = 0. d(a, b) is a predeﬁned divergence measure between a  2.1 Conventional Recurrent Neural Networks  (cid:0)W(cid:62)ht−1 + U(cid:62)xt (cid:1) , (cid:0)V(cid:62)ht  (cid:1)  A conventional RNN is constructed by deﬁning the transition function and the output function as  ht = fh(xt, ht−1) = φh yt = fo(ht, xt) = φo  (4) (5) where W, U and V are respectively the transition, input and output matrices, and φh and φo are element-wise nonlinear functions. It is usual to use a saturating nonlinear function such as a logistic sigmoid function or a hyperbolic tangent function for φh. An illustration of this RNN is in Fig. 2 (a). The parameters of the conventional RNN can be estimated by, for instance, stochastic gradient de- scent (SGD) algorithm with the gradient of the cost function in Eq. (3) computed by backpropagation through time (Rumelhart et al., 1986).  3 Deep Recurrent Neural Networks  3.1 Why Deep Recurrent Neural Networks?  Deep learning is built around a hypothesis that a deep, hierarchical model can be exponentially more efﬁcient at representing some functions than a shallow one (Bengio, 2009). A number of recent theoretical results support this hypothesis (see, e.g., Le Roux and Bengio, 2010; Delalleau and Bengio, 2011; Pascanu et al., 2013b). For instance, it has been shown by Delalleau and Bengio (2011) that a deep sum-product network may require exponentially less units to represent the same function compared to a shallow sum-product network. Furthermore, there is a wealth of empirical evidences supporting this hypothesis (see, e.g., Goodfellow et al., 2013; Hinton et al., 2012b,a). These ﬁndings make us suspect that the same argument should apply to recurrent neural networks.  2  3.2 Depth of a Recurrent Neural Network  Figure 1: A conventional recurrent neural network unfolded in time.  The depth is deﬁned in the case of feedforward neural networks as having multiple nonlinear layers between input and output. Unfortunately this deﬁnition does not apply trivially to a recurrent neural network (RNN) because of its temporal structure. For instance, any RNN when unfolded in time as in Fig. 1 is deep, because a computational path between the input at time k < t to the output at time t crosses several nonlinear layers. A close analysis of the computation carried out by an RNN (see Fig. 2 (a)) at each time step individ- ually, however, shows that certain transitions are not deep, but are only results of a linear projection followed by an element-wise nonlinearity. It is clear that the hidden-to-hidden (ht−1 → ht), hidden- to-output (ht → yt) and input-to-hidden (xt → ht) functions are all shallow in the sense that there exists no intermediate, nonlinear hidden layer. We can now consider different types of depth of an RNN by considering those transitions separately. We may make the hidden-to-hidden transition deeper by having one or more intermediate nonlinear layers between two consecutive hidden states (ht−1 and ht). At the same time, the hidden-to- output function can be made deeper, as described previously, by plugging, multiple intermediate nonlinear layers between the hidden state ht and the output yt. Each of these choices has a different implication.  3.2.1 Deep Input-to-Hidden Function  A model can exploit more non-temporal structure from the input by making the input-to-hidden function deep. Previous work has shown that higher-level representations of deep networks tend to better disentangle the underlying factors of variation than the original input (Goodfellow et al., 2009; Glorot et al., 2011b) and ﬂatten the manifolds near which the data concentrate (Bengio et al., 2013). We hypothesize that such higher-level representations should make it easier to learn the temporal structure between successive time steps because the relationship between abstract features can gen- erally be expressed more easily. This has been, for instance, illustrated by the recent work (Mikolov et al., 2013b) showing that word embeddings from neural language models tend to be related to their temporal neighbors by simple algebraic relationships, with the same type of relationship (adding a vector) holding over very different regions of the space, allowing a form of analogical reasoning. This approach of making the input-to-hidden function deeper is in the line with the standard practice of replacing input with extracted features in order to improve the performance of a machine learning model (see, e.g., Bengio, 2009). Recently, Chen and Deng (2013) reported that a better speech recognition performance could be achieved by employing this strategy, although they did not jointly train the deep input-to-hidden function together with other parameters of an RNN.  3.2.2 Deep Hidden-to-Output Function  A deep hidden-to-output function can be useful to disentangle the factors of variations in the hidden state, making it easier to predict the output. This allows the hidden state of the model to be more compact and may result in the model being able to summarize the history of previous inputs more efﬁciently. Let us denote an RNN with this deep hidden-to-output function a deep output RNN (DO-RNN). Instead of having feedforward, intermediate layers between the hidden state and the output, Boulanger-Lewandowski et al. (2012) proposed to replace the output layer with a conditional gen-  3  yt-1xt-1ht-1xthtytxt+1ht+1yt+1(a) RNN  (b) DT-RNN  (b*) DT(S)-RNN  (c) DOT-RNN  (d) Stacked RNN  Figure 2: Illustrations of four different recurrent neural networks (RNN). (a) A conventional RNN. (b) Deep Transition (DT) RNN. (b*) DT-RNN with shortcut connections (c) Deep Transition, Deep Output (DOT) RNN. (d) Stacked RNN  erative model such as restricted Boltzmann machines or neural autoregressive distribution estima- tor (Larochelle and Murray, 2011). In this paper we only consider feedforward intermediate layers.  3.2.3 Deep Hidden-to-Hidden Transition  The third knob we can play with is the depth of the hidden-to-hidden transition. The state transition between the consecutive hidden states effectively adds a new input to the summary of the previous inputs represented by the ﬁxed-length hidden state. Previous work with RNNs has generally limited the architecture to a shallow operation; afﬁne transformation followed by an element-wise nonlin- earity. Instead, we argue that this procedure of constructing a new summary, or a hidden state, from the combination of the previous one and the new input should be highly nonlinear. This nonlinear transition could allow, for instance, the hidden state of an RNN to rapidly adapt to quickly changing modes of the input, while still preserving a useful summary of the past. This may be impossible to be modeled by a function from the family of generalized linear models. However, this highly non- linear transition can be modeled by an MLP with one or more hidden layers which has an universal approximator property (see, e.g., Hornik et al., 1989). An RNN with this deep transition will be called a deep transition RNN (DT-RNN) throughout re- mainder of this paper. This model is shown in Fig. 2 (b). This approach of having a deep transition, however, introduces a potential problem. As the in- troduction of deep transition increases the number of nonlinear steps the gradient has to traverse when propagated back in time, it might become more difﬁcult to train the model to capture long- term dependencies (Bengio et al., 1994). One possible way to address this difﬁculty is to introduce shortcut connections (see, e.g., Raiko et al., 2012) in the deep transition, where the added shortcut connections provide shorter paths, skipping the intermediate layers, through which the gradient is propagated back in time. We refer to an RNN having deep transition with shortcut connections by DT(S)-RNN (See Fig. 2 (b*)). Furthermore, we will call an RNN having both a deep hidden-to-output function and a deep transi- tion a deep output, deep transition RNN (DOT-RNN). See Fig. 2 (c) for the illustration of DOT-RNN. If we consider shortcut connections as well in the hidden to hidden transition, we call the resulting model DOT(S)-RNN. An approach similar to the deep hidden-to-hidden transition has been proposed recently by Pinheiro and Collobert (2014) in the context of parsing a static scene. They introduced a recurrent convolu- tional neural network (RCNN) which can be understood as a recurrent network whose the transition between consecutive hidden states (and input to hidden state) is modeled by a convolutional neural network. The RCNN was shown to speed up scene parsing and obtained the state-of-the-art result in Stanford Background and SIFT Flow datasets. Ko and Dieter (2009) proposed deep transitions for Gaussian Process models. Earlier, Valpola and Karhunen (2002) used a deep neural network to model the state transition in a nonlinear, dynamical state-space model.  3.2.4 Stack of Hidden States  An RNN may be extended deeper in yet another way by stacking multiple recurrent hidden layers on top of each other (Schmidhuber, 1992; El Hihi and Bengio, 1996; Jaeger, 2007; Graves, 2013).  4  xtht-1htytxtht-1htytxtht-1htytxtht-1htytxtht-1htytzt-1ztWe call this model a stacked RNN (sRNN) to distinguish it from the other proposed variants. The goal of a such model is to encourage each recurrent level to operate at a different timescale. It should be noticed that the DT-RNN and the sRNN extend the conventional, shallow RNN in different aspects. If we look at each recurrent level of the sRNN separately, it is easy to see that the transition between the consecutive hidden states is still shallow. As we have argued above, this limits the family of functions it can represent. For example, if the structure of the data is sufﬁciently complex, incorporating a new input frame into the summary of what had been seen up to now might be an arbitrarily complex function. In such a case we would like to model this function by something that has universal approximator properties, as an MLP. The model can not rely on the higher layers to do so, because the higher layers do not feed back into the lower layer. On the other hand, the sRNN can deal with multiple time scales in the input sequence, which is not an obvious feature of the DT-RNN. The DT-RNN and the sRNN are, however, orthogonal in the sense that it is possible to have both features of the DT-RNN and the sRNN by stacking multiple levels of DT-RNNs to build a stacked DT-RNN which we do not explore more in this paper.  3.3 Formal descriptions of deep RNNs  Here we give a more formal description on how the deep transition recurrent neural network (DT- RNN) and the deep output RNN (DO-RNN) as well as the stacked RNN are implemented.  3.3.1 Deep Transition RNN  We noticed from the state transition equation of the dynamical system simulated by RNNs in Eq. (1) that there is no restriction on the form of fh. Hence, we propose here to use a multilayer perceptron to approximate fh instead. In this case, we can implement fh by L intermediate layers such that  (cid:0)W(cid:62)  (cid:0)W(cid:62)  (cid:0)··· φ1  (cid:0)W(cid:62)  1 ht−1 + U(cid:62)xt  (cid:1)(cid:1)(cid:1)(cid:1) ,  ht = fh(xt, ht−1) = φh  L φL−1  L−1φL−2  where φl and Wl are the element-wise nonlinear function and the weight matrix for the l-th layer. This RNN with a multilayered transition function is a deep transition RNN (DT-RNN). An illustration of building an RNN with the deep state transition function is shown in Fig. 2 (b). In the illustration the state transition function is implemented with a neural network with a single intermediate layer. This formulation allows the RNN to learn a non-trivial, highly nonlinear transition between the consecutive hidden states.  3.3.2 Deep Output RNN  Similarly, we can use a multilayer perceptron with L intermediate layers to model the output function fo in Eq. (2) such that  (cid:0)V(cid:62)  (cid:0)V(cid:62)  (cid:0)··· φ1  (cid:0)V(cid:62)  1 ht  (cid:1)(cid:1)(cid:1)(cid:1) ,  yt = fo(ht) = φo  L φL−1  L−1φL−2  where φl and Vl are the element-wise nonlinear function and the weight matrix for the l-th layer. An RNN implementing this kind of multilayered output function is a deep output recurrent neural network (DO-RNN). Fig. 2 (c) draws a deep output, deep transition RNN (DOT-RNN) implemented using both the deep transition and the deep output with a single intermediate layer each.  3.3.3 Stacked RNN  The stacked RNN (Schmidhuber, 1992; El Hihi and Bengio, 1996) has multiple levels of transition functions deﬁned by  (cid:16)  (cid:17)  W(cid:62)  t−1 + U(cid:62)  l h(l−1)  l h(l)  t  ,  h(l) t = f (l)  h (h(l−1)  t  , h(l)  t−1) = φh  5  t  is the hidden state of the l-th level at time t. When l = 1, the state is computed using xt . The hidden states of all the levels are recursively computed from the bottom level  where h(l) t instead of h(l−1) l = 1. Once the top-level hidden state is computed, the output can be obtained using the usual formula- tion in Eq. (5). Alternatively, one may use all the hidden states to compute the output (Hermans and Schrauwen, 2013). Each hidden state at each level may also be made to depend on the input as well (Graves, 2013). Both of them can be considered approaches using shortcut connections discussed earlier. The illustration of this stacked RNN is in Fig. 2 (d).  4 Another Perspective: Neural Operators  In this section, we brieﬂy introduce a novel approach with which the already discussed deep tran- sition (DT) and/or deep output (DO) recurrent neural networks (RNN) may be built. We call this approach which is based on building an RNN with a set of predeﬁned neural operators, an operator- based framework. In the operator-based framework, one ﬁrst deﬁnes a set of operators of which each is implemented by a multilayer perceptron (MLP). For instance, a plus operator ⊕ may be deﬁned as a function receiving two vectors x and h and returning the summary h(cid:48) of them:  where we may constrain that the dimensionality of h and h(cid:48) are identical. Additionally, we can deﬁne another operator (cid:66) which predicts the most likely output symbol x(cid:48) given a summary h, such that  h(cid:48) = x ⊕ h,  x(cid:48) = (cid:66)h  It is possible to deﬁne many other operators, but in this paper, we stick to these two operators which are sufﬁcient to express all the proposed types of RNNs.  It is clear to see that the plus operator ⊕ and the predict operator (cid:66) correspond to the transition function and the output function in Eqs. (1)–(2). Thus, at each step, an RNN can be thought as performing the plus operator to update the hidden state given an input (ht = xt ⊕ ht−1) and then the predict operator to compute the output (yt = (cid:66)ht = (cid:66)(xt ⊕ ht−1)). See Fig. 3 for the illustration of how an RNN can be understood from the operator-based framework. Each operator can be parameterized as an MLP with one or more hidden layers, hence a neural operator, since we cannot simply expect the operation will be linear with respect to the input vector(s). By using an MLP to im- plement the operators, the proposed deep transition, deep output RNN (DOT-RNN) naturally arises. This framework provides us an insight on how the con- structed RNN be regularized. For instance, one may regularize the model such that the plus operator ⊕ is commutative. However, in this paper, we do not explore further on this approach. Note that this is different from (Mikolov et al., 2013a) where the learned embeddings of words happened to be suitable for algebraic operators. The operator-based framework proposed here is rather geared toward learning these operators directly.  Figure 3: A view of an RNN under the operator-based framework: ⊕ and (cid:66) are the plus and predict operators, respec- tively.  5 Experiments  We train four types of RNNs described in this paper on a number of benchmark datasets to evaluate their performance. For each benchmark dataset, we try the task of predicting the next symbol.  6  xtht-1htyt+The task of predicting the next symbol is equivalent to the task of modeling the distribution over a sequence. For each sequence (x1, . . . , xT ), we decompose it into  p(x1, . . . , xT ) = p(x1)  p(xt | x1, . . . , xt−1),  T(cid:89)  t=2  and each term on the right-hand side will be replaced with a single timestep of an RNN. In this setting, the RNN predicts the probability of the next symbol xt in the sequence given the all previous symbols x1, . . . xt−1. Then, we train the RNN by maximizing the log-likelihood. We try this task of modeling the joint distribution on three different tasks; polyphonic music predic- tion, character-level and word-level language modeling. We test the RNNs on the task of polyphonic music prediction using three datasets which are Notting- ham, JSB Chorales and MuseData (Boulanger-Lewandowski et al., 2012). On the task of character- level and word-level language modeling, we use Penn Treebank Corpus (Marcus et al., 1993).  5.1 Model Descriptions  We compare the conventional recurrent neural network (RNN), deep transition RNN with shortcut connections in the transition MLP (DT(S)-RNN), deep output/transition RNN with shortcut connec- tions in the hidden to hidden transition MLP (DOT(S)-RNN) and stacked RNN (sRNN). See Fig. 2 (a)–(d) for the illustrations of these models.  RNN  DT(S)-RNN DOT(S)-RNN  sRNN 2 layers  Music  Language  Notthingam  JSB Chorales  MuseData  Char-level  Word-level  # units  # parameters  # units  # parameters  # units  # parameters  # units  # parameters  # units  # parameters  600 465K 200 75K 600 465K 600 420K 200 4.04M  400,400 585K 400,400 585K 400,400 585K 400,400 540K 200,200 6.12M  400,400,400  400,400,400  400,400,400  745K  745K  745K  790K  400,400,600  200,200,200  6.16M  400 550K 400 550K 600  1185K  400 520K 400 8.48M  Table 1: The sizes of the trained models. We provide the number of hidden units as well as the total number of parameters. For DT(S)-RNN, the two numbers provided for the number of units mean the size of the hidden state and that of the intermediate layer, respectively. For DOT(S)-RNN, the three numbers are the size of the hidden state, that of the intermediate layer between the consecutive hidden states and that of the intermediate layer between the hidden state and the output layer. For sRNN, the number corresponds to the size of the hidden state at each level The size of each model is chosen from a limited set {100, 200, 400, 600, 800} to minimize the val- idation error for each polyphonic music task (See Table. 1 for the ﬁnal models). In the case of language modeling tasks, we chose the size of the models from {200, 400} and {400, 600} for word-level and character-level tasks, respectively. In all cases, we use a logistic sigmoid function as an element-wise nonlinearity of each hidden unit. Only for the character-level language modeling we used rectiﬁed linear units (Glorot et al., 2011a) for the intermediate layers of the output function, which gave lower validation error.  5.2 Training  We use stochastic gradient descent (SGD) and employ the strategy of clipping the gradient proposed by Pascanu et al. (2013a). Training stops when the validation cost stops decreasing. Polyphonic Music Prediction: For Nottingham and MuseData datasets we compute each gradient step on subsequences of at most 200 steps, while we use subsequences of 50 steps for JSB Chorales.  7  We do not reset the hidden state for each subsequence, unless the subsequence belongs to a different song than the previous subsequence. The cutoff threshold for the gradients is set to 1. The hyperparameter for the learning rate schedule1 is tuned manually for each dataset. We set the hyperparameter β to 2330 for Nottingham, 1475 for MuseData and 100 for JSB Chroales. They correspond to two epochs, a single epoch and a third of an epoch, respectively. The weights of the connections between any pair of hidden layers are sparse, having only 20 non- zero incoming connections per unit (see, e.g., Sutskever et al., 2013). Each weight matrix is rescaled to have a unit largest singular value (Pascanu et al., 2013a). The weights of the connections between the input layer and the hidden state as well as between the hidden state and the output layer are initialized randomly from the white Gaussian distribution with its standard deviation ﬁxed to 0.1 and 0.01, respectively. In the case of deep output functions (DOT(S)-RNN), the weights of the connections between the hidden state and the intermediate layer are sampled initially from the white Gaussian distribution of standard deviation 0.01. In all cases, the biases are initialized to 0. To regularize the models, we add white Gaussian noise of standard deviation 0.075 to each weight parameter every time the gradient is computed (Graves, 2011). Language Modeling: We used the same strategy for initializing the parameters in the case of lan- guage modeling. For character-level modeling, the standard deviations of the white Gaussian distri- butions for the input-to-hidden weights and the hidden-to-output weights, we used 0.01 and 0.001, respectively, while those hyperparameters were both 0.1 for word-level modeling. In the case of DOT(S)-RNN, we sample the weights of between the hidden state and the rectiﬁer intermediate layer of the output function from the white Gaussian distribution of standard deviation 0.01. When using rectiﬁer units (character-based language modeling) we ﬁx the biases to 0.1. In language modeling, the learning rate starts from an initial value and is halved each time the vali- dation cost does not decrease signiﬁcantly (Mikolov et al., 2010). We do not use any regularization for the character-level modeling, but for the word-level modeling we use the same strategy of adding weight noise as we do with the polyphonic music prediction. For all the tasks (polyphonic music prediction, character-level and word-level language modeling), the stacked RNN and the DOT(S)-RNN were initialized with the weights of the conventional RNN and the DT(S)-RNN, which is similar to layer-wise pretraining of a feedforward neural network (see, e.g., Hinton and Salakhutdinov, 2006). We use a ten times smaller learning rate for each parameter that was pretrained as either RNN or DT(S)-RNN.  RNN DT(S)-RNN DOT(S)-RNN sRNN DOT(S)-RNN*  Notthingam 3.225 JSB Chorales 8.338 6.990  MuseData  3.206 8.278 6.988  3.215 8.437 6.973  3.258 8.367 6.954  2.95 7.92 6.59  Table 2: The performances of the four types of RNNs on the polyphonic music prediction. The numbers represent negative log-probabilities on test sequences. (*) We obtained these results using DOT(S)-RNN with Lp units in the deep transition, maxout units in the deep output function and dropout (Gulcehre et al., 2013).  5.3 Result and Analysis  5.3.1 Polyphonic Music Prediction  The log-probabilities on the test set of each data are presented in the ﬁrst four columns of Tab. 2. We were able to observe that in all cases one of the proposed deep RNNs outperformed the conventional, shallow RNN. Though, the suitability of each deep RNN depended on the data it was trained on. The best results obtained by the DT(S)-RNNs on Notthingam and JSB Chorales are close to, but  1 We use at each update τ, the following learning rate ητ =  , where τ0 and β indicate respec- tively when the learning rate starts decreasing and how quickly the learning rate decreases. In the experiment, we set τ0 to coincide with the time when the validation error starts increasing for the ﬁrst time.  max(0,τ−τ0)  1+  1  β  8  worse than the result obtained by RNNs trained with the technique of fast dropout (FD) which are 3.09 and 8.01, respectively (Bayer et al., 2013). In order to quickly investigate whether the proposed deeper variants of RNNs may also beneﬁt from the recent advances in feedforward neural networks, such as the use of non-saturating activation functions2 and the method of dropout. We have built another set of DOT(S)-RNNs that have the recently proposed Lp units (Gulcehre et al., 2013) in deep transition and maxout units (Goodfellow et al., 2013) in deep output function. Furthermore, we used the method of dropout (Hinton et al., 2012b) instead of weight noise during training. Similarly to the previously trained models, we searched for the size of the models as well as other learning hyperparameters that minimize the validation performance. We, however, did not pretrain these models. The results obtained by the DOT(S)-RNNs having Lp and maxout units trained with dropout are shown in the last column of Tab. 2. On every music dataset the performance by this model is sig- niﬁcantly better than those achieved by all the other models as well as the best results reported with recurrent neural networks in (Bayer et al., 2013). This suggests us that the proposed variants of deep RNNs also beneﬁt from having non-saturating activations and using dropout, just like feedforward neural networks. We reported these results and more details on the experiment in (Gulcehre et al., 2013). We, however, acknowledge that the model-free state-of-the-art results for the both datasets were obtained using an RNN combined with a conditional generative model, such as restricted Boltz- mann machines or neural autoregressive distribution estimator (Larochelle and Murray, 2011), in the output (Boulanger-Lewandowski et al., 2012).  Character-Level  Word-Level  RNN DT(S)-RNN DOT(S)-RNN sRNN 1.412 1.414 117.7 110.0  1.386 107.5  1.409 112.0  ∗ 1.411 1232  (cid:63)  1.243 1173  Table 3: The performances of the four types of RNNs on the tasks of language modeling. The numbers represent bit-per-character and perplexity computed on test sequence, respectively, for the character-level and word-level modeling tasks. ∗ The previous/current state-of-the-art results obtained with shallow RNNs. (cid:63) The previous/current state-of-the-art results obtained with RNNs having long-short term memory units.  5.3.2 Language Modeling  On Tab. 3, we can see the perplexities on the test set achieved by the all four models. We can clearly see that the deep RNNs (DT(S)-RNN, DOT(S)-RNN and sRNN) outperform the conventional, shal- low RNN signiﬁcantly. On these tasks DOT(S)-RNN outperformed all the other models, which suggests that it is important to have highly nonlinear mapping from the hidden state to the output in the case of language modeling. The results by both the DOT(S)-RNN and the sRNN for word-level modeling surpassed the previous best performance achieved by an RNN with 1000 long short-term memory (LSTM) units (Graves, 2013) as well as that by a shallow RNN with a larger hidden state (Mikolov et al., 2011), even when both of them used dynamic evaluation3. The results we report here are without dynamic evaluation. For character-level modeling the state-of-the-art results were obtained using an optimization method Hessian-free with a speciﬁc type of RNN architecture called mRNN (Mikolov et al., 2012a) or a regularization technique called adaptive weight noise (Graves, 2013). Our result, however, is better than the performance achieved by conventional, shallow RNNs without any of those advanced  2 Note that it is not trivial to use non-saturating activation functions in conventional RNNs, as this may cause the explosion of the activations of hidden states. However, it is perfectly safe to use non-saturating activation functions at the intermediate layers of a deep RNN with deep transition.  1Reported by Mikolov et al. (2012a) using mRNN with Hessian-free optimization technique. 2Reported by Mikolov et al. (2011) using the dynamic evaluation. 3Reported by Graves (2013) using the dynamic evaluation and weight noise. 3Dynamic evaluation refers to an approach where the parameters of a model are updated as the valida-  tion/test data is predicted.  9  regularization methods (Mikolov et al., 2012b), where they reported the best performance of 1.41 using an RNN trained with the Hessian-free learning algorithm (Martens and Sutskever, 2011).  6 Discussion  In this paper, we have explored a novel approach to building a deep recurrent neural network (RNN). We considered the structure of an RNN at each timestep, which revealed that the relationship be- tween the consecutive hidden states and that between the hidden state and output are shallow. Based on this observation, we proposed two alternative designs of deep RNN that make those shallow re- lationships be modeled by deep neural networks. Furthermore, we proposed to make use of shortcut connections in these deep RNNs to alleviate a problem of difﬁcult learning potentially introduced by the increasing depth. We empirically evaluated the proposed designs against the conventional RNN which has only a single hidden layer and against another approach of building a deep RNN (stacked RNN, Graves, 2013), on the task of polyphonic music prediction and language modeling. The experiments revealed that the RNN with the proposed deep transition and deep output (DOT(S)- RNN) outperformed both the conventional RNN and the stacked RNN on the task of language modeling, achieving the state-of-the-art result on the task of word-level language modeling. For polyphonic music prediction, a different deeper variant of an RNN achieved the best performance for each dataset. Importantly, however, in all the cases, the conventional, shallow RNN was not able to outperform the deeper variants. These results strongly support our claim that an RNN beneﬁts from having a deeper architecture, just like feedforward neural networks. The observation that there is no clear winner in the task of polyphonic music prediction suggests us that each of the proposed deep RNNs has a distinct characteristic that makes it more, or less, suitable for certain types of datasets. We suspect that in the future it will be possible to design and train yet another deeper variant of an RNN that combines the proposed models together to be more robust to the characteristics of datasets. For instance, a stacked DT(S)-RNN may be constructed by combining the DT(S)-RNN and the sRNN. In a quick additional experiment where we have trained DOT(S)-RNN constructed using non- saturating nonlinear activation functions and trained with the method of dropout, we were able to improve the performance of the deep recurrent neural networks on the polyphonic music prediction tasks signiﬁcantly. This suggests us that it is important to investigate the possibility of applying recent advances in feedforward neural networks, such as novel, non-saturating activation functions and the method of dropout, to recurrent neural networks as well. However, we leave this as future research. One practical issue we ran into during the experiments was the difﬁculty of training deep RNNs. We were able to train the conventional RNN as well as the DT(S)-RNN easily, but it was not trivial to train the DOT(S)-RNN and the stacked RNN. In this paper, we proposed to use shortcut connections as well as to pretrain them either with the conventional RNN or with the DT(S)-RNN. We, however, believe that learning may become even more problematic as the size and the depth of a model increase. In the future, it will be important to investigate the root causes of this difﬁculty and to explore potential solutions. We ﬁnd some of the recently introduced approaches, such as advanced regularization methods (Pascanu et al., 2013a) and advanced optimization algorithms (see, e.g., Pascanu and Bengio, 2013; Martens, 2010), to be promising candidates.  Acknowledgments  We would like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012). We also thank Justin Bayer for his insightful comments on the paper. We would like to thank NSERC, Compute Canada, and Calcul Qu´ebec for providing computational resources. Razvan Pascanu is supported by a DeepMind Fellowship. Kyunghyun Cho is supported by FICS (Finnish Doctoral Programme in Computational Sciences) and “the Academy of Finland (Finnish Centre of Excellence in Computational Inference Research COIN, 251170)”.  10  References Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A., Bouchard, N., and Bengio, Y. (2012). Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.  Bayer, J., Osendorfer, C., Korhammer, D., Chen, N., Urban, S., and van der Smagt, P. (2013). On  fast dropout and its applicability to recurrent networks. arXiv:1311.0701 [cs.NE].  Bengio, Y. (2009). Learning deep architectures for AI. Found. Trends Mach. Learn., 2(1), 1–127. Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient  descent is difﬁcult. IEEE Transactions on Neural Networks, 5(2), 157–166.  Bengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. (2013). Better mixing via deep representations.  In ICML’13.  Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde- In  Farley, D., and Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. Proceedings of the Python for Scientiﬁc Computing Conference (SciPy). Oral Presentation.  Boulanger-Lewandowski, N., Bengio, Y., and Vincent, P. (2012). Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription. In ICML’2012.  Chen, J. and Deng, L. (2013). A new method for learning deep recurrent neural networks.  arXiv:1311.6091 [cs.LG].  Delalleau, O. and Bengio, Y. (2011). Shallow vs. deep sum-product networks. In NIPS. El Hihi, S. and Bengio, Y. (1996). Hierarchical recurrent neural networks for long-term dependen-  cies. In NIPS 8. MIT Press.  Glorot, X., Bordes, A., and Bengio, Y. (2011a). Deep sparse rectiﬁer neural networks. In AISTATS. Glorot, X., Bordes, A., and Bengio, Y. (2011b). Domain adaptation for large-scale sentiment classi-  ﬁcation: A deep learning approach. In ICML’2011.  Goodfellow, I., Le, Q., Saxe, A., and Ng, A. (2009). Measuring invariances in deep networks. In  NIPS’09, pages 646–654.  Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. (2013). Maxout  networks. In ICML’2013.  Graves, A. (2011). Practical variational inference for neural networks. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 2348–2356.  Graves, A. (2013). Generating sequences with recurrent neural networks. arXiv:1308.0850  [cs.NE].  Graves, A., Liwicki, M., Fernandez, S., Bertolami, R., Bunke, H., and Schmidhuber, J. (2009). A novel connectionist system for improved unconstrained handwriting recognition. IEEE Transac- tions on Pattern Analysis and Machine Intelligence.  Graves, A., Mohamed, A., and Hinton, G. (2013). Speech recognition with deep recurrent neural  networks. ICASSP.  Gulcehre, C., Cho, K., Pascanu, R., and Bengio, Y. (2013). Learned-norm pooling for deep feedfor-  ward and recurrent neural networks. arXiv:1311.1780 [cs.NE].  Hermans, M. and Schrauwen, B. (2013). Training and analysing deep recurrent neural networks. In  Advances in Neural Information Processing Systems 26, pages 190–198.  Hinton, G., Deng, L., Dahl, G. E., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T., and Kingsbury, B. (2012a). Deep neural networks for acoustic modeling in speech recognition. IEEE Signal Processing Magazine, 29(6), 82–97.  Hinton, G. E. and Salakhutdinov, R. (2006). Reducing the dimensionality of data with neural net-  works. Science, 313(5786), 504–507.  Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2012b). Im- proving neural networks by preventing co-adaptation of feature detectors. Technical report, arXiv:1207.0580.  11  Hornik, K., Stinchcombe, M., and White, H. (1989). Multilayer feedforward networks are universal  approximators. Neural Networks, 2, 359–366.  Jaeger, H. (2007). Discovering multiscale dynamical features with hierarchical echo state networks.  Technical report, Jacobs University.  Ko, J. and Dieter, F. (2009). Gp-bayesﬁlters: Bayesian ﬁltering using gaussian process prediction  and observation models. Autonomous Robots.  Larochelle, H. and Murray, I. (2011). The Neural Autoregressive Distribution Estimator. In Pro- ceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics (AIS- TATS’2011), volume 15 of JMLR: W&CP.  Le Roux, N. and Bengio, Y. (2010). Deep belief networks are compact universal approximators.  Neural Computation, 22(8), 2192–2207.  Marcus, M. P., Marcinkiewicz, M. A., and Santorini, B. (1993). Building a large annotated corpus  of english: The Penn Treebank. Computational Linguistics, 19(2), 313–330.  Martens, J. (2010). Deep learning via Hessian-free optimization. In L. Bottou and M. Littman, ed- itors, Proceedings of the Twenty-seventh International Conference on Machine Learning (ICML- 10), pages 735–742. ACM.  Martens, J. and Sutskever, I. (2011). Learning recurrent neural networks with Hessian-free opti-  mization. In Proc. ICML’2011. ACM.  Mikolov, T. (2012). Statistical Language Models based on Neural Networks. Ph.D. thesis, Brno  University of Technology.  Mikolov, T., Karaﬁ´at, M., Burget, L., Cernocky, J., and Khudanpur, S. (2010). Recurrent neural network based language model. In Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH 2010), volume 2010, pages 1045–1048. International Speech Communication Association.  Mikolov, T., Kombrink, S., Burget, L., Cernocky, J., and Khudanpur, S. (2011). Extensions of recur- rent neural network language model. In Proc. 2011 IEEE international conference on acoustics, speech and signal processing (ICASSP 2011).  Mikolov, T., Sutskever, I., Deoras, A., Le, H., Kombrink, S., and Cernocky, J. (2012a). Subword  language modeling with neural networks. unpublished.  Mikolov, T., Sutskever,  I., Deoras, A., Le, H.-S., Kombrink, S.,  (2012b).  J. (http://www.ﬁt.vutbr.cz/ imikolov/rnnlm/char.pdf).  language modeling with  Subword  neural  networks.  and Cernocky, preprint  Mikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean, J. (2013a). Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26, pages 3111–3119.  Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013b). Efﬁcient estimation of word represen- tations in vector space. In International Conference on Learning Representations: Workshops Track.  Pascanu, R. and Bengio, Y. (2013). Revisiting natural gradient for deep networks. Technical report,  arXiv:1301.3584.  Pascanu, R., Mikolov, T., and Bengio, Y. (2013a). On the difﬁculty of training recurrent neural  networks. In ICML’2013.  Pascanu, R., Montufar, G., and Bengio, Y. (2013b). On the number of response regions of deep feed  forward networks with piece-wise linear activations. arXiv:1312.6098[cs.LG].  Pinheiro, P. and Collobert, R. (2014). Recurrent convolutional neural networks for scene labeling.  In Proceedings of The 31st International Conference on Machine Learning, pages 82–90.  Raiko, T., Valpola, H., and LeCun, Y. (2012). Deep learning made easier by linear transformations in perceptrons. In Proceedings of the Fifteenth Internation Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2012), volume 22 of JMLR Workshop and Conference Proceedings, pages 924–932. JMLR W&CP.  Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning representations by back-  propagating errors. Nature, 323, 533–536.  12  Schmidhuber, J. (1992). Learning complex, extended sequences using the principle of history com-  pression. Neural Computation, (4), 234–242.  Sutskever, I., Martens, J., and Hinton, G. (2011). Generating text with recurrent neural networks. In L. Getoor and T. Scheffer, editors, Proceedings of the 28th International Conference on Machine Learning (ICML 2011), pages 1017–1024, New York, NY, USA. ACM.  Sutskever, I., Martens, J., Dahl, G., and Hinton, G. (2013). On the importance of initialization and  momentum in deep learning. In ICML.  Valpola, H. and Karhunen, J. (2002). An unsupervised ensemble learning method for nonlinear  dynamic state-space models. Neural Comput., 14(11), 2647–2692.  13  ","In this paper, we explore different ways to extend a recurrent neural network(RNN) to a \textit{deep} RNN. We start by arguing that the concept of depth inan RNN is not as clear as it is in feedforward neural networks. By carefullyanalyzing and understanding the architecture of an RNN, however, we find threepoints of an RNN which may be made deeper; (1) input-to-hidden function, (2)hidden-to-hidden transition and (3) hidden-to-output function. Based on thisobservation, we propose two novel architectures of a deep RNN which areorthogonal to an earlier attempt of stacking multiple recurrent layers to builda deep RNN (Schmidhuber, 1992; El Hihi and Bengio, 1996). We provide analternative interpretation of these deep RNNs using a novel framework based onneural operators. The proposed deep RNNs are empirically evaluated on the tasksof polyphonic music prediction and language modeling. The experimental resultsupports our claim that the proposed deep RNNs benefit from the depth andoutperform the conventional, shallow RNNs."
1312.6098,2014,On the number of inference regions of deep feed forward networks with piece-wise linear activations  ,"['Razvan Pascanu', 'Guido F. Montufar', 'Yoshua Bengio']",https://arxiv.org/pdf/1312.6098.pdf,"4 1 0 2     b e F 4 1         ]  G L . s c [      5 v 8 9 0 6  .  2 1 3 1 : v i X r a  On the number of response regions of deep feedforward networks with piecewise linear  activations  Razvan Pascanu  Universit´e de Montr´eal  Montr´eal QC H3C 3J7 Canada r.pascanu@gmail.com  Guido Mont´ufar  Max Planck Institute for Mathematics in the Sciences  Inselstraße 22, 04103 Leipzig, Germany  montufar@mis.mpg.de  Yoshua Bengio  Universit´e de Montr´eal  Montr´eal QC H3C 3J7 Canada  yoshua.bengio@umontreal.ca  Abstract  This paper explores the complexity of deep feedforward networks with linear pre- synaptic couplings and rectiﬁed linear activations. This is a contribution to the growing body of work contrasting the representational power of deep and shallow network architectures. In particular, we offer a framework for comparing deep and shallow models that belong to the family of piecewise linear functions based on computational geometry. We look at a deep rectiﬁer multi-layer perceptron (MLP) with linear outputs units and compare it with a single layer version of the model. In the asymptotic regime, when the number of inputs stays constant, if the shallow model has kn hidden units and n0 inputs, then the number of linear regions is O(kn0nn0 ). For a k layer model with n hidden units on each layer it is Ω((cid:98)n/n0(cid:99)k−1 nn0 ). The number (cid:98)n/n0(cid:99)k−1 grows faster than kn0 when n tends to inﬁnity or when k tends to inﬁnity and n ≥ 2n0. Additionally, even when k is small, if we restrict n to be 2n0, we can show that a deep model has considerably more linear regions that a shallow one. We consider this as a ﬁrst step towards understanding the complexity of these models and speciﬁcally towards providing suitable mathematical tools for future analysis. Keywords: Deep learning, artiﬁcial neural network, rectiﬁer unit, hyperplane ar- rangement, representational power  1  Introduction  Deep systems are believed to play an important role in information processing of intelligent agents. A common hypothesis underlying this belief is that deep models can be exponentially more efﬁcient at representing some functions than their shallow counterparts (see Bengio, 2009). The argument is usually a compositional one. Higher layers in a deep model can re-use primitives constructed by the lower layers in order to build gradually more complex functions. For example, on a vision task, one would hope that the ﬁrst layer learns Gabor ﬁlters capable to detect edges of different orientation. These edges are then put together at the second layer to form part-of-object shapes. On higher layers, these part-of-object shapes are combined further to obtain detectors for more complex part-of-object shapes or objects. Such a behaviour is empirically illustrated, for  1  instance, in Zeiler and Fergus (2013); Lee et al. (2009). On the other hand, a shallow model has to construct detectors of target objects based only on the detectors learnt by the ﬁrst layer. The representational power of computational systems with shallow and deep architectures has been studied intensively. A well known result Hajnal et al. (1993) derived lower complexity bounds for shallow threshold networks. Other works have explored the representational power of generative models based on Boltzmann machines Mont´ufar et al. (2011); Martens et al. (2013) and deep belief networks (Sutskever and Hinton, 2008; Le Roux and Bengio, 2010; Mont´ufar and Ay, 2011), or have compared mixtures and products of experts models (Mont´ufar and Morton, 2012). In addition to such inspections, a wealth of evidence for the validity of this hypothesis comes from deep models consistently outperforming shallow ones on a variety of tasks and datasets (see, e.g., Goodfellow et al., 2013; Hinton et al., 2012b,a). However, theoretical results on the representational power of deep models are limited, usually due to the composition of nonlinear functions in deep models, which makes mathematical analysis difﬁcult. Up to now, theoretical results have focussed on circuit operations (neural net unit computations) that are substantially different from those being used in real state-of-the-art deep learning applications, such as logic gates (H˚astad, 1986), linear + threshold units with non-negative weights (H˚astad and Goldmann, 1991) or polynomials (Bengio and Delalleau, 2011). Bengio and Delalleau (2011) show that deep sum-product networks (Poon and Domingos, 2011) can use exponentially less nodes to express some families of polynomials compared to the shallow ones. The present note analyzes the representational power of deep MLPs with rectiﬁer units. Rectiﬁer units (Glorot et al., 2011; Nair and Hinton, 2010) and piecewise linearly activated units in general (like the maxout unit (Goodfellow et al., 2013)), are becoming popular choices in designing deep models, and most current state-of-the-art results involve using one of such activations (Goodfellow et al., 2013; Hinton et al., 2012b). Glorot et al. (2011) show that rectiﬁer units have several properties that make the optimization problem easier than the more traditional case using smooth and bounded activations, such as tanh or sigmoid. In this work we take advantage of the piecewise linear nature of the rectiﬁer unit to mathematically analyze the behaviour of deep rectiﬁer MLPs. Given that the model is a composition of piecewise linear functions, it is itself a piecewise linear function. We compare the ﬂexibility of a deep model with that of a shallow model by counting the number of linear regions they deﬁne over the input space for a ﬁxed number of hidden units. This is the number of pieces available to the model in order to approximate some arbitrary nonlinear function. For example, if we want to perfectly approximate some curved boundary between two classes, a rectiﬁer MLP will have to use inﬁnitely many linear regions. In practice we have a ﬁnite number of pieces, and if we assume that we can perfectly learn their optimal slopes, then the number of linear regions becomes a good proxy for how well the model approximates this boundary. In this sense, the number of linear regions is an upper bound for the ﬂexibility of the model. In practice, the linear pieces are not independent and the model may not be able to learn the right slope for each linear region. Speciﬁcally, for deep models there is a correlation between regions, which results from the sharing of parameters between the functions that describe the output on each region. This is by no means a negative observation. If all the linear regions of the deep model were inde- pendent of each other, by having many more linear regions, deep models would grossly overﬁt. The correlation of the linear regions of a deep model results in its ability to generalize, by allowing it to better represent only a small family of structured functions. These are functions that look compli- cated (e.g., a distribution with a huge number of modes) but that have an underlying structure that the network can ‘compress’ into its parameters. The number of regions, which indicates the number of variations that the network can represent, provides a measure of how well it can ﬁt this family of structured functions (whose approximation potentially needs inﬁnitely many linear regions). We believe that this approach, based on counting the number of linear regions, is extensible to any other piecewise linear activation function and also to other architectures, including the maxout activation and the convolutional networks with rectiﬁer activations. We know the maximal number of regions of linearity of functions computable by a shallow model with a ﬁxed number of hidden units. This number is given by a well studied geometrical problem. The main insight of the present work is to provide a geometrical construction that describes the regions of linearity of functions computed by deep models. We show that in the asymptotic regime,  2  these functions have many more linear regions than the ones computed by shallow models, for the same number of hidden units. For the single layer case, each hidden unit divides the input space in two, whereby the boundary is given by a hyperplane. For all input values on one side of the hyperplane, the unit outputs a positive value. For all input values on the other side of the hyperplane, the unit outputs 0. Therefore, the question that we are asking is: Into how many regions do n hyperplanes split space? This question is studied in geometry under the name of hyperplane arrangements, with classic results such as Zaslavsky’s theorem. Section 3 provides a quick introduction to the subject. For the multilayer version of the model we rely on the following intuition. By using the rectiﬁer nonlinearity, we identify multiple regions of the input space which are mapped by a given layer into an equivalent set of activations and represent thus equivalent inputs for the next layers. That is, a hidden layer can perform a kind of or operation by reacting similarly to several different inputs. Any subsequent computation made on these activations is replicated on all equivalent inputs.  This paper is organized as follows. In Section 2 we provide deﬁnitions and basic observations about piecewise linear functions. In Section 3 we discuss rectiﬁer networks with one single hidden layer and describe their properties in terms of hyperplane arrangements which are fairly well known in the literature. In Section 4 we discuss deep rectiﬁer networks and prove our main result, Theorem 1, which describes their complexity in terms of the number of regions of linearity of functions that they represent. Details about the asymptotic behaviour of the results derived in Sections 3 and 4 are given in the Appendix A. In Section 5 we analyze a special type of deep rectiﬁer MLP and show that even for a small number of hidden layers it can generate a large number of linear regions. In Section 6 we offer a discussion of the results.  2 Preliminaries  We consider classes of functions (models) deﬁned in the following way. Deﬁnition 1. A rectiﬁer feedforward network is a layered feedforward network, or multilayer per- ceptron (MLP), as shown in Fig. 1, with following properties. Each hidden unit receives as inputs the real valued activations x1, . . . , xn of all units in the previous layer, computes the weighted sum  (cid:88)  i∈[n]  s =  wixi + b,  and outputs the rectiﬁed value  rect(s) = max{0, s}.  The real parameters w1, . . . , wn are the input weights and b is the bias of the unit. The output layer is a linear layer, that is, the units in the last layer compute a linear combination of their inputs and output it unrectiﬁed. Given a vector of naturals n = (n0, n1, . . . , nL), we denote by Fn the set of all functions Rn0 → RnL that can be computed by a rectiﬁer feedforward network with n0 inputs and nl units in layer l for l ∈ [L]. The elements of Fn are continuous piecewise linear functions. We denote by R(n) the maximum of the number of regions of linearity or response regions over all functions from Fn. For clarity, given a function f : Rn0 → RnL, a connected open subset R ⊆ Rn0 is called a region of linearity or linear region or response region of f if the restriction f|R is a linear function and for any open set ˜R (cid:41) R the restriction f| ˜R is not a linear function. In the next sections we will compute bounds on R(n) for different choices of n. We are especially interested in the comparison of shallow networks with one single very wide hidden layer and deep networks with many narrow hidden layers.  In the remainder of this section we state three simple lemmas. The next lemma states that a piecewise linear function f = (fi)i∈[k] has as many regions of linearity as there are distinct intersections of regions of linearity of the coordinates fi. Lemma 1. Consider a width k layer of rectiﬁer units. Let Ri = {Ri Ni} be the regions of linearity of the function fi : Rn0 → R computed by the i-th unit, for all i ∈ [k]. Then the regions of  1, . . . , Ri  3  Figure 1: Illustration of a rectiﬁer feedforward network with two hidden layers.  j1 ∩ ··· ∩ Rk  jk}(j1,...,jk)∈[N1]×···×[Nk].  linearity of the function f = (fi)i∈[k] : Rn0 → Rk computed by the rectiﬁer layer are the elements of the set {Rj1,...,jk = R1 Proof. A function f = (f1, . . . , fk) : Rn → Rk is linear iff all its coordinates f1, . . . , fk are. In regard to the number of regions of linearity of the functions represented by rectiﬁer networks, the number of output dimensions, i.e., the number of linear output units, is irrelevant. This is the statement of the next lemma. Lemma 2. The number of (linear) output units of a rectiﬁer feedforward network does not affect the maximal number of regions of linearity that it can realize. Proof. Let f : Rn0 → Rk be the map of inputs to activations in the last hidden layer of a deep feedforward rectiﬁer model. Let h = g ◦ f be the map of inputs to activations of the output units, given by composition of f with the linear output layer, h(x) = W(out)f (x) + b(out). If the row span of W(out) is not orthogonal to any difference of gradients of neighbouring regions of linearity of f, then g captures all discontinuities of ∇f. In this case both functions f and h have the same number of regions of linearity. If the number of regions of f is ﬁnite, then the number of differences of gradients is ﬁnite and there is a vector outside the union of their orthogonal spaces. Hence a matrix with a single row (a single output unit) sufﬁces to capture all transitions between different regions of linearity of f. Lemma 3. A layer of n rectiﬁer units with n0 inputs can compute any function that can be computed by the composition of a linear layer with n0 inputs and n(cid:48) 0 inputs and n1 outputs, for any n0, n(cid:48) Proof. A rectiﬁer layer computes functions of the form x (cid:55)→ rect(Wx + b), with W ∈ Rn1×n0 and b ∈ Rn1. The argument Wx + b is an afﬁne function of x. The claim follows from the fact that any composition of afﬁne functions is an afﬁne function.  0 outputs and a rectiﬁer layer with n(cid:48)  0, n1 ∈ N.  3 One hidden layer  Let us look at the number of response regions of a single hidden layer MLP with n0 input units and n hidden units. We ﬁrst formulate the rectiﬁer unit as follows:  rect(s) = I(s) · s,  4  (1)  1sthiddenlayer2ndhiddenlayerInputOutputh(1)=rect(W(1)x+b(1))h(2)=rect(W(2)h(1)+b(2))xW(out)h(2)where I is the indicator function deﬁned as:  We can now write the single hidden layer MLP with ny outputs as the function f : Rn0 → Rny;  if s > 0  0, otherwise .  I(s) =  (cid:26)1,  I(W(1)  ...    1: x + b(1) 1 )  I(W(1)  n1:x + b(1) n1 )   (cid:0)W(1)x + b(1)(cid:1) + b(out) .  (2)  (3)  f (x) = W(out)diag  i,: x + b(1)  From this formulation it is clear that each unit i in the hidden layer has two operational modes. One is when the unit takes value 0 and one when it takes a non-zero value. The boundary between these two operational modes is given by the hyperplane Hi consisting of all inputs x ∈ Rn0 with W(1) i = 0. Below this hyperplane, the activation of the unit is constant equal to zero, and above, it is linear with gradient equal to W(1) i,: . It follows that the number of regions of linearity of a single layer MLP is equal to the number of regions formed by the set of hyperplanes {Hi}i∈[n1]. A ﬁnite set of hyperplanes in a common n0-dimensional Euclidian space is called an n0-dimensional hyperplane arrangement. A region of an arrangement A = {Hi ⊂ Rn0}i∈[n] is a connected component of the complement of the union of the hyperplanes, i.e., a connected component of Rn0 \ (∪i∈[n]Hi). To make this clearer, consider an arrangement A consisting of hyperplanes Hi = {x ∈ Rn0 : Wi,:x + bi = 0} for all i ∈ [n], for some W ∈ Rn×n0 and some b ∈ Rn. A region of A is a set of points of the form R = {x ∈ Rn0 : sgn(Wx + b) = s} for some sign vector s ∈ {−, +}n. A region of an arrangement is relatively bounded if its intersection with the space spanned by the normals of the hyperplanes is bounded. We denote by r(A) the number of regions and by b(A) the number of relatively bounded regions of an arrangement A. The essentialization of an arrangement A = {Hi}i is the arrangement consisting of the hyperplanes Hi ∩N for all i, deﬁned in the span N of the normals of the hyperplanes Hi. For example, the essentialization of an arrangement of two non-parallel planes in R3 is an arrangement of two lines in a plane. Problem 1. How many regions are generated by an arrangement of n hyperplanes in Rn0? The general answer to Problem 1 is given by Zaslavsky’s theorem (Zaslavsky, 1975, Theorem A), which is one of the central results from the theory of hyperplane arrangements. We will only need the special case of hyperplanes in general position, which realize the maximal possible number of regions. Formally, an n-dimensional arrangement A is in general position if for any subset {H1, . . . Hp} ⊆ A the following holds. (1) If p ≤ n, then dim(H1 ∩ ··· ∩ Hp) = n − p. (2) If p > n, then H1 ∩ ··· ∩ Hp = ∅. An arrangement is in general position if the weights W, b deﬁning its hyperplanes are generic. This means that any arrangement can be perturbed by an arbitrarily small perturbation in such a way that the resulting arrangement is in general position. For arrangements in general position, Zaslavsky’s theorem can be stated in the following way (see Stanley, 2004, Proposition 2.4). Proposition 1. Let A be an arrangement of m hyperplanes in general position in Rn0. Then  In particular, the number of regions of a 2-dimensional arrangement Am of m lines in general position is equal to (4)  + m + 1.  r(A) =  b(A) =  s  s=0  (cid:18)m (cid:19) n0(cid:88) (cid:18)m − 1 (cid:19) (cid:18)m (cid:19)  n0  .  r(Am) =  2  5  Figure 2: Induction step of the hyperplane sweep method for counting the regions of line arrange- ments in the plane.  Induction step. Assume that for m lines the number of regions is r(Am) =(cid:0)m  For the purpose of illustration, we sketch a proof of eq. (4) using the sweep hyperplane method. We proceed by induction over the number of lines m. Base case m = 0. It is obvious that in this case there is a single region, corresponding to the entire plane. Therefore, r(A0) = 1. a new line Lm+1 to the arrangement. Since we assumed the lines are in general position, Lm+1 intersects each of the existing lines Lk at a different point. Fig. 2 depicts the situation for m = 2. The m intersection points split the line Lm+1 into m + 1 segments. Each of these segments cuts a region of Am in two pieces. Therefore, by adding the line Lm+1 we get m + 1 new regions. In Fig. 2 the two intersection points result in three segments that split each of the regions R1, R01, R0 in two. Hence  (cid:1) + m + 1, and add  2  r(Am+1) = r(Am) + m + 1 =  m(m − 1)  2  + m + 1 + m + 1 =  m(m + 1)  2  + (m + 1) + 1  (cid:18)m + 1  (cid:19)  =  2  + (m + 1) + 1.  For the number of response regions of MLPs with one single hidden layer we obtain the following. Proposition 2. The regions of linearity of a function in the model F(n0,n1,1) with n0 inputs and n1 hidden units are given by the regions of an arrangement of n1 hyperplanes in n0-dimensional space.  The maximal number of regions of such an arrangement is R(n0, n1, ny) =(cid:80)n0  (cid:0)n1  (cid:1).  j=0  j  Proof. This is a consequence of Lemma 1. The maximal number of regions is produced by an n0- dimensional arrangement of n1 hyperplanes in general position, which is given in Proposition 1.  4 Multiple hidden layers  In order to show that a k hidden layer model can be more expressive than a single hidden layer one with the same number of hidden units, we will need the next three propositions. Proposition 3. Any arrangement can be scaled down and shifted such that all regions of the ar- rangement intersect the unit ball.  Proof. Let A be an arrangement and let S be a ball of radius r and center c. Let d be the supremum of the distance from the origin to a point in a bounded region of the essentialization of the arrange- (cid:48) = φ(A) is an ment A. Consider the map φ : Rn0 → Rn0 deﬁned by φ(x) = r arrangement satisfying the claim. It is easy to see that any point with norm bounded by d is mapped to a point inside the ball S.  2d · x + c. Then A  6  L1L2R12R1R∅R2L1L2R12R1R∅R2R23R123R13L3Figure 3: An arrangement A and a scaled-shifted version A  (cid:48) whose regions intersect the ball S.  The proposition is illustrated in Fig. 3. We need some additional notations in order to formulate the next proposition. Given a hyperplane H = {x : w(cid:62)x + b = 0}, we consider the region H− = {x : w(cid:62)x + b < 0}, and the region H + = {x : w(cid:62)x + b ≥ 0}. If we think about the corresponding rectiﬁer unit, then H + is the region where the unit is active and H− is the region where the unit is dead. Let R be a region delimited by the hyperplanes {H1, . . . , Hn}. We denote by R+ ⊆ {1, . . . , n} the j . In other words, R+ is the list of hidden units that are set of all hyperplane-indices j with R ⊂ H + active (non-zero) in the input-space region R. The following proposition describes the combinatorics of 2-dimensional arrangements in general position. More precisely, the proposition describes the combinatorics of n-dimensional arrange- ments with 2-dimensional essentialization in general position. Recall that the essentialization of an arrangement is the arrangement that it deﬁnes in the subspace spanned by the normals of its hyperplanes. The proposition guarantees the existence of input weights and bias for a rectiﬁer layer such that for any list of consecutive units, there is a region of inputs for which exactly the units from that list are active. Proposition 4. For any n0, n ∈ N, n ≥ 2, there exists an n0-dimensional arrangement A of n hyperplanes such that for any pair a, b ∈ {1, . . . , n} with a < b, there is a region R of A with R+ = {a, a + 1, . . . , b}. We show that the hyperplanes of a 2-dimensional arrangement in general position can be indexed in such a way that the claim of the proposition holds. For higher dimensional arrangements the state- ment follows trivially, applying the 2-dimensional statement to the intersection of the arrangement with a 2-subspace.  Proof of Proposition 4. Consider ﬁrst the case n0 = 2. We deﬁne the ﬁrst line L1 of the arrangement to be the x-axis of the standard coordinate system. To deﬁne the second line L2, we consider a circle S1 of radius r ∈ R+ centered at the origin. We deﬁne L2 to be the tangent of S1 at an angle α1 to the y-axis, where 0 < α1 < π 2 . The top left panel of Fig. 4 depicts the situation. In the ﬁgure, R∅ corresponds to inputs for which no rectiﬁer unit is active, R1 corresponds to inputs where the ﬁrst unit is active, R2 to inputs where the second unit is active, and R12 to inputs where both units are active. This arrangement has the claimed properties. Now assume that there is an arrangement of n lines with the claimed properties. To add an (n + 1)-th line, we ﬁrst consider the maximal distance dmax from the origin to the intersection of two lines Li ∩ Lj with 1 ≤ i < j < n. We also consider the radius-(dmax + r) circle Sn centered at the origin. The circle Sn contains all intersection of any of the ﬁrst n lines. We now choose an angle αn with 0 < αn < αn−1 and deﬁne Ln+1 as the tangent of Sn that forms an angle αn with the y-axis. Fig. 4 depicts adding the third and fourth line to the arrangement. After adding line Ln+1, we have that the arrangement  1. is in general position. 2. has regions R(cid:48) 1, . . . , R(cid:48)  n+1 with R  (cid:48)+ i = {i, i + 1, . . . , n + 1} for all i ∈ [n + 1].  7  AA0SFigure 4: Illustration of the hyperplane arrangement discussed in Proposition 4, in the 2-dimensional case. On the left we have arrangements of two and three lines, and on the right an arrangement of four lines.  The regions of the arrangement are stable under perturbation of the angles and radii used to deﬁne the lines. Any slight perturbation of these parameters preserves the list of regions. Therefore, the arrangement is in general position. The second property comes from the order in which Ln+1 intersects all previous lines. Ln+1 in- tersects the lines in the order in which they were added to the arrangement: L1, L2, . . . , Ln. The intersection of Ln+1 and Li, Bin+1 = Ln+1 ∩ Li, is above the lines Li+1, Li+2, . . . , Ln, and hence the segment Bi−1n+1Bin+1 between the intersection with Li−1 and with Li, has to cut the region in which only units i to n are active. The intersection order is ensured by the choice of angles αi and the fact that the lines are tangent to the circles Si. For any i < j and Bij = Li ∩ Lj let Tij be the line parallel to the y-axis passing through Bij. Each line Tij divides the space in two. Let Hij be the half-space to the right of Tij. Within any half-space Hij, the intersection Hij ∩ Li is above Hij ∩ Lj, because the angle αi−1 of Li with the y-axis is larger than αj−1 (this means Lj has a stepper decrease). Since Ln+1 is tangent to the circle that contains all points Bij, the line Ln+1 will intersect lines Li and Lj in Hij, and therefore it has to intersect Li ﬁrst. For n0 > 2 we can consider an arrangement that is essentially 2-dimensional and has the properties of the arrangement described above. To do this, we construct a 2-dimensional arrangement in a 2-subspace of Rn0 and then extend each of the lines Li of the arrangement to a hyperplane Hi that crosses Li orthogonally. The resulting arrangement satisﬁes all claims of the proposition.  The next proposition guarantees the existence of a collection of afﬁne maps with shared bias, which map a collection of regions to a common output. Proposition 5. Consider two integers n0 and p. Let S denote the n0-dimensional unit ball and let R1, . . . , Rp ⊆ Rn0 be some regions with non-empty interiors. Then there is a choice of weights c ∈ Rn0 and U1, . . . , Up ∈ Rn0×n0 for which gi(Ri) ⊇ S for all i ∈ [p], where gi : Rn0 → Rn0; y (cid:55)→ Uiy + c.  8  R∅R1L1L2R12R2S1y-axisα1R∅R1L1L2L3R12R123R23R3R2S1S2y-axisα1α2R4R∅R1L1L2L3L4R12R123R1234R23R234R3R34R2S1S2S3y-axisα3α2α1Figure 5: Illustration of Example 1. The units represented by squares build an intermediary layer of linear units between the ﬁrst and the second hidden layers. The computation of such an intermediary linear layer can be absorbed in the second hidden layer of rectiﬁer units (Lemma 3). The connectivity map depicts the maps g1 by dashed arrows and g2 by dashed-dotted arrows.  Proof. To see this, consider the following construction. For each region Ri consider a ball Si ⊆ Ri of radius ri ∈ R+ and center si = (si1, . . . , sin0 ) ∈ Rn0. For each j = 1, . . . , n0, consider p positive numbers u1j, . . . , upj such that uijsij = ukjskj for all 1 ≤ k < i ≤ p. This can be done ﬁxing u1j equal to 1 and solving the equation for all other numbers. Let η ∈ R be such that riηuij > 1 for any j and i. Scaling each region Ri by Ui = diag(ηui0, . . . , ηuin0) transforms the center of Si to the same point for all i. By the choice of η, the minor radius of all transformed balls is larger than 1. We can now set c to be minus the common center of the scaled balls, to obtain the map:  gi(x) = diag (ηui1, . . . , ηuin0 ) x − diag (ηu11, . . . , ηu1n0) s1,  for all 1 ≤ i ≤ p.  These gi satisfy claimed property, namely that gi(Ri) contains the unit ball, for all i.  Before proceeding, we discuss an example illustrating how the previous propositions and lemmas are put together to prove our main result below, in Theorem 1. Example 1. Consider a rectiﬁer MLP with n0 = 2, such that the input space is R2, and assume that the network has only two hidden layers, each consisting of n = 2n(cid:48) units. Each unit in the ﬁrst hidden layer deﬁnes a hyperplane in R2, namely the hyperplane that separates the inputs for which it is active, from the inputs for which it is not active. Hence the ﬁrst hidden layer deﬁnes an arrangement of n hyperplanes in R2. By Proposition 4, this arrangement can be made such that it delimits regions of inputs R1, . . . , Rn(cid:48) ⊆ R2 with the following property. For each input in any given one of these regions, exactly one pair of units in the ﬁrst hidden layer is active, and, furthermore, the pairs of units that are active on different regions are disjoint. By the deﬁnition of rectiﬁer units, each hidden unit computes a linear function within the half-space of inputs where it is active. In turn, the image of Ri by the pair of units that is active in Ri is a polyhedron in R2. For each region Ri, denote corresponding polyhedron by Si. Recall that a rectiﬁer layer computes a map of the form f : Rn → Rm; x (cid:55)→ rect(Wx + b). Hence a rectiﬁer layer with n inputs and m outputs can compute any composition f(cid:48) ◦ g of an afﬁne map g : Rn → Rk and a map f(cid:48) computed by a rectiﬁer layer with k inputs and m outputs (Lemma 3). Consider the map computed by the rectiﬁer units in the second hidden layer, i.e., the map that takes activations from the ﬁrst hidden layer and outputs activations from the second hidden layer. We think of this map as a composition f(cid:48) ◦ g of an afﬁne map g : Rn → R2 and a map f(cid:48) computed by a rectiﬁer layer with 2 inputs. The map g can be interpreted as an intermediary layer consisting of two linear units, as illustrated in Fig. 5.  9  1sthiddenlayerIntermediatelayerInput2ndhiddenlayerh(1)g(h(1))rect(V(2)g(h(1))+d(2))Figure 6: Constructing  (cid:106) n1  n0  (cid:107)(cid:80)n0  k=0  (cid:0)n2  k  (cid:1) response regions in a model with two layers.  Within each input region Ri, only two units in the ﬁrst hidden layer are active. Therefore, for each input region Ri, the output of the intermediary layer is an afﬁne transformation of Si. Furthermore, the weights of the intermediary layer can be chosen in such a way that the image of each Ri contains the unit ball. Now, f(cid:48) is the map computed by a rectiﬁer layer with 2 inputs and n outputs. It is possible to deﬁne this map in such a way that it has R regions of linearity within the unit ball, where R is the number of regions of a 2-dimensional arrangement of n hyperplanes in general position. We see that the entire network computes a function which has R regions of linearity within each one of the input regions R1, . . . , Rn(cid:48). Each input region Ri is mapped by the concatenation of ﬁrst and intermediate (notional) layer to a subset of R2 which contains the unit ball. Then, the second layer computes a function which partitions the unit ball into many pieces. The partition computed by the second layer gets replicated in each of the input regions Ri, resulting in a subdivision of the input space in exponentially many pieces (exponential in the number of network layers).  Now we are ready to state our main result on the number of response regions of rectiﬁer deep feedforward networks: Theorem 1. A model with n0 inputs and k hidden layers of widths n1, n2, . . . , nk can divide the input space in  (cid:1) or possibly more regions.  (cid:107)(cid:17)(cid:80)n0  (cid:0)nk  i=0  i  n0  Proof of Theorem 1. Let the ﬁrst hidden layer deﬁne an arrangement like the one from Proposition 4. input-space regions Ri ⊆ Rn0, i ∈ [p] with the following property. For Then there are p = each input vector from the region Ri, exactly n0 units from the ﬁrst hidden layer are active. We denote this set of units by Ii. Furthermore, by Proposition 4, for inputs in distinct regions Ri, the corresponding set of active units is disjoint; that is, Ii ∩ Ij = ∅ for all i, j ∈ [p], i (cid:54)= j. To be more speciﬁc, for an input vectors from R1, exactly the ﬁrst n0 units of the ﬁrst hidden layer are active, that is, for these input vectors the value of h(1) is non-zero if and only if j ∈ I1 = {1, . . . , n0}. For input vectors from R2, only the next n0 units of the ﬁrst hidden layer are active, that is, the units with index in I2 = {n0 + 1, . . . , 2n0}, and so on. Now we consider a ‘ﬁctitious’ intermediary layer consisting of n0 linear units between the ﬁrst and second hidden layers. As this intermediary layer computes an afﬁne function, it can be absorbed into the second hidden layer (see Lemma 3). We use it only for making the next arguments clearer.  j  i=1  (cid:16)(cid:81)k−1 (cid:106) ni (cid:107) (cid:106) n1  n0  10  g1(R1)g2(R2)R1R2g1g2SThe map taking activations from the ﬁrst hidden layer to activations from the second hidden layer is rect(W(2)x + b(2)), where W(2) ∈ Rn2×n1, b(2) ∈ Rn2. We can write the input and bias weight matrices as W(2) = U(2)V(2) and b(2) = d(2) + V(2)c(2), where U(2) ∈ Rn0×n1, c ∈ Rn0, and V(2) ∈ Rn2×n0, d ∈ Rn2. The weights U(2) and c(2) describe the afﬁne function computed by the intermediary layer, x (cid:55)→ U(2)x + c. The weights V(2) and d(2) are the input and bias weights of the rectiﬁer layer following the intermediary layer.  (cid:104)  p | ˜U(2)(cid:105)  i  i  i  i∈[p] gi + c(2),  U(2) 1 |···|U(2)  i x, for all i ∈ [p].  and bias c(2) such that the image of Si by x → U(2)  of U(2) consisting of the columns of U(2) with indices Ii, for , where ˜U(2) is the sub-matrix of U(2) consisting  The map g : Rn1 → Rn0; g(x) = U(2)x + c(2) is thus written as the sum g =(cid:80)  We now consider the sub-matrix U(2) all i ∈ [p]. Then U(2) = of its last n1 − pn0 columns. In the sequel we set all entries of ˜U(2) equal to zero. where gi : Rn0 → Rn0 ; gi(x) = U(2) Let Si be the image of the input-space region Ri by the ﬁrst hidden layer. By Proposition 5, there is a choice of the weights U(2) (x) + c(2) contains the n0-dimensional unit ball. Now, for all inputs vectors from Ri, only the units Ii of the ﬁrst hidden layer are active. Therefore, g|Ri = gi|Ri + c(2). This implies that the image g(Ri) of the input-space region Ri by the intermediary layer contains the unit ball, for all i ∈ [p]. We can now choose V(2) and d(2) in such a way that the rectiﬁer function Rn0 → Rn2 ; y (cid:55)→ rect(V(2)y + d(2)) deﬁnes an arrangement A of n2 hyperplanes with the property that each region of A intersects the unit ball at an open neighborhood. In consequence, the map from input-space to activations of the second hidden layer has r(A) regions of linearity within each input-space region Ri. Fig. 6 illustrates the situation. All inputs that are mapped to the same activation of the ﬁrst hidden layer, are treated as equivalent on the subsequent layers. In this sense, an arrangement A deﬁned on the set of common outputs of R1, . . . , Rp at the ﬁrst hidden layer, is ‘replicated’ in each input region R1, . . . , Rp. The subsequent layers of the network can be analyzed in a similar way as done above for the ﬁrst two layers. In particular, the weights V(2) and d(2) can be chosen in such a way that they deﬁne an arrangement with the properties from Proposition 4. Then, the map taking activations from the second hidden layer to activations from the third hidden layer, can be analyzed by considering again a ﬁctitious intermediary layer between the second and third layers, and so forth, as done above. For the last hidden layer we choose the input weights V(k) and bias d(k) deﬁning an n0-dimensional arrangement of nk hyperplanes in general position. The map of inputs to activations of the last hid- den layer has thus the maximal number of regions of linearity of functions computable by the network. This completes the proof. The intuition of the construction is illustrated in Fig. 7.  (cid:1) regions of linearity. This number is a lower bound on  (cid:107)(cid:17)(cid:80)n0  (cid:16)(cid:81)k−1  (cid:106) ni  (cid:0)nk  i=0  i=1  n0  i  In the Appendix A we derive an asymptotic expansion of the bound given in Theorem 1.  5 A special class of deep models  In this section we consider deep rectiﬁer models with n0 input units and hidden layers of width n = 2n0. This restriction allows us to construct a very efﬁcient deep model in terms of number of response regions. The analysis that we provide in this section complements the results from the previous section, showing that rectiﬁer MLPs can compute functions with many response regions, even when deﬁned with relatively few hidden layers. Example 2. Let us assume we have a 2-dimensional input, i.e., n0 = 2, and a layer of n = 4 rectiﬁers f1, f2, f3, and f4, followed by a linear projection. We construct the rectiﬁer layer in such a way that it divides the input space into four ‘square’ cones; each of them corresponding to the inputs  11  where two of the rectiﬁer units are active. We deﬁne the four rectiﬁers as:  Figure 7: Constructing  (cid:106) n2  n0  (cid:107)(cid:106) n1  n0  (cid:107)(cid:80)n0  k=0  f1(x) = max  f2(x) = max  f3(x) = max  f4(x) = max  ,  k  (cid:62)  x (cid:62)  0, [1, 0]  (cid:111) (cid:111)  (cid:0)n3 (cid:1) response regions in a model with three layers. (cid:110) (cid:110) (cid:110) (cid:110) (cid:21) f1(x)  (cid:111) (cid:111)  =  (cid:20) abs(x1)  0, [−1, 0] (cid:62) 0, [0, 1]  0, [0,−1]  (cid:21)  x (cid:62)  abs(x2)  0 1  x  x  ,  ,  ,  .  f2(x) f3(x) f4(x)  (cid:62) where x = [x1, x2] effectively mimic a layer consisting of two absolute-value units g1 and g2:  ∈ Rn0. By adding pairs of coordinates of f = [f1, f2, f3, f4] (cid:20) g1(x)  (cid:20) 1  (cid:21)  (cid:62), we can  (5)  g2(x)  =  0  1 0  0 1  The absolute-value unit gi divides the input space along the i-th coordinate axis, taking values which are symmetric about that axis. The combination of g1 and g2 is then a function with four regions of linearity;  S1 ={(x1, x2) | x1 ≥ 0, x2 ≥ 0} S2 ={(x1, x2) | x1 ≥ 0, x2 < 0} S3 ={(x1, x2) | x1 < 0, x2 ≥ 0} S4 ={(x1, x2) | x1 < 0, x2 < 0} .  Since the values of gi are symmetric about the i-th coordinate axis, each point x ∈ Si has a corre- sponding point y ∈ Sj with g(x) = g(y), for all i and j. We can apply the same procedure to the image of [g1, g2] to recursively divide the input space, as illustrated in Fig. 8. For instance, if we apply this procedure one more time, we get four regions  12  g(2)1(R1)g(2)2(R1)R1R2g(3)1(g(2)2(R2)))g(3)1(g(2)1(R2))g(3)1(g(2)1(R1))g(3)1(g(2)1(R1))g(2)1g(2)2g(3)1g(3)2SS(a)  (b)  Figure 8: Illustration of Example 2. (a) A rectiﬁer layer with two pairs of units, where each pair computes the absolute value of one of two input coordinates. Each input quadrant is mapped to the positive quadrant. (b) Depiction of a two layer model. Both layers simulate the absolute value of their input coordinates.  within each Si, resulting in 16 regions in total, within the input space. On the last layer, we may place rectiﬁers in any way suitable for the task of interest (e.g., classiﬁcation). The partition computed by the last layer will be copied to each of the input space regions that produced the same input for the last layer. Fig. 9 shows a function that can be implemented efﬁciently by a deep model using the previous observations.  (a)  (b)  (c)  Figure 9: (a) Illustration of the partition computed by 8 rectiﬁer units on the outputs (x1, x2) of the preceding layer. The color is a heat map of x1 − x2. (b) Heat map of a function computed by a rectiﬁer network with 2 inputs, 2 hidden layers of width 4, and one linear output unit. The black lines delimit the regions of linearity of the function. (c) Heat map of a function computed by a 4 layer model with a total of 24 hidden units. It takes at least 137 hidden units on a shallow model to represent the same function.  The foregoing discussion can be easily generalized to n0 > 2 input variables and k hidden layers, each consisting of 2n0 rectiﬁers. In that case, the maximal number of linear regions of functions computable by the network is lower-bounded as follows. Theorem 2. The maximal number of regions of linearity of functions computable by a rec- tiﬁer neural network with n0 input variables and k hidden layers of width 2n0 is at least  2(k−1)n0(cid:80)n0  (cid:0)2n0  (cid:1).  j=0  j  Proof. We prove this constructively. We deﬁne the rectiﬁer units in each hidden layer in pairs, with the sum of each pair giving the absolute value of a coordinate axis. We interpret also the sum of such pairs as the actual input coordinates of the subsequent hidden layers. The rectiﬁers in the ﬁrst hidden layer are deﬁned in pairs, such that the sum of each pair is the absolute value of one of the input 2 ). In the next hidden layers, the rectiﬁers are deﬁned dimensions, with bias equal to (− 1  2 , . . . ,− 1  13  PS1PS3PS4PS2-4-242x0x1x0x1Pin a similar way, with the difference that each pair computes the absolute value of the sum of two of their inputs. The last hidden layer is deﬁned in such a way that it computes a piece-wise linear function with the maximal number of pieces, all of them intersecting the unit cube in Rn0. The  maximal number of regions of linearity of m rectiﬁer units with n0-dimensional input is(cid:80)n0  (cid:0)m (cid:1).  j=0  j  This partition is multiplied in each previous layer 2n0 times.  The theorem shows that even for a small number of layers k, we can have many more linear regions in a deep model than in a shallow one. For example, if we set the input dimensionality to n0 = 2, a shallow model with 4n0 units will have at most 37 linear regions. The equivalent deep model with two layers of 2n0 units can produce 44 linear regions. For 6n0 hidden units the shallow model computes at most 79 regions, while the equivalent three layer model can compute 176 regions.  6 Discussion and conclusions  In this paper we introduced a novel way of understanding the expressiveness of neural networks with piecewise linear activations. We count the number of regions of linearity, also called response regions, of the functions that they can represent. The number of response regions tells us how well the models can approximate arbitrary curved shapes. Computational Geometry provides us the tool to make such statements. We found that deep and narrow rectiﬁer MPLs can generate many more regions of linearity than their shallow counterparts with the same number of computational units or of parameters. We can express this in terms of the ratio between the maximal number of response regions and the number of parameters of both model classes. For a deep model with n0 = O(1) inputs and k hidden layers of width n, the maximal number of response regions per parameter behaves as  Ω  (cid:33)  .  (cid:32)(cid:22) n (cid:23)k−1 nn0−2 O(cid:0)kn0−1nn0−1(cid:1) .  n0  k  For a shallow model with n0 = O(1) inputs, the maximal number of response regions per parameter behaves as  targ) = ftarg, when σ has an inverse (e.g., with sigmoid), f(cid:48)  We see that the deep model can generate many more response regions per parameter than the shallow model; exponentially more regions per parameter in terms of the number of hidden layers k, and at least order (k − 2) polynomially more regions per parameter in terms of the layer width n. In particular, there are deep models which use fewer parameters to produce more linear regions than their shallow counterparts. Details about the asymptotic expansions are given in the Appendix A. In this paper we only considered linear output units, but this is not a restriction, as the output ac- tivation itself is not parametrized. If there is a target function ftarg that we want to model with a rectiﬁer MLP with σ as its output activation function, then there exists a function f(cid:48) targ such that σ(f(cid:48) targ = σ−1(ftarg). For activations that do not have an inverse, like softmax, there are inﬁnitely many functions f(cid:48) targ that work. We just need to pick one, e.g., for softmax we can pick log(ftarg). By analyzing how well we can model f(cid:48) targ with a linear output rectiﬁer MLP we get an indirect measure of how well we can model ftarg with an MLP that has σ as its output activation. Another interesting observation is that we recover a high ratio of n to n0 if the data lives near a low- dimensional manifold (effectively like reducing the input size n0). One-layer models can reach the upper bound of response regions only by spanning all the dimensions of the input. In other words, shallow models are not capable of concentrating linear response regions in any lower dimensional subspace of the input. If, as commonly assumed, data lives near a low dimensional manifold, then we care only about the number of response regions that a model can generate in the directions of the data manifold. One way of thinking about this is principal component analysis (PCA), where one ﬁnds that only few input space directions (say on the MNIST database) are relevant to the underlying data. In such a situation, one cares about the number of response regions that a model can generate only within the directions in which the data does change. In such situations n (cid:29) n0, and our results show a clear advantage of using deep models.  14  We believe that the proposed framework can be used to answer many other interesting questions about these models. For example, one can look at how the number of response regions is affected by different constraints of the model, like shared weights. We think that this approach can also be used to study other kinds of piecewise linear models, such as convolutional networks with rectiﬁer units or maxout networks, or also for comparing between different piecewise linear models.  A Asymptotic  Here we derive asymptotic expressions of the formulas contained in Proposition 2 and Theorem 1. We use following standard notation:  n larger than some N.  • f (n) = O(g(n)) means that there is a positive constant c2 such that f (n) ≤ c2g(n) for all • f (n) = Θ(g(n)) means that there are two positive constants c1 and c2 such that c1g(n) ≤ • f (n) = Ω(g(n)) means that there is a positive constant c1 such that f (n) ≥ c1g(n) for all  f (n) ≤ c2g(n) for all n larger than some N. n larger than some N.  Proposition 6.  • Consider a single layer rectiﬁed MLP with kn units and n0 inputs. Then the maximal  number of regions of linearity of the functions represented by this network is  n0(cid:88)  (cid:18)kn  (cid:19)  s  s=0  ,  R(n0, kn, 1) =  • Consider a k layer rectiﬁed MLP with hidden layers of width n and n0 inputs. Then the maximal number of regions of linearity of the functions represented by this network satisﬁes  R(n0, kn, 1) = O(kn0nn0), when n0 = O(1). (cid:19)  R(n0, n, . . . , n, 1) ≥  (cid:18)n  s  ,  and  and  R(n0, n, . . . , n, 1) = Ω  , when n0 = O(1).  Proof. Here only the asymptotic expressions remain to be shown. It is known that  (cid:32)k−1(cid:89) (cid:23)k−1  i=1  (cid:23)(cid:33) n0(cid:88) (cid:22) n (cid:33)  n0  s=0  n0  nn0  (cid:19)(cid:33)  (cid:32)(cid:22) n (cid:19)−1(cid:18)m m )(cid:1) , when s = O(1). nn0(cid:0)1 + O(cid:0) 1  , when n0 ≤  (cid:1)(cid:1) .  n0  kn  = Θ (kn0nn0)  and also  15  m  2 − √m.  (6)  (7)  (cid:18)n  (cid:19)  s  n0(cid:88)  s=0  = Θ(nn0).  n0(cid:88)  (cid:32)(cid:18) (cid:19) (cid:18)m Furthermore, it is known that(cid:18)m (cid:19)  = Θ  s=0  s  2n0 m  1 −  (cid:0)1 + O( 1  =  ms s!  s  When n0 is constant, n0 = O(1), we have that kn0 n0!  n0  =  In this case, it follows that  n0(cid:88)  (cid:18)kn (cid:19)  s  s=0  (cid:32)(cid:18)  = Θ  2n0 kn  1 −  (cid:19) (cid:18)kn (cid:19)−1(cid:18)kn  n0  (cid:19)(cid:33)  Furthermore,  (cid:32)k−1(cid:89)  (cid:22) n  (cid:23)(cid:33) n0(cid:88)  (cid:18)n  (cid:19)  n0  i=1  s  s=0  (cid:32)(cid:22) n  (cid:23)k−1  n0  (cid:33)  nn0  .  = Θ  We now analyze the number of response regions as a function of the number of parameters. When  k and n0 are ﬁxed, then (cid:98)n/n0(cid:99)k−1 grows polynomially in n, and kn0 is constant. On the other hand, when n is ﬁxed with n > 2n0, then (cid:98)n/n0(cid:99)k−1 grows exponentially in k, and kn0 grows polynomially in k. Proposition 7. The number of parameters of a deep model with n0 = O(1) inputs, nout = O(1) outputs, and k hidden layers of width n is  (k − 1)n2 + (k + n0 + nout)n + nout = O(kn2).  The number of parameters of a shallow model with n0 = O(1) inputs, nout = O(1) outputs, and kn hidden units is  (n0 + nout)kn + n + nout = O(kn).  Proof. For the deep model, each layer, except the ﬁrst and last, has an input weight matrix with n2 entries and a bias vector of length n. This gives a total of (k− 1)n2 + (k− 1)n parameters. The ﬁrst layer has nn0 input weights and n bias. The output layer has nnout input weight matrix and nout bias. If we sum these together we get  (k − 1)n2 + n(k + n0 + nout) + nout = O(kn2).  For the shallow model, the hidden layer has knn0 input weights and kn bias. The output weights has knnout input weights and nout bias. Summing these together we get  kn(n0 + nout) + n + nout = O(kn).  The number of linear regions per parameter can be given as follows. Proposition 8. Consider a ﬁxed number of inputs n0 and a ﬁxed number of outputs nout. The maximal ratio of the number of response regions to the number of parameters of a deep model with k layers of width n is  In the case of a shallow model with kn hidden units, the ratio is  Ω  (cid:32)(cid:22) n (cid:33) (cid:23)k−1 nn0−2 O(cid:0)kn0−1nn0−1(cid:1) .  n0  k  .  Proof. This is by combining Proposition 6 and Proposition 7.  We see that ﬁxing the number of parameters, deep models can compute functions with many more regions of linearity that those computable by shallow models. The ratio is exponential in the number of hidden layers k and thus in the number of hidden units.  Acknowledgments  We would like to thank KyungHyun Cho, C¸ a˘glar G¨ulc¸ehre, and anonymous ICLR reviewers for their comments. Razvan Pascanu is supported by a DeepMind Fellowship.  16  References Y. Bengio. Learning deep architectures for AI. Foundations and trends R(cid:13) in Machine Learning, 2(1):1–127,  2009.  Y. Bengio and O. Delalleau. On the expressive power of deep architectures.  In J. Kivinen, C. Szepesvri, E. Ukkonen, and T. Zeugmann, editors, Algorithmic Learning Theory, volume 6925 of Lecture Notes in Computer Science, pages 18–36. Springer Berlin Heidelberg, 2011.  X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectiﬁer neural networks. In AISTATS, 2011. I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio. Maxout networks. In ICML’2013,  2013.  A. Hajnal, W. Maass, P. Pudlk, M. Szegedy, and G. Turn. Threshold circuits of bounded depth. Journal of  Computer and System Sciences, 46(2):129–154, 1993.  J. H˚astad. Almost optimal lower bounds for small depth circuits.  In Proceedings of the 18th annual ACM  Symposium on Theory of Computing, pages 6–20, Berkeley, California, 1986. ACM Press.  J. H˚astad and M. Goldmann. On the power of small-depth threshold circuits. Computational Complexity, 1:  113–129, 1991.  G. Hinton, L. Deng, G. E. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal Processing Magazine, 29(6):82–97, Nov. 2012a.  G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. Salakhutdinv. Improving neural networks by  preventing co-adaptation of feature detectors. Technical report, arXiv:1207.0580, 2012b.  N. Le Roux and Y. Bengio. Deep belief networks are compact universal approximators. Neural Computation,  22(8):2192–2207, 2010.  H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolutional deep belief networks for scalable unsupervised  learning of hierarchical representations. Montreal (QC), Canada, 2009.  J. Martens, A. Chattopadhya, T. Pitassi, and R. Zemel. On the expressive power of restricted boltzmann  machines. In Advances in Neural Information Processing Systems 26, pages 2877–2885. 2013.  G. Mont´ufar and N. Ay. Reﬁnements of universal approximation results for deep belief networks and restricted  Boltzmann machines. Neural Computation, 23(5):1306–1319, 2011.  G. Mont´ufar and J. Morton. When does a mixture of products contain a product of mixtures? arXiv preprint  arXiv:1206.0387, 2012.  G. Mont´ufar, J. Rauh, and N. Ay. Expressive power and approximation errors of restricted Boltzmann machines.  Advances in Neural Information Processing Systems, 24:415–423, 2011.  V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted Boltzmann machines. pages 807–814, 2010. H. Poon and P. Domingos. Sum-product networks: A new deep architecture. In Computer Vision Workshops  (ICCV Workshops), 2011 IEEE International Conference on, pages 689–690, 2011.  R. Stanley. An introduction to hyperplane arrangements. In Lect. notes, IAS/Park City Math. Inst., 2004. I. Sutskever and G. E. Hinton. Deep, narrow sigmoid belief networks are universal approximators. Neural  Computation, 20(11):2629–2636, 2008.  T. Zaslavsky. Facing Up to Arrangements: Face-Count Formulas for Partitions of Space by Hyperplanes. Number no. 154 in Memoirs of the American Mathematical Society. American Mathematical Society, 1975. M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. Technical report,  arXiv:1311.2901, 2013.  17  ","This paper explores the complexity of deep feedforward networks with linearpre-synaptic couplings and rectified linear activations. This is a contributionto the growing body of work contrasting the representational power of deep andshallow network architectures. In particular, we offer a framework forcomparing deep and shallow models that belong to the family of piecewise linearfunctions based on computational geometry. We look at a deep rectifiermulti-layer perceptron (MLP) with linear outputs units and compare it with asingle layer version of the model. In the asymptotic regime, when the number ofinputs stays constant, if the shallow model has $kn$ hidden units and $n_0$inputs, then the number of linear regions is $O(k^{n_0}n^{n_0})$. For a $k$layer model with $n$ hidden units on each layer it is $\Omega(\left\lfloor{n}/{n_0}\right\rfloor^{k-1}n^{n_0})$. The number$\left\lfloor{n}/{n_0}\right\rfloor^{k-1}$ grows faster than $k^{n_0}$ when $n$tends to infinity or when $k$ tends to infinity and $n \geq 2n_0$.Additionally, even when $k$ is small, if we restrict $n$ to be $2n_0$, we canshow that a deep model has considerably more linear regions that a shallow one.We consider this as a first step towards understanding the complexity of thesemodels and specifically towards providing suitable mathematical tools forfuture analysis."
1312.6108,2014,Modeling correlations in spontaneous activity of visual cortex with centered Gaussian-binary deep Boltzmann machines  ,"['nan wang', 'Laurenz Wiskott', 'Dirk Jancke']",https://arxiv.org/pdf/1312.6108.pdf,"4 1 0 2     b e F 7 1         ] E N . s c [      3 v 8 0 1 6  .  2 1 3 1 : v i X r a  Modeling correlations in spontaneous activity of visual cortex with centered Gaussian-binary deep  Boltzmann machines  Nan Wang  Institut f¨ur Neuroinformatik Ruhr-Universit¨at Bochum Bochum, 44780, Germany nan.wang@ini.rub.de  Dirk Jancke  Institut f¨ur Neuroinformatik Ruhr-Universit¨at Bochum Bochum, 44780, Germany  jancke@neurobiologie.rub.de  Laurenz Wiskott  Institut f¨ur Neuroinformatik Ruhr-Universit¨at Bochum Bochum, 44780, Germany  laurenz.wiskott@rub.de  Abstract  Spontaneous cortical activity – the ongoing cortical activities in absence of in- tentional sensory input – is considered to play a vital role in many aspects of both normal brain functions [1] and mental dysfunctions [2]. We present a cen- tered Gaussian-binary Deep Boltzmann Machine (GDBM) for modeling the spon- taneous activity in early cortical visual area and relate the random sampling in GDBMs to the spontaneous cortical activity. After training the proposed model on natural image patches, we show that the samples collected from the model’s probability distribution encompass similar activity patterns as found in the spon- taneous activity. Speciﬁcally, ﬁlters having the same orientation preference tend to be active together during random sampling. Our work demonstrates GDBM is a meaningful model approach for basic receptive ﬁeld properties and the emergence of spontaneous activity patterns in early cortical visual areas. Besides, we show empirically that centered GDBMs do not suffer from the difﬁculties during train- ing as GDBMs do and can be properly trained without the layer-wise pretraining as described in [3].  1 Introduction  Spontaneous cortical activity has been studied in various contexts ranging from somato-sensory to visual and auditory perception [4]. These ongoing cortical activities in absence of intentional sensory input are considered to play a vital role in many aspects of both normal brain functions [1], and mental dysfunctions [2]. In [1], the spontaneous activity in the early cortical visual area is reported to have a set of states, several of which resemble cortical representation of orientation, i.e. neurons in the visual cortex having similar orientation preference tend to be active together. One hypothesis about this ﬁnding is that these states reﬂect expectations about the sensory input, which reﬂects an aspect of approaches such as Bayesian models[5] and generative models. Concretely, a brain learn to synthesize (or generate) representations of sensory inputs.  Previous studies have considered Deep Boltzmann Machines (DBMs) as a potential model frame- work for modeling the relevant aspects of this generative process and have related the inference in  1  DBM to the mechanisms of cortical perception[6, 7]. The authors have trained a DBM on binary im- ages and have shown that trained DBMs can qualitatively reproduce several aspects of hallucinations in Charles Bonnet Syndrome. In this work, we have chosen a variant of DBM, a Gaussian-Binary DBM (GDBM), as a model for early cortical visual areas, which extends DBMs to modeling con- tinuous data. In particular, we are interested in modeling the spontaneous activity found in [1] and demonstrate how GDBM reproduces the ﬁndings of spontaneous activity in early visual cortex.  As for training GDBMs on the visual inputs, we adapt the centering proposed in [8] to GDBMs by rewriting the energy function as centered states. We show empirically the proposed centered GDBMs can learn features from the natural image patches without the layer-wise pretraining proce- dure and do not suffer from the difﬁculties during training of GDBMs as reported in [3].  In Section 2, the proposed centered GDBM is introduced. Then we describe the training procedure in Section 3.1, and show that the centered GDBM can not only learn Gabor-like ﬁlters, but also meaningful features in the higher layer. Finally, by considering the samples of the hidden units from the model distribution as spontaneous activity, we demonstrate in Section 3.2 that these random samples present similar activity patterns as the spontaneous visual cortical activity reported in [1].  2 Algorithm  Like Deep Belief Networks (DBN) [9] and deep autoencoders [10], DBMs have been proposed for learning multi-layer representations that are increasingly complex. In particular, DBM incorporates both the top-down messages and the bottom-up passes during the inference for each layer, which gives DBM an advantage in propagating the input uncertainties. DBMs have been applied to many problems and show promising results[11, 12, 13].  To model the natural image patches that have continuous values, a variant of DBMs is required since the original DBM is designed for binary data. There are two common ways to extend DBMs to modeling continuous values. The most common way is to train a Gaussian-binary restricted Boltzmann machine (GRBM) as a preprocessing model and use the output of the trained model as the input data for training a DBM [14]. However, this practice loses the ability to train the model as a whole, i.e. the preprocessing part needs to be trained beforehand. An alternative is a Gaussian- binary deep Boltzmann machine (also known as Gaussian-Bernoulli DBM [3]), in which the binary units in the bottom layer are replaced by the real-value ones as in GRBMs. Moreover, GDBMs have been proved to be a universal approximator. The pitfall of GDBMs is the difﬁculty in training it [3].  The centering has been proposed in [8] for DBMs and has been shown to improve learning. The idea is to have the output of each unit to all the other units to be centered around zero and take effects only when its activation deviate from its mean activation. In [8], the authors show that centering produces a better conditioned optimization problem. Therefore, we adapted the centering to the GDBM and refer to the new model as centered GDBM. Compared with the training recipe in [3], we ﬁnd empirically that a centered GDBM can easily be trained even without pre-training phase and is insensitive to the choice of hyper-parameters.  2.1 Centered GDBM  To avoid cluttering, we present a centered GDBM of two hidden layers as an example, although the model can be extended to an arbitrary number of layers. A two-layer centered GDBM as illustrated in Figure 1, consisted of an input layer X and two hidden layers, has an energy deﬁned as:  E (X, Y, Z; Θ, C) : =  M  Xi  (Xi − cxi)2  2σ2 i  −  L,M  Xi,j  (Xi − cxi)wij (Yj − cyj )  σ2 i  −  M  Xj  byj (Yj − cyj )  N  M,N  −  bzk (Zk − czk ) −  Xk Xj,k (X − cx)T Λ−1(X − cx) − (X − cx)T Λ−1W(Y − cy)  (Yj − cyj )ujk(Zk − czk )  2  =  (1)  −(Y − cy)T by − (Z − cz)T bz − (Y − cy)T U(Z − cz),  (2)  2  where Y represents the ﬁrst hidden layer, and Z denotes the second hidden layer. L, M , N are the dimensionality of X, Y and Z respectively. Θ := {W, U, by, bz} denotes the parameters trained for maximizing the loglikelihood. C := {cx, cy, cz} represents the offsets for centering the units and is set to the mean activation of each unit. Λ is a diagonal matrix with the elements σ2 i . The probability of any given state (x, y, z) in the DBM is  P (x, y, z; Θ, C) : =  1  Z(Θ)  exp (−E(x, y, z; Θ, C)) .  (3)  Here Z (Θ) is the partition function depended on the parameters of the model. Inference in centered GDBM is simple, because the states of the units in each layer are independent of the other ones in the same layer given the adjacent upper and lower layer.  P (Xi|y) = N (cid:0)Xi; wi∗(y − cy) + cxi , σ2 i(cid:1) ,  (4) (5) (6) where f (·) is a sigmoid function and N (·; µ, σ2) denotes a normal distribution with mean µ and variance σ2. wi∗ and w∗j denote the ith row and the jth column of matrix W. uj∗ and u∗k are deﬁned correspondingly.  P (Yj = 1|x, z) = f (cid:0)(x − cx)T w∗j + uj∗(z − cz) + byj(cid:1) , P (Zk = 1|y) = f (cid:0)(y − cy)T u∗k + bzk(cid:1) ,  (a)  (b)  (c)  Z  Zk  Y Yj  X Xi  U  W  N  M  L  Z  Y  X  Z  Y  X  Figure 1: (a) A graphical description of a two-layer GDBM. (b) Illustration of the sampling pro- cedure for estimating model dependent expectation. (c) Illustration of the sampling procedure for estimating data dependent expectation.  For training a centered GDBM, the objective is to maximize the loglikelihood ˆℓ, of which the partial derivative for each parameter θ ∈ Θ is  ∂ ˆℓ ∂θ  = (cid:28) ∂(−E(x, y, z)  ∂θ  (cid:29)data  −(cid:28) ∂(−E(x, y, z)  ∂θ  (cid:29)model  ,  (7)  where h·idata and h·imodel represent the expectation with respect to the data and the model distri- bution, respectively. By using the mean-ﬁeld approximation, we can estimate the data-dependent expectation. And the model-dependent expectation is usually approximated by using persistent Markov Chains with Gibbs sampling. See [14] for details.  As for the offsets, we adjust them along the training procedure with a moving average. To ensure the units are centered from the beginning of training, cy and cz are set to f (by) and f (bz), respectively. And cX is set to the data mean hxidata. In general, we follow the learning algorithm in [8] with several modiﬁcations as shown in Algorithm 1.  3 Experiments and results  3.1 Learning from natural image patches  We applied the centered GDBM to image patches of 32 × 32 pixels1, taken randomly from the van Hateren natural image dataset [15]. The patches were ﬁrstly whitened by principal component analysis and reduced to the dimensionality of 256 in order to avoid aliasing [16].  1We generated 60,000 image patches by randomly taken patches of 32 × 32 pixels from 2,000 natural  images. A subset of size 50,000 was used for training. The rest was used for testing.  3  Algorithm 1 Training algorithm for centered GDBMs  (cid:0)i.e. wij ∼ U(cid:2) −q 6  L+M ,q 6  L+M(cid:3), ujk ∼ U(cid:2) −q 6  M+N ,q 6 M+N(cid:3)(cid:1) i ∼ N (0.5, 0.01)(cid:1) (cid:0)i.e. byj , bzk ∼ N (−4.0, 0.01)(cid:1) (cid:0)i.e. cx ←< x >data, cy ← f (by), cz ← f (bz) (cid:1)  (cid:0)i.e. σ2  ymodel ← cy for all batches xdata do  1: Initialize W, U  zdata ← cz loop  2: Initialize Λ 3: Initialize by, bz 4: Initialize cx, cy, cz 5: loop 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21:  ydata ← P (Y|xdata, zdata) zdata ← P (Z|ydata)  end loop until stop criteria is met loop  zmodel ∼ P (Z|ymodel) xmodel ∼ P (X|ymodel) ymodel ∼ P (Y|xmodel, zmodel) end loop until stop criteria is met cy ← (1 − ν) · cy + ν · hydatai cz ← (1 − ν) · cz + ν · hzdatai  σ3 2(xi,model−cxi )wi∗(ymodel−cy)  W ← W + η(cid:0)h(xdata − cx)Λ−1(ydata − cy)T i − h(xmodel − cx)Λ−1(ymodel − cy)T i(cid:1) U ← U + η(cid:0)h(ydata − cy)(zdata − cz)T i − h(ymodel − cy)(zmodel − cz)T i(cid:1) σi ← σi + ησ(cid:0)h (xi,data−cxi )2 i − h (xi,model−cxi )2 i(cid:1) by ← by + η(cid:0)hymodeli − hymodeli(cid:1) + νWT Λ−1(xdata − cx) + νU(cid:0)zdata − cz(cid:1) bz ← bz + η(cid:0)hzdatai − hzmodeli(cid:1) + νUT(cid:0)ydata − cy(cid:1)  − 2(xi,data−cxi )wi∗(ydata−cy)  ymodel ← P (Y|xmodel, zmodel)  σ3  σ3  σ3  −  22:  23: 24: 25: end for 26: 27: end loop  Afterwards, we trained a centered GDBM with 256 visible units and two hidden layers. There were 900 units in the ﬁrst hidden layer and 100 units in the second hidden layer2. Despite the difﬁculties during training of GDBMs, we found empirically the centered GDBM can be trained much easier. Without the layer-wise pretraining, the centered GDBM did not suffer from the issue that the higher layer units are either always inactive or always active as reported in [3]. Since any centered GDBM can be reparameterized as a normal GDBM [18], this may imply that the centering in GDBM plays an important role in the optimization procedure.  After training, the centered GDBM had oriented, Gabor-like ﬁlters in the ﬁrst hidden layer (Fig- ure 2a). Most of the units in the second hidden layer had either strong positive or negative con- nections to the ﬁlters in the ﬁrst layer that have similar patterns. As shown in Figure 2b, the ﬁlters having strong connections to the same second-layer units either have the similar orientation or the same location. The results further suggest that the model learned to encode more complex features such as contours, angles, and junctions of edges. These results resemble the properties of the neu- rons in V1 of visual cortex and imply that centered GDBM is a meaningful model for basic receptive ﬁeld properties in early cortical visual areas. Despite the resemblance of these results to those from sparse DBNs [19], sparse DBNs show worse match to the biological ﬁndings in [1]. A quantitative comparison between centered GDBMs and sparse DBNs is still open for future studies.  2The size of the hidden layers were chosen to get a good model of the spontaneous cortical activity as described in Section 3.2. The training procedure started with a learning rate of 0.03 and a momentum of 0.9, which is annealed to 0.001 and 0.0, respectively. But the standard deviation σi have a different learning rate, which is only one-tenth of the other’s learning rate [17]. Neither weight decay nor sparse penalty is used during training. Mini-batch learning is used with a batch size of 100. The updating rate for centering parameters was 0.001. The training procedure was stopped when the reconstruction error stopped decreasing.  4  (a)  (b)  Z81  Z86  Z41  Z66  Z28  Z50  Z46  Z64  Z65  Z40  Figure 2: (a) 100 randomly selected ﬁrst-layer ﬁlters learned from the natural image. (b) The leading six columns visualize the ﬁrst-layer ﬁlters that have strongest connections to one selected second- layer unit. The ﬁlters are arranged from left to right in descending order by the absolute value of their weight to the selected unit. The last column depicts the weighted sum of the six strongest- connected ﬁlters, which can be considered as an approximation of the receptive ﬁelds of the selected second-layer units.  3.2 Comparing with biological experiments  After successful training of centered GDBM, we quantitatively analyzed the results of the trained model3 with the methods used in [1]. For conducting the measurements, we made two basic as- sumptions. Firstly, since only the ﬁrst-layer ﬁlters of centered GDBM present strong orientation preferences, we assumed these ﬁlters correspond to the visual cortical neurons recorded in the liter- ature. Secondly, the samples of the hidden units collected according to the model distribution was assumed to be the counterpart of the neuron’s activity.  3.2.1 Generating orientation maps  To compare with the original experiments, we ﬁrstly generated full-ﬁeld gratings as input data and measured the response of the centered GDBM model. We collected the responses of the ﬁrst-layer hidden units to eight orientations from 0◦ to 157.5◦ with a difference of 22.5◦. The amplitude of the gratings was chosen such that the average norm of the input stimuli is the same as that of the natural image patches before whitening. For each orientation, the grating stimuli of various frequencies and phases was fed to the model. To be consist with the learning procedure, we used the mean-ﬁeld variational approximation to approximate the responses of the model to each stimulus, which can also be estimated by using the Gibbs sampling with the visible units clamped to the input. An illustration of the sampling procedure in given in Figure 1(c). After collecting the responses, the average response over all the stimuli of each orientation were calculated and considered to be the model’s response to the corresponding orientation. These activity patterns correspond to the single-condition orientation maps in [1]. Figure 3 (top) visualizes the most active ﬁlters in the single-condition orientation maps of four selected orientations.  3.2.2 Generating spontaneous frames  To simulate spontaneously emerging cortical activity, we sampled the states of the trained model starting from a random initialization. Neither the visible nor the hidden unit was clamped to ﬁxed value during sampling. Figure 1(b) shows an illustration of the sampling procedure. This sampling procedure generated samples from the model’s distribution P (Y, X, Z), which were considered as  3Considering the authors of the original paper [1] only presented the results from one hemisphere of a selected cat, here we only present the results of one centered GDBM. However, all the centered GDBMs trained in our experience showed consistent results.  5  0 ◦  45 ◦  90 ◦  135 ◦  No. 11432  No. 1097  No. 17372  No. 14014  No. 15  No. 22  No. 28  No. 19  Figure 3: (top) The 25 most active ﬁlters in each of the single-condition orientation maps. For each map the ﬁlters are arranged in a descending order of activity level. (middle) The same but using the ﬁlters in the spontaneous frames that are best correlated to the corresponding single-condition maps. (bottom) The same but using the ﬁlters of the best-correlated nodes in the self-organizing map.  the expectated states of the units in the model. This differes from the sampling procedure in [6], where the visible units were clamped to zeroes and the hidden biases were adapted during sampling. The difference is discussed at length in Section 4.  In total, there were 100 Markov chains running Gibbs sampling for 2,000 iterations. For each Markov chain, the initial states of the ﬁrst-hidden-layer units were set to be active with a probability that is equal to the average P (Y|xdata, zdata) over the training data. By recording the samples every 10 sampling steps, we collected 20,000 samples of Y and considered the corresponding probabilities P (Y|x, z) as spontaneous frames. Such a sampling procedure is referred to as a session. In the experiments we repeated the session for many times with every trained model.  3.2.3 Correlation between spontaneous frames and orientation maps  To establish the similarity between the single-condition orientation maps and the spontaneous frames, we calculated the spatial correlation coefﬁcients between them. Figure 4(a, red) presents an example of the distribution of these correlation coefﬁcients for four selected orientations4. In order to show that these correlations are stronger than expected by chances, we generated random activity patterns of the ﬁrst hidden layer with the same probability as the one used for initializing the Markov chains, and the same correlation coefﬁcients were calculated with these random ac- tivity patterns as shown in Figure 4(a, blue). Speciﬁcally, the maximal correlation coefﬁcients is 0.50 ± 0.06 whereas the correlation coefﬁcients between the spontaneous frames and the random generated patterns seldom reach 0.2. The same observation were made in Figure 2 in [1]. In [1], the authors observed that there were more spontaneous activity patterns corresponding to the cardinal orientations than to the oblique ones. Therefore we further calculated the orientation preference of these spontaneous frames. By this point, only the spontaneous frames that are signif- icantly correlated were chosen. As in the biological experiment [1], we chose a signiﬁcance level of P < 0.01, resulting in a threshold of |0.182| and a selection of 18 ± 9% spontaneous frames. We then calculated the orientation preference of these frames by searching the orientation that is maximally correlated for each frame. Figure 4(b) plots the relative occurrences of the different ori- entation preferences together with the maximal correlation coefﬁcients. The results match those from the cats’ visual cortex in [1] fairly well, i.e. the spontaneous frames corresponding to the car-  4The following results were collected in a single session, but the results are consistent across simulation runs. In all sessions, the similar observation as shown in Figure 2–4 can be observed. The only difference is the shape of the curves in Figure 4b might vary in different sessions. However, the dominance of the cardinal orientation is always present.  6  dinal orientations emerged more often than those corresponding to the oblique ones and the former also have larger correlation coefﬁcients.                           (a)  0 ◦  90 ◦  10000  8000  6000  4000  2000  0  10000  8000  6000  4000  2000  s e c n a r u c c o   f o   #  s e c n a r u c c o   f o   #  (b)  45 ◦  135 ◦  1.0  0.8  0.6  0.4  0.2  e c n e r r u c c o   e v i t a e R  l  0.55  0.50  0.45  0.40  0.35  0.30  0.25  t n e i c i f f e o c   n o i t a e r r o c     a M  l  0  −0.5  0.0  0.5  −0.5  0.0  Correlation coefficient  Correlation coefficient  0.5  0.0  0.0  22.5  45.0  67.5  90.0  112.5  135.0  157.5  0.20  180.0  Figure 4: (a) Red, example distribution of the correlation coefﬁcients between the spontaneous frames and the four selected single-condition orientation maps. Blue, the same, using random gen- erated activity patterns with the same average probability. The threshold for signiﬁcant correlation with the signiﬁcance level of P < 0.01 is |0.182|. (b) The relationship between the relative occur- rence of the different orientation preferences (red) and the maximal correlation coefﬁcient (blue). The results from the centered GDBM are plotted by solid lines in comparison with the results in [1] plotted by dotted lines. The relative occurrence is calculated relative to the occurrence of the hori- zontal orientation. The ﬁgure is adapted to Figure 3b in [1].  Next we compared the most active ﬁlters in the spontaneous frames with those in the single-condition orientation map. Figure 3 (middle) visualizes these ﬁlters for four spontaneous frames that are best correlated with the selected single-condition orientation maps. These ﬁlters demonstrate similar features as those in the corresponding orientation maps shown in Figure 3 (top). Thus the result sup- ports the similarity between the spontaneous frames and the orientation maps in centered GDBMs.  3.2.4 Learning SOM from the spontaneous frames  We followed the methods in [1] and applied Self-Organizing Map (SOM) algorithm [20] to the spon- taneous frames in order to study the intrinsic structures of these spontaneous activities. We trained a SOM on the 20,000 spontaneous frames collected from a single session. The SOM projects the spontaneous frames onto 40 nodes that were arranged on a 1-D circle. See [1] for details of training. After training the SOM, we examined the correlation between the weight vectors of the 40 nodes and the single-condition orientation maps. Figure 3 (bottom) illustrates the most active ﬁlters in the weight vectors of four nodes that are best correlated with the selected single-condition orientation maps. The remarkable resemblance between these ﬁlters and those in the single-condition orienta- tion maps suggests that the spontaneous frames encompass several states of the hidden variables in the ﬁrst layer, which resemble the model’s representation of orientation.  4 Discussion  In this work, we present a variant of DBMs, centered Gaussian-binary deep Boltzmann machines (GDBM) for modeling spontaneous activity in visual cortex. We show empirically that the proposed centered GDBM does not require the layer-wise pretraining procedure by virtue of centering the units’ activation. An intuitive explanation for the success of centering is that centering prevents the units from being either always active or inactive and thus forming a bias for the other units. Because the centering offsets keep the output of each unit to zero unless its activation differs from its mean activation. Formally, the authors in [8] show that the centering produces improved learning conditions. Nevertheless, the authors show that a centered DBM can be reparameterized as a normal DBM with the same probability distribution in [18]. Therefore, centered GDBM can be considered as a GDBM with better optimization conditions. In other word, the results presented in this work can also be expected in a GDBM, despite its optimization difﬁculties.  7  The results of modeling natural image patches with centered GDBM in this work suggest centering helps to overcome the commonly observed difﬁculties during training. In addition, we also trained a centered GDBM on the Olivetti face dataset [21] following the same setting as in [3]. We achieved a reconstruction error on the test set of 41.6 ± 0.40 compared to the result of about 40 in [3]. Although the results are not sufﬁcient to claim the superiority of centered GDBMs over the proposed training algorithm in [3], we argue that a centered GDBM is an alternative to GDBM.  Our main contribution is to consider centered GDBMs as a model for the early cortical visual ar- eas and to reproduce the ﬁndings of the spontaneous cortical activity in[1]. Compared to previous work [6] of modeling cortical activity with DBMs, we extend the model from binary data to con- tinuous data with the proposed centered GDBMs. This extension makes it possible to use centered GDBMs to model other cortical activities besides vision.  For modeling spontaneous activity, we’ve also tested other models, i.e. GRBMs and DBN. None of them can match the results in [1] as well as centered GDBMs. The correlation between the spontaneous frames and the orientation maps are signiﬁcantly less than in centered GDBMs, where 18 ± 9% of the frames are signiﬁcantly correlated to the maps compared to 2% or less in other models. A possible explanation is that centered GDBMs is the only model using both top-down and bottom-up interactions during inference. In comparison, GRBMs and DBN only use either the bottom-up or the top-down information. On one hand, this suggests that the observed spontaneous activity is a result of interactions between incoming stimulation and feedback from higher areas. On the other hand, our results also predicts the states of spontaneous activity found in [1] are the result of the interactions within the early cortical visual areas. Because a centered GDBM with two hidden layers is enough to faithfully reproduce the reported results.  In [6], the authors clamped the visible units to zero in order to model profound visual impairment or blindness. This is equivalent to clamping the visible units of a centered GDBM to the center- ing offsets, which leads to zero inputs to the ﬁrst hidden layer from bottom-up. However, in this work we sampled the visible units freely during generating spontaneous frames. The main reason is that spontaneous cortical activity is the ongoing activity in the absence of intentional sensory in- put, which does not exclude incoming stimuli [22]. The authors also reported no difference in their ﬁndings when a uniform grey screen is used instead of having the room darkened [1]. From the model’s persepctive, our sampling procedure is supposed to approximate samples from the model’s prior distribution P (Y), which are the expectated states of the ﬁrst-hidden-layer units without any knowledge of bottom-up or top-down information. As a result, our results suggest that the spon- taneous activity patterns appear to be an expectation of internal states in brain and indicate that a brain might learn to generate such expectations as a generative model. Moreover, we observed that the correlation between the spontaneous frames and the orientation maps disappeared when we clamped either the visible or the second hidden layer units to the centering offsets during generat- ing the spontaneous frames. This supports our prediction that generating the observed spontaneous activity needs both incoming stimulation and feedback from higher areas.  5 Conclusion  We present centered GDBMs by applying the centering to GDBMs and used the centered GDBM as a model of spontaneous activity in early cortical visual areas. Our work extend the previous work of using DBM for modeling cortical activities to continuous data. The results demonstrate that a centered GDBM is a meaningful model approach for basic receptive ﬁeld properties and the emergence of spontaneous activity patterns in early cortical visual area and has the potential to give further insights to spontaneous activity in brain. Our work also show empirically centered GDBMs can be properly trained without layer-wise pretraining.  Acknowledgments  We would like to thank Jan Melchior for helpful comments. This work is funded by a grant from the German Research Foundation (Deutsche Forschungsgemeinschaft, DFG) to L. Wiskott (SFB 874, TP B3) and D. Jancke (SFB-874, TP A2).  8  References  [1] Tal Kenet, Dmitri Bibitchkov, Misha Tsodyks, Amiram Grinvald, and Amos Arieli. Spontaneously emerg-  ing cortical representations of visual attributes. Nature, 424:954–956, 2003.  [2] W Burke. The neural basis of Charles Bonnet hallucinations: a hypothesis. Journal of Neurology, Neuro-  surgery & Psychiatry, 73(5):535–541, 2002.  [3] KyungHyun Cho, Tapani Raiko, and Alexander Ilin. Gaussian-Bernoulli deep Boltzmann machines. In  the proceedings of the International Joint Conference on Neural Networks (IJCNN), 2013.  [4] R´emy Lestienne. Spike timing, synchronization and information processing on the sensory side of the  central nervous system. Progress in Neurobiology, 65(6):545 – 591, 2001.  [5] Iris Vilares and Konrad Kording. Bayesian models: the structure of the world, uncertainty, behavior, and  the brain. Annals of the New York Academy of Sciences, 1224(1):22–39, 2011.  [6] David Reichert, Peggy Series, and Amos Storkey. Hallucinations in Charles Bonnet syndrome induced In the proceedings of the Conference on Neural  by homeostasis: a deep Boltzmann machine model. Information Processing Systems (NIPS), pages 2020–2028, 2010.  [7] David Reichert, Peggy Series, and Amos Storkey. Charles Bonnet syndrome: Evidence for a generative  model in the cortex? PLoS Comput Biol, 9(7):e1003134, 07 2013.  [8] Gr´egoire Montavon and Klaus-Robert M¨uller. Deep Boltzmann machines and the centering trick.  In Neural Networks: Tricks of the Trade, volume 7700 of Lecture Notes in Computer Science, pages 621– 637. Springer Berlin Heidelberg, 2012.  [9] Geoffrey Hinton and Ruslan Salakhutdinov. Reducing the dimensionality of data with neural networks.  Science, 313(5786):504–507, 2006.  [10] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and compos- ing robust features with denoising autoencoders. In the proceedings of the International Conference on Machine Learning (ICML), pages 1096–1103. ACM, 2008.  [11] Nitish Srivastava and Ruslan Salakhutdinov. Multimodal learning with deep Boltzmann machines. In the proceedings of the Conference on Neural Information Processing Systems (NIPS), pages 2231–2239, 2012.  [12] Nitish Srivastava, Ruslan Salakhutdinov, and Geoffrey Hinton. Modeling documents with deep Boltz- mann machines. In the proceedings of the Internation Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2013.  [13] Ruslan Salakhutdinov, J Tenenbaum, and Antonio Torralba. Learning with hierarchical-deep models.  IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1958–1971, 2013.  [14] Ruslan Salakhutdinov and Geoffrey Hinton. Deep Boltzmann machines. In the proceedings of the inter-  nation conference on Artiﬁcial Intelligence and Statistics (AISTATS), volume 5, pages 448–455, 2009.  [15] A. van der Schaaf and J.H. van Hateren. Modelling the power spectra of natural images: Statistics and  information. Vision Research, 36(17):2759 – 2770, 1996.  [16] Aapo. Hyv¨arinen, Juha Karhunen, and Erkki Oja. Independent Component Analysis. Adaptive and Learn-  ing Systems for Signal Processing, Communications and Control Series. Wiley, 2004.  [17] Alex Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, University of  Toronto, Toronto, 4 2009.  [18] Jan Melchior, Asja Fischer, Nan Wang, and Laurenz Wiskott. How to center binary restricted Boltzmann  machines. arXiv preprint arXiv:1311.1354, 2013.  [19] Honglak Lee, Chaitanya Ekanadham, and Andrew Y.Ng. Sparse deep belief net model for visual area v2. In the proceedings of the Conference on Neural Information Processing Systems (NIPS). MIT Press, 2007.  [20] Teuvo Kohonen. Self-Organizing Maps. Physics and astronomy online library. Springer Berlin Heidel-  berg, 2001.  [21] F. S. Samaria and A. C. Harter. Parameterisation of a stochastic model for human face identiﬁcation. In  the proceedings of IEEE Workshop on Applications of Computer Vision, pages 138–142, 1994.  [22] Amos Arieli, Alexander Sterkin, Amiram Grinvald, and Ad Aertsen. Dynamics of ongoing activity:  explanation of the large variability in evoked cortical responses. Science, 273:1868–1871, 1996.  9  ","Spontaneous cortical activity -- the ongoing cortical activities in absenceof intentional sensory input -- is considered to play a vital role in manyaspects of both normal brain functions and mental dysfunctions. We present acentered Gaussian-binary Deep Boltzmann Machine (GDBM) for modeling theactivity in early cortical visual areas and relate the random sampling in GDBMsto the spontaneous cortical activity. After training the proposed model onnatural image patches, we show that the samples collected from the model'sprobability distribution encompass similar activity patterns as found in thespontaneous activity. Specifically, filters having the same orientationpreference tend to be active together during random sampling. Our workdemonstrates the centered GDBM is a meaningful model approach for basicreceptive field properties and the emergence of spontaneous activity patternsin early cortical visual areas. Besides, we show empirically that centeredGDBMs do not suffer from the difficulties during training as GDBMs do and canbe properly trained without the layer-wise pretraining."
1312.5813,2014,Why does the unsupervised pretraining encourage moderate-sparseness?  ,"['Jun Li', 'Wei Luo', 'Jian Yang', 'Xiaotong Yuan']",https://arxiv.org/pdf/1312.5813.pdf,"Unsupervised Pretraining Encourages Moderate-Sparseness  4 1 0 2     n u J    9      ]  G L . s c [      2 v 3 1 8 5  .  2 1 3 1 : v i X r a  Jun Li, Wei Luo, and Jian Yang School of Computer Science and Technology, Nanjing University of Science and Technology, Nanjing, China, 210094 Xiaotong Yuan XTYUAN1980@GMAIL.COM School of Information and Control, Nanjing University of Information Science and Technology, Nanjing, China, 210044  JUNL.NJUST@GMAIL.COM;CSWLUO@GMAIL.COM;CSJYANG@NJUST.EDU.CN  Abstract  It is well known that direct training of deep neu- ral networks will generally lead to poor results. A major progress in recent years is the inven- tion of various pretraining methods to initialize network parameters and it was shown that such methods lead to good prediction performance. However, the reason for the success of pretrain- ing has not been fully understood, although it was argued that regularization and better opti- mization play certain roles. This paper provides another explanation for the effectiveness of pre- training, where we show pretraining leads to a sparseness of hidden unit activation in the result- ing neural networks. The main reason is that the pretraining models can be interpreted as an adaptive sparse coding. Compared to deep neural network with sigmoid function, our experimental results on MNIST and Birdsong further support this sparseness observation.  1. Introduction Deep neural networks (DNNs) have found many success- ful applications in recent years. However, it is well-known that if one trains such networks with the standard back- propagation algorithm from randomly initialized param- eters, one typically ends up with models that have poor prediction performance. A major progress in DNNs re- search is the invention of pretraining techniques for deep learning (Hinton et al., 2006; Hinton & Salakhutdinov, 2006; Bengio et al., 2006; Bengio, 2009; Bengio et al., 2012). The main strategy is to employ layer-wise unsu- pervised learning procedures to initialize the DNN model parameters. A number of such unsupervised training tech- niques have been proposed, such as restricted Boltzmann  Proceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copy- right 2014 by the author(s).  machines (RBMs) in (Hinton et al., 2006), and denoising autoencoders (DAEs) in (Vincent et al., 2008). Although these methods show strong empirical performance, the rea- son of their success has not been fully understood. Two reasons were offered in the literature to explain the ad- vantages of unsupervised learning procedure (Erhan et al., 2010; Larochelle et al., 2009): the regularization effect and the optimization effect. The regularization effect says that pretraining provides regularization which initialize the pa- rameters in the basin of attraction to a “good” local min- imum. The optimization effect says that the pretraining leads to better optimization so that the initial value is close to a local minimum with a lower objective value than that can be achieved with random initialization. Based on ex- perimental evidences, some researchers conﬁrm that the pretraining can learn invariant representations and selective units (Goodfellow et al., 2009). Our Contributions: We study why the pretraining encour- ages moderate-sparseness. The main reason is that the pre- training models can be interpreted as an adaptive sparse coding. This coding is approximated by a sparse encoder, which is implemented by adaptively ﬁltering out a lot of features that are not present in the input and suppressing the responses of some features that are not signiﬁcant in the input. We further conduct experiments to demonstrate that it is a sparse regularization (the hidden units become more sparsely activated).  2. Previous Works In this part we review some advantages of the pretraining methods. Distributed representations and deep architectures play an important role in deep learning methods. A distributed rep- resentation (an old idea) can capture a very great number of possible input conﬁgurations (Bengio, 2009). Deep ar- chitectures can promote the re-use of features and lead to abstract more invariant features for most local changes of the inputs (Bengio et al., 2012). However, it is hard to use  Submission and Formatting Instructions for uLearnBio 2014  the Back-Propagation to train DNNs with two traditional activation functions (the sigmoid function 1/(1 + e−x) and the hyperbolic tangent tanh(x)). Luckily, (Hinton et al., 2006) proposes a unsupervised pretraining method to ini- tialize the DNNs model parameters and learn good rep- resentations. The regularization effect and the optimiza- tion effect are used to explain the main advantages of the pretraining method (Erhan et al., 2010; Larochelle et al., 2009). To better understand what the pretraining models learn in deep architectures, (Goodfellow et al., 2009) ﬁnd that the pretraining methods can learn invariant representations and selective units. Some researchers use the linear combina- tion of previous units (Lee et al., 2009) and the maximizing activation (Erhan et al., 2010) to visualize the feature detec- tors (or invariance manifolds or ﬁlters) in an arbitrary layer. Fig. 1 of (Erhan et al., 2010) and Fig. 3 of (Lee et al., 2009) show that the ﬁrst, second and third layer can learn edge detectors, object parts, and objects respectively. Based the distributed and invariant representations, (Larochelle et al., 2009; Bengio et al., 2012; 2013) further conﬁrm that the pretraining methods tend to do a better job at disentangling the underlying factors of variation, such as objects or object parts. Compared to DNNs with sigmoid function, we conﬁrm that the pretraining methods encourage moderate-sparseness as the detectors ﬁlter out a lot of features that are not present in the input. In general, there is an illusion that unsuper- vised pretraining methods tend to learn non-sparse repre- sentations because it does not meet the conventional sparse methods (Zhang et al., 2011; Yang et al., 2012a; 2013). The conventional methods consider the idea of introducing a form of sparsity regularization. Most ways have been pro- posed by directly penalizing the outputs of hidden units, such as L1 penalty, L1/L2 penalty and Student-t penalty. But, the pretraining methods implement sparseness by ﬁl- tering out a lot of irrelevant features.  3. Pretraining Model There is a classic pretraining models: RBMs. An RBMs is an energy-based generative model deﬁned over a visible layer and a hidden layer. The visible layer is fully con- nected to the hidden layer via symmetric weights W , while there have no connections between units of the same layer. The number of visible units x and hidden units h are de- noted by dx and dh, respectively. Additionally, visible units and hidden units receive input from bias - c and b respec- tively. The energy function is denoted by η(x, h):  η(x, h) = −hTW x − cTx − bTh  (1)  The probability that the network assigns to visible units x  dx(cid:89) (cid:16)(cid:80)dh  i=1  is  (cid:88)  h  1 Z  p(x) =  e−η(x,h)  Z =  (cid:88)  x,h  e−η(,h)  (2)  where Z is the partition function or normalizing constant. Because there are no direct connections between hidden (or visible) units, it is very easy to sample from the conditional functions taking the form:  p(x|h) =  p(xi|h)  p(h|x) =  p(hj|x)  (3)  dh(cid:89)  j=1  (cid:17)  (cid:16)(cid:80)dx (cid:17)  where p(hj = 1|x) = f 1|h) = f moid functions: f (t) = 1/(1 + exp(−t)). The training is to use CD − 1 algorithm (Hinton., 2002) to minimize the likelihood of the data: − log p(x).  , p(xi = i=1 Wjixi + bj and f (t) is a logistic sig-  j=1 Wjihj + ci  4. Unsupervised Pretraining Encourages  Moderate-Sparseness  In this section, we denote that the sparse regularization with more overlapping groups in low layer or less in high layer is called the Moderate-Sparseness. We mainly consider the multi-class problem to explain why the unsupervised pre- training encourages moderate-sparseness since DNNs with the pretraining has been used to achieve state-of-the-art re- sults on classiﬁcation tasks. There are two reasons. First, we show a new viewing that the pretraining model is an adaptive sparse coding. Second, because the pretraining can train the ”good” feature detectors, we discuss that how the feature detectors can lead to moderate-sparseness. Fi- nally, we measure the moderate-sparseness. To start off the discussion, there are two natural assump- tions to m-class training set. Assumption 1: Every class has a balanced number of samples and there are a lot of common raw features (pixels or MFCC features) among samples of the same class (Zhang et al., 2012). Assump- tion 2: There are some similar raw features among samples of different classes since they share some common ones (Amit et al., 2007).  4.1. A New Viewing of Pretraining Model  Pretraining Model (such as RBMs) is an adaptive sparse coding. The explanation is as follow. By the results of (Bengio & Delalleau., 2009), the pretraining model (RBMs training is to minimize − log p(x)) is also approximated by minimizing a reconstruction error criterion:  −(log p(x|(cid:98)h) + log p(h|(cid:98)x))  (4)  Submission and Formatting Instructions for uLearnBio 2014  (cid:17)  i=1 Wjixi + bj  (cid:16)(cid:80)dx  (cid:17) . The − log p(x|(cid:98)h) and − log p(h|(cid:98)x) (cid:16)(cid:80)dx i=1 Wji(cid:98)xi + bj  where Eh[p(h|x)] is the mean-ﬁeld output of the hid- den units given the observed input x and Ex[p(x|h)] is the mean-ﬁeld output of the visible units given the representation h sampled from p(hj = 1|x) = f can be regard as a decoder and an encoder, respectively. From the second parts of (3) and (4), every hidden unit , (j = 1,··· , dh) p(hj = 1|x) = f can be further interpreted as a feature detector (or invari- ance manifolds or ﬁlters) because the hidden unit is ac- tive (or non-active), that means, the detector should re- spond strongly (or weakly) when the corresponding feature is present (or absent) in the input (Goodfellow et al., 2009). Amazedly the pretraining can train edge feature detectors in low layer and objects (or object parts) in high layer (Lee et al., 2009; Larochelle et al., 2009; Bengio et al., 2012). Given an input, the feature detectors naturally ﬁlter out a lot of features that are not present in the input and suppress the responses of some features that are not signiﬁcant in the input. Clearly, those detectors result in sparseness. Relationship with sparse coding: Sparse coding is to ﬁnd the dictionary D and the sparse representation h to mini- mize the most popular form:  (cid:107)x − Dh(cid:107)2 + λsc(cid:107)h(cid:107)1  (5)  where λsc also is a hyper-parameter. Obviously, the ﬁrst part of RBMs (??) is similar to the ﬁrst part of sparse cod- ing (5) as they are decoders. The sparse coding is directly to penalize the L1 norm of the hidden representation h. But in RBMs (??) the h is approximated by the sparse encoders (feature detectors), which ﬁlter out a lot of irrelevant fea- tures. In next subsection we shall discuss that how the fea- ture detectors can lead to moderate-sparseness.  4.2. Lead To Moderate-Sparseness Low-layer: Based on the assumptions the pretraining mod- els averagely distribute all edge feature detectors to the hid- den units in low layer as every class has a same number of samples. Assumption 1 shows that every class has a same number of edge feature detectors and there are a lot of com- mon edge ones in the same class. Clearly, the edge fea- ture detectors ﬁnd out the edge features belonged to self- class, suppress the responses of some nonsigniﬁcant edge features, and ﬁlter out a lot of edge features related to the other classes. Suppose that there are N hidden units (edge feature detectors) and a m-class dataset, every class ideally m ones. Given input samples of a class, thus, the N has N m hidden units belonged to the class are activated or weakly responded and the remaining N − N m units are not activated (corresponding sparseness that is measured by (6)). More- over, there are the common activation units (corresponding  group). Simultaneously, assumption 2 shows that there are some similar edge feature detectors among different classes. Dif- ferent classes share some edge feature detectors corre- sponded to hidden units, which are also activated. The activated units results in more overlapping activation units in low layer. The activation overlapping degree is mea- sured by (10). Combined with regularization effect (Erhan et al., 2010), therefore, we obtain the ﬁrst result (A1) that the unsupervised pretraining is a sparse regularization with more-overlapping groups in low layer. High-layer: In high layer the pretraining goes on to train object or object part features detectors from the edge fea- tures. Similarly to the analysis in low layer, the hidden units are more sparsely activated or weakly responded in high layer. Moreover, the activation overlapping degree is lower than one in low layer because the pretraining can po- tentially lead to do a better job at disentangling the objects or object parts (Larochelle et al., 2007; Bengio et al., 2013). Thus, we obtain the second result (A2) that the unsuper- vised pretraining is a sparse regularization with less (or no)-overlapping groups in high layer. In DNNs without the pretraining the most hidden units of every layer are always activated and correspond to terrible feature detectors, which are the important causes of difﬁ- cult classiﬁcation problems. For classiﬁcation tasks, it is al- ways desirable to extract features that are most effective for preserving class separability (Wong & Sun, 2011) and col- laboratively representing objects (Zhang et al., 2011; Yang et al., 2012b). The pretraining ﬁrmly grasps the those ben- eﬁts. The more activation overlapping units can capture the collaborative features in low layer and the less or no activa- tion overlapping units can capture the separability in high layer.  4.3. Sparseness Measure  For better understanding the pretraining, we tried to ﬁnd sparseness, more-overlapping and no-overlapping charac- teristics of DNNs with or without the pretraining. So, Hoyer’s sparseness measure and activation overlapping de- gree are deﬁned as followings. The Hoyer’s sparseness measure (HSPM) (Hoyer, 2004) is based on the relationship between the L1 norm and the L2 norm. The HSPM of a n dimensional vector h is deﬁned as follows:  n − ((cid:80)n  i=1 |hi|)/(cid:112)(cid:80)n  √  √  n − 1  i=1 h2 i  ,  (6)  HSP M (h) =  This measure has good properties, which is in the interval [0, 1] and on a normalized scale. It’s value more close to 1 means that there are more zero components in the vector h.  Submission and Formatting Instructions for uLearnBio 2014  We denote | · | absolute value of a real number and give the following deﬁnitions about AOD. Deﬁnition 1: A hidden unit i is said to be active if the absolute value of its activation hi is above a threshold τ, that is |hi| > τ. And a hidden unit i is called un-active if |hi| < τ. Deﬁnition 2: A vector z is said to be an activation binary- vector of a d dimensional representation h if some repre- sentation units are active when the corresponding features are present in x, and otherwise are not active when they are absent. Formally, the activation binary-vector z = z(h) is deﬁned as:  zi = zi(hi) =  |hi| ≥ τ; |hi| < τ.  ; i = 1,··· , dh  (7)  (cid:26) 1,  0,  where τ is a threshold. To indicate the present feature in the input, we select a threshold τ that does not change the reconstruction data, that is (cid:107)f (W T s + c) − f (W T h + c)(cid:107)2 < 0.051, where h = f (W x + b) and the vector s = s(x) of a sample x is deﬁned as:  |hi| ≥ τ; |hi| < τ.  ; i = 1,··· , dh  (8)  (cid:26) hi,  0,  si =  (cid:80)  Deﬁnition 3: An activation binary-vector Z(X ) of a sam- ple set X is an activation binary-vector of the mean value x among all samples x(x ∈ X ). It is deﬁned as:  Z(X ) = z(x) x =  x∈X x m  (9)  where z(x) is deﬁned in (7) and m is the number of sam- ple in the set X . The activation overlapping degree (AOD) simply calculates the percentage of activation unites that are simultaneously selected by different classes Xi(i = 1,··· , m). AOD among a set H is deﬁned as:  AOD(X1,··· ,Xm) =  z =  Z(Xi)  (10)  (cid:80)n  j=1 zj n  m(cid:94)  i=1  where H = {X1,··· ,Xm}, z is a binary-vector that is a logical conjunction on all activation binary-vectors Z(Xi), i = 1,··· , m and Z(Xi) is deﬁned in (9). AOD, which is in the interval [0, 1], is used to measure the percentage of activation overlapping units in different classes. It’s value more close to 0 means that there are few activation overlapping units and it is easier to separate the different classes.  1We select 0.05 because it is small enough.  Table 1. Hoyer’s sparseness measures (HSPM) of DNNs (500- 500-2000) on MNIST.  DBNs  DpRBMs Dsigm  dataset 0.63 0.63 0.63  1st 0.39 0.39 0.17  2nd 0.53 0.58 0.18  3rd 0.63 0.67 0.06  error 1.17%  2.01%  5. Experiments In this section, we use deep neural networks to do experi- ments. A standard architecture for DNNs consists of multi- ple layers of units in a directed graph, with each layer fully connected to the next one. The nodes of the inter-layers are called hidden units. Each hidden unit is passed through a standard sigmoid functions. The objective of learning is to ﬁnd the optimal network parameters so that the network output matches the target closely. The output can be com- pared to a target vector through a squared loss function or an negative log-likelihood loss function. We employ the standard back-propagation algorithm to train the model pa- rameters (the connected weights) (Bishop, 2006). We denote that Dsigm: DNNs with standard sigmoid func- tions, DpRBMs: DNNs only pretrained with RBMs and DBNs: deep belief networks pretrained with RBMs and ﬁnely tuned. Datasets: We present experimental results on standard benchmark datasets: MNIST2 and Birdsong3 The pixel in- tensities of all datasets are normalized to [0, 1]. MNIST dataset has 60,000 training samples and 10,000 test sam- ples with 28 × 28 pixel greyscale images of handwritten digits 0-9. Birdsong4 dataset has 70,000 training samples and 200,690 test samples with 16 MFCC features. To speed-up training, we subdivide training sets into mini- batches, each containing 100 cases, and the model param- eter is updated after each minibatch using the averages. Weights are initialized with small random values sampled from a normal distribution with zero mean and standard deviation of 0.01. Biases are initialized with zeros. For simplicity, we use a constant learning rate chosen from {1, 0.1, 0.05, 0.01}. Momentum is also used to speed up learning. The momentum starts at a value of 0.5 and lin- early increases to 0.90 over half epochs, and stays at 0.9 thereafter. The L2 regularization parameter for the weights is ﬁxed at 0.0001.  2http://yann.lecun.com/exdb/mnist/ 3http://sabiod.univ-tln.fr/icml2013/BIRD-SAMPLES/ 4TRAIN SET has 30 sec × 35 bird recordings and TEST SET has 150sec × 3 mics × 90 recordings. There are not labels in TEST SET. So we divide the TRAIN SET to a new train set and a new test set(We randomly select 3,000 train samples with 16 MFCC features, the rest are test samples in every recording.).  Submission and Formatting Instructions for uLearnBio 2014  Figure 1. (a-c) Hoyer’s sparseness measures (HSPM) of RBMs only pretrained on MNIST. (a) HSPM of three layers RBMs as the pretraining epoch increases (the momentum is 0.5 in the ﬁrst 25 epochs and 0.9 in the rest 25 epochs). From down to top: RBMs from the 1st, 2nd and 3rd layers, respectively. (b) HSPM of RBMs with 500-5000 hidden units after 1000 training epochs. (c) HSPM of ﬁve layers RBMs with 500 and 1000 hidden units after 1000 training epochs.  Figure 2. Activation overlapping degree (AOD) on MNIST. The left, middle and right respectively plot the average AOD among k classes (k changes from 2 to 10) in ﬁrst, second and third layer. 5.1. Sparseness Comparison  Before presenting the comparison of activation overlapping units, we ﬁrst show the sparseness of pretraining compared to the more traditional sigmoid activation function. The sparseness metric HSPM is the averaged value over the def- inition of (6). We perform comparisons on MNIST, and results after ﬁne- tuning training for 200 epochs are reported in Table 1. The results show that compared to Dsigm the pretraining leads to models with higher sparseness, and smaller test er- rors. Table 1 compares the network HSPM of DBNs and DpRBMs to that of Dsigm. From Table 1, we observe that the average sparseness of three layer DpRBMs is about 0.68; the resulting DBNs has similar sparseness. In Fig 1, (a) also shows that the feature of every layer RBMs is more sparse as the train epoch increases. In contract, the HSPM of Dsigm is on average below 0.14. When the pretraining are trained longer enough and the number of hidden unites increases, HSPM of the pretrain- ing models will become more sparse and also has an upper bound. In Fig 1, (b) shows that when the number of hidden units changes from 500 to 5000, an upper bounds of RBMs is 0.68 after 1000 training epochs. As the number of layers increases, HSPM of the pretraining  Table 2. Hoyer’s sparseness measures (HSPM) of DNNs (50-100- 100) on Birdsong.  DBNs  DpRBMs Dsigm  dataset 0.29 0.29 0.29  1st 0.12 0.62 0.08  2nd 0.12 0.68 0.11  3rd 0.35 0.59 0.43  error 9.6%  13.7%  models also has an upper bound. From Fig 1, (c) shows that upper bounds of ﬁve hidden (500 and 1000) layers RBMs are 0.58 and 0.66, respectively. We observe that the HSPM of the third layer pretraining is lower than one of the second layer. We empirically obtain the high HSPM by increasing the number of the hidden units of high layer, for example, DBNs (500-500-2000). This observation maybe can ex- plain why the top layer should be big. We perform comparisons on Birdsong, results after ﬁne- tuning training for 100 epochs are reported in Table 2. The results also show that the pretraining leads to models with higher sparseness, and smaller test errors. From Table 2, we observe that the sparseness of three layer DpRBMs is higher than that of Dsigm and database. Although after tun- ing the sparseness is close to Dsigm, the pretraining learn ”good” initial values to initialize the DNN model parame- ters. This illustrates that the pretraining also is an optimiza- tion effect.  Submission and Formatting Instructions for uLearnBio 2014  The HSPM in 3th layer are lower than 2nd layer. When training 2 layers networks, the resulting has similar test er- rors. So, there is a inspiration that the HSPM can be used to guide the number of layers and the number of hidden units.  learning and deep learning: A review and new perspectives. arXiv preprint arXiv:1206.5538v1, 2012.  Bengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. Better mixing  via deep representations. In ICML, 2013.  5.2. Comparison of Selective-Overlapping Units  10).  1 ,··· , Sk  }, where C k  j (j = 1,··· , C k  We perform comparisons on the test set of MNIST. For convenience, the test set S is denoted by {X0,··· ,X9}, where Xi(i = 0,··· , 9) represents a set of all digits i. k-combinations of the set S is denoted by a set Sk = {Sk j ,··· , Sk 10 is the number of k- Ck 10 combinations and Sk j is a subset of k distinct elements of S. The average AOD among k classes is an average of all AOD among a subset Sk We compare the average AOD of DpRBMs to that of Dsigm. We found that the pretraining can capture the char- acteristics (A1) that there are many overlapping units in low layer and (A2) that there are few (or no) overlapping units in high layer. From Fig 2, the results show that average AOD among k classes (k changes from 2 to 10) is high in low layer and is low (or zero) in high layer. The average AOD gets closer to 0 as the number of layer increases in DpRBMs. Particularly, the average AOD gets closer to 0 than data-self in the third layer. This reveals that it is easier to classify. But it is very approximate to 1 in every layer of Dsigm.  6. Conclusion Since the pretraining is known to perform well on MNIST, this paper mainly discusses why the unsupervised pretrain- ing encourages moderate-sparseness. Our observations make us suspect that sparseness and activation overlapping degree play more important roles in deep neural networks. From Table 1, Table 2 and Fig 2, the pretraining can cap- ture the sparse hidden units, the more activation overlap- ping units in low layer and the less (or no) activation over- lapping units in high layers.  References Amit, Y., Fink, M., Srebro, N., and Ullman, S. Uncovering shared  structures in multiclass classiﬁcation. In ICML, 2007.  Bengio, Y. Learning deep architectures for ai. Foundations and  Trends in Machine Learning, 2:1–127, 2009.  Bengio, Y. and Delalleau., O.  Justifying and generalizing contrastive divergence. Neural Computation, 21:1601–1621, 2009.  Bengio, Y., Lamblin, P., Popovici, D., and Larochelle., H. Greedy layer-wise training of deep networks. In NIPS, pp. 153–160, 2006.  Bengio, Y., Courville, A., and Vincent, P. Unsupervised feature  Bishop, C. M.  In Pattern Recognition and Machine Learning.  Springer Press, 2006.  Erhan, D., Courville, A., and Bengio, Y. Understanding represen- tations learned in deep architectures. In Technical Report 1355, 2010.  Goodfellow, I.J., Le, Q.V., Saxe, A.M., Lee, H., and Ng, A.Y.  Measuring invariances in deep networks. In NIPS, 2009.  Hinton., G. Training products of experts by minimizing con- Neural Computation, 14:1771–1800,  trastive divergence. 2002.  Hinton, G. and Salakhutdinov, R. Reducing the dimensionality of  data with neural networks. Science, 313:504–507, 2006.  Hinton, G., Osindero, S., and Teh., Y. W. A fast learning al- gorithm for deep belief nets. Neural Computation, 18:1527– 1554, 2006.  Hoyer, P.O. Non-negative matrix factorization with sparseness constraints. Journal of Machine Learning Research, 5:1457– 1469, 2004.  Larochelle, H., Erhan, D., Courville, A., Bergstra, J., and Bengio, Y. An empirical evaluation of deep architectures on problems with many factors of variation. In ICML, pp. 473–480, 2007.  Larochelle, H., Bengio, Y., Louradour, J., and Lamblin, P. Ex- ploring strategies for training deep neural networks. Journal of Machine Learning Research, 10:1–40, 2009.  Lee, H., Grosse, R., Ranganath, R., and Ng, A. Convolutional deep belief networks for scalable unsupervised learning of hi- erarchical representations. In ICML, pp. 609–616, 2009.  Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.A. Ex- tracting and composing robust features with denoising autoen- coders. In ICML, pp. 1096–1103, 2008.  Wong, W.K. and Sun, M.M. Deep learning regularized ﬁsher mappings. IEEE Trans. on Neural Networks, 22:1668–1675, 2011.  Yang, J., Zhang, L., Xu, Y., and Yang, J.Y. Beyond sparsity: The role of l1-optimizer in pattern classiﬁcation. Pattern Recogni- tion, 45:1104–1118, 2012a.  Yang, J., Chu, D.L., Zhang, L., Xu, Y., and Yang, J.Y. Sparse representation classiﬁer steered discriminative projection with applications to face recognition. IEEE Trans. on Neural Net- works and Learning Systems, 24:1023–1035, 2013.  Yang, M., Zhang, L., Zhang, D., and Wang, S. Relaxed collabora- tive representation for pattern classiﬁcation. In CVPR, 2012b.  Zhang, C.J., Liu, J., Tian, Q., Xu, C.S., Lu, H.Q., and Ma, S.D. Image classiﬁcation by non-negative sparse coding, low-rank and sparse decomposition. In CVPR, 2012.  Zhang, L., Yang, M., and Feng, X. Sparse representation or col- In  laborative representation: Which helps face recognition? ICCV, 2011.  ","It is well known that direct training of deep neural networks will generallylead to poor results. A major progress in recent years is the invention ofvarious pretraining methods to initialize network parameters and it was shownthat such methods lead to good prediction performance. However, the reason forthe success of pretraining has not been fully understood, although it wasargued that regularization and better optimization play certain roles. Thispaper provides another explanation for the effectiveness of pretraining, wherewe show pretraining leads to a sparseness of hidden unit activation in theresulting neural networks. The main reason is that the pretraining models canbe interpreted as an adaptive sparse coding. Compared to deep neural networkwith sigmoid function, our experimental results on MNIST and Birdsong furthersupport this sparseness observation."
1312.5857,2014,A Generative Product-of-Filters Model of Audio  ,"['Dawen Liang', 'Mathew D. Hoffman', 'Gautham Mysore']",https://arxiv.org/pdf/1312.5857.pdf,"4 1 0 2     v o N 5 2         ] L M  . t a t s [      5 v 7 5 8 5  .  2 1 3 1 : v i X r a  A Generative Product-of-Filters Model of Audio  Dawen Liang∗  Columbia University New York, NY 10027  dliang@ee.columbia.edu  Matthew D. Hoffman  Adobe Research  San Francisco, CA 94103 mathoffm@adobe.com  Gautham J. Mysore  Adobe Research  San Francisco, CA 94103 gmysore@adobe.com  Abstract  We propose the product-of-ﬁlters (PoF) model, a generative model that decom- poses audio spectra as sparse linear combinations of “ﬁlters” in the log-spectral domain. PoF makes similar assumptions to those used in the classic homomorphic ﬁltering approach to signal processing, but replaces decompositions built of basic signal processing operations with a learned decomposition based on statistical in- ference. This paper formulates the PoF model and derives a mean-ﬁeld method for posterior inference and a variational EM algorithm to estimate the model’s free parameters. We demonstrate PoF’s potential for audio processing on a bandwidth expansion task, and show that PoF can serve as an effective unsupervised feature extractor for a speaker identiﬁcation task.  1 Introduction  Some of the most successful approaches to audio signal processing of the last ﬁfty years have been based on decomposing complicated systems into an excitation signal and some number of simpler linear systems. One of the simplest (and most widely used) examples is linear predictive coding (LPC), which uses a simple autoregressive model to decompose an audio signal into an excitation signal and linear ﬁlter [17]. More broadly, homomorphic ﬁltering methods such as cepstral analysis [16] try to decompose complicated linear systems into a set of simpler linear systems that can then be analyzed, interpreted, and manipulated independently.  One reason that this broad approach has been successful is because it is consistent with the way many real-world objects actually generate sound. An important example is the human voice: human vocal sounds are generated by running vibrations generated by the vocal folds through the rest of the vocal tract (tongue, lips, jaw, etc.), which approximately linearly ﬁlters the vibrations that come from the larynx or lungs.  Traditional approaches typically rely on hand-designed decompositions built of basic operations such as Fourier transforms, discrete cosine transforms, and least-squares solvers. In this paper we take a more data-driven approach, and derive a generative product-of-ﬁlters (PoF) model that learns a statistical-inference-based decomposition that is tuned to be appropriate to the data being ana- lyzed. Like traditional homomorphic ﬁltering approaches, PoF decomposes audio spectra as linear combinations of ﬁlters in the log-spectral domain. Unlike previous approaches, these ﬁlters are learned from data rather than selected from convenient families such as orthogonal cosines, and the PoF model learns a sparsity-inducing prior that prefers decompositions that use relatively few ﬁlters to explain each observed spectrum. The result when applied to speech data is that PoF discovers some ﬁlters that model excitation signals and some that model the various ﬁltering operations that the vocal tract can perform. Given a set of these learned ﬁlters, PoF can infer how much each ﬁl- ter contributed to a given audio magnitude spectrum, resulting in a sparse, compact, interpretable representation.  ∗This work was performed while Dawen Liang was an intern at Adobe Research.  1  The rest of the paper proceeds as follows. First, we formally introduce the product-of-ﬁlters (PoF) model, and give more rigorous intuitions about the assumptions that it makes. Next, we review some previous work and show how it relates to the PoF model. Then, we derive a mean-ﬁeld variational inference algorithm that allows us to do approximate posterior inference in PoF, as well as a variational EM algorithm that ﬁts the model’s free parameters to data. Finally, we demonstrate PoF’s potential for audio processing on a bandwidth expansion task, where it achieves better results than a recently proposed technique based on non-negative matrix factorization (NMF). We also evaluate PoF as an unsupervised feature extractor, and ﬁnd that the representation learned by the model yields higher accuracy on a speaker identiﬁcation task than the widely used mel-frequency cepstral coefﬁcient (MFCC) representation.  2 Product-of-Filters Model  We are interested in modeling audio spectrograms, which are collections of Fourier magnitude spec- tra W taken from some set of audio signals, where W is an F × T non-negative matrix; the cell Wf t gives the magnitude of the audio signal at frequency bin f and time window (often called a frame) t. Each column of W is the magnitude of the fast Fourier transform (FFT) of a short window of an audio signal, within which the spectral characteristics of the signal are assumed to be roughly stationary.  The motivation for our model comes from the widely used homomorphic ﬁltering approach to audio and speech signal processing [16], where a short window of audio w[n] is modeled as a convolution between an excitation signal e[n] (which might come from a speaker’s vocal folds) and the impulse response h[n] of a series of linear ﬁlters (such as might be implemented by a speaker’s vocal tract):  In the spectral domain after taking the FFT, this is equivalent to:  w[n] = (e ∗ h)[n]  |W[k]| = |E[k]| ◦ |H[k]| = exp{log |E[k]| + log |H[k]|}  (1)  (2)  where ◦ denotes element-wise multiplication and | · | denotes the magnitude of a complex value pro- duced by the FFT. Thus, the convolution between these two signals corresponds to a simple addition of their log-spectra. Another attractive feature is the symmetry between the excitation signal e[n] and the impulse response h[n] of the vocal-tract ﬁlter—convolution commutes, so mathematically (if not physiologically) the vocal tract could just as well be exciting the “ﬁlter” implemented by vocal folds.  We will likewise model the observed magnitude spectra as a product of ﬁlters. We assume each observed log-spectrum is approximately obtained by linearly combining elements from a pool of L log-ﬁlters1 U ≡ [u1|u2| · · · |uL] ∈ RF ×L:  log Wf t ≈ PlUf lalt,  (3)  where alt denotes the activation of ﬁlter ul in frame t. We will impose some sparsity on the activa- tions, to allow us to encode the intuition that not all ﬁlters are always active. This assumption ex- pands on the expressive power of the simple excitation-ﬁlter model of equation 1; we could recover that model by partitioning the ﬁlters into “excitations” and “vocal tracts”, requiring that exactly one “excitation ﬁlter” be active in each frame, and combining the weighted effects of all “vocal tract ﬁlters” into a single ﬁlter.  We have two main reasons for relaxing the classic excitation-ﬁlter model to include more than two ﬁlters, one computational and one statistical. The statistical rationale is that the parameters that deﬁne the human voice (pitch, tongue position, etc.) are inherently continuous, and so a very large dictionary of excitations and ﬁlters might be necessary to explain observed inter- and intra-speaker variability with the classic model. The computational rationale is that clustering models (which might try to determine which excitation is active) can be more fraught with local optima than fac- torial models such as ours (which tries to determine how active each ﬁlter is), and there is some precedent for relaxing clustering models into factorial models [3].  1We will use the term “ﬁlter” when referring to U for the rest of the paper.  2  Formally, we deﬁne the product-of-ﬁlters model:  alt ∼ Gamma(αl, αl)  Wf t ∼ Gamma(cid:16)γf , γf / exp(PlUf lalt)(cid:17)  where γf is the frequency-dependent noise level. We restrict the activations at (but not the ﬁlters ul) to be non-negative; if we allowed negative alt, then the the ﬁlters would be inverted, reducing the interpretability of the model.  Under this model  E[alt] = 1  E[Wf t] = exp(PlUf lalt).  For l ∈ {1, 2, · · · , L}, αl controls the sparseness of the activations associated with ﬁlter ul; smaller values of αl indicate that ﬁlter ul is used more rarely. From a generative point of view, one can view the model as ﬁrst drawing activations alt from a sparse prior, then applying multiplicative gamma  noise with expected value 1 to the expected value exp(Pl Uf lalt). A graphical model representation  of the PoF model is shown in Figure 1.  (4)  (5)  at  α  wt  T  U, γ  Figure 1: Graphical model representation of the PoF model.  In this paper we focus on speech applications, but the homomorphic ﬁltering approach has also been successfully applied to model other kinds of sounds such as musical instruments. For example, [13] treat the effect of the random excitation, string, and body as a chain of linear systems, which can therefore be modeled as a product of ﬁlters.  3 Related Work  The PoF model can be interpreted as a matrix factorization model, where we are trying to decompose the log-spectrogram. A closely related model is non-negative matrix factorization (NMF) [14] and its variations, e.g., NMF with sparseness constrains [10], convex NMF [4], and fully Bayesian NMF [2]. In NMF, a F × T non-negative matrix W is approximately decomposed into the product of two non-negative matrices: a F × K matrix V (often called the dictionary) and a K × T matrix H (often called the activations). NMF is widely used to analyze audio spectrograms [6, 19], largely due to its additivity property and the parts-based representation that it induces. It also often provides a semantically meaningful interpretation. For example, given the NMF decomposition of a piano sonata, the components in the dictionary are likely to correspond to notes with different pitches, and the activations will indicate when and how strongly each note is played. NMF’s ability to isolate energy coming from different sources in mixed recordings has made it a popular tool for addressing source separation problems.  Although both models decompose audio spectra using a linear combination of dictionary elements, NMF and PoF make fundamentally different modeling assumptions. NMF models each frame of a spectrogram as an additive combination of dictionary elements, which approximately corresponds to modeling the corresponding time-domain signal as a summation of parts. On the other hand, PoF models each frame of the spectrogram as a product of ﬁlters (sum of log-ﬁlters), which corresponds to modeling the corresponding time-domain signal as a convolution of ﬁlters. NMF is well suited to decomposing polyphonic sounds into mixtures of independent sources, whereas PoF is well suited to decomposing monophonic sounds into simpler systems.  In the compressive sensing literature, there has been a great deal of work on matrix factorization and dictionary learning by solving an optimization problem with sparseness constraints—adding ℓ1 norm penalty as a convex relaxation of ℓ0 norm penalty [5]. Online algorithms have also been  3  proposed to handle large data sets [15]. In principle, we could have formulated the PoF model similarly, using an ℓ1 penalty and convex optimization in place of a gamma prior and Bayesian inference. However, in such a formulation it is unclear how we might ﬁt the L hyperparameters α controlling the amount of sparsity in the activations associated with each ﬁlter. We found that PoF best explains speech training data when each ﬁlter ul has its own sparsity hyperparameter αl, and performing cross-validation to select so many hyperparameters would be impractical.  4 Inference and Parameter Estimation  From Figure 1, we can see that there are two computational problems that will arise when using the PoF model. First, given ﬁxed U, α, and γ and input spectrum wt, we must (approximately) compute the posterior distribution p(at|wt, U, α, γ). This will enable us to ﬁt the PoF model to unseen data and obtain a different representation in the latent ﬁlter space. Second, given a collection of training spectra W = {wt}1:T , we want to ﬁnd the maximum likelihood estimates of the free parameters U, α, and γ. In this section, we will tackle these two problems respectively. The detailed derivations can be found in the appendix. The source code in Python is available on Github2.  4.1 Mean-Field Posterior Inference  The posterior p(at|wt, U, α, γ) is intractable to compute due to the nonconjugacy of the model. Therefore, we employ mean-ﬁeld variational inference [12].  Variational inference is a deterministic alternative to Markov Chain Monte Carlo (MCMC) meth- ods. The basic idea behind variational inference is to choose a tractable family of variational dis- tributions q(at) to approximate the intractable posterior p(at|wt, U, α, γ), so that the Kullback- Leibler (KL) divergence between the variational distribution and the true posterior KL(qakpa|W) is minimized. In particular, we are using the mean-ﬁeld family which is completely factorized, i.e.,  q(at) = Ql q(alt). For each alt, we choose a variational distribution from the same family as alt’s t are free parameters that we will tune to  prior distribution: q(alt) = Gamma(alt; νa t and ρ minimize the KL divergence between q and the posterior. We can lower bound the marginal likelihood of the input spectrum wt:  lt). ν  lt, ρa  a  a  log p(wt|U, α, γ)  ≥ Eq[log p(wt, at|U, α, γ)] − Eq[log q(at)]  △ = L(ν  a t , ρ  a t )  (6)  To compute the variational lower bound L(ν lt/ρa and Eq[log alt] = ψ(νa lt, where ψ(·) is the digamma function, are both easy to compute. For Eq[exp(−Uf lalt)], we seek for the moment-generating function of a gamma-distributed random variable and obtain the expectation as:  t ), the necessary expectations Eq[alt] = νa  lt) − log ρa  a t , ρ  lt  a  Eq[exp(−Uf lalt)] = (cid:16)1 + Uf l  ρa  lt  lt (cid:17)−νa  (7)  lt, and +∞ otherwise3.  for Uf l > −ρa The nonconjugacy and the exponents in the likelihood model preclude optimizing the lower bound by traditional closed-form coordinate ascent updates. Instead, we compute the gradient of L(ν a t ) with respect to variational parameters ν t and use Limited-memory BFGS (L-BFGS) to opti- mize the variational lower bound, which guarantees to ﬁnd a local optimum and optimal variational parameters { ˆν Note that in the posterior inference, the optimization problem is independent for different frame t. Therefore, given input spectra {wt}1:T , we can break down the whole problem into T independent sub-problems which can be solved in parallel.  t and ρ  a a t , ˆρ  a t , ρ  t }.  a  a  2https://github.com/dawenl/pof 3Technically the expectation for Uf l ≤ −ρa  the variational lower bound goes to −∞ and the optimization can be carried out seamlessly.  lt is undeﬁned. Here we treat it as +∞ so that when Uf l ≤ −ρa  lt  4  4.2 Parameter Estimation  Given a collection of training audio spectra {wt}1:T , we carry out parameter estimation for the PoF model by ﬁnding the maximum likelihood estimates of the free parameters U, α, and γ, approxi- mately marginalizing out at. We formally deﬁne the parameter estimation problem as  ˆU, ˆα, ˆγ = arg max  U,α,γ Xt U,α,γ Xt  = arg max  log p(wt|U, α, γ)  (8)  logZat  p(wt, at|U, α, γ)dat  This problem can be solved by variational Expectation-Maximization (EM) which maximizes a lower bound on marginal likelihood in equation 6 with respect to the variational parameters, and then for the ﬁxed values of variational parameters, maximizes the lower bound with respect to the model’s free parameters U, α, and γ.  E-step For each wt where t = 1, 2, · · · , T , perform posterior inference by optimizing the values of the variational parameters { ˆν  t }. This is done as described in Section 4.1.  a a t , ˆρ  M-step Maximize the variational lower bound in equation 6, which is equivalent to maximizing the following objective:  Q(U, α, γ) = Pt  Eq[log p(wt, at|U, α, γ)]  (9)  This is accomplished by ﬁnding the maximum likelihood estimates using the expected sufﬁcient statistics for each at that were computed in the E-step. There are no simple closed-form updates for the M-step. Therefore, we compute the gradient of Q(U, α, γ) with respect to U, α, γ, respectively, and use L-BFGS to optimize the bound in equation 9.  The most time-consuming part for M-step is updating U, which is a F × L matrix. However, note that the optimization problem is independent for different frequency bins f ∈ {1, 2, · · · , F }. Therefore, we can update U by optimizing each row independently, and in parallel if desired.  5 Evaluation  We conducted experiments to assess the PoF model on two different tasks. We evaluate PoF’s ability to infer missing data in the bandwidth expansion task. We also explore the potential of the PoF model as an unsupervised feature extractor for the speaker identiﬁcation task.  Both tasks require pre-trained parameters U, α, and γ, which we learned from the TIMIT Speech Corpus [7]. It contains speech sampled at 16000 Hz from 630 speakers of eight major dialects of American English, each reading ten phonetically rich sentences. The parameters were learned from 20 randomly selected speakers (10 males and 10 females). We performed a 1024-point FFT with Hann window and 50% overlap, thus the number of frequency bins was F = 513. We performed the experiments on magnitude spectrograms except where speciﬁed otherwise.  We tried different model orders L ∈ {10, 20, · · · , 80} and evaluated the lower bound on the marginal likelihood log p(wt|U, α, γ) in equation 6. In general, larger L will give us a larger variational lower bound and will be slower to train. In our experiments, we set L = 50 as a compromise between performance and computational efﬁciency. We initialized all the variational parameters ν a t and ρ t with random draws from a gamma distribution with shape parameter 100 and inverse-scale parameter 100. This yields a diffuse and smooth initial variational posterior, which helped avoid bad local optima. We ran variational EM until the variational lower bound increased by less than 0.01%.  a  Figure 2 demonstrates some representative ﬁlters learned from the PoF model with L = 50. The six ﬁlters ul associated with the largest values of αl are shown in Figure 2a, and the six ﬁlters associated with the smallest values of αl are shown in Figure 2b. Small values of αl indicate a prior preference to use the associated ﬁlters less frequently, since the Gamma(αl, αl) prior places more mass near 0 when αl is smaller. The ﬁlters in Figure 2b, which are used relatively rarely, tend to have the  5  ) B d (   e d u t i n g a M  ) B d (   e d u t i n g a M  10 5 0 −5 −10 −15 −20 −25  0  20 15 10 5 0 −5 −10 −15 −20  0  ) B d (   e d u t i n g a M  40 20 0 −20 −40  0  ) B d (   e d u t i n g a M  30 20 10 0 −10 −20 −30  0  2  2  2  2  α = 1.68  4  Frequency (kHz) α = 0.96  4  Frequency (kHz)  6  6  6 4 2 0 −2 −4 −6 −8 −10 −12 0  15 10 5 0 −5 −10 −15 −20  0  8  8  α = 1.37  2  2  4  Frequency (kHz) α = 0.94  4  Frequency (kHz)  6  6  10 5 0 −5 −10 −15 −20 −25  0  25 20 15 10 5 0 −5 −10 −15 −20  0  8  8  α = 1.06  2  2  4  Frequency (kHz) α = 0.89  4  Frequency (kHz)  (a) The top 6 ﬁlters ul with the largest αl values (shown above each plot). α = 0.01  α = 0.01  α = 0.02  4  Frequency (kHz) α = 0.02  4  Frequency (kHz)  6  6  8  8  2  2  4  Frequency (kHz) α = 0.02  4  Frequency (kHz)  6  6  8  8  2  2  4  Frequency (kHz) α = 0.02  4  Frequency (kHz)  25 20 15 10 5 0 −5 −10 −15 −20  0  25 20 15 10 5 0 −5 −10 −15 −20  0  60 40 20 0 −20 −40 −60  0  30 20 10 0 −10 −20 −30  0  6  6  6  6  8  8  8  8  (b) The top 6 ﬁlters ul with the smallest αl values (shown above each plot).  Figure 2: The representative ﬁlters learned from the PoF model with L = 50.  strong harmonic structure displayed by the log-spectra of periodic signals, while the ﬁlters in Figure 2a tend to vary more smoothly, suggesting that they are being used to model the ﬁltering induced by the vocal tract. The periodic “excitation” ﬁlters tend to be used more rarely, which is consistent with the fact that normally there is not more than one excitation signal contributing to a speaker’s voice. (Very few people can speak or sing more than one pitch simultaneously.) The model has more freedom to use several of the coarser “vocal tract” ﬁlters per spectrum, which is consistent with the fact that several aspects of the vocal tract may be combined to ﬁlter the excitation signal generated by a speaker’s vocal folds.  Despite the non-convexity inherent to all dictionary-learning problems, which causes the details of the ﬁlters vary from run to run, training with multiple random restarts and different speakers produced little impact on the ﬁlters that the PoF model learned; in all cases with different model order L, we found the same “excitation/ﬁlter” structure, similar to what is shown in Figure 2.  5.1 Bandwidth Expansion  We demonstrate PoF’s potential in audio processing applications on a bandwidth expansion task, where the goal is to infer the contents of a full-bandwidth signal given only the contents of a band- limited version of that signal. Bandwidth expansion has applications to restoration of low-quality audio such as might be recorded from a telephone or cheap microphone.  bl t }1:T , the model implies a posterior distribution p(at|w  Given the parameters U, α, and γ ﬁt to full-bandwidth training data, we can treat the bandwidth expansion problem as a missing data problem. Given spectra from a band-limited recording Wbl = bl t ) over the activations at associated {w with the band-limited signal, for t = {1, 2, · · · , T }. We can approximate this posterior using the variational inference algorithm from Section 4.1 by only using the band-limited part of U and γ. Then we can reconstruct the full-bandwidth spectra by combining the inferred {at}1:T with the full-bandwidth U. Following the model formulation in equation 4, we might either estimate the full-bandwidth spectra using  or  Eq[W fb  f t] = Ql  Eq[exp(Uf lalt)]  Eq[W fb  f t] = exp{PlUf l · Eq[alt]}.  6  (10)  (11)  We use equation 11, both because it is more stable and because human auditory perception is log- arithmic; if we are summarizing the posterior distribution with a point estimate, the expectation on the log-spectral domain is more perceptually natural.  As a comparison, NMF is widely used for bandwidth expansion [1, 18, 20]. The full-bandwidth training spectra Wtrain, which are also used to learn the parameters U, α, and γ for the PoF model, are decomposed by NMF as Wtrain ≈ VH, where V is the dictionary and H is the activation. Then given the band-limited spectra Wbl, we can use the band-limited part of V to infer the activation Hbl. Finally, we can reconstruct the full-bandwidth spectra by computing VHbl. Based on how the loss function is deﬁned, there can be different types of NMF models: KL-NMF [14] which is based on Kullback-Leibler divergence, and IS-NMF [6] which is based on Itakura- Saito divergence, are among the most commonly used NMF decomposition models in audio signal processing. We compare the PoF model with both KL-NMF and IS-NMF with different model orders K = 25, 50, and 100. We used the standard multiplicative updates for NMF and stopped the iterations when the decrease in the cost function was less than 0.01%. For IS-NMF, we used power spectra instead of magnitude spectra, since the power spectrum representation is more consistent with the statistical assumptions that underlie the Itakura-Saito divergence.  We randomly selected 10 speakers (5 male and 5 female) from TIMIT that do not overlap with the speakers used to ﬁt the model parameters U, α, and γ, and took 3 sentences from each speaker as test data. We cut off all the contents below 400 Hz and above 3400 Hz to obtain band-limited recordings of approximately telephone-quality speech.  In previous NMF-based bandwidth expansion work [1, 18, 20], all experiments are done in a speaker- dependent setting, which means the model is trained from the target speaker. What we are doing here, on the other hand, is speaker-independent: we use no prior knowledge about the speciﬁc speaker whose speech is being restored4. To our knowledge, little if any work has been done on speaker-independent bandwidth expansion based on NMF decompositions.  To evaluate the quality of the reconstructed recordings, we used the composite objective measure [11] and short-time objective intelligibility [21] metrics. These metrics measure different aspects of the “distance” between the reconstructed speech and the original speech. The composite objective measure (will be abbreviated as OVRL, as it reﬂects the overall sound quality) was originally pro- posed as a quality measure for speech enhancement. It aggregates different basic objective measures and has been shown to correlate with humans’ perceptions of audio quality. OVRL is based on the predicted perceptual auditory rating and is in the range of 1 to 5 (1: bad; 2: poor; 3: fair; 4: good; 5: excellent). The short-time objective intelligibility measure (STOI) is a function of the clean speech and reconstructed speech, which correlates with the intelligibility of the reconstructed speech, that is, it predicts the ability of listeners to understand what words are being spoken rather than per- ceived sound quality. STOI is computed as the average correlation coefﬁcient from 15 one-third octave bands across frames, thus theoretically should be in the range of -1 to 1, where larger values indicate higher expected intelligibility. However, in practice, even when we ﬁlled out the missing contents with random noise, the STOI is 0.306 ± 0.016, which can be interpreted as a practical lower bound on the test data. The average OVRL and STOI with two standard errors5 across 30 sentences for different methods, along with these from the band-limited input speech as baseline, are reported in Table 1. We can see that NMF improves STOI by a small amount, and PoF improves it slightly more, but the improve- ment in both cases is fairly small. This may be because the band-limited input speech already has a relatively high STOI (telephone-quality speech is fairly intelligible). On the other hand, PoF pro- duces better predicted perceived sound quality as measured by OVRL than KL-NMF and IS-NMF by a large margin regardless of the model order K, improving the sound quality from poor-to-fair (2.71 OVRL) to fair-to-good (3.25 OVRL).  4When we conducted speaker-dependent experiments, both PoF and NMF produced nearly ceiling-level results. Thus we only report results on the harder and more practically relevant speaker-independent problem.  5For both OVRL and STOI, we used the MATLAB implementation from the original authors.  7  Table 1: Average OVRL (composite objective measure) and STOI (short-time objective intelligibil- ity) score with two standard errors (in parenthesis) for the bandwidth expansion task from different methods. OVRL is in the range of 1 to 5 [1: bad; 2: poor; 3: fair; 4: good; 5: excellent]. STOI is the average correlation coefﬁcient, thus theoretically should be in the range of -1 to 1, where larger values indicate higher expected intelligibility.  KL-NMF  Band-limited input K=25 K=50 K=100 K=25 K=50 K=100  IS-NMF  PoF  OVRL  1.72 (0.16) 2.60 (0.12) 2.71 (0.14) 2.41 (0.10) 2.43 (0.15) 2.62 (0.12) 2.15 (0.10) 3.25 (0.13)  STOI  0.762 (0.012) 0.786 (0.013) 0.790 (0.013) 0.759 (0.012) 0.779 (0.013) 0.774 (0.014) 0.751 (0.012) 0.804 (0.010)  5.2 Feature Learning and Speaker Identiﬁcation  We explore PoF’s potential as an unsupervised feature extractor. One way to interpret the PoF model is that it attempts to represent the data in a latent ﬁlter space. Therefore, given spectra {wt}1:T , we can use the coordinates in the latent ﬁlter space {at}1:T as features. Since we believe the inter- and intra-speaker variability is well-captured by the PoF model, we use speaker identiﬁcation to evaluate the effectiveness of these features.  We compare our learned representation with mel-frequency cepstral coefﬁcients (MFCCs), which are widely used in various speech and audio processing tasks including speaker identiﬁcation. MFCCs are computed by taking the discrete cosine transform (DCT) on mel-scale log-spectra and using only the low-order coefﬁcients. PoF can be understood in similar terms; we are also trying to explain the variability in log-spectra in terms of a linear combination of dictionary elements. However, instead of using the ﬁxed, orthogonal DCT basis, PoF learns a ﬁlter space that is tuned to the statistics of the input. Therefore, it seems reasonable to hope that the coefﬁcients at from the PoF model, which will be abbreviated as PoFC, might compare favorably with MFCCs as a feature representation.  We evaluated speaker identiﬁcation under the following scenario: identify different speakers from recordings where each speaker may start and ﬁnish talking at random time, but at any given time there is only one speaker speaking (like during a meeting). This is very similar to speaker diarization [8], but here we assume we know a priori the number of speakers and certain amount of training data is available for each speaker. Our goal in this experiment was to demonstrate that the PoFC repre- sentation captures information that is missing or difﬁcult to extract from the MFCC representation, rather than trying to build a state-of-the-art speaker identiﬁcation system.  We randomly selected 10 speakers (5 males and 5 females) from TIMIT outside the training data we used to learn the free parameters U, α, and γ. We used the ﬁrst 13 DCT coefﬁcients which is a standard choice for computing MFCC. We obtained the PoFC by doing posterior inference as described in Section 4.1 and used Eq[at] as a point estimate summary. For both MFCC and PoFC, we computed the ﬁrst-order and second-order differences and concatenated them with the original feature.  We treat speaker identiﬁcation problem as a classiﬁcation problem and make predictions for each frame. We trained a multi-class (one-vs-the-rest) linear SVM using eight sentences from each speaker and tested with the remaining two sentences, which gave us about 7800 frames of train- ing data and 1700 frames of test data. The test data was randomly permuted so that the order in which sentences appear is random, which simulates the aforementioned scenario.  The frame level accuracy is reported in the ﬁrst row of Table 2. We can see PoFC increases the accuracy by a large margin (from 49.1% to 60.5%). To make use of temporal information, we used a simple median ﬁlter smoother with length 25, which boosts the performance for both representations equally; these results are reported in the second row of Table 2.  8  Table 2: 10-speaker identiﬁcation accuracy using PoFC, MFCC, and combined. The ﬁrst row shows the raw frame-level test accuracy. The second row shows the result after applying a simple median ﬁlter with length 25 on the frame-level prediction.  Frame-level Smoothing  MFCC 49.1% 60.5% 74.2% 85.0%  65.5% 89.5%  PoFC MFCC + PoFC  Although MFCCs and PoFCs capture similar information, concatenating both sets of features yields better accuracy than that obtained by either feature set alone. The results achieved by combining the features are summarized in the last column of Table 2, which indicates that MFCCs and PoFCs capture complementary information. These results, which use a relatively simple frame-level classi- ﬁer, suggest that PoFC could produce even better accuracy when used in a more sophisticated model (e.g. a deep neural network).  6 Discussion and Future Work  In this paper, we proposed the product-of-ﬁlters (PoF) model, a generative model which makes sim- ilar assumptions to those used in the classic homomorphic ﬁltering approach to signal processing. We derived variational inference and parameter estimation algorithms, and demonstrated experi- mental improvements on a bandwidth expansion task and showed that PoF can serve as an effective unsupervised feature extractor for speaker identiﬁcation.  In this paper, we derived PoF as a standalone model. However, it can also be used as a building block and integrated into a larger model, e.g., as a prior for the dictionary in a probabilistic NMF model.  Although the optimization in the variational EM algorithm can be parallelized, currently we cannot ﬁt PoF to large-scale speech data on a single machine. Leveraging recent developments in stochastic variational inference [9], it would be possible to learn the free parameters from a much larger, more diverse speech corpus, or even from streams of data.  References  [1] D. Bansal, B. Raj, and P. Smaragdis. Bandwidth expansion of narrowband speech using non-  negative matrix factorization. In INTERSPEECH, pages 1505–1508, 2005.  [2] A. T. Cemgil. Bayesian inference for nonnegative matrix factorisation models. Computational  Intelligence and Neuroscience, 2009.  [3] C. Ding and X. He. K-means clustering via principal component analysis. In Proceedings of  the International Conference on Machine learning, page 29. ACM, 2004.  [4] C. Ding, T. Li, and M. I. Jordan. Convex and semi-nonnegative matrix factorizations. Pattern  Analysis and Machine Intelligence, IEEE Transactions on, 32(1):45–55, 2010.  [5] D. L. Donoho and M. Elad. Optimally sparse representation in general (nonorthogonal) dictio- naries via ℓ1 minimization. Proceedings of the National Academy of Sciences, 100(5):2197– 2202, 2003.  [6] C. F´evotte, N. Bertin, and J.-L. Durrieu. Nonnegative matrix factorization with the Itakura- Saito divergence: with application to music analysis. Neural Computation, 21(3):793–830, Mar. 2009.  [7] W. M. Fisher, G. R. Doddington, and K. M. Goudie-Marshall. The DARPA speech recognition research database: speciﬁcations and status. In Proc. DARPA Workshop on speech recognition, pages 93–99, 1986.  [8] E.B. Fox, E.B. Sudderth, M.I. Jordan, and A.S. Willsky. A Sticky HDP-HMM with Application  to Speaker Diarization. Annals of Applied Statistics, 5(2A):1020–1056, 2011.  [9] M. Hoffman, D. Blei, C. Wang, and J. Paisley. Stochastic variational inference. Journal of  Machine Learning Research, 14:1303–1347, 2013.  9  [10] P. O. Hoyer. Non-negative matrix factorization with sparseness constraints. The Journal of  Machine Learning Research, 5:1457–1469, 2004.  [11] Y. Hu and P. C. Loizou. Evaluation of objective quality measures for speech enhancement.  Audio, Speech, and Language Processing, IEEE Transactions on, 16(1):229–238, 2008.  [12] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational  methods for graphical models. Machine learning, 37(2):183–233, 1999.  [13] M. Karjalainen, V. V¨alim¨aki, and Z. J´anosy. Towards high-quality sound synthesis of the guitar and string instruments. In Proceedings of the International Computer Music Conference, pages 56–56, 1993.  [14] D.D. Lee and H.S. Seung. Algorithms for non-negative matrix factorization. Advances in  Neural Information Processing Systems, 13:556–562, 2001.  [15] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online learning for matrix factorization and sparse  coding. The Journal of Machine Learning Research, 11:19–60, 2010.  [16] A. Oppenheim and R. Schafer. Homomorphic analysis of speech. Audio and Electroacoustics,  IEEE Transactions on, 16(2):221–226, 1968.  [17] D. O’Shaughnessy. Linear predictive coding. Potentials, IEEE, 7(1):29–32, 1988. [18] B. Raj, R. Singh, M. Shashanka, and P. Smaragdis. Bandwidth expansionwith a P´olya urn model. In Acoustics, Speech and Signal Processing, IEEE International Conference on, vol- ume 4, pages IV–597. IEEE, 2007.  [19] P. Smaragdis and J. C. Brown. Non-negative matrix factorization for polyphonic music tran- scription. In Applications of Signal Processing to Audio and Acoustics, IEEE Workshop on., pages 177–180. IEEE, 2003.  [20] D. L. Sun and R. Mazumder. Non-negative matrix completion for bandwidth extension: A In Machine Learning for Signal Processing (MLSP), IEEE  convex optimization approach. International Workshop on, pages 1–6. IEEE, 2013.  [21] C. H. Taal, R. C. Hendriks, R. Heusdens, and J. Jensen. An algorithm for intelligibility pre- diction of time–frequency weighted noisy speech. Audio, Speech, and Language Processing, IEEE Transactions on, 19(7):2125–2136, 2011.  10  A Variational EM for Product-of-Filters Model  A.1 E-step (Posterior Inference)  Following Section 4.1, the variational lower bound for the E-step (equation 6):  log p(wt|U, α, γ)  q(at)  p(wt, at|U, α, γ)  q(at)  dat  = logZat ≥Zat  q(at) log  p(wt, at|U, α, γ)  q(at)  dat  (12)  = Eq[log p(wt, at|U, α, γ)] − Eq[log q(at)] ≡ L(ν  a t , ρ  a t )  The second term is the entropy of a gamma-distributed random variable:  − Eq[log q(at)]  =Xl (cid:16)νa  lt − log ρa  lt + log Γ(νa  lt) + (1 − νa  lt)(cid:17) lt)ψ(νa  For the ﬁrst term, we can keep the parts which depend on {ν  a t , ρ  a  t }:  Eq[log p(wt, at|U, α, γ)]  = Eq[log p(wt|at, U, γ)] + Eq[log p(at|α)]  γfnWf tYl = const +Xl n(αl − 1)Eq[log alt] − αlEq[alt]o −Xf lt and ρa lt:  Take the derivative of L(ν  t ) with respect to νa  a t , ρ  a  Eq[exp(−Uf lalt)] +Xl  Uf lEq[alt]o  ∂L ∂νa lt  = Xf  γf(cid:26)Wf t log(1 +  Uf l ρa lt  )  L  Yj=1  Eq[exp(−Uf jajt)] −  Uf l ρa  lt (cid:27)+(αl − νa  lt)ψ1(νa  lt) + 1 −  αl ρa lt  ∂L ∂ρa lt  =  νlt (ρa  lt)2 Xf  γf(cid:26)Uf l − Wf t(1 +  Uf l ρa lt  )−1Uf l  L  Yj=1  Eq[exp(−Uf jajt)](cid:27)+αl(  νlt lt)2 − (ρa  1 ρa lt  )  where ψ1(·) is the trigamma function.  A.2 M-step  The objective function for M-step is:  Q(U, α, γ) = Xt = Xt  Eq[log p(wt, at|U, α, γ)]  Eq[log p(wt|at, U, γ)] + Eq[log p(at|α)]  (13)  where  Eq[log p(wt|at, U, γ)]  =Xf (cid:16)γf log γf − γf Xl  Uf lEq[alt] − log Γ(γf ) + (γf − 1) log Wf t − Wf tγf Yl  Eq[exp(−Uf lalt)](cid:17)  Eq[log p(at|α)]  =Xl (cid:16)αl log αl − log Γ(αl) + (αl − 1)Eq[log alt] − αlEq[alt](cid:17)  11  Take the derivative with respect to U, α, γ, we obtain the following gradients:  ∂Q ∂Uf l  ∂Q ∂αl  ∂Q ∂γf  Uf l ρa lt  )−(νa  = Xt (cid:16) − Eq[alt] + Wf tEq[alt](1 + lt+1)Yj6=l = Xt (cid:16) log αl + 1 − ψ(αl) + Eq[log alt] − Eq[alt](cid:17) = Xt (cid:16) log γf −Xl  Uf lEq[alt] + 1 − ψ(γf ) + log Wf t − Wf tYl  Eq[exp(−Uf jajt)](cid:17)  (14)  (15)  Eq[exp(−Uf lalt)](cid:17)  (16)  Note that the optimization problem for U is independent for different frequency bin f ∈ {1, 2, · · · , F }, as reﬂected by the gradient.  12  ","We propose the product-of-filters (PoF) model, a generative model thatdecomposes audio spectra as sparse linear combinations of ""filters"" in thelog-spectral domain. PoF makes similar assumptions to those used in the classichomomorphic filtering approach to signal processing, but replaces hand-designeddecompositions built of basic signal processing operations with a learneddecomposition based on statistical inference. This paper formulates the PoFmodel and derives a mean-field method for posterior inference and a variationalEM algorithm to estimate the model's free parameters. We demonstrate PoF'spotential for audio processing on a bandwidth expansion task, and show that PoFcan serve as an effective unsupervised feature extractor for a speakeridentification task."
1312.5578,2014,Multimodal Transitions for Generative Stochastic Networks  ,"['Sherjil Ozair', 'Li Yao', 'Yoshua Bengio']",https://arxiv.org/pdf/1312.5578.pdf,"4 1 0 2     n a J    4 2      ]  G L . s c [      4 v 8 7 5 5  .  2 1 3 1 : v i X r a  Multimodal Transitions for Generative Stochastic  Networks  Sherjil Ozair, Li Yao, & Yoshua Bengio  D´epartement d’informatique et de recherche op´erationnelle  Universit´e de Montr´eal Montr´eal, QC H3C 3J7  Abstract  Generative Stochastic Networks (GSNs) have been recently introduced as an al- ternative to traditional probabilistic modeling: instead of parametrizing the data distribution directly, one parametrizes a transition operator for a Markov chain whose stationary distribution is an estimator of the data generating distribution. The result of training is therefore a machine that generates samples through this Markov chain. However, the previously introduced GSN consistency theorems suggest that in order to capture a wide class of distributions, the transition oper- ator in general should be multimodal, something that has not been done before this paper. We introduce for the ﬁrst time multimodal transition distributions for GSNs, in particular using models in the NADE family (Neural Autoregressive Density Estimator) as output distributions of the transition operator. A NADE model is related to an RBM (and can thus model multimodal distributions) but its likelihood (and likelihood gradient) can be computed easily. The parameters of the NADE are obtained as a learned function of the previous state of the learned Markov chain. Experiments clearly illustrate the advantage of such multimodal transition distributions over unimodal GSNs.  1  Introduction  A new approach to unsupervised learning of probabilistic models has recently been proposed [11, 10], called Generative Stochastic Networks (GSNs), based on learning the operator (transition dis- tribution) of a Markov chain that generates samples from the learned distribution. In Section 3, GSNs are formally deﬁned and mathematical results are provided on the consistency achieved when training GSNs with a denoising criterion. Denoising had previously been proposed as a criterion for unsupervised feature learning of denoising auto-encoders [31, 32], one of the successful building blocks for deep learning [5]. The motivation for GSNs is that the transition distribution of a Markov chain, given the previous state, is generally simpler than the stationary distribution of the Markov chain, i.e., it has a simpler structure and less major modes (the stationary distribution can be viewed as a huge mixture, over all states, of these transition distributions). This could make it easier to learn GSNs because one factor in the difﬁculty of training probabilistic models is the complexity of ap- proximating their normalizing constant (partition function). If a distribution has fewer major modes or less complicated structure in them (in the extreme, if it is factorized or unimodal), then its parti- tion function can be computed or approximated more easily. However, previous work with GSNs has focused only on the extreme case where the transition distribution is parametrized by a factorized or unimodal distribution. This paper starts by reminding the reader of good reasons for unsupervised learning of generative models (Section 2) and two of the fundamental challenges involved, namely the difﬁculty of mixing between modes and presence of a potentially very large number of non-tiny modes. It argues in favor of GSNs with multi-modal transition distributions (Section 4) to address the second problem and proposes (Section 5) to use conditional NADE [23, 15] models (similar to  1  conditional RBMs [30, 29] but with a tractable likelihood) to represent multi-modal output distribu- tions for these transition operators. Experiments are performed with denoising auto-encoder neural networks, which were originally proposed as simple implementations of GSNs (Section 6). The results on both artiﬁcial (2D for easy visualization) and real data (MNIST) clearly show that mul- timodality helps GSNs to better capture the data generating distribution, reducing spurious modes by allowing the transition operator to not be forced to put probability mass in the middle of two or more major modes of the true transition distribution.  2 Promises and Challenges of Unsupervised Learning of Generative Models  Unsupervised learning remains one of the core challenges for progress in deep learning algorithms, with current applications of deep learning mostly based on supervised deep nets. Unsupervised learning could hold the key to many advantages for which purely supervised deep learning is inade- quate:  • It allows a learner to take advantage of unlabeled data. Most of the data available to ma- chines (and to humans and animals) is unlabeled, i.e., without a precise and symbolic char- acterization of its semantics and of the outputs desired from a learner. • It allows a learner to capture enough information about the observed variables so as to be able to answer new questions about them in the future, that were not anticipated at the time of seeing the training examples. • It has been shown to be a good regularizer for supervised learning [17], meaning that it can help the learner generalize better, especially when the number of labeled examples is small. This advantage clearly shows up (e.g., as illustrated in the transfer learning competitions with unsupervised deep learning [6, 25, 18]) in practical applications where the distribution changes or new classes or domains are considered (transfer learning, domain adaptation), when some classes are frequent while many others are rare (fat tail or Zipf distribution), or when new classes are shown with zero, one or very few examples (zero-shot and one-shot learning [24, 22]). • There is evidence suggesting that unsupervised learning can be successfully achieved mostly from a local training signal (as indicated by the success of the unsupervised layer- wise pre-training procedures [5], semi-supervised embedding [33], and intermediate-level hints [21]), i.e., that it may suffer less from the difﬁculty of propagating credit across a large network, which has been observed for supervised learning. • A recent trend in machine learning research is to apply machine learning to problems where the output variable is very high-dimensional (structured output models), instead of just a few numbers or classes. For example, the output could be a sentence or an image. In that case, the mathematical and computational issues involved in unsupervised learning also arise.  What is holding us back from fully taking advantage of all these advanges? We hypothesise that this is mainly because unsupervised learning procedures for ﬂexible and expressive models based on probability theory and the maximum likelihood principle or its close approximations [26] are almost all facing the challenge of marginalizing (summing, sampling, or maximizing) across many expla- nations or across many conﬁgurations of random variables. This is also true of structured output models, or anytime we care about capturing complicated joint probabilistic relationships between many variables. This is best exempliﬁed with the most classical of the deep learning algorithms, i.e., Boltzmann machines [2] with latent variables. In such models, one faces the two common intractable sums involved when trying to compute the data likelihood or its gradient (which is nec- essary for learning), or simply to use the trained model for new inputs:  • The intractable sum or maximization over latent variables, given inputs, which is necessary for inference, i.e., when using the model to answer questions about new examples, or as part of the training procedure. • The intractable sum or maximization over all the variables (the observed ones and the latent ones), which is necessary for learning, in order to compute or estimate the gradient of the log-likelihood, in particular the part due to the normalization constant of the probability  2  distribution. This intractable sum is often approximated with Monte-Carlo Markov Chain (MCMC) methods, variational methods, or optimization methods (the MAP or Maximum A Posteriori approximation).1  In principle, MCMC methods should be able to handle both of these challenges. However, MCMC methods can suffer from two fundamental issues, discussed in Bengio et al. [10] in more detail:  • Mixing between well-separated modes. A mode is a local maximum of probability (by extension, we refer to a connected region of high probability). According to the manifold hypothesis2 [16, 27], high-probability modes are separated by vast areas of low probability. Unfortunately, this makes it exponentially difﬁcult for MCMC methods to travel between such modes, making the resulting Monte-Carlo averages unreliable (i.e., of very high vari- ance). • A potentially huge number of large modes. It may be the case that the number of high- probability modes is very large (up to exponentially large), when considering problems with many explanatory factors. When this is so, the traditional approximations based on MCMC, variational methods, or MAP approximations could all fall on their face. In particular, since MCMC methods can only visit one mode at a time, the resulting estimates could be highly unreliable.  These are of course hypotheses, they need to be better tested, and may be more or less severe depending on the type of data generating distribution being estimated. The work on GSNs aims at giving us a tool to test these hypotheses by providing algorithms where the issue of normalization constant can be greatly reduced.  3 Generative Stochastic Networks  A very recent result [11, 10] shows both in theory and in experiments that it is possible to capture the data generating distribution with a training principle that is completely different from the maximum likelihood principle. Instead of directly modeling P (x) for observed variables x, one learns the tran- sition operator of a Markov chain whose stationary distribution estimates P (x). The idea is that a conditional distribution of a Markov chain typically makes only “small” or “local” moves3, making the conditional distribution simpler in the sense of having fewer dominant modes. The surprising result is that there is at least one simple training criterion for such a transition operator, based on probabilistically undoing the perturbations introduced by some noise source. The transition operator of the Markov chain, P (xt, ht|xt−1, ht−1) (with observed variable x and latent state variables h) can be trained by using a denoising objective, in which xt−1 = x is ﬁrst mapped into a learned f (xt−1, ht−1, zt) (with a noise source zt) that destroys information about x, and from which the clean data point x is predicted and its probability P (xt = x|f (xt−1 = x, ht−1, zt)) approximately maximized. Since the corruption only leaves a relatively small number of conﬁgurations of x as probable explanations for the computed value of f (xt−1 = x, ht−1, zt), the reconstruction distribu- tion P (xt = x|f (xt−1 = x, ht−1, zt)) can generally be approximated with a much simpler model (compared to P (x)). In a sense, this is something that we are directly testing in this paper. The GSN framework therefore only addresses the issue of a potentially huge number of modes (problem 2, above). However, by training GSNs with deep representations, one can hope to take advantage of the recently observed superiority of trained representations (such as in deep auto- encoders or deep belief nets and deep Boltzmann machines) in terms of faster mixing between well separated modes [8]. The core mathematical result justifying this procedure is the following theorem, proven in Bengio et al. [10], and stating sufﬁcient conditions for the denoising criterion to yield a consistent estimator of the data generating distribution, as the stationary distribution of the Markov chain:  1The partition function problem goes away in directed graphical models, shifting completely the burden to  inference.  2stating that probability is concentrated in some low-dimensional regions; the manifold hypothesis holds  for most AI tasks of interest because most conﬁgurations of input variables are unlikely.  3if it did not make a move sampled from a distribution with small entropy it would have to reject most of  the time, according to the manifold hypothesis stating that probability mass is concentrated  3  for a given sequence of Xt’s.  training data X ∼ P(X) and independent noise Z ∼ P(Z) and in- Theorem 1 Let troduce a sequence of latent variables H deﬁned iteratively through a function f with Consider a model Ht = fθ1(Xt−1, Zt−1, Ht−1) Pθ2(X|fθ1(X, Zt−1, Ht−1)) trained (over both θ1 and θ2) so that Pθ2(X|H), for a given θ1, is a consistent estimator of the true P(X|H). Consider the Markov chain deﬁned above and assume that it converges to a stationary distribution πn over the X and H and with marginal πn(X), even in the limit as the number of training examples n → ∞. Then πn(X) → P(X) as n → ∞. A particularly simple family of GSNs is the denoising auto-encoder, studied in [11] as a GSN, i.e., as a generative model. In that case, Ht = fθ1(Xt−1, Zt−1, Ht−1) is just the parameter-less corruption of Xt−1 according to a predetermined corruption process such as adding Gaussian noise or multiplying by masking noise, also known as dropout (and Ht does not depend on a previous Ht−1). In Bengio et al. [10], this approach is used to successfully train an unsupervised architecture mim- icking the structure of a Deep Boltzmann Machine. In Goodfellow et al. [19] the same principle is used to provide a successful sampling scheme for the Multi-Prediction Deep Boltzmann Machine (MP-DBM), when its training procedure is viewed as a particular form of GSN. Note that the MP- DBM was shown to be particularly successful at classiﬁcation tasks, in spite of being trained with both the image and class being treated as visible variables in a single integrated training framework, unlike previous approaches for training DBMs [28]. Much more work is needed to better understand the GSN approach and expand it to larger models, structured output problems, and applications, and more importantly, to explore the space of architectures and models that can be trained by this novel principle. In this paper, we focus on one particular aspect of research on GSNs, namely the need for multimodality of the transition distribution, the one that computes the probability of the next visible state given the previous state, P (xt|f (xt−1, ht−1, zt)).  4 The Need for Multimodality of the Transition Distribution  Close inspection of Theorem 1 reveals that if the “true” data generating distribution requires P (xt = x|f (xt−1 = x, ht−1, zt)) to be multimodal, then that capability is required of our parametrization of the transition probability in order to get consistency. Otherwise we only get consistency in the family of functions that can be represented by such unimodal transition operators. We already know from a mathematical result in Alain and Bengio [3] that when the amount of corruption noise converges to 0 and the input variables have a smooth continuous density, then a unimodal Gaussian reconstruction density sufﬁces to fully capture the joint distribution. The price to pay for this easier parametrization of the conditional distribution is that the associated Markov chain would mix very slowly, making it much less useful as a generative model. At the other extreme, if the amount of corruption noise is “inﬁnite” (destroying all information about the previous state), then P (xt = x|f (xt−1 = x, ht−1, zt)) = P (xt = x) and we are back to a normal probabilistic model, which generally has a lot of modes. The advantage of multimodal GSNs such as discussed in this paper is that they allow us to explore the realm in between these extremes, where the transition operator is multimodal but yet the number of such modes remains small, making it relatively easy to learn the required conditional distributions. This is illustrated in Figure 2, which shows an example where having a unimodal reconstruction dis- tribution (factorial GSN, i.e., a regular denoising auto-encoder with factorized Gaussian or factorized Bernoulli output units) would yield a poor model of the data, with spurious samples generated in between the arms of the spiral near which the data generating distribution concentrates.  5 NADE and GSN-NADE Models  The Neural Auto-regressive Density Estimator (NADE) family of probability models [23] can cap- ture complex distributions (more so by increasing capacity, just like other neural nets or mixture models) while allowing the analytic and tractable computation of the likelihood. It is a descendant of previous neural “auto-regressive” neural networks [7], also based on decomposing P (x1, x2, . . . xn) i P (xi|x1, . . . xi−1), with a group of output units for each variable i and an architecture that prevents input xj to enter into the computation of P (xi|x1, . . . xi−1) when j ≥ i. Whereas the  into(cid:81)  4  Figure 1: Example illustrating the need for a multimodal reconstruction distribution. The red dots are samples from the model (P (X)), the yellow dot is a sample from the latent variable (the corrupted sample ˜X of a denoising auto-encoder, obtained by adding isotropic Gaussian noise with standard deviation 0.3) and the blue dots show samples of the multimodal reconstruction distribution P (X| ˜X). Clearly, there are two main modes from which X could have come to produce ˜X. A unimodal (e.g., factorial) distribution for P (X| ˜X) could not capture that and one would obtain as estimator a wide Gaussian covering both modes, i.e., corresponding to generating a lot of spurious samples in between the real modes.  models of Bengio and Bengio [7], Larochelle and Murray [23] were developed for discrete vari- ables, NADE was recently extended [12] to continuous variables by making P (xi|x1, . . . xi−1) a mixture of k Gaussians (whose means and variances may depend on x1, . . . xi−1). Like other parametric models, NADE can be used as the output distribution of a conditional prob- ability. This has already been done in the context of modeling musical sequences [15], with the outputs of a recurrent neural networks being the bias parameters of a NADE model capturing the conditional distribution of the next frame (a set of musical notes), given previous frames (as cap- tured by the state of the recurrent neural network). In this paper we propose to use a NADE or RNADE model to capture the output distribution of a transition operator for the Markov chain asso- ciated with a GSN. More speciﬁcally, the experiments performed here are for the case of a denoising auto-encoder, i.e., a neural network takes the corrupted previous state ˜X as input and attempts to denoise it probabilistically, i.e., it outputs the parameters of a NADE model (here, again the hidden and output layer biases) associated with the distribution of the next state (i.e. the clean input X), given ˜X. The weights of the NADE model are kept unconditional: because these weight matrices can be large, it makes more sense to try to only condition the biases, like has been done before with conditional RBMs [30, 29].  6 Experiments  In order to demonstrate the advantages of using a multimodal reconstruction distribution over the unimodal one, we ﬁrstly report experimental results on a 2D real-valued dataset. Two types of GSNs are trained and then used to generate samples, shown in Figure 2. Both GSN-NADE and fatorial GSN use Gaussian corruption p( ˜X|X) = X + N (0, σ). To make the comparison fair, both models use σ = 0.3. When the added noise is signiﬁcantly large, using a multimodal reconstruction distribution p(X| ˜X) is particularly advantageous, as is clearly demonstrated in the Figure. The samples generated by the GSN-NADE model resemble closely to the original training examples.  5  Figure 2: Top: Training examples. Bottom left: Samples from a GSN-NADE model with 5 Gaussian com- ponents per NADE output unit. Bottom right: samples from a GSN model with factorized reconstruction distribution.  The second set of experiments are conducted on MNIST, the handwritten digits dataset. The real- valued pixels are binarized with the threshold 0.5. A 784-2000-2000-784 NADE model is chosen as the reconstruction distribution of the GSN-NADE model. The biases of the ﬁrst 2000 hidden units of the NADE network are outputs of a 784-2000 neural network, which has ˜X as the input. The training of model uses the same procedure as in [13]. In addition, a dynamic noise is added on input pixels: each training example is corrupted with a salt-and-pepper noise that is uniformly sampled between 0 and 1. We ﬁnd in practice that using dynamic noise removes more spurious modes in the samples. To evaluate the quality of trained GSNs as generative models, we adopt the Conservative Sampling-based Log-likelihood (CSL) estimator proposed in [9]. The CSL estimates are computed on different numbers of consecutive samples, all starting from a random intialization, as opposed to computing the estimates based on samples collected every 100 steps originally reported in [9]. The detailed comparison is shown in Table 1. In the table, GSN-NADE denotes the model we propose in this work. The GSN-1-w is a GSN with factorial reconstruction distribution trained with the walkback procedure proposed in Bengio et al. [11]. GSN-1 denotes a GSN with a factorial reconstruction distribution with no walkback training. For the details of models being compared, please refer to [9]. Figure 3, Figure 4 and Figure 5 show collections of consecutively generated samples from all 3 types of models after training.  Table 1: The CSL estimates obtained using different numbers of consecutive samples of latent variables: H are collected with consecutive sampling steps of one Markov chain.  # samples GSN-NADE GSN-1 GSN-1-w -127 10k -109 50k 100k -107 -106 150k  -143 -124 -118 -114  -148 -131 -125 -121  Experiments conﬁrm that the GSN model with a NADE output distribution does a much better job than the GSN model with a factorized output distribution on learning and generating samples for the  6  Figure 3: Consecutive samples generated by training GSN-1, the single hidden layer GSN with no walkbacks.  Figure 4: Consecutive samples generated by training GSN-1-w, the single hidden layer GSN with walkbacks.  Figure 5: Consecutive samples generated by training GSN-NADE with no walkback training.  real-valued 2 dimensional spiral manifold (Figure 2). On MNIST, the differences between generated samples are also visually striking. As demonstrated in the picture, a factorial GSN without the help of the walkback training (Figure 3) tends to generate sharper digits but suffers from often getting stuck in the non-digit-like spurious modes. On the contrary, a GSN-NADE without the walkback training (Figure 5) alleviates this issue. In addition, GSN-NADE mixes much better among digits, It generates samples that are visually much better than the rest two factorial models. To show that GSN-NADE mixes better, we also computed the CSL estimate of the same GSN-NADE reported in Table 1 with 10k samples but all of which are collected every 100 steps. Surprisingly, the CSL estimate remains exactly the same as the one where samples are collected after every step of the Markov chain (shown in Table 1). Furthermore, as shown in Figure 5, its samples present more diversity in writing styles, directly resulting a signiﬁcantly better CSL estimate (CSL estimates are in log scale) than its factorial counterpart. GSN-NADE, however, is not able to outperform the factorial GSN trained with the walkback training (Figure 4) in terms of the CSL estimates. This is because factorial GSNs trained with walkbacks win by suppressing almost all the spurious modes, resulting in higher CSL estimates of the testset log-likelihood (CSL prefers blury digits to spurious modes).  7 Conclusion  In this paper we have motivated a form of generative stochastic networks (GSNs) in which the next- state distribution is multimodal and we have proposed to use conditional NADE distributions to capture this multimodality. Experiments conﬁrm that this allows the GSN to more easily capture the data distribution, avoiding the spurious samples that would otherwise be generated by a unimodal (factorial) output distribution (the kind that has been used in the past for GSNs and denoising auto- encoders).  7  In the MNIST experiments, we found that the beneﬁt of the walkback procedure was actually greater than the beneﬁt of the multimodality brought by the NADE output distribution. How about combin- ing both NADE and walkback training? The basic procedure for sampling from NADE is expensive because it involves sampling each input variable given all the previous ones, each time having to run through the NADE neural net. Applying the walkback training procedure to the GSN-NADE models therefore poses a computational problem because the walkback procedure involves sam- pling a few steps from the chain in the middle of the training loop (in a way similar to contrastive divergence training): whereas getting the likelihood gradient is fast with NADE (order of number of inputs times number of hidden units), sampling from NADE is much more expensive (multiply that by the number of inputs). Future work should therefore investigate other multimodal distributions (for which sampling is cheap) or ways to approximate the NADE sampling procedure with faster procedures.  Acknowledgments  We would like to thank the developers of Theano [14, 4] and Pylearn2 [20]. We would also like to thank NSERC, the Canada Research Chairs, and CIFAR for funding, as well as Compute Canada, and Calcul Qu´ebec for providing computational resources.  References [1] (-1). Advances in neural information processing systems 26 (nips’13). In Advances in Neural  Information Processing Systems 26 (NIPS’13). Nips Foundation (http://books.nips.cc).  [2] Ackley, D. H., Hinton, G. E., and Sejnowski, T. J. (1985). A learning algorithm for Boltzmann  machines. Cognitive Science, 9, 147–169.  [3] Alain, G. and Bengio, Y. (2013). What regularized auto-encoders learn from the data generating  distribution. In International Conference on Learning Representations (ICLR’2013).  [4] Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A., Bouchard, N., and Bengio, Y. (2012). Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.  [5] Bengio, Y. (2009). Learning deep architectures for AI. Now Publishers. [6] Bengio, Y. (2011). Deep learning of representations for unsupervised and transfer learning. In  JMLR W&CP: Proc. Unsupervised and Transfer Learning.  [7] Bengio, Y. and Bengio, S. (2000). Modeling high-dimensional discrete data with multi-layer  neural networks. In NIPS’99, pages 400–406. MIT Press.  [8] Bengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. (2013a). Better mixing via deep represen- tations. In Proceedings of the 30th International Conference on Machine Learning (ICML’13). ACM.  [9] Bengio, Y., Yao, L., and Cho, K. (2013b). Bounding the test log-likelihood of generative models.  Technical report, U. Montreal, arXiv.  [10] Bengio, Y., Thibodeau-Laufer, E., and Yosinski, J. (2013c). Deep generative stochastic net-  works trainable by backprop. Technical Report arXiv:1306.1091, Universite de Montreal.  [11] Bengio, Y., Yao, L., Alain, G., and Vincent, P. (2013d). Generalized denoising auto-encoders  as generative models. In NIPS’2013.  [12] Benigno, U., Iain, M., and Hugo, L. (2013). Rnade: The real-valued neural autoregressive  density-estimator. In NIPS’2013.  [13] Benigno Uria, Iain Murray, H. L. (2013). A deep and tractable density estimator. Technical  Report arXiv:1310.1757.  [14] Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-Farley, D., and Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Conference (SciPy). Oral Presentation.  [15] Boulanger-Lewandowski, N., Bengio, Y., and Vincent, P. (2012). Modeling temporal depen- dencies in high-dimensional sequences: Application to polyphonic music generation and tran- scription. In ICML’2012.  8  [16] Cayton, L. (2005). Algorithms for manifold learning. Technical Report CS2008-0923, UCSD. [17] Erhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vincent, P., and Bengio, S. (2010). Why does unsupervised pre-training help deep learning? Journal of Machine Learning Research, 11, 625–660.  [18] Goodfellow, I. J., Courville, A., and Bengio, Y. (2011). Spike-and-slab sparse coding for unsu- pervised feature discovery. In NIPS Workshop on Challenges in Learning Hierarchical Models. [19] Goodfellow, I. J., Mirza, M., Courville, A., and Bengio, Y. (2013a). Multi-prediction deep  Boltzmann machines. In [1].  [20] Goodfellow, I. J., Warde-Farley, D., Lamblin, P., Dumoulin, V., Mirza, M., Pascanu, R., Bergstra, J., Bastien, F., and Bengio, Y. (2013b). Pylearn2: a machine learning research library. arXiv preprint arXiv:1308.4214.  [21] Gulcehre, C. and Bengio, Y. (2013). Knowledge matters: Importance of prior information for  optimization. In International Conference on Learning Representations (ICLR’2013).  [22] Lake, B., Salakhutdinov, R., and Tenenbaum, J. (2013). One-shot learning by inverting a  compositional causal process. In NIPS’2013.  [23] Larochelle, H. and Murray, I. (2011). The Neural Autoregressive Distribution Estimator. In Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS’2011), volume 15 of JMLR: W&CP.  [24] Larochelle, H., Erhan, D., and Bengio, Y. (2008). Zero-data learning of new tasks. In AAAI  Conference on Artiﬁcial Intelligence.  [25] Mesnil, G., Dauphin, Y., Glorot, X., Rifai, S., Bengio, Y., Goodfellow, I., Lavoie, E., Muller, X., Desjardins, G., Warde-Farley, D., Vincent, P., Courville, A., and Bergstra, J. (2011). Un- supervised and transfer learning challenge: a deep learning approach. In JMLR W&CP: Proc. Unsupervised and Transfer Learning, volume 7.  [26] Murphy, K. P. (2012). Machine Learning: a Probabilistic Perspective. MIT Press, Cambridge,  MA, USA.  [27] Narayanan, H. and Mitter, S. (2010). Sample complexity of testing the manifold hypothesis.  In NIPS’2010.  [28] Salakhutdinov, R. and Hinton, G. (2009). Deep Boltzmann machines.  In Proceedings of the Twelfth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2009), volume 8.  [29] Taylor, G. and Hinton, G. (2009). Factored conditional restricted Boltzmann machines for modeling motion style. In L. Bottou and M. Littman, editors, ICML 2009, pages 1025–1032. ACM.  [30] Taylor, G., Hinton, G. E., and Roweis, S. (2007). Modeling human motion using binary latent  variables. In NIPS’06, pages 1345–1352. MIT Press, Cambridge, MA.  [31] Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. (2008). Extracting and composing  robust features with denoising autoencoders. In ICML 2008.  [32] Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol, P.-A. (2010). Stacked de- noising autoencoders: Learning useful representations in a deep network with a local denoising criterion. J. Machine Learning Res., 11.  [33] Weston, J., Ratle, F., and Collobert, R. (2008). Deep learning via semi-supervised embedding.  In ICML 2008.  9  ","Generative Stochastic Networks (GSNs) have been recently introduced as analternative to traditional probabilistic modeling: instead of parametrizing thedata distribution directly, one parametrizes a transition operator for a Markovchain whose stationary distribution is an estimator of the data generatingdistribution. The result of training is therefore a machine that generatessamples through this Markov chain. However, the previously introduced GSNconsistency theorems suggest that in order to capture a wide class ofdistributions, the transition operator in general should be multimodal,something that has not been done before this paper. We introduce for the firsttime multimodal transition distributions for GSNs, in particular using modelsin the NADE family (Neural Autoregressive Density Estimator) as outputdistributions of the transition operator. A NADE model is related to an RBM(and can thus model multimodal distributions) but its likelihood (andlikelihood gradient) can be computed easily. The parameters of the NADE areobtained as a learned function of the previous state of the learned Markovchain. Experiments clearly illustrate the advantage of such multimodaltransition distributions over unimodal GSNs."
1301.3584,2014,Revisiting Natural Gradient for Deep Networks  ,"['Razvan Pascanu', 'Yoshua Bengio']",https://arxiv.org/pdf/1301.3584.pdf,"Revisiting natural gradient for deep networks  Razvan Pascanu  Universit´e de Montr´eal  Montr´eal QC H3C 3J7 Canada r.pascanu@gmail.com  Yoshua Bengio  Universit´e de Montr´eal  Montr´eal QC H3C 3J7 Canada  yoshua.bengio@umontreal.ca  4 1 0 2     b e F 7 1         ]  G L . s c [      7 v 4 8 5 3  .  1 0 3 1 : v i X r a  Abstract  We evaluate natural gradient descent, an algorithm originally proposed in Amari (1997), for learning deep models. The contributions of this paper are as follows. We show the connection between natural gradient descent and three other recently proposed methods for training deep models: Hessian-Free Optimization (Martens, 2010), Krylov Subspace Descent (Vinyals and Povey, 2012) and TONGA (Le Roux et al., 2008). We describe how one can use unlabeled data to improve the generalization error obtained by natural gradient and empirically evaluate the ro- bustness of the algorithm to the ordering of the training set compared to stochastic gradient descent. Finally we extend natural gradient descent to incorporate second order information alongside the manifold information and provide a benchmark of the new algorithm using a truncated Newton approach for inverting the metric ma- trix instead of using a diagonal approximation of it.  Introduction  1 Several recent papers tried to address the issue of using better optimization techniques for machine learning, especially for training deep architectures or neural networks of various kinds. Hessian- Free Optimization (Martens, 2010; Martens and Sutskever, 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012; Mizutani and Demmel, 2003), natural gradient descent (Amari, 1997; Park et al., 2000), TONGA (Le Roux et al., 2008; Roux and Fitzgibbon, 2010) are just a few of such recently proposed algorithms. They usually can be split in different categories: those which make use of second order information, those which use the geometry of the underlying parameter manifold (e.g. natural gradient descent) or those that use the uncertainty in the gradient (e.g. TONGA). One particularly interesting pipeline to scale up such algorithms was originally proposed in Pearl- mutter (1994) – ﬁnetuned in Schraudolph (2002) – and represents the backbone behind both Hessian- Free Optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012; Mizu- tani and Demmel, 2003). The core idea behind it is to make use of the forward pass (renamed to R-operator in Pearlmutter (1994)) and backward pass of automatic differentiation to compute efﬁ- cient products between Jacobian or Hessian matrices and vectors. These products are used within a truncated-Newton approach (Nocedal and Wright, 2000) which considers the exact Hessian and only inverts it approximately without the need for explicitly storing the matrix in memory, as op- posed to other approaches which perform a more crude approximation of the Hessian (or Fisher) matrix (either diagonal or block-diagonal). The original contributions of this paper to the study of natural gradient descent are as follows. Section 5 describes the connection between natural gradient and Hessian Free (Martens, 2010), section 6 looks at the relationship with Krylov Subspace Descent (KSD) (Vinyals and Povey, 2012). Section 7 describes how unlabeled data can be incorporated into natural gradient descent in order to improve generalization error. Section 8 explores empirically natural gradient descent’s robustness to permutation of the training set. In section 9 we extend natural gradient descent to incorporate second order information. Finally in section 10 we provide a benchmark of the algorithm discussed,  1  (cid:105)  where natural gradient descent is implemented using a truncated Newton approach for inverting the full metric matrix instead of the traditional diagonal or band-diagonal approximation. 2 Natural gradient descent Natural gradient descent can be traced back to Amari’s work on information geometry (Amari, 1985) and its application to various neural networks (Amari et al., 1992; Amari, 1997), though a more in depth introduction can be found in Amari (1998); Park et al. (2000); Arnold et al. (2011). The algorithm has also been successfully applied in the reinforcement learning community (Kakade, 2001; Peters and Schaal, 2008) and for stochastic search (Sun et al., 2009). Let us consider a family of density functions F that maps a parameter θ ∈ RP to a probability density function p(z), p : RN → [0,∞), where z ∈ RN . Any choice of θ ∈ RP deﬁnes a particular density function pθ(z) = F(θ)(z) and by considering all possible θ values, we explore the set F, which is our functional manifold. In its inﬁnitesimal form, the KL-divergence behaves like a distance measure, so we can deﬁne a similarity measure between nearby density functions. Hence F is a Riemannian manifold whose metric is given by the Fisher Information matrix Fθ deﬁned in the equation below:  Fθ = Ez  (∇ log pθ(z))T (∇ log pθ(z))  .  (1)  That is, locally around some point θ, the metric deﬁnes an inner product between vectors u and v:  < u, v >θ= uFθv,  and it hence provides a local measure of distance. It what follows we will write F for the Fisher Information Matrix, assuming an implicit dependency of this matrix on θ. Given a loss function L parametrized by θ, natural gradient descent attempts to move along the manifold by correcting the gradient of L according to the local curvature of the KL-divergence surface, i.e. moving some given distance in the direction ∇NL(θ):  ∇NL(θ)  def  =∇L(θ)Ez  (∇ log pθ(z))T (∇ log pθ(z))  (cid:105)−1  def  = ∇L(θ)F−1.  (2) We use ∇N for natural gradient, ∇ for gradients and F is the metric matrix given by the Fisher Information Matrix. Partial derivatives are usually denoted as row vectors in this work. We can derive this result by considering natural gradient descent to be deﬁned as the algorithm which, at each step, attempts to pick a descent direction such that the amount of change (in the KL-sense) induced in our model is some given value. Speciﬁcally we look for a small ∆θ that minimizes a ﬁrst order Taylor expansion of L when the second order Taylor series of the KL-divergence between pθ and pθ+∆θ has to be constant:  arg min∆θ L(θ + ∆θ) s. t. KL(pθ||pθ+∆θ) = const.  (3)  Using this constraint we ensure that we move along the functional manifold with constant speed, without being slowed down by its curvature. This also makes learning locally robust to re- parametrizations of the model, as the functional behaviour of p does not depend on how it is parametrized. Assuming ∆θ → 0, we can approximate the KL divergence by its second order Taylor series:  (cid:104)  (cid:104)  KL(pθ (cid:107) pθ+∆θ) ≈ (Ez [log pθ] − Ez [log pθ])  − Ez [∇ log pθ(z)] ∆θ − 1 2  (cid:2)−∇2 log pθ(z)(cid:3) ∆θ  ∆θT Ez  ∆θT Ez  (cid:2)∇2 log pθ  (cid:3) ∆θ  =  =  1 2 1 2  ∆θT F∆θ  (4)  2  The ﬁrst term cancels out and because Ez [∇ log pθ(z)] = 01, we are left with only the last term. The Fisher Information Matrix form can be obtained from the expected value of the Hessian through algebraic manipulations (see the appendix). We now express equation (3) as a Lagrangian, where the KL divergence is approximated by (4) and L(θ + ∆θ) by its ﬁrst order Taylor series L(θ) + ∇L(θ)∆θ:  L(θ) + ∇L(θ)∆θ +  1 2  λ∆θT F∆θ  (5)  Solving equation (5) for ∆θ gives us the natural gradient decent formula (2). Note that we get a scalar factor of 2 1 λ times the natural gradient. We fold this scalar into the learning rate, which now also controls the weight we put on preserving the KL-distance between pθ and pθ+∆θ. The approximations we use are meaningful only around θ: in Schaul (2012) it is shown that taking large steps might harm convergence. We deal with such issues both by using damping (i.e. setting a trust region around θ) and by properly selecting a learning rate.  2.1 Adapting natural gradient descent for neural networks In order to use natural gradient descent for deterministic neural networks we rely on their probabilis- tic interpretation (see Bishop (2006), chapter 3.1.1 for linear regression and 4.2 for classiﬁcation). For example, the output of an MLP with linear activation function can be interpreted as the mean of a conditional Gaussian distribution with a ﬁxed variance, where we condition on the input. Mini- mizing the squared error, under this assumption, is equivalent to maximum likelihood. For classiﬁcation, depending on the activation function, we can deﬁne the output units as being success probability of each trial or event probability of a binomial or multinoulli distribution condi- tioned on the input x. By an abuse of notation2, we will use pθ(t|x) to deﬁne this conditional probability density function described above. Because it is a conditional probability function, the formulation of natural gradient descent, equation (3), changes into the following equation:  arg min∆θ L(θ + ∆θ) s. t. Ex∼˜q(x) [KL(pθ(t|x)||pθ+∆θ(t|x))] = const.  (6)  Each value of x now deﬁnes a different family of density functions pθ(t|x), and hence a different manifold. In order to measure the functional behaviour of pθ(t|x) for different values of x, we use the expected value (over x) of the KL-divergence between pθ(t|x) and pθ+∆θ(t|x). In deﬁning the constraint of equation 6, we have chosen to allow ourselves the freedom to compute the expectation over x using some distribution ˜q instead of the empirical distribution q. Usually we want ˜q to be q, though one can imagine situations when this would not be true. E.g. when we want our model to look more carefully at certain types of inputs, which we can do by biasing ˜q towards that type of inputs. Applying the same steps as in section 2 we can recover the formula for natural gradient descent. This formula can be massaged further (similar to Park et al. (2000)) for speciﬁc activations and error functions. We exclude these derivations due to space constraints (they are provided in the appendix). For convenience we show the formulas for two typical pairs of output activations and corresponding error function, where Jy stands for the Jacobian matrix ∂y ∂θ , y being the output layer. By an abuse of notation, 1 y indicates the element-wise division of 1 by yi.  Flinear = β2Ex∼˜q  Fsigmoid = Ex∼˜q  y diag JT  1Proof: Ez [∇ log pθ(z)] =(cid:80)  (cid:16)  (cid:34)  (cid:20)  (cid:35)  (cid:3)  (cid:2)JT (cid:21)  ∂y ∂θ  T ∂y ∂θ  (cid:18) (cid:17)  1  y Jy  = β2Ex∼˜q  (cid:19) y(1 − y) (cid:0)(cid:80) θ pθ(z)(cid:1) = ∂1  = ∂ ∂θ  Jy  the continuous case as well, replacing sums for integrals.  2E.g., for softmax output layer the random variable sampled from the multinoulli is a scalar not a vector  pθ(z)  1  ∂pθ (z)  pθ (z)  ∂θ  z  ∂θ = 0. The proof holds for  (7)  (8)  3  3 Properties of natural gradient descent  In this section we summarize a few properties of natural gradient descent that have been discussed in the literature. Natural gradient descent can be used in the online regime. We assume that even though we only have a single example available at each time step one also has access to a sufﬁciently large set of held out examples. If we are to apply a second order method in this regime, computing the gradient on the single example and the Hessian on the held out set would be conceptually problematic. The gradient and Hessian will be incompatible as they do not refer to the same function. However, for natural gradient descent, the metric comes from evaluating an independent expectation that is not related to the prediction error. It measures an intrinsic property of the model. It is therefore easier to motivate using a held-out set (which can even be unlabeled data as discussed in section 7). Desjardins et al. (2013) shows a straight forward application of natural gradient to Deep Boltz- mann Machines. It is not obvious how the same can be done for a standard second order method. Probabilistic models like RBMs and DBMs not have direct access to the cost they are minimizing, something natural gradient does not require. Natural gradient descent is robust to local re-parametrization of the model. This comes from the constraint that we use. The KL-divergence is a measure of how the probability density function changes, regardless on how it was parametrized. Sohl-Dickstein (2012) explores this idea, deﬁning natural gradient descent as doing whitening in the parameter space. The metric F has two different forms as can be seen in equation (4). Note however that while the metric can be seen as both a Hessian or as a covariance matrix, it is not the Hessian of the cost, nor the covariance of the gradients we follow to a local minimum. The gradients are of pθ which acts as a proxy for the cost L. The KL-surface considered at any point p during training always has its minimum at pθ, and the metric we obtain is always positive semi-deﬁnite by construction which is not true for the Hessian of L. Because F can be interpreted as an expected Hessian, it measures how much a change in θ will affect the gradients of Et∼p(t|x) [log pθ(t|x)]. This means that, in the same way second order methods do, natural gradient descent can jump over plateaus of pθ(t|x). Given the usual form of the loss function L, which is just an evaluation of p for certain pairs (x, t), plateaus of p usually match those of L and hence the method can jump over plateaus of the error function. If we look at the KL constraint that we enforce at each time step, it does not only ensure that we induce at least some epsilon change in the model, but also that the model does not change by more than epsilon. We argue that this provides some kind of robustness to overﬁtting. The model is not allowed to move too far in some direction d if moving along d changes the density computed by the model substantially.  4 Natural gradient descent and TONGA  In Le Roux et al. (2008) one assumes that the gradients computed over different minibatches are distributed according to a Gaussian centered around the true gradient with some covariance matrix C. By using the uncertainty provided by C we can correct the step that we are taking to maximize the probability of a downward move in generalization error (expected negative log-likelihood), resulting in a formula similar to that of natural gradient descent. While the probabilistic derivation requires the centered covariance, in Le Roux et al. (2008) it is argued that one can use the uncentered covariance U resulting in a simpliﬁed formula which is sometimes confused with the metric derived by Amari:  (cid:20)(cid:16) ∂ log p(t|x)  (cid:17)T(cid:16) ∂ log p(t|x)  (cid:17)(cid:21)  ∂θ  ∂θ  U ≈ E(x,t)∼q  (9)  The discrepancy comes from the fact that the equation is an expectation, though the expectation is over the empirical distribution q(x, t) as opposed to x ∼ q(x) and t ∼ p(t|x). It is therefore not clear if U tells us how pθ would change, whereas it is clear that Amari’s metric does.  4  5 Natural gradient descent and Hessian-Free Optimization Hessian-Free Optimization as well as Krylov Subspace Descent3 rely on the extended Gauss-Newton approximation of the Hessian, GN, instead of the actual Hessian (see Schraudolph (2002)):  (cid:34)(cid:18) ∂r (cid:19)T ∂2 log p(t(i)|x(i)) n(cid:88) (cid:0)Et∼˜q(t|x) [HL◦r](cid:1) Jr (cid:2)JT (cid:3)  ∂r2  ∂θ  i=1  r  GN =  1 n  = Ex∼˜q  (cid:18) ∂r  (cid:19)(cid:35)  ∂θ  (10)  The last step of equation (10) assumes that (x(i), t(i)) are i.i.d samples, and ˜q stands for the distri- bution represented by the minibatch over which the matrix is computed. J stands for Jacobian and H for Hessian. The subscript describes for which variable the quantity is computed. A composition in the subscript, as in HL◦r, implies computing the Hessian of L with respect to r, with r being the output layer before applying the activation function. The reason for choosing this approximation over the Hessian is not computational, as computing both can be done equally fast. The extended Gauss-Newton approximation is better behaved during learning. This is assumed to hold because GN is positive semi-deﬁnite by construction, so one needs not worry about negative curvature issues. It is known that the Gauss-Newton approximation (for linear activation function and square error) matches the Fisher Information matrix. In this section we show that this identity holds also for other matching pairs like sigmoid and cross-entropy or softmax and negative log-likelihood for which the extended Gauss-Newton is deﬁned. By choosing this speciﬁc approximation, one can therefore view both Hessian-Free Optimization and KSD as being implementations of natural gradient descent. We make the additional note that Heskes (2000) makes similar algebraic manipulations as the ones provided in this section, however for different reasons, namely to provide a new justiﬁcation of the algorithm that relies on distance measures. The original contribution of this section is in describing the relationship between Hessian-Free Optimization and Krylov Subspace Descent on one side and natural gradient descent on the other. This relation was not acknowledged anywhere in the literature as far as we are aware of. While Heskes (2000) precedes both Schraudolph (2002); Martens (2010) both Hessian Free and Krylov Subspace Descent are introduced as purely approximations to second order methods. In the case of sigmoid units with cross-entropy objective (σ is the sigmoid function), HL◦r is  HL◦rij,i(cid:54)=j = ∂2(cid:80)  = ∂σ(ri)−ti = 0 = ... = ∂σ(ri)−ti  ∂rj  ∂ri  HL◦rii  k(−tk log(σ(rk))−(1−tk) log(1−σ(rk)))  ∂ri∂rj  = σ(ri)(1 − σ(ri))  (11)  If we insert this back into the Gauss-Newton approximation of the Hessian and re-write the equation in terms of Jy instead of Jr, we get the corresponding natural gradient metric, equation (8). diag(v) stands for the diagonal matrix constructed from the values of the vector v and we make an abuse y we understand the vector obtain by applying the division element-wise (the of notation, where by 1 i-th element of is 1 yi  ).  (cid:80) (cid:80)  GN = 1 n = 1 n = Ex∼˜q  (cid:104)  r HL◦rJr  x(i),t(i) JT x(i) JT JT  (cid:105) r diag (y(1 − y)) diag y diag  (cid:16)  (cid:17)  y(1−y)  Jy  1  (cid:16)  (cid:17)  1  y(1−y)  diag (y(1 − y)) Jr  (12)  The last matching activation and error function that we consider is the softmax with cross-entropy. The derivation of the Gauss-Newton approximation is given in equation (13).  3By an abuse of languge, the term Hessian-Free Optimization refers to the speciﬁc algorithm proposed by  Martens (2010), whily KSD stands for the speciﬁc algorithm proposed by Vinyals and Povey (2012)  5  k(tkφ(ri))−ti  ∂rj  = φ(ri) − φ(ri)φ(ri)  = ∂(cid:80) (cid:21) (cid:17)T(cid:16) ∂yk  ∂r  ∂yk ∂θ  k(−tk log(φ(rk)))  ∂ri∂rj  = −φ(ri)φ(rj) = ... = ∂φ(ri)−ti  ∂ri  HL◦rii  HL◦rij,i(cid:54)=j = ∂2(cid:80) (cid:20)(cid:80)o (cid:20) (cid:18)(cid:80)o (cid:0)JT  F = Ex∼˜q = Ex∼˜q = 1 N  JT r  k=1  x(i)  1 yk  (cid:80) Mij,i(cid:54)=j =(cid:80)o Mii = (cid:80)o  1 yk  ∂yk ∂ri  ∂yk ∂rj  k=1 r MJr  ∂θ  1 yk  (cid:16) ∂yk (cid:17)T (cid:16) ∂yk (cid:1) =(cid:80)o i ((cid:80)o  ∂r  k=1  = yiyj − yiyj − yiyj = −φ(ri)φ(rj) = y2 = φ(ri) − φ(ri)φ(ri)  ∂yk ∂yi  ∂yk ∂rj  1 yk  k=1  k=1(δki − yi)yk(δkj − yj) k=1 yk) + yi − 2y2  i  (cid:17)(cid:19)  (cid:21)  Jr  (13)  (14)  (15)  Equation (14) starts from the natural gradient metric and singles out a matrix M in the formula such that the metric can be re-written as the product JT r MJr (similar to the formula for the Gauss- Newton approximation). In equation (15) we show that indeed M equals HL◦r and hence the natural gradient metric is the same as the extended Gauss-Newton matrix for this case as well. Note that δ is the Kronecker delta, where δij,i(cid:54)=j = 0 and δii = 1. There is also a one to one mapping for most of the other heuristics used by Hessian-Free Opti- mization. Following the functional manifold interpretation of the algorithm, we can recover the Levenberg-Marquardt heuristic used in Martens (2010). This is a heuristic for adapting α in the damping term αI that is added to the Hessian before inverting it. This additive term helps mak- ing the matrix easier to invert (for example when it is close to singular) or to deal with negative curvature. I is the identity matrix, α ∈ R is a scalar. The justiﬁcation of the method comes from a trust region approach. We look at how well our second order Taylor approximation of the function predicts the change when taking a step. If it does well, then we can trust our approximation (and decrease the damping α). If our approximation does not predict well the change in the error, we increase the damping factor. We can see that in the limit, when α → 0, we completely trust our approximation and use no damping. When α is very large, there are two things happening. First of all, we are taking much smaller steps (scaled by α). Secondly, the direction that we follow is mostly that of the gradient (ignoring the curvature 1 information of the Hessian). A similar argument can be carried on for natural gradient descent, where we consider only a ﬁrst order Taylor approximation on the manifold. For any function f, if p depicts the picked descent direction and η the step size  f (θt − ηp) ≈ f (θt) − η  ∂f (θt)  ∂θt  p  (16)  This gives the reduction ratio given by equation (17) which can, under the assumption that p = F−1, be shown to behave identically with the one in Martens (2010) (under the same assump- ∂f (θt) tion, namely that CG is close to convergence).  ∂θt  ρ =  f (θt − ηp) − f (θt)  −η ∂f (θt)  ∂θt  p  θ − t − ηF−1 ∂f (θt)  ∂θt  −η ∂f (θt)  ∂θt  F−1 ∂f (θt)  ∂θt  T  (17)  Structural damping (Martens and Sutskever, 2011), a speciﬁc regularization term used to improve training of recurrent neural network, can also be explained from the natural gradient descent per- spective. Roughly it implies using the joint probability density function p(t, h|x), where h is the log p(t, h|x) will break in the sum of two terms, hidden state, when writing the KL-constraint. one being the Fisher Information Matrix, while the other measures the change in h and forms the  6  T(cid:17) − f (θt)  (cid:16) ≈ f (  structural damping term. While theoretically pleasing, however this derivation results in a ﬁxed coefﬁcient of 1 for the regularization term. We can ﬁx this by using two constraints when deriving the natural gradient descent algorithm, namely:  arg min∆θ L(θ + ∆θ) s. t. Ex∼˜q(x) [KL(pθ(t|x)||pθ+∆θ(t|x))] = const. and Ex∼˜q(x) [KL(pθ(h|x)||pθ+∆θ(h|x))] = const.  (18)  If we apply the same steps as before for both constraints (i.e. replace them by a second order Taylor expansion), the second term will give rise to the structural damping term.  6 Natural gradient descent and Krylov Subspace Descent Instead of using linear conjugate gradient descent for computing the inverse of the metric, Krylov Subspace Descent(KSD) Vinyals and Povey (2012) 4 opts for restricting ∆θ to a lower dimensional Krylov subspace given by Fx = ∇L and then, using some other second order method like BFGS, to solve for ∆θ within this space. Formally we are looking for γ1, γ2, .., γk such that:  θ +   γ1  γ2 . . . γk     ∇L  ∇LF . . .∇LFk−1     L  min  γ  Because it relies on the extended Gauss-Newton approximation of the Hessian, like Hessian-Free Optimization, KSD implements natural gradient descent. But there is an additional difference be- tween KSD and HF that can be interpreted from the natural gradient descent perspective. In order to mimic Hessian-Free Optimization’s warm restart, this method adds to the Krylov Sub- space the previous search direction. We hypothesize that due to this change, KSD is more similar to natural conjugate gradient than natural gradient. Natural conjugate gradient (see section 9) is an extension of the natural gradient descent algorithm that incorporates second order information by applying the nonlinear conjugate gradient algorithm on top of the natural gradients. To show this we can rewrite the new subspace as:  θ + βdt−1 + α   γ1  αγ2 α . . . γk α     ∇L  ∇LF . . .∇LFk−1     minL  From this formulation one can see that the previous direction plays a different role than the one played when doing warm restart of CG. The algorithm is reminiscent of the nonlinear conjugate gradient. The descent directions we pick, besides incorporating the natural gradient direction, also tend to be locally conjugate to the Hessian of the error with respect to the functional behaviour of the model. Additionally, compared to nonlinear conjugate gradient BFGS is used to compute β rather then some known formula.  7 Using unlabeled data When computing the metric of natural gradient descent, the expectation over the target t is computed where t is taken from the model distribution for some given x: t ∼ p(t|x). For the standard neural network models this expectation can be evaluated analytically (given the form of p(t|x)). This means that we do not need target values to compute the metric of natural gradient descent. Furthermore, to compute the natural gradient descent direction we need to evaluate two different expectations over x. The ﬁrst one is when we evaluate the expected (Euclidean) gradient, while the  4KSD stands for the exact algorithm proposed by Vinyals and Povey (2012)  7  (19)  (20)  Figure 1: The plot depicts the training error (left) on a log scale and test error (right) as percentage of misclassiﬁed examples for fold 4 of TFD. On the x-axis we have the number of updates. Dotted green line (top, worst) stands for using the same minibatch (of 256 examples) to compute the gradi- ent and evaluate the metric. Dashed blue line (middle) uses a different minibatch of 384 examples from the training set to evaluate the metric. The solid red line (bottom, best) relies on a randomly sampled batch of unlabeled data to estimate the metric.  second is when we evaluate the metric. In this section we explore the effect of re-using the same samples in computing these two expectations as well as the effect of improving the accuracy of the metric F by employing unlabeled data. Statistically, if both expectations over x are computed from the same samples, the two estimations are not independent from each other. We hypothesize that this can lead to the overﬁtting of the current minibatch. Fig. 1 provides empirical evidence that our hypothesis is correct. Vinyals and Povey (2012) make a similar empirical observation. As discussed in section 3, enforcing a constant change in the model distribution helps ensuring stable progress but also protects from large changes in the model which can be detrimental (could result in overﬁtting a subset of the data). We get this effect as long as the metric provides a good measure of how much the model distribution changes. Unfortunately the metric is computed over training examples, and hence it will focus on how much p changes at these points. When learning overﬁts the training set we usually observe reduction in the training error that result in larger increases of the generalization error. This behaviour can be avoided by natural gradient descent if we can measure how much p changes far away from the training set. To explore this idea we propose to increase the accuracy of the metric F by using unlabeled data, helping us to measure how p changes far away from the training set. We explore empirically these two hypotheses on the Toronto Face Dataset (TFD) (Susskind et al., 2010) which has a small training set and a large pool of unlabeled data. Fig. 1 shows the training and test error of a model trained on fold 4 of TFD, though similar results are obtained for the other folds. We used a two layer model, where the ﬁrst layer is convolutional. It uses 512 ﬁlters of 14X14, and applies a sigmoid activation function. The next layer is a dense sigmoidal one with 1024 hidden units. The output layer uses sigmoids as well instead of softmax. The data is pre-processed by using local contrast normalization. Hyper-parameters have been selected using a grid search (more details in the appendix). We notice that re-using the same samples for the metric and gradient results in worse global training error (training error over the entire train set) and worse test error. Our intuition is that we are seeing the model overﬁtting, at each step, the current training minibatch. At each step we compute the gradient and the metric on the same example. There can be, within this minibatch, some direction in which we could overﬁt some speciﬁc property of these examples. Because we only look at how the model changes at these points when computing the metric, all gradients will agree with this direction. Speciﬁcally, the model believes that moving in this direction will lead to steady change in the KL (the KL looks at the covariance between the gradeints from the output layer to the parameters, which will be low in the picked direction) and the model will believe it can take a large step, learning this speciﬁc feature. However it is not useful for generalization, nor is it useful for the other training examples (e.g. if the particular deformation is not actually  8  02.5K5K8K876543log of train error02.5K5K8K1520253035test error (%)correlated with the classes it needs to predict). This means that on subsequent training examples the model will under-perform, resulting in a worse overall error. On the other hand, if we use a different minibatch for the gradient and for the metric, it is less likely that the same particular feature to be present in both the set of examples used for the gradient and those used for the metric. So either the gradient will not point in said direction (as the feature is not present in the gradient), or, if the feature is present in the gradient, it may not be present in the examples used for computing the metric. That would lead to a larger variance, and hance the model is less likely to take a large step in this direction. Using unlabeled data results in better test error for worse training error, which means that this way of using the unlabeled data acts like a regularizer.  8 Robustness to reorderings of the train set We want to test the hypothesis that, by forcing the change in the model, in terms of the KL, to be constant, the model avoids to take large steps, being more robust to the reordering of the train set. We repeat the experiment from Erhan et al. (2010), using the NISTP dataset introduced in Bengio et al. (2011) (which is just the NIST dataset plus deformations) and use 32.7M samples of this data. The original experiment attempts to measure the importance of the early examples in the learnt model. To achieve this we respect the same protocol as the original paper described below:  for steps between 1 and 5 do  Algorithm 1 Protocol for the robustness to the order of training examples 1: Split the training data into two large chunks of 16.4M data points 2: Split again the ﬁrst chunk into 10 equal size segments 3: for i between 1 and 10 do 4: 5: 6: 7: 8: 9:  Replace segment i by new randomly sampled examples that have not been used before Train the model from scratch Evaluate the model on 10K heldout examples  end for Compute the segment i mean variance, among the 5 runs in the output of the trained model (the variance in the activations of the output layer)  10: end for 11: Plot the mean variance as a function of which segment was resampled  Fig. 2 shows these curves for minibatch stochastic gradient descent and natural gradient descent. Note that the variance at each point on the curve depends on the speed with which we move in func- tional space. For a ﬁxed number of examples one can artiﬁcially tweak the curves by decreasing the learning rate. With a smaller learning rate we move slower, and hence the model, from a functional point of view, does not change by much, resulting in low variance. In the limit, with a learning rate of 0, the model always stays the same. In order to be fair to the two algorithms compared in the plot, natural gradient descent and stochastic gradient descent, we use the error on a different validation set as a measure of how much we moved in the functional space. This helps us to chose hyper- parameters such that after 32.7M samples both methods achieve about the same validation error of 49.8% (see appendix for hyper-parameters). Both models are run on minibatches of the same size, 512 samples. We use the same minibatch to compute the metric as we do to compute the gradient for natural gradient descent, to avoid favorazing NGD over MSGD. The results are consistent with our hypothesis that natural gradient descent avoids making large steps in function space during training, staying on the path that induces least variance. Note that relative variance might not be affected when switching to natural gradient descent. That is to be expected, as, in the initial stages of learning, any gradient descent algorithm needs to choose a basin of attraction (a local minima) while subsequent steps make progress towards these minima. That is, the ﬁrst examples must have, relatively speaking, a big impact on the learnt model. However what we are trying to investigate is the overall variance. Natural gradient descent (in global terms) has lower variance, resulting in more consistent functional behaviour regardless of the order of the input examples.  9  Figure 2: The plot describes how much the model is inﬂuenced by different parts of an online training set. The variance induced by re-shufﬂing of data for natural gradient descent is order of magnitudes lower than for SGD. See text for more information.  SGD might move in a direction early on that could possibly yield overﬁtting (e.g. getting forced into some quadrant of parameter space based only on a few examples) resulting in different models at the end. This suggests that natural gradient descent can deal better with nonstationary data and can be less sensitive to the particular examples selected early on during training.  9 Natural conjugate gradient  Natural gradient descent is a ﬁrst order method in the space of functions, which raises the question: can the algorithm be improved by considering second order information? Unfortunately computing the Hessian on the manifold can be daunting (especially from a computational perspective). That is the Hessian of the error as a function of the density probability functions pθ(t|x) that give rise to the manifold structure. Note that in Roux and Fitzgibbon (2010) a method for combining second order information and TONGA is proposed. It is not clear how this algorithm can be applied to natural gradient descent. Also in Roux and Fitzgibbon (2010) a different Hessian matrix is considered, namely that of the error as a function of the parameter. A simpler option is, however, to use an optimization method such as Nonlinear Conjugate Gradient, that is able to take advantage of second order structure (by following locally conjugate directions) while not requiring to actually compute the Hessian at any point in time, only the gradient. Absil et al. (2008) describes how second order methods can be generalized to the manifold case. In Honkela et al. (2008, 2010) a similar idea is proposed in the context of variational inference and speciﬁc assumptions on the form of pθ are made. Gonzalez and Dorronsoro (2006) is more similar in spirit with this work, though their approach is deﬁned for the diagonal form of the Fisher Information Matrix and differs in how they propose to compute a new conjugate direction. Natural conjugate gradient, the manifold version of nonlinear conjugate gradient, is deﬁned follow- ing the same intuitions (Shewchuck, 1994). The only problematic part of the original algorithm is how to obtain a new conjugate direction given the previous search direction and the current natural gradient. The problem arises from the fact that the two vectors belong to different spaces. The lo- cal geometry around the point where the previous search direction dt−1 was computed is deﬁned by Ft−1, while the geometry around the new direction is deﬁned by Ft, where, in principle, Ft−1 (cid:54)= Ft. Following Absil et al. (2008) we would need to “transport” dt−1 into the space of ∇Nt, an expen- sive operation, before we can compute a new direction using a standard formula like Polak-Ribiere. Furthermore, the line search should be done along the geodesic of the manifold. Gonzalez and Dorronsoro (2006); Honkela et al. (2010) address these issues by making the assump- tion that Ft−1 and Ft are identical (so dt−1 does not need to be transported). This assumption is detrimental to the algorithm because it goes against what we want to achieve. By employing a con- jugate gradient method we hope to make large steps, from which it follows that the metric is very likely to change. Hence the assumption can not hold.  10  0.00.10.20.30.40.50.60.70.80.9Which fraction of the dataset is replaced10-510-410-310-210-1mean(variance) on a log scaleNGD validation error 49.8%MSGD validation error 49.8%Computing the right direction is difﬁcult, as the transportation operator for dt−1 is hard to compute in a generic manner, without enforcing strict constraints on the form of pθ. Inspired by our new interpretation of the KSD algorithm, we propose instead to solve for the coefﬁcients β and α that minimizes our cost, where β α is the coefﬁcient required in computing the new direction and α is the step size:  The new direction therefore is:  L  min α,β  θt−1 +  (cid:18)  (cid:20) αt  βt  (cid:21)(cid:20) ∇Nt  dt−1  (cid:21)(cid:19)  dt = ∇Nt +  βt αt  dt−1  (21)  (22)  used to compute the new direction.  Note that the resulting algorithm looks identical to standard nonlinear conjugate gradient. The dif- ference are that we use the natural direction ∇Nt instead of the gradients ∇t everywhere and that we use an off the shelf solver to optimize simultaneously for both the step size αt and the correction term βt αt We can show that in the Euclidean space, given a second order Taylor approximation of L(θt−1), under the assumption that the Hessian Ht−1 is symmetric, this approach will result in conjugate direction. Because we do not do the multidimensional line search along geodesics or transport the gradients in the same space, in general, this approach is also just an approximation to the locally conjugate direction on the manifold. The advantage of our approach over using some formula, like Polak-Ribiere, and ignoring the dif- ference between Ft−1 and Ft, is that we are guaranteed to minimize the cost L at at each step. Let us show that the resulting direction is conjugate to the previous one in Euclidean space. Let dt−1 be the previous direction and γt−1 the step size such that L(θt−1 + γt−1dt−1) is minimal for ﬁxed dt−1. If we approximate L by its second order Taylor expansion and compute the derivative with respect to the step size γt−1 we have that:  ∂L ∂θ  dt−1 + γt−1dT  t−1Hdt−1 = 0  (23)  Suppose now that we take the next step which takes the form of  L(θt + βtdt−1 + αt∇T  )  = L(θt−1 + γt−1dt−1 + βtdt−1 + αt∇T  Nt  )  Nt  t−1Hdt−1 = 0  where we minimize for αt and βt. If we replace L by its second order Taylor series around θt−1, compute the derivative with respect to βt and use the assumption that Ht−1 (where we drop the subscript) is symmetric, we get: ∂θ dt−1 + αt∇NtHdt−1 + (γt−1 + βt)dT ∂L Using the previous relation 23 this implies that (αt∇NtLT + βtdt−1)T Hdt−1 = 0, i.e. that the new direction is conjugate to the last one. 10 Benchmark We carry out a benchmark on the Curves dataset, using the 6 layer deep auto-encoder from Martens (2010). The dataset is small, has only 20K training examples of 784 dimensions each. All methods except SGD use the truncated Newton pipeline. The benchmark is run on a GTX 580 Nvidia card, using Theano (Bergstra et al., 2010a) for cuda kernels. Fig. 3 contains the results. See supplemen- tary materials for details. For natural gradient we present two runs, one in which we force the model to use small minibatches of 5000 examples, and one in which we run in batch mode. In both cases all other hyper-parameters were manually tuned. The learning rate is ﬁxed at 1.0 for the model using a minibatch size of 5000 and a line search was used when running in batch mode. For natural conjugate gradient we use scipy.optimize.fmin cobyla to solve the two dimensional line search. This corresponds to the new algorithm introduced in section 9. We additionally run the same algorithm, where we rely on the Polak-Riebiere formula for computing the new direction and use standard line search for the  11  learning rate. This corresponds to our implementation of the algorithm proposed in Gonzalez and Dorronsoro (2006), where instead of a diagonal approximation of the metric we reply on a truncated Newton approach. In both cases we run in full batch mode. For SGD we used a minibatch of 100 examples and a learning rate of 0.01. Note that our implementation of natural gradient does not directly matches James Martens’ Hessian Free as there are components of the algorithm that we do not use like backtracking and preconditioning.  Figure 3: The plots depict the training curves on a log scale (x-axis) for different learning algorithms on the Curves dataset. On the y-axis we have training error on the whole dataset. The left plot shows the curves in terms of iterations, while the right plot show the curves as a function of clock time (seconds). NGD stands for natural gradient descent descent with a minibatch of 1000 and a ﬁxed learning rate. NGD-L is natural gradient descent in batch mode where we use a line search for the learning rate. NCG-L uses an scipy.optimize.fmin cobyla for ﬁnding both learning rate and the new conjugate direction. NCG-F employs the Polak-Ribiere formula, ignoring the change in the metric. SGD stands for stochastic gradient descent.  The ﬁrst observation that we can make is that natural gradient descent can run reliably with smaller minibatches, as long as the gradient and the metric are computed on different samples and the learn- ing rate used is sufﬁciently small to account for the noise on the natural direction (or the damping coefﬁcient is sufﬁciently large). This can be seen from comparing the two runs of natural gradient descent descent. Our result is in the spirit of the work of Kiros (2013), though the exact approach of dealing with small minibatches is slightly different. The second observation that we can draw is that incorporating second order information into natu- ral gradient descent might be beneﬁcial. We see a speedup in convergence. The result agrees with the observation made in Vinyals and Povey (2012), where Krylov Subspace Descent (KSD) was shown to converge slightly faster than Hessian-Free Optimization. Given the relationship between natural gradient and Hessian-Free Optimization, and between KSD and natural conjugate gradient, we expected to see the same trend between natural gradient descent and natural conjugate gradient. We regard our algorithm as similar to KSD, with its main advantage being that it avoids storing in memory the Krylov subspace. Furthermore, from the comparison between NCG-L and NCG-F we can see evidence for our argumentation that it is useful to use an optimizer for ﬁnding the new di- rection. The reason why NCG-F under performs is that the smooth change in the metric assumption contradicts what the algorithm is trying to do, namely move as far as possible in parameter space. 11 Discussion and conclusions In this paper we have made the following original contributions. We showed that by employing the extended Gauss-Newton approximation of the Hessian both Hessian-Free Optimization and Krylov Subspace Descent can be interpreted as implementing natural gradient descent. Furthermore, by adding the previous search direction to the Krylov subspace, KSD does something akin to an ap- proximation to nonlinear conjugate gradient on the manifold. We proposed an approximation of nonlinear conjugate gradient on the manifold which is similar in spirit to KSD, with the difference that we rely on linear conjugate gradient to invert the metric F  12  102103104105106Iterations012345Square error103104105Time (seconds)012345Square errorSGDNGDNGD-LNCG-LNCG-Finstead of using a Krylov subspace. This allows us to save on the memory requirements which can become prohibitive for large models, while still retaining some of the second order information of the cost and hence converging faster than vanilla natural gradient. Lastly we highlighted the difference between natural gradient descent and TONGA, and brought forward two new properties of the algorithm compared to stochastic gradient descent. The ﬁrst one is an empirical evaluation of the robustness of natural gradient descent to the order of examples in the training set, robustness that can be crucial when dealing with non-stationary data. The second property is the ability of natural gradient descent to guard against large drops in generalization error especially when the accuracy of the metric is increased by using unlabeled data. We believe that these properties as well as others found in the literature and shortly summarized in section 3 might provide some insight in the recent success of Hessian Free and KSD for deep models. Finally we make the empirical observation that natural gradient descent can perform well even when its metric and gradients are estimated on rather small minibatches. One just needs to use different samples for computing the gradients from those used for the metric. Also the step size and damping coefﬁcient need to be bounded to account for the noise in the computed descent direction.  Acknowledgments  We would like to thank the developers of Theano (Bergstra et al., 2010b; Bastien et al., 2012) and Guillaume Desjardins and Yann Dauphin for their insightful comments. We would also like to thank NSERC, Compute Canada, and Calcul Qu´ebec for providing computational resources. Razvan Pascanu is supported by a DeepMind Fellowship.  References Absil, P.-A., Mahony, R., and Sepulchre, R. (2008). Optimization Algorithms on Matrix Manifolds. Princeton  University Press, Princeton, NJ.  Amari, S. (1985). Differential geometrical methods in statistics. Lecture notes in statistics, 28. Amari, S. (1997). Neural learning in structured parameter spaces - natural Riemannian gradient. In In Advances  in Neural Information Processing Systems, pages 127–133. MIT Press.  Amari, S., Kurata, K., and Nagaoka, H. (1992). Information geometry of Boltzmann machines. IEEE Trans.  on Neural Networks, 3, 260–271.  Amari, S.-I. (1998). Natural gradient works efﬁciently in learning. Neural Comput., 10(2), 251–276. Arnold, L., Auger, A., Hansen, N., and Olivier, Y. (2011). Information-geometric optimization algorithms: A  unifying picture via invariance principles. CoRR, abs/1106.3708.  Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I., Bergeron, A., Bouchard, N., and Bengio, Y. (2012). Theano: new features and speed improvements. Submited to Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.  Bengio, Y., Bastien, F., Bergeron, A., Boulanger-Lewandowski, N., Breuel, T., Chherawala, Y., Cisse, M., Cˆot´e, M., Erhan, D., Eustache, J., Glorot, X., Muller, X., Pannetier Lebeuf, S., Pascanu, R., Rifai, S., Savard, F., and Sicard, G. (2011). Deep learners beneﬁt more from out-of-distribution examples. In JMLR W&CP: Proc. AISTATS’2011.  Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-Farley, D., and Bengio, Y. (2010a). Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Conference (SciPy).  Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-Farley, D., and Bengio, Y. (2010b). Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Conference (SciPy). Oral Presentation.  Bishop, C. M. (2006). Pattern Recognition and Machine Learning (Information Science and Statistics).  Springer-Verlag New York, Inc.  Chapelle, O. and Erhan, D. (2011). Improved Preconditioner for Hessian Free Optimization. In NIPS Workshop  on Deep Learning and Unsupervised Feature Learning.  Choi, S.-C. T., Paige, C. C., and Saunders, M. A. (2011). MINRES-QLP: A Krylov subspace method for  indeﬁnite or singular symmetric systems. 33(4), 1810–1836.  Desjardins, G., Pascanu, R., Courville, A., and Bengio, Y. (2013). Metric-free natural gradient for joint-training  of boltzmann machines. ICLR, abs/1301.3545.  13  Erhan, D., Courville, A., Bengio, Y., and Vincent, P. (2010). Why does unsupervised pre-training help deep  learning? In JMLR W&CP: Proc. AISTATS’2010, volume 9, pages 201–208.  Gonzalez, A. and Dorronsoro, J. (2006). Natural conjugate gradient training of multilayer perceptrons. Artiﬁcial  Neural Networks ICANN 2006, pages 169–177.  Heskes, T. (2000). On natural learning and pruning in multilayered perceptrons. Neural Computation, 12,  1037–1057.  Honkela, A., Tornio, M., Raiko, T., and Karhunen, J. (2008). Natural conjugate gradient in variational inference.  In Neural Information Processing, pages 305–314.  Honkela, A., Raiko, T., Kuusela, M., Tornio, M., and Karhunen, J. (2010). Approximate riemannian conjugate gradient learning for ﬁxed-form variational bayes. Journal of Machine Learning Research, 11, 3235–3268.  Kakade, S. (2001). A natural policy gradient. In NIPS, pages 1531–1538. MIT Press. Kiros, R. (2013). Training neural networks with stochastic hessian-free optimization. ICLR. Le Roux, N., Manzagol, P.-A., and Bengio, Y. (2008). Topmoumoute online natural gradient algorithm. In  NIPS’07.  Martens, J. (2010). Deep learning via hessian-free optimization. In ICML, pages 735–742. Martens, J. and Sutskever, I. (2011). Learning recurrent neural networks with hessian-free optimization. In  ICML, pages 1017–1024.  Mizutani, E. and Demmel, J. (2003). Iterative scaled trust-region learning in krylov subspaces via peralmutter’s  implicit sparse hessian-vector multiply. In NIPS, pages 209–216.  Nocedal, J. and Wright, S. J. (2000). Numerical Optimization. Springer. Park, H., Amari, S.-I., and Fukumizu, K. (2000). Adaptive natural gradient learning algorithms for various  stochastic models. Neural Networks, 13(7), 755 – 764.  Pearlmutter, B. A. (1994). Fast exact multiplication by the hessian. Neural Computation, 6, 147–160. Peters, J. and Schaal, S. (2008). Natural actor-critic. (7-9), 1180–1190. Roux, N. L. and Fitzgibbon, A. W. (2010). A fast natural newton method. In ICML, pages 623–630. Schaul, T. (2012). Natural evolution strategies converge on sphere functions.  In Genetic and Evolutionary  Computation Conference (GECCO).  Schraudolph, N. N. (2002). Fast curvature matrix-vector products for second-order gradient descent. Neural  Computation, 14(7), 1723–1738.  Shewchuck, J. (1994). An introduction to the conjugate gradient method without the agonizing pain. Technical  report, CMU.  Sohl-Dickstein, J. (2012). The natural gradient by analogy to signal whitening, and recipes and tricks for its  use. CoRR, abs/1205.1828.  Sun, Y., Wierstra, D., Schaul, T., and Schmidhuber, J. (2009). Stochastic search using the natural gradient. In  ICML, page 146.  Susskind, J., Anderson, A., and Hinton, G. E. (2010). The Toronto face dataset. Technical Report UTML TR  2010-001, U. Toronto.  Vinyals, O. and Povey, D. (2012). Krylov Subspace Descent for Deep Learning. In AISTATS.  Appendix  Expected Hessian to Fisher Information Matrix  The Fisher Information Matrix form can be obtained from the expected value of the Hessian :  (cid:20)  (cid:21)  Ez  − ∂2 log pθ  ∂θ  = Ez  (cid:34)  (cid:35)  (cid:34)  ∂pθ ∂θ  − ∂ 1  pθ ∂θ  (cid:32)(cid:88) (cid:34)(cid:18) ∂ log pθ(z)  pθ(z)  = Ez  pθ(z)  − 1  ∂2pθ ∂θ2 +  (cid:33) (cid:19)T(cid:18) ∂ log pθ(z)  (cid:34)(cid:18) ∂ log pθ(z) (cid:19)(cid:35)  + Ez  ∂θ  z  = − ∂2 ∂θ2  = Ez  (cid:19)(cid:35)  (cid:18) 1 (cid:19)T(cid:18) 1 (cid:19)T(cid:18) ∂ log pθ(z)  ∂pθ ∂θ  pθ  pθ  ∂pθ ∂θ  (cid:19)(cid:35)  ∂θ  (24)  ∂θ  ∂θ  14  Derivation of the natural gradient descent metrics  Linear activation function  In the case of linear outputs we assume that each entry of the vector t, ti comes from a Gaussian distribution centered around yi(x) with some standard deviation β. From this it follows that:  o(cid:89) (cid:20)(cid:80)o  i=1  i=1  pθ(t|x) =  N (ti|y(x, θ)i, β2)  Et∼N (t|y(x,θ),β2I)  i=1  i=1  (cid:20) (cid:20)(cid:80)o (cid:20)(cid:80)o (cid:20)(cid:80)o (cid:20)(cid:80)o (cid:20)(cid:80)o (cid:20)(cid:80)o (cid:2)JT  i=1  i=1  i=1  F = Ex∼˜q = Ex∼˜q = Ex∼˜q = Ex∼˜q = Ex∼˜q = Ex∼˜q = β2Ex∼˜q = β2Ex∼˜q  (cid:20) (cid:20) (cid:20) (cid:20) (cid:20)  i=1 y Jy  Et∼N (t|y(x,θ),β2I) Et∼N (t|y(x,θ),β2I) Et∼N (t|y(x,θ),βI) Et∼N (t|y(x,θ),β2I) Et∼N (t|y(x,θ),β2I)  (cid:17)T(cid:16) ∂yi  (cid:20)(cid:16) ∂yi (cid:3)  ∂θ  ∂θ  ∂θ  ∂θ  ∂θ  ∂θ  (cid:17)T(cid:16) ∂ log pθ(ti|y(x)i (cid:17)(cid:21)(cid:21)(cid:21) (cid:17)T(cid:16) ∂(ti−yi)2 (cid:17)(cid:21)(cid:21)(cid:21) (cid:17)T(cid:16) ∂yi (cid:17)(cid:21)(cid:21) (cid:17)T(cid:16) ∂yi (cid:17)(cid:21)(cid:21) (cid:17)T(cid:16) ∂yi (cid:17)(cid:21)(cid:21) (cid:17)T(cid:16) ∂yi  (cid:16) ∂ logθ p(ti|y(x)i (cid:20)(cid:16) ∂(ti−yi)2 (cid:20) (ti − yi)2(cid:16) ∂yi (cid:2)(ti − yi)2(cid:3)(cid:16) ∂yi (cid:2)(ti − yi)2(cid:3)(cid:16) ∂yi (cid:2)(ti − yi)2(cid:3)(cid:16) ∂yi (cid:17)(cid:21)(cid:21)  ∂θ  ∂θ  ∂θ  ∂θ  ∂θ  ∂θ  ∂θ  ∂θ  (cid:17)(cid:21)(cid:21)  (25)  (26)  Sigmoid activation function  In the case of the sigmoid units, i,e, y = sigmoid(r), we assume a binomial distribution which gives us:  p(t|x) =  i (1 − yi)1−ti yti  (27)  log p gives us the usual cross-entropy error used with sigmoid units. We can compute the Fisher information matrix as follows:  (cid:20) (cid:20)(cid:80)o (cid:104)  F = Ex∼˜q = Ex∼˜q = Ex∼˜q  Et∼p(t|x)  i=1  (cid:16)  1  yi(1−yi) 1  i=1  (cid:16) ∂yi (cid:17)  (ti−yi)2 i (1−yi)2 y2 ∂yi ∂θ  (cid:17)T (cid:105)  ∂θ  JT y diag  y(1−y)  Jy  (cid:21)(cid:21)  ∂yi ∂θ  (cid:17)T  (cid:16) ∂yi (cid:21)  ∂θ  (28)  (cid:89)  i  (cid:20)(cid:80)o  Note that diag(v) stands for the diagonal matrix constructed from the values of the vector v and we make an abuse of notation, where by 1 y we understand the vector obtain by applying the division element-wise (the i-th element of is 1 yi  ).  Softmax activation function For the softmax activation function, y = softmax(r), p(t|x) takes the form of a multinoulli:  p(t|x) =  yti i  (29)  o(cid:89)  i=1  15  (cid:34) o(cid:88)  i=1  1 yi  (cid:35)  (cid:18) ∂yi  (cid:19)T ∂yi  ∂θ  ∂θ  (cid:20)  JT y diag  (cid:18) 1  (cid:19)  y  (cid:21)  Jy  = Ex∼˜q  F = Ex∼˜q  (30)  Implementation Details  We have implemented natural gradient descent descent using a truncated Newton approach similar to the pipeline proposed by Pearlmutter (1994) and used by Martens (2010). In order to better deal with singular and ill-conditioned matrices we use the MinRes-QLP algorithm (Choi et al., 2011) instead of linear conjugate gradient for certain experiments. Both Minres-QLP as well as linear conjugate gradient can be found implemented in Theano at https://github.com/pascanur/theano optimize. We used the Theano library (Bergstra et al., 2010a) which allows for a ﬂexible implementation of the pipeline, that can automatically generate the computational graph of the metric times some vector for different models:  import theano.tensor as TT # ‘params‘ is the list of Theano variables containing the parameters # ‘vs‘ is the list of Theano variable representing the vector ‘v‘ # with whom we want to multiply the metric # ‘Gvs‘ is the list of Theano expressions representing the product # between the metric and ‘vs‘  # ‘out_smx‘ is the output of the model with softmax units Gvs = TT.Lop(out_smx,params,  TT.Rop(out_smx,params,vs)/(out_smx*out_smx.shape[0]))  # ‘out_sig‘ is the output of the model with sigmoid units Gvs = TT.Lop(out_sig,params,  TT.Rop(out_sig,params,vs)/(out_sig*  (1-out_sig)* out_sig.shape[0]))  # ‘out‘ is the output of the model with linear units Gvs = TT.Lop(out,params,TT.Rop(out,params,vs)/out.shape[0])  The full pseudo-code of the algorithm (which is very similar to the one for Hessian-Free Optimiza- tion) is given below.  Algorithm 2 Pseudocode for natural gradient descent algorithm 1: # ‘gfn‘ is a function that computes the metric times some vector 2: gfn ← (lambda v → Fv) 3: while not early stopping condition do 4: 5: 6: 7: 8: 9: end while  g ← ∂L # linear cg solves the linear system Gx = ∂L ng← linear cg(gfn, g, max iters = 20, rtol=1e-4) # γ is the learning rate θ ← θ − γ ng  ∂θ  ∂θ  Note that we do not usually do a line search for each step. While it could be useful, conceptually we like to think of the algorithm as a ﬁrst order method. Doing a line search has the side effect that can lead to overﬁtting of the current minibatch. To a certain extent this can be ﬁxed by using new samples of data to compute the error, though we ﬁnd that we need to use reasonably large batches to get an accurate measure. Using a learning rate as for a ﬁrst order method is a good alternative if one wants to apply this algorithm using small minibatches of data. Even though we are ensured that F is positive semi-deﬁnite by construction, and MinRes-QLP is able to ﬁnd a suitable solutions in case of singular matrices, we still use a damping strategy for two  16  reasons. The ﬁrst one is that we want to take in consideration the inaccuracy of the metric (which is approximated only over a small minibatch). The second reason is that natural gradient descent makes sense only in the vicinity of θ as it is obtained by using a Taylor series approximation, hence (as for ordinary second order methods) it is appropriate to enforce a trust region for the gradient. See Schaul (2012), where the convergence properties of natural gradient descent (in a speciﬁc case) are studied. Following the functional manifold interpretation of the algorithm, we can recover the Levenberg- Marquardt heuristic used in Martens (2010). We consider only a ﬁrst order Taylor approximation, where, due to the functional manifold, for any function f,  (cid:32)  T(cid:33)  f  θt − ηF−1 ∂f (θt) ∂θt  ≈ f (θt) − η  ∂f (θt)  ∂θt  F−1 ∂f (θt) ∂θt  T  (31)  We now can check how well this approximation predicts the change in the error for the step taken during an iteration. Based on this comparison we can deﬁne our reduction ratio given by equation (32). Based on this ratio we can decide if we want to increase or decrease the damping applied to the metric before inverting. As we will show below, this ratio behaves identically with the one in Martens (2010).  (cid:16)  T(cid:17) − f (θt)  θt − ηF−1 ∂f (θt) −η ∂f (θt)  ∂θt  ∂θt  F−1 ∂f (θt)  T  ∂θt  (32)  f  ρ =  Additional experimental results  TFD experiment  We used a two layer model, where the ﬁrst layer is convolutional. It uses 512 ﬁlters of 14X14, and applies a sigmoid activation function. The next layer is a dense sigmoidal one with 1024 hidden units. The output layer uses sigmoids as well instead of softmax. The data is pre-processed by using local contrast normalization. Hyper-parameters such as learning rate, starting damping coefﬁcient have been selected using a grid search, based on the validation cost obtained for each conﬁguration. We ended up using a ﬁxed learning rate of 0.2 (with no line search) and adapting the damping coefﬁcient using the Levenberg-Marquardt heuristic. When using the same samples for evaluating the metric and gradient we used minibatches of 256 samples, otherwise we used 384 other samples randomly picked from either the training set or the unlabeled set. We use MinResQLP as a linear solver, the picked initial damping factor is 5., and we allow a maximum of 50 iterations to the linear solver.  NISTP experiment (robustness to the order of training samples)  The model we experimented with was an MLP of only 500 hidden units. We compute the gradients for both MSGD and natural gradient descent over minibatches of 512 examples. In the case of natu- ral gradient descent we compute the metric over the same input batch of 512 examples. Additionally we use a constant damping factor of 3 to account for the noise in the metric (and ill-conditioning since we only use batches of 512 samples). The learning rates were kept constant, and we use 0.2 for the natural gradient descent and 0.1 for MSGD.  Curves experiment - Benchmark  For the curves experiment we used a deep autoencoder with 400-200-100-50-25-6 hidden units respectively where we apply sigmoid activation function everywhere except on the top layer. We used the sparse initialization proposed by Martens (2010) for all runs. For natural conjugate gradient we use the full batch of 20000 examples to compute the gra- dients or metric as well as for evaluating the model when optimizing for α and β using  17  scipy.optimize.fmin cobyla. We use the Levenberg-Marquardt heuristic to adapt damping (and start with a damping value of 3). We use MinRes-QLP that we allow to run for a maximum of 250 steps. Batch size, number of linear solver iterations and damping coefﬁcient have been selected by hand tuning, looking at validation set performance. For natural gradient we end up reporting two runs. One uses full batch and a line search while the other uses small batches of 5000 examples and a ﬁxed learning rate of 1.0. For the SGD case we use a smaller batch size of 100 examples. The optimum learning rate, obtained by a grid search, is 0.01 which was kept constant through training. We did not use any additional enhancement like momentum. A ﬁnal note is that we show square error (to be consistent with Martens (2010)) we minimize cross- entropy in practice (i.e. the gradients are computed based on the cross-entropy cost, square error is only computed for visualization reasons).  18  ","We evaluate natural gradient, an algorithm originally proposed in Amari(1997), for learning deep models. The contributions of this paper are asfollows. We show the connection between natural gradient and three otherrecently proposed methods for training deep models: Hessian-Free (Martens,2010), Krylov Subspace Descent (Vinyals and Povey, 2012) and TONGA (Le Roux etal., 2008). We describe how one can use unlabeled data to improve thegeneralization error obtained by natural gradient and empirically evaluate therobustness of the algorithm to the ordering of the training set compared tostochastic gradient descent. Finally we extend natural gradient to incorporatesecond order information alongside the manifold information and provide abenchmark of the new algorithm using a truncated Newton approach for invertingthe metric matrix instead of using a diagonal approximation of it."
1312.4740,2014,Learning High-level Image Representation for Image Retrieval via Multi-Task DNN using Clickthrough Data  ,"['Wei-Ying Ma', 'Tiejun Zhao', 'Kuiyuan Yang', 'Wei Yu', 'Yalong Bai']",https://arxiv.org/pdf/1312.4740.pdf,"3 1 0 2   c e D 1 2         ]  V C . s c [      2 v 0 4 7 4  .  2 1 3 1 : v i X r a  Learning High-level Image Representation for Image Retrieval via Multi-Task DNN using Clickthrough  Data  Yalong Bai  Harbin Institute of Technology ylbai@mtlab.hit.edu.cn  Kuiyuan Yang  Microsoft Research  kuyang@microsoft.com  Wei Yu  Harbin Institute of Technology  w.yu@hit.edu.cn  Wei-Ying Ma  Microsoft Research  wyma@microsoft.com  Tiejun Zhao  Harbin Institute of Technology  tjzhao@hit.edu.cn  Abstract  Image retrieval refers to ﬁnding relevant images from an image database for a query, which is considered difﬁcult for the gap between low-level representation of images and high-level representation of queries. Recently further developed Deep Neural Network sheds light on automatically learning high-level image rep- resentation from raw pixels. In this paper, we proposed a multi-task DNN for image retrieval, which contains two parts, i.e., query-sharing layers for image rep- resentation computation and query-speciﬁc layers for relevance estimation. The weights of multi-task DNN are learned on clickthrough data by Ring Training. Experimental results on both simulated and real dataset show the effectiveness of the proposed method.  1  Introduction  Image retrieval is a challenge task in current information retrieval systems, as relevance between query (high-level semantic representation) and image (low-level visual representation) is hard to compute for the well-known semantic gap problem. In current image retrieval system, images are indirectly represented by their surrounding texts from web pages, which contain many noises and bring irrelevant images in the search results (Fig. 1 shows an example query’s search results to illustrate the shortcomings of surrounding texts as representation). To improve the search results by lowering the rank of irrelevant images, binary classiﬁer based on visual representations has been trained for each query to rerank the search results[6]. However the used visual representations such as SIFT [11], HOG [3], and LBP [12] are still too low-level to capture the semantic information in images [14]. With large number of training data, convolutional deep neural network [7, 15, 10, 13] has demon- strated its great success in learning high-level image representation from raw pixels [9, 8], and achieved superior performance in image classiﬁcation task on ImageNet [17]. For image retrieval task, large scale clickthrough data (contains millions queries and their clicked images by users) is available as training data [2]. The clickthrough data is different from training data for classiﬁcation in the following three aspects:  1. The query set is much larger than category set. 2. The image number distribution on queries is signiﬁcant heavy-tailed, while distributions on training data for classiﬁcation (i.e., ImageNet, CIFAR-10, MINIST etc) are relatively  1  Figure 1: The top ranked images for query “kenny dance shoe” from a popular commer- cial image search engine at September 15th, 2013. Though the surrounding texts of these images all contain “kenny dance shoe”, the images marked with red boxes are all irrele- vant ones.  Figure 2: Data distributions on ImageNet and clickthrough data.  Figure 3: The architectures of binary DNN, multi-class DNN and multi-task DNN.  uniform. Fig. 2 statistics the image number distributions on ImageNet and clickthrough data from one year’s search log of Bing. Compared with ImageNet, the clickthrough data is signiﬁcant heavy-tailed with more than 96% queries have less than 1000 clicked images.  3. The concept of many queries are not exclusive, e.g. “dog” vs “puppy”.  The three differences make exiting binary DNN and multi-class DNN not suitable as models. Binary DNN suffers from the limited training data for each query especially the large number of tail queries, while multi-class DNN cannot handle millions queries and inclusive problem between queries. To leverage the supervised information in clickthrough data, we proposed a new DNN model named as multi-task DNN. In multi-task DNN, ranking images for a query is treated as a task, while all queries share the same image representation layers. In addition, we proposed ring training which simultaneously updates the shared weights and query speciﬁc weights. The rest of paper is structured as follows. In Section 2, we introduce multi-task DNN for image retrieval and deﬁne the objective function based on the clickthrough data. In Section 3, we introduce the Ring Training, a transfer learning mechanism for training multi-task DNN. Section 4 and Section 5 give the simulated and real experimental veriﬁcation of the proposed method. Finally, we conclude the paper in Section 6.  2 Multi-task DNN  Multi-task DNN as illustrated in Fig. 3(c) contains two parts: query-sharing layers and query- speciﬁc layers. Based on multi-task DNN, relevance score between query q and image I is deﬁned as,  r(I, q) = ψ(cid:0)φ(I; Ws); Wq  (cid:1),  2  (1)  00.511.522.5x 1040500100015002000250030003500Query/Term Index# of image  ImageNetBing Click Log…a) Binary DNN………………………………… ……………b) Multi-class DNNc) Multi-task DNNTMT2T1T1-MTMT1T2where φ(I; Ws) generates the image representation, Ws are the weights of query-sharing layers, and ψ(·; Wq) computes the relevance, Wq are the weights of query-speciﬁc layers. Given a clickthrough dataset contains M queries denoted by {qi}M ing images denoted as {I i following  i=1, each query qi with ni train- j}ni j=1, we deﬁne the objective function that will be used for training as  min  ni(cid:88)  M(cid:88)  L(cid:0)g(I i  j, qi)(cid:1), i=1} denotes all the weights in the model, N =(cid:80)M  j, qi), r(I i  J(Θ) =  1 N  j=1  i=1  Θ  (2)  j, qi), r(I i  i=1 ni is the total number i is clicked by  j, qi)(cid:1) is the loss function  j, qi) ∈ {−1, +1} is the groundtruth denotes whether image I j  where Θ = {Ws,{Wqi}M of training images, g(I i query qi, r(I i penalizes the inconsistence between groundtruth and prediction. To optimize the objective function, we resort to gradient descent method. The gradient of the objec- tive function with respect to Wqi is ∇Wqi  j, qi) is the ranking score deﬁned in Eq. 1, L(cid:0)g(I i j, qi)(cid:1) j, qi)(cid:1)  which only need to average the gradients over training images of query qi. The gradient of the objective function with respect to Ws is  ∂L(cid:0)g(I i  ∂L(cid:0)g(I i  j, qi), r(I i ∂Wqi  ni(cid:88)  M(cid:88)  ni(cid:88)  1 N  J =  (3)  j=1  ,  ,  (4)  ∇WsJ =  1 N  i=1  j=1  j, qi), r(I i ∂Ws  where gradients of all training images from all queries are averaged. Computing gradient in batch mode is computational intensive over large-scale clickthrough data, which is also computational inefﬁcient as many queries share similar concept (As an extreme example, the dataset only contains two queries “cat” and “kitten”, and the training images for each query are exactly the same, updates the weights iteratively using average gradients of “cat” and “kitten” will be two times faster than batch mode, which shares similar advantage as mini-batch mode).  3 Ring Training  Based on the above observation, we proposed ring training to update the weights as illustrated in Fig. 4. Ring training loops several rounds over queries, each query qi updates both Wqi and Ws several epoches with the average gradients of the query’s training images {Iij}ni j=1 in batch or mini-batch mode. Ring training shares similar advantage as mini-batch mode and ensures faster convergence rate. From the viewpoint of transfer learning, ring training transferred the image repre- sentation from previously learned queries to current query, and can avoid overﬁtting even the query with few training images. The detailed algorithm is summarized in Algorithm 1. In practice, the learning rate ηs for Ws is gradually reduced to 0 several rounds before ηq, after ηs reduced to 0, the layers related to image representation are ﬁxed.  4 Experiments on CIFAR-10  In this section, we did several simulations based on a small dataset CIFRA-10 to verify the effec- tiveness of the proposed method. CIFAR-10 is a dataset composed of 10 classes of natural objects, each class contains 5,000 training images and 1,000 testing images. Images are with small size of 32 × 32.  4.1 Overview  The architecture of network for CIFRA-10 contains four layers in order, three convolutional layers and one fully-connected layer, the ﬁrst two convolutional layers have 32 ﬁlters with size of 5× 5, the  3  Figure 4: Schematic illustration of Ring Training. Green layers are shared between all queries, and layers of other colors are query speciﬁc layers. Ring training loops over the queries to update the weights.  Algorithm 1 Procedure of Ring Training For multi-task DNN  i=1, R rounds, E epochs each round, learning rate ηq and ηs  j}ni j=1 for query qi, shared weights Ws, query speciﬁc  Input: M queries, traning images {I i weights {Wqi}M Initialize Ws randomly. for r = 1 to R do  for i = 1 to M do  for e = 1 to E do Forward Pass: Extract public feature: v ← φ(I i Compute output: r(I i Backpropagate Gradients: Compute gradient A(cid:48) Update category special layers: Wqi = Wqi − ηqA(cid:48) Update shared layers: Ws = Ws − ηsA(cid:48)  j; Ws) j, qi) = ψ(v; Wqi )  and A(cid:48)  Wqi  Ws  Ws  Wqi  with respect to parameters Wqi and Ws:  end for  end for  end for  last convolutional layer has 64 ﬁlters with size of 5 × 5. The ﬁrst convolutional layer is followed by a max-pooling layer, and the other two convolutional layers are followed by average-pooling layer, overlap pooling is used in pooling layer with window size of 3 × 3 and stride of 2(i.e., neighboring pooling windows overlap by 1 element). The deﬁned network achieves 78.2% accuracy on standard CIFAR-10 task, which is comparable to work of [16] 78.9% without using dropout and dropconnect. All experiments are based on mini-batch SGD (stochastic gradient descent) with batch size of 128 images, the momentum is ﬁxed at 0.9, and weight decay is set as 0.004. The update rule for weights is deﬁned as  vt+1 = 0.9vt − 0.004 · (cid:15) · wt + (cid:15) · gt wt+1 = wt + vt+1  where gt is the gradient of the cost function with respect to that parameter averaged over the mini- batch, t is the iteration index and (cid:15) is the learning rate of weights and biases are initialized with 0.001 and 0.002 respectively.  4.2 CIFAR-10  To simulate the heavy-tail distribution of real dataset, we construct a dataset denoted as dataset1 by sampling different amounts of images for each class. The number of training images for the ten categories of dataset1 is: [5000, 4000, 3000, 2000, 1000, 500, 400, 300, 200, 100]. Considering  4  DNN finetuned on Task 1..................DNN finetuned on Task M..................DNN finetuned on Task 2.................. Table 1: Train and test set misclassiﬁcation rate for binary DNN(separately trained), multi-class DNN and multi-task DNN.  Dataset  dateset1  dateset2  dateset3  Model  Binary DNN  Multi-class DNN  Mutli-task DNN + ring training  Binary DNN  Multi-class DNN  Mutli-task DNN + ring training  Binary DNN  Multi-class DNN  Mutli-task DNN + ring training  Train Error %  31.19 6.22 32.79 31.09 10.80 32.36 38.79 10.41 30.4  Test Error %  43.27 49.82 39.16 43.53 52.53 39.89 43.4 31.97 36.68  there are categories with similar concept in real dataset, we construct another datasets dataset2 by randomly splitting images of “cat” in dataset1 into two parts named as “cat” and “kitten”. For comparison, dataset3 is constructed with same total number of training images as dataset1 by randomly sampling 1650 images per category. For each category, negative examples are randomly selected from the other categories with the same size as positive examples. Before feeding images to the neural network, we subtracted the per-pixel mean computed over the training set for each category [5]. The following three methodsare compared on the three datasets:  1. Binary DNN, a separate DNN is trained for each category 2. Multi-class DNN 3. Multi-task DNN with ring training, the proposed method  The results are summarized in Table 1. In general, binary DNN performs consistently worse for the severe overﬁtting problem. Comparing error rates on dataset1 and dataset3, multi-class DNN performs worse when the dataset is with heavy-tailed distribution.The performance of multi-class DNN is ever worse by comparing error rates on dataset1 and dataset2, where a class named cat is split into two similar class called cat and kitten 1. This demonstrates that trying to discriminate similar categories in multi-class DNN will hurt the performance. The reason is that multi-class DNN is designed to discriminate all categories, while trying to discriminate categories describing the same concept will lead overﬁtting problem. dataset1 and dataset3 are with the same number of images but with much lower test error, it is the nonuniform distribution affects the learning of multi-class DNN. Fig. 5 shows the number of predicted images vs the number of training images, top categories are overemphasized and tend to have more predicted images. In summary, all of above experiments show multi-class DNN is not suitable for real dataset, especially in image retrieval task, there is no requirement to discriminate a query from all the others, where only the relevance between image and queries is required. In general, the proposed method achieves the lowest error rate except dataset3 which is not real case. Multi-task DNN with ring training signiﬁcantly outperforms the binary DNN and multi-class DNN on all nonuniform distributed datasets. Additionally, to verify how ring training improves classiﬁcation error of tail categories, Fig. 6 shows the convergence property of a category with only 100 images in dataset1. Comparing to binary DNN, multi-task DNN with ring training converges much faster (test error is table after ten epochs) and with much lower test error, which further verifys the efﬁciency and effectiveness of the ring training.  5 Experiment on image retrieval  In this section, we verify the effectiveness of multi-task DNN in the real image retrieval task.  1For fair comparison, images predicted as cat and kitten are all treated as cat during evaluation.  5  Figure 5: Amount of the predicted images for each category in dataset1, dataset2 and dataset3, using multi-class architecture  Figure 6: Training progression of baseline (bi- nary DNN) and multi-task DNN with ring train- ing for a tail categorie.  5.1 Clickthrough dataset  The clickthrough dataset, which contains 11 million queries and 1 million images and 21 million click pairs, collected from one year’s search log of Bing image search, is publicly available from MSR-Bing Image Retrieval Challenge [1], and the data distributed is same with the “Bing Click Log” in Fig. 2. The dev set contains 1000 queries and 79,655 images, and the relevance between image and query are manually judged. The judgment guidelines and procedure are established to ensure high data quality and consistency. Multi-class DNN is infeasible for such large number of queries. In this experiment, multi-task DNN with ring training is used to learn weights based on the clickthrough data.  5.2 Experimental Setting  The network is with ﬁve convolutional layers and two fully-connected layers, drop out with rate 0.5 is added to the ﬁrst fully-connected layers during training for avoiding overﬁtting [5]. The output of the last fully-connected layer is fed to softmax to represent the relevance degree between image and query. Our network maximized the average log-probability of correct judgment about whether the image related to query. The ﬁrst, second and ﬁfth convolutional layers are followed by max-pooling layers, while the ﬁrst and second max-pooling layers are followed by response-normalization layers. To accelerate the learning of early stage, the ReLU [4] non-linearity is applied as activation function in every convolutional and full-connected layers. The input image is with size of 224 × 224. The ﬁrst convolutional layer is with 96 ﬁlters with size of 11 × 11 and stride of 4. The second convolutional layer is with 256 ﬁlters with size of 5 × 5. The third, fourth and ﬁfth convolutional layers are with 384 ﬁlters with size of 3× 3. The ﬁrst fully- connected layers following the ﬁfth convolutional layer are with 4096 neurons. Three max-pooling layers are with window size of 3 × 3 and stride of 2.  5.3 Experimental Results  ranking list for a query, the DCG is calculated as DCG25 = 0.01757(cid:80)25  Discounted Cumulated Gain (DCG) is adopt to as the performance metric for a ranking list. Given a 2reli−1 log2(i+1) where reli = Excellent = 3, Good = 2, Bad = 0 is the manually judged relevance for each image with respect to the query, and 0.01757 is a normalizer to make the DCG score of 25 Excellent images to be 1. The ﬁnal metric is the average of DCG25 over all test queries. For ring training, ﬁve convolutional layers and the ﬁrst fully-connected layer are shared among all queries, the second fully-connected layer and 2-way softmax layer are used as query-speciﬁc layers. After ﬁnishing ring training, weights of sharing layers are ﬁxed, outputs of the ﬁrst fully-connected  i=1  6  01000200030004000500005001000150020002500# of training data# of prediction  # of prediction in dataset1# of prediction in dataset2# of prediction in dataset300.050.10.150.20.250.30.350.4102030405060708090100110120130140150160170180190200Error RateEpochsRing-Training TrainRing-Training TestBaseline TrainBaseline TestFigure 7: Five randomly chose example queries with their top ranked images, the ranking of each image is computed by SVM based on learned feature.  Table 2: Rank result comparison between the three ranker method  Method  Random ranker  SVM based on bag of word SVM based on learned feature  DCG25 of all queries  0.468 0.484 0.502  layer are used as feature for each images, then SVM to used to learn the relevance between image and query based on the extracted feature. We compared the following three ranking methods, 1) Random ranker, images are randomly ranked for each query 2) SVM based on bag of visual words, which preselect SIFT [11] as visual feature. 3) SVM based on learned feature. The results are summarized in Table 2, where the learned feature achieved the best performance. Fig. 7 shows the ranking results of ﬁve queries based on feature learned by multi-task DNN with ring training.  6 Discuss and Conclusion  In this work, multi-task DNN learned by ring training is proposed for image retrieval. The model treats each query as a speciﬁc task, and exploits the commonalities between different tasks for image representation learning. Experimental results on both CIFAR-10 and MSR-Bing Image Retrieval Challenge show the improvement by the proposed method.  References  [1] http://acmmm13.org/submissions/call-for-multimedia-grand-challenge-solutions/msr-bing-  grand-challenge-on-image-retrieval-scientiﬁc-track/.  [2] N. Craswell and M. Szummer. Random walks on the click graph. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 239–246. ACM, 2007.  [3] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 886–893. IEEE, 2005.  7  silk twist hair stylelightedfrisbeepregnancy week by weektowelibanezk15 custom built 15 string bass guitar[4] X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectiﬁer networks. In Proceedings of the 14th International Conference on Artiﬁcial Intelligence and Statistics. JMLR W&CP Volume, volume 15, pages 315–323, 2011.  [5] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov.  Im- proving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.  [6] V. Jain and M. Varma. Learning to re-rank: query-dependent image re-ranking using click data. In Proceedings of the 20th international conference on World wide web, pages 277–286. ACM, 2011.  [7] A. Krizhevsky. Convolutional deep belief networks on cifar-10. Unpublished manuscript,  2010.  [8] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1106–1114, 2012.  [9] Q. V. Le, R. Monga, M. Devin, K. Chen, G. S. Corrado, J. Dean, and A. Y. Ng. Building  high-level features using large scale unsupervised learning. In ICML, 2012.  [10] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolutional deep belief networks for In Proceedings of the 26th  scalable unsupervised learning of hierarchical representations. Annual International Conference on Machine Learning, pages 609–616. ACM, 2009.  [11] D. G. Lowe. Distinctive image features from scale-invariant keypoints. International journal  of computer vision, 60(2):91–110, 2004.  [12] T. Ojala, M. Pietikainen, and T. Maenpaa. Multiresolution gray-scale and rotation invariant texture classiﬁcation with local binary patterns. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 24(7):971–987, 2002.  [13] N. Pinto, D. Doukhan, J. J. DiCarlo, and D. D. Cox. A high-throughput screening approach to discovering good forms of biologically inspired visual representation. PLoS computational biology, 5(11):e1000579, 2009.  [14] F. Schroff, A. Criminisi, and A. Zisserman. Harvesting image databases from the web. IEEE  Transactions on Pattern Analysis and Machine Intelligence, 2011.  [15] S. C. Turaga, J. F. Murray, V. Jain, F. Roth, M. Helmstaedter, K. Briggman, W. Denk, and H. S. Seung. Convolutional networks can learn to generate afﬁnity graphs for image segmentation. Neural Computation, 22(2):511–538, 2010.  [16] L. Wan, M. Zeiler, S. Zhang, Y. L. Cun, and R. Fergus. Regularization of neural networks using dropconnect. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 1058–1066, 2013.  [17] M. Zeiler and R. Fergus. Visualizing and understanding convolutional networks.  1311.2901, 2013.  In Arxiv  8  ","Image retrieval refers to finding relevant images from an image database fora query, which is considered difficult for the gap between low-levelrepresentation of images and high-level representation of queries. Recentlyfurther developed Deep Neural Network sheds light on automatically learninghigh-level image representation from raw pixels. In this paper, we proposed amulti-task DNN learned for image retrieval, which contains two parts, i.e.,query-sharing layers for image representation computation and query-specificlayers for relevance estimation. The weights of multi-task DNN are learned onclickthrough data by Ring Training. Experimental results on both simulated andreal dataset show the effectiveness of the proposed method."
1312.5355,2014,Generative NeuroEvolution for Deep Learning  ,"['Phillip Verbancsics', 'Josh Harguess']",https://arxiv.org/pdf/1312.5355.pdf,"Generative NeuroEvolution for Deep Learning  Phillip Verbancsics & Josh Harguess  Space and Naval Warfare Systems Center – Paciﬁc  {phillip.verbancsics,joshua.harguess}@navy.mil  San Diego, CA 92101  3 1 0 2    c e D 8 1         ] E N . s c [      1 v 5 5 3 5  .  2 1 3 1 : v i X r a  Abstract  An important goal for the machine learning (ML) community is to create ap- proaches that can learn solutions with human-level capability. One domain where humans have held a signiﬁcant advantage is visual processing. A signiﬁcant ap- proach to addressing this gap has been machine learning approaches that are in- spired from the natural systems, such as artiﬁcial neural networks (ANNs), evo- lutionary computation (EC), and generative and developmental systems (GDS). Research into deep learning has demonstrated that such architectures can achieve performance competitive with humans on some visual tasks; however, these sys- tems have been primarily trained through supervised and unsupervised learning algorithms. Alternatively, research is showing that evolution may have a signiﬁ- cant role in the development of visual systems. Thus this paper investigates the role neuro-evolution (NE) can take in deep learning. In particular, the Hypercube- based NeuroEvolution of Augmenting Topologies is a NE approach that can ef- fectively learn large neural structures by training an indirect encoding that com- presses the ANN weight pattern as a function of geometry. The results show that HyperNEAT struggles with performing image classiﬁcation by itself, but can be effective in training a feature extractor that other ML approaches can learn from. Thus NeuroEvolution combined with other ML methods provides an intriguing area of research that can replicate the processes in nature.  1  Introduction  Evolution has a signiﬁcant role in creating biological visual systems [1, 2], thus a potential path for training artiﬁcial vision as effective as the biological counterparts is neuro-evolution (NE; [3]) . However, NE has been challenged because of the required structure sizes. While the human brain includes trillions of connections [4, 5], traditional NE approaches produce networks signiﬁcantly smaller in size [3, 6]. Thus an active area of research is evolutionary approaches that address this gap [6]. One signiﬁcant area of interest is generative and developmental systems (GDS), which are inspired by principles that map genotype to phenotype in nature. Such principles are enabled by reusing genetic information [4, 7]. For example, biological neural networks exhibit several important organizational principles, such as modularity, regularity, and hierarchy [8, 9], which can be exploited through the indirect encoding and enable evolution of complex structures. The GDS approach investigated in this paper is called the Hypercube-based NeuroEvolution of Augmenting Topologies (HyperNEAT; [6, 10, 11]). HyperNEAT trains an indirect encoding, called a compositional pattern producing network (CPPN; [12]) that represents artiﬁcial neural network (ANN) connection weights. HyperNEAT has shown promise in simple visual discrimination tasks [13, 14, 15, 16], but has been limited to two or fewer hidden layers. This paper extends the in- vestigation to deeper architectures by applying HyperNEAT to the MNIST benchmark dataset. In- terestingly, the results show that HyperNEAT alone has difﬁculty in classifying the images, but HyperNEAT is effective at training a feature extractor for backward propagation learning. That  1  is, HyperNEAT can learn the ‘deep’ parts of the neural network, while backward propagation can perform the ﬁne tuning to make classiﬁcations from the generated features.  2 Background  The geometry-based methods that underlie the approach are described in this section.  2.1 NeuroEvolution of Augmenting Topologies (NEAT)  Neuro-evolution (NE; [3]) methods train artiﬁcial neural networks (ANNs) through evolutionary algorithms. As opposed to updating weights according to a learning rule, candidate ANNs are eval- uated in a task and assigned ﬁtness to allow selection and creation of a new generation of ANNs by mutating and recombining their selected genomes. The NeuroEvolution of Augmenting Topologies (NEAT) algorithm [17] is a popular neuroevolutionary approach that has been proven in a variety of challenging tasks, including particle physics [18, 19], simulated car racing [20], RoboCup Keep- away [21], function approximation [22], and real-time agent evolution [23], among others [17]. NEAT starts with a population of small, simple ANNs that increase their complexity over gen- erations by adding new nodes and connections through mutation. That way, the topology of the network does not need to be known a priori; NEAT searches through increasingly complex networks as it evolves their connection weights to ﬁnd a suitable level of complexity. The techniques that facilitate evolving a population of diverse and increasingly complex networks are described in detail in Stanley and Miikkulainen [17]; the important concept for the approach in this paper is that NEAT is an evolutionary method that discovers the right topology and weights of a network to maximize performance on a task. The next section reviews the extension of NEAT called HyperNEAT that allows it to effectively train large neural structures.  2.2 CPPNs and HyperNEAT  Hypercube-based NEAT (HyperNEAT; [6, 11]) is a GDS extension of NEAT that enables effective evolution of high-dimensional ANNs. The effectiveness of the geometry-based learning in Hyper- NEAT has been demonstrated in multiple domains, such as multi-agent predator prey [24, 25] and RoboCup Keepaway [26]. A full description of HyperNEAT is in Stanley et al. [6]. The main idea in HyperNEAT is that geometric relationships are learned though an indirect encoding that describes how the weights of the ANN can be generated as a function of geometry. Unlike a direct representation, wherein every connection in the ANN is described individually, an indirect representation describes a pattern of parameters without explicitly enumerating each such parameter. That is, information is reused in such an encoding, which is a major focus in the ﬁeld of GDS from which HyperNEAT originates [27, 28]. Such information reuse allows indirect encoding to search a compressed space. HyperNEAT discovers the regularities in the geometry and learns from them. The indirect encoding in HyperNEAT is called a compositional pattern producing network (CPPN; [12]), which encodes the weight pattern of an ANN [6, 10]. The idea behind CPPNs is that geometric patterns can be encoded by a composition of functions that are chosen to represent common regular- ities. In this way, a set of simple functions can be composed into more elaborate functions through hierarchical composition.Formally, CPPNs are functions of geometry (i.e. locations in space) that output connectivity patterns for nodes situated in n dimensions. Consider a CPPN that takes four inputs labeled x1, y1, x2, and y2; this point in four-dimensional space can also denote the connection between the two-dimensional points (x1, y1) and (x2, y2). The output of the CPPN for that input thereby represents the weight of that connection (ﬁgure 1). Because the connection weights are produced as a function of their endpoints, the ﬁnal pattern is produced with knowledge of the domain geometry, which is literally depicted geometrically within the constellation of nodes. Weight patterns produced by a CPPN in this way are called substrates so that they can be verbally distinguished from the CPPN itself. It is important to note that the structure of the substrate is independent of the structure of the CPPN. The substrate is an ANN whose nodes are situated in a coordinate system, while the CPPN deﬁnes the connectivity among the nodes of the ANN. The experimenter deﬁnes both the location and role (i.e. hidden, input, or output) of each node in the substrate.  2  Figure 1: A CPPN Describes Connectivity. A grid of nodes, called the ANN substrate, is assigned coordinates. (1) Every connection between layers in the substrate is queried by the CPPN to deter- mine its weight; the line connecting layers in the substrate represents a sample such connection. (2) For each such query, the CPPN inputs the coordinates of the two endpoints. (3) The weight between them is output by the CPPN. Thus, CPPNs can generate regular patterns of connections.  In summary, HyperNEAT evolves the topology and weights of the CPPN that encodes ANN weight patterns. An extension of HyperNEAT called HyperNEAT with Link Expression Output (HyperNEAT-LEO) was introduced to constrain connectivity with a bias towards modularity [15]. This extension separates the decision of weight magnitude and expression into two different CPPN outputs and seeds the LEO with the concept of locality. The HyperNEAT-LEO variant is shown in Algorithm 1. The next section reviews deep learning that has been applied to image classiﬁcation.  Input: Substrate Conﬁguration Output: Solution CPPN Initialize population of minimal CPPNs with random weights; while Stopping criteria is not met do  foreach CPPN in the population do  foreach Possible connection in the substrate do  Query the CPPN for weight w of connection and LEO expression value e; if e > 0.0 then  Create connection with a weight w;  Run the substrate as an ANN in the task domain to ascertain ﬁtness;  Reproduce CPPNs according to the NEAT method to produce the next generation;  Output the champion CPPN.  Algorithm 1: HyperNEAT-LEO Algorithm  2.3 Deep Learning in Image Classiﬁcation  Neural networks have experienced a resurgence thanks to breakthroughs in deep learning that have led to state of the art results in a number of challenging domains [29]. In particular, deep learning approaches have achieved remarkable performance in a number of object recognition benchmarks, often achieving the current best performance on these tasks. Such object recognition tasks where deep learning has achieved the best results include the MNIST hand-written digit dataset [30, 31], trafﬁc sign recognition [32], and the ImageNet Large-Scale Visual Recognition Challenge [33]. The challenge for deep learning is how to effectively train such large neural structures. Traditional supervised learning approaches for neural networks, such as backward propagation, face problems that include the “curse of dimensionality” or vanishing gradient. Thus a popular alternative is to perform pre-training on the deep architecture through unsupervised learning. Restricted Boltzmann Machines [34] and auto-encoders [35] are among the common approaches to performing this unsu- pervised training. This pre-training can either act as an initial starting point for supervised learning of a deep network or a feature extractor from which machine learning approaches can learn [29].  3   X2 Y2 Y1 x1 x2 y1 y2 bias ANN Substrate 2) Feed coordinate pair to CPPN 3) Set connection weight                       to output X1 1) Query each substrate connection CPPN Figure 2: HyperNEAT Feature Learning. To learn features, HyperNEAT trains CPPNs (1) that generate the connectivity for a deﬁned ANN substrate (2). The ANN substrate processes the inputs from a data set to produced a set of features (3). These features are given to another machine learning algorithm (4) that learns to perform the task (e.g. image classiﬁcation). Machine learning then produces a solution that is evaluated on testing data (5). The performance of the solution on data not seen during training provides the ﬁtness score of the CPPN for HyperNEAT. In this way, HyperNEAT not only discovers better learning features, but also better features for generalization.  Alternatively, supervised learning performance can be enhanced through the selection of the deep architecture, such as convolutional neural networks (CNNs). Beginning with the Neocognitron [36], these deep architectures have been inspired by proposed models of the human visual cortex [37] and the concept of local receptive ﬁelds that take advantage of the input topology. That is, CNNs enforce a particular geometric knowledge by constructing an architecture that learns features based upon locality. Through local receptive ﬁelds, shared weights, and sub-sampling [38], CNNs enable backward propagation (supervised learning) to effectively train deep architectures. The next section introduces approaches to deep learning through HyperNEAT.  3 Approach: Deep Learning HyperNEAT  Although the HyperNEAT method succeeds in a number of challenging tasks [6, 10, 11, 26] by exploiting geometric regularities, it has not yet been applied to tasks where deep learning is showing promise or to deep architectures. Because HyperNEAT learns as a function of domain geometry, it is well-suited towards architectures similar to CNNs. This approach introduces two modiﬁcations to HyperNEAT training. The ﬁrst modiﬁcation introduces the idea of HyperNEAT as a feature learner. Conventional Hy- perNEAT trains a CPPN that deﬁnes an ANN that is the solution, that is, the produced ANN is applied directly to the task and then the ANN’s performance on that task determines the CPPN’s ﬁtness score. However, this HyperNEAT modiﬁcation trains an ANN that transforms inputs into features based upon domain geometry and then the features are given to another machine learn- ing approach to solve the problem, the performance of this learned solution then deﬁnes the ﬁtness score of the CPPN for HyperNEAT (ﬁgure 2). In this way, HyperNEAT acts as a reinforcement learning approach that determines the best features to extract for another machine learning approach to maximize performance on the task. The second modiﬁcation introduces alternative architectures to HyperNEAT. Traditionally, Hyper- NEAT produces the weight pattern for a ANN substrate that is feed forward, fully connected, and containing only sigmoid activation functions. However, the only information HyperNEAT sees about the substrate is the geometric coordinates of the neurons, thus HyperNEAT could be applied to any graph like structure wherein the nodes have coordinates associated with them. Because CNNs have been demonstrably successful in a number of domains, HyperNEAT is extended to include this alternative ANN architecture. Each of these extensions to HyperNEAT is explored in experiments described in the next section.  4   Hyper NEAT CPPN ANN Inputs Features ML Training Solution ML Testing (1) (2) (3) (4) (5) 4 Experimental Setup  These investigations are conducted on the MNIST dataset, which is a popular benchmark for ma- chine learning because it is a challenging and relevant real world problem. MNIST is a set of 28×28 pixel images of handwritten digits (0 − 9), separated into 60,000 training images and 10,000 test- ing images. The goal for machine learning is to correctly classify the handwritten digit contained within each image. To this end, we explore HyperNEAT with four combinations of experimental settings on the benchmark MNIST dataset. These settings are traditional HyperNEAT ANN ar- chitecture (feed-forward, fully-connection, sigmoid activation functions) or CNN architecture and HyperNEAT training the solution (an ANN for classiﬁcation) or feature extractor (ANN that trans- forms images into features). The architecture for HyperNEAT in these experiments is a multi-layer neural network wherein the layers travel along the z-axis, each layer consists of a number of features (f-axis), and each feature has a constellation of neurons on the x, y-plane corresponding to pixel locations. Thus the CPPN represents points in an eight-dimensional Hyper-cube that correspond to connections in the four- dimensional substrate and each neuron is located at a particular (x, y, f, z) coordinate. Each layer is presented by a triple (X, Y, F ), wherein F is the number of features and X, Y are the pixel dimensions. Thus the input layer is (28, 28, 1), because the input image is 28× 28 and contains only grayscale pixel values. The traditional HyperNEAT architecture for these experiments is a seven layer neural network with one input, one output, and ﬁve hidden layers, represented by the triples (28, 28, 1), (16, 16, 3), (8, 8, 3), (6, 6, 8), (3, 3, 8), (1, 1, 100), (1, 1, 64), and (1, 1, 10). Each layer in the traditional HyperNEAT ANN architecture is fully connected to the adjacent layers and each neuron has a bipolar sigmoid activation function. The CNN architecture to which HyperNEAT is applied replicates the LeNet-5 architecture that was previously applied to the MNIST dataset [38]. To operate as a feature extractor, the above architectures are modiﬁed such that the ANN substrate is cut off before the last hidden layer, that is, for the traditional ANN architecture the (1, 1, 100) layer becomes the new output layer and for the CNN architecture the (1, 1, 120) layer becomes the outputs. Thus each image is passed through the ANN substrate architecture to produce an associated feature vector. These feature vectors are then given to backward propagation to train an ANN with an architecture identical to the architecture that was removed from the substrates. The next section presents the results of these HyperNEAT variants.  5 Results  For each of these experiments, results are averaged over 30 independent runs of 2500 generations with a HyperNEAT population size of 256. The ﬁtness score is the sum of the true positive rate, true negative rate, positive predictive value, negative predictive value, and accuracy for each class plus the fraction correctly classiﬁed overall and the inverse of the mean square error from the correct label outputs. Each run randomly selects 300 images, evenly spread across the classes, from the MNIST training set for training. For regular HyperNEAT (i.e. not feature learning), ﬁtness is determined by applying the ANN substrate to the training images. For HyperNEAT feature learning an additional 1000 images are randomly selected (again evenly spread across classes) from the MNIST training set. Backward propagation training is run for 250 epochs on the 300 selected images and then tested on the different set of 1000 images. The testing performance of the backward propagation trained ANN becomes the ﬁtness of the CPPN for HyperNEAT. After evolution completes, the generation champions are evaluated on the MNIST testing set. In the HyperNEAT for feature learning case, the backward propagation trained ANN learns from the full MNIST training set. As seen in ﬁgure 3, HyperNEAT by itself quickly plateaus at a particular performance level and only gradually learns, achieving an average ﬁtness score of 4.2 by the end of training. The accuracy of these trained solutions on the testing data is also not impressive, achieving only a 23.9% correct classiﬁcations. By applying HyperNEAT as a feature learner and allowing backward propagation to train, both ﬁtness score during training and classiﬁcation correctness during testing increase to 5.7 and 58.4%, respectively. It is interesting to note that in both cases, the ﬁtness score and the correct classiﬁcations are not completely correlated, that is, improvements in the ﬁtness score can lead to decreases in classiﬁcation accuracy.  5  Figure 3: HyperNEAT Performance with Traditional ANN Architecture. HyperNEAT can learn to classify images by itself; however, learning quickly plateaus at a low performance and then learn- ing slows, reaching a ﬁtness score of 4.2 and testing performance of 23.9% correct classiﬁcations. By acting as a feature learner, HyperNEAT does not plateau and achieves improved performance over HyperNEAT alone, ﬁnishing training with a ﬁtness score of 5.7 and a testing score of 58.4%. Thus HyperNEAT as a feature learner is more promising for than HyperNEAT alone.  Figure 4: HyperNEAT Performance with CNN Architecture. The CNN architecture has a small effect on HyperNEAT only performance, lowering the ﬁtness performance plateau to 4.1, but in- creasing testing performance of 27.7% correctness. Shifting to CNN architecture signiﬁcantly im- proves HyperNEAT’s ability as a feature learner, allowing HyperNEAT to ﬁnd features that achieve a ﬁtness score 7.0 and a testing score of 92.1%. Thus HyperNEAT can learn effective levels of performance given careful selection of the ANN substrate.  Changing HyperNEAT to learn the weights of a CNN architecture, rather than the normal ANN architecture of HyperNEAT, does change performance (ﬁgure 4). In this case, HyperNEAT by it- self plateaus faster at a lower ﬁtness level, achieving an average ﬁtness score of 4.1 by the end of training, but the accuracy of these trained solutions on the testing data improves to 27.7% correct classiﬁcations. On the other hand, feature learning with HyperNEAT on a CNN architecture signif- icantly improves both metrics, achieving a 7.0 ﬁtness score and a 92.1% correct classiﬁcations. An example of the feature maps generated by these HyperNEAT champions can be seen in ﬁgure 5.  6   0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 0 250 500 750 1000 1250 1500 1750 2000 2250 2500 Correct Classifications Fitness Score Generation HyperNEAT Fitness HyperNEAT + BP Fitness HyperNEAT Correct % HyperNEAT + BP Correct %  0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 0 250 500 750 1000 1250 1500 1750 2000 2250 2500 Correct Classifications Fitness Score Generation HyperNEAT Fitness HyperNEAT + BP Fitness HyperNEAT Correct % HyperNEAT + BP Correct % (a)  (b)  Figure 5: Visualization of Feature Maps Generated by HyperNEAT. Example feature maps are shown for the digits nine (a) and three (b). Interestingly, there is a distinct pattern along the feature dimension (top to bottom), demonstrating patterns in the geometry of the feature maps.  6 Discussion and Future Work  Evolution is a signiﬁcant factor in the creation of biological systems, including the visual cortex [1, 2]. However, biological neural networks are often deep architectures and conventional Neu- roEvolution approaches have been challenged in effectively training ANNs order of magnitude smaller than those found in nature. Emerging research from generative and developmental sys- tems has provided an answer in the form of HyperNEAT. HyperNEAT can effectively learn weight patterns for an ANN substrate by training CPPNs, an indirect encoding that computes weights as a function of geometry. The challenge for HyperNEAT is that, by training an indirect encoding, the ability to control precise weights is diminished, thereby creating difﬁculties in making ﬁne adjust- ments for tasks such as classiﬁcation. That is, a change in the indirect encoding will cause changes across the entire weight pattern, when a change to a single weight is needed. This challenge can be addressed by applying HyperNEAT as a feature learner for another ML approach that then makes the ﬁne adjustments for the task, as shown in this paper. Indeed, the results in this paper demonstrate that HyperNEAT successfully learn features to train a backward propagation trained ANN. Interestingly, changing to the convolutional neural network architecture had a signiﬁcant impact on HyperNEAT’s performance. Two interesting questions arise: (1) Are there more effective architec- ture choices? and; (2) How do can they be discovered? A path to answering these questions may be an extension of HyperNEAT known as Evolvable Substrate HyperNEAT (ES-HyperNEAT; [39]). In ES-HyperNEAT, the ANN substrate is not deﬁned a priori (except for inputs and outputs); instead, the pattern generated by the CPPN determines the placement neurons in the hidden layers. An exciting implication of this work is that evolution provides a means to learn a representation that is well-suited for the target machine learning approach. Through reinforcement learning, evolution can measure ﬁtness as a function of how well an approach performs on the generated representa- tion. Because the ﬁtness measure does not depend on a particular error signal or gradient, multiple metrics can be incorporated. For example, the results in the paper incorporated several classiﬁcation characteristics into the ﬁtness function, such as measures of true positives, true negatives, false posi- tives, and false negatives. By combining measures, evolution can explore multiple paths through the different metrics. In addition, the many candidate solutions that evolution generates will perform differently from each other and these solutions may be combined to create an enhanced feature set. Finally, because the ﬁtness measure can be based upon performance of the trained solution on data not seen by ML approach, the discovered features may encourage learning that generalizes. Thus this approach provides anopportunity to encourage representations that enhance generalizability.  7   7 Conclusion  This paper investigated deep learning through NeuroEvolution. While NE has been limited in the size of ANNs that could be effectively trained in the past, novel algorithms that operate on indirect encodings, such as HyperNEAT, allow effective evolution of large ANNs. Prior work with Hyper- NEAT has shown promise in simple vision tasks that operate on a raw visual ﬁeld, but with non-deep architectures. By itself, HyperNEAT struggles to ﬁnd ANNs that perform well in image classiﬁca- tion; however, HyperNEAT demonstrates an effective ability to act as a feature extractor by being combined with backward propagation. Thus HyperNEAT provides a potentially interesting path for combining reinforcement learning and supervised learning in image classiﬁcation, as evolution and lifetime learning combine to create the capabilities in biological neural networks.  Acknowledgments  This work was supported and funded by the SSC Paciﬁc Naval Innovative Science and Engineering (NISE) Program. References [1] Q. V. Le, L. A. Isbell, J. Matsumoto, M. Nguyen, E. Hori, R. S. Maior, C. Tomaz, A. H. Tran, T. Ono, and H. Nishijo, “Pulvinar neurons reveal neurobiological evidence of past selection for rapid detection of snakes,” Proceedings of the National Academy of Sciences, vol. 110, November 2013.  [2] T. Gliga and G. Dehaene-Lambertz, “Structural encoding of body and face in human infants and adults,”  Journal of Cognitive Neuroscience, vol. 17, August 2005.  [3] X. Yao, “Evolving artiﬁcial neural networks,” Proceedings of the IEEE, vol. 87, no. 9, pp. 1423–1447,  1999.  [4] E. R. Kandel, J. H. Schwartz, and T. M. Jessell, Principles of Neural Science. New York: McGraw-Hill,  fourth ed., 2000.  [5] M. J. Zigmond, F. E. Bloom, S. C. Landis, J. L. Roberts, and L. R. Squire, eds., Fundamental Neuro-  science. London: Academic Press, 1999.  [6] K. O. Stanley, D. B. D’Ambrosio, and J. Gauci, “A hypercube-based indirect encoding for evolving large-  scale neural networks,” Artiﬁcial Life, vol. 15, 2009.  [7] P. J. Bentley and S. Kumar, “The ways to grow designs: A comparison of embryogenies for an evolution-  ary design problem,” in Proc. of the Genetic and Evo. Comp. Conf., pp. 35–43, ACM, 1999.  [8] L. Hartwell, J. Hopﬁeld, S. Leibler, and A. Murray, “From molecular to modular cell biology,” Nature,  vol. 402, 1999.  [9] G. Wagner and L. Altenberg, “Complex adapatations and the evolution of evolovability,” Evolution,  vol. 50, 1996.  [10] J. Gauci and K. O. Stanley, “A case study on the critical role of geometric regularity in machine learning,”  in Proc. of the 23rd AAAI Conf. on AI (AAAI-2008), (Menlo Park, CA), AAAI Press, 2008.  [11] J. Gauci and K. O. Stanley, “Autonomous evolution of topographic regularities in artiﬁcial neural net-  works,” Neural Computation, p. 38, 2010.  [12] K. O. Stanley, “Compositional pattern producing networks: A novel abstraction of development,” Genetic  Programming and Evolvable Machines Special Issue on Dev. Sys., vol. 8, no. 2, pp. 131–162, 2007.  [13] J. Gauci and K. O. Stanley, “Generating large-scale neural networks through discovering geometric regu-  larities,” in Proc. of the Genetic and Evo. Comp. Conf., (New York, NY), p. 8, ACM, 2007.  [14] O. J. Coleman, Evolving Neural Networks for Visual Processing. PhD thesis, The University of New  South Wales, Kensingtion, Austrailia, 2010.  [15] P. Verbancsics and K. O. Stanley, “Constraining connectivity to encourage modularity in hyperneat,” in  Proc. of the Genetic and Evo. Comp. Conf. , (New York, NY), ACM Press, 2011.  [16] M. Hausknecht, P. Khandelwal, R. Miikkulainen, and P. Stone, “Hyperneat-gpp: A hyperneat-based atari general game player,” in Proceedings and the Genetic and Evolutionary Computation Conference, (Philadelphia, Pennsylvania), p. 8, ACM Press, July 2012.  [17] K. O. Stanley and R. Miikkulainen, “Competitive coevolution through evolutionary complexiﬁcation,”  Journal of Artiﬁcial Intelligence Research, vol. 21, pp. 63–100, 2004.  [18] T. Aaltonen et al., “Measurement of the top quark mass with dilepton events selected using neuroevolution  at CDF,” Physical Review Letters, 2009.  8  [19] S. Whiteson and D. Whiteson, “Stochastic optimization for collision selection in high energy physics,” in IAAI 2007: Proceedings of the Nineteenth Annual Innovative Applications of Artiﬁcial Intelligence Conference, (Vancouver, British Columbia, Canada), AAAI Press, July 2007.  [20] L. Cardamone, D. Loiacono, and P. L. Lanzi, “On-line neuroevolution applied to the open racing car simulator,” in Proceedings of the 2009 IEEE Congress on Evolutionary Computation (IEEE CEC 2009), (Piscataway, NJ, USA), IEEE Press, 2009.  [21] M. E. Taylor, S. Whiteson, and P. Stone, “Comparing evolutionary and temporal difference methods in a reinforcement learning domain,” in Proceedings of the Genetic and Evolutionary Computation Confer- ence (GECCO 2006), (New York, NY), pp. 1321–1328, ACM Press, July 2006.  [22] S. Whiteson, “Improving reinforcement learning function approximators via neuroevolution,” in AAMAS ’05: Proceedings of the fourth international joint conference on Autonomous agents and multiagent sys- tems, (New York, NY, USA), pp. 1386–1386, ACM, 2005.  [23] K. O. Stanley, B. D. Bryant, and R. Miikkulainen, “Real-time neuroevolution in the NERO video game,” IEEE Transactions on Evolutionary Computation Special Issue on Evolutionary Computation and Games, vol. 9, no. 6, pp. 653–668, 2005.  [24] D. D’Ambroiso and K. O. Stanley, “Evolving policy geometry for scalable multiagent learning,” in Pro- ceedings of the Ninth International Conference on Autonomous Agents and Multiagent Systems (AAMAS- 2010), (New York, NY, USA), p. 8, ACM Press, 2010.  [25] D. B. D’Ambrosio and K. O. Stanley, “Generative encoding for multiagent learning,” in Proc. of the  Genetic and Evo. Comp. Conf., (New York, NY), ACM Press, 2008.  [26] P. Verbancsics and K. O. Stanley, “Evolving static representations for task transfer,” Journal of Machine  Learning Research, vol. 11, 2010.  [27] K. O. Stanley and R. Miikkulainen, “A taxonomy for artiﬁcial embryogeny,” Artiﬁcial Life, vol. 9, no. 2,  pp. 93–130, 2003.  [28] A. M. Turing, “The Chemical Basis of Morphogenesis,” Royal Society of London Philosophical Transac-  tions Series B, vol. 237, pp. 37–72, Aug. 1952.  [29] Y. Bengio, A. Courville, and P. Vincent, “Representation learning: A review and new perspectives,” IEEE  Transactions on Pattern Analysis and Machine Intelligence, vol. 35, pp. 1798–1828, August 2013.  [30] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning algorithm for deep belief nets,” Neural Com-  putation, vol. 18, pp. 1527–1554, 2006.  [31] S. Rifai, Y. N. Dauphin, P. Vincent, Y. Bengio, and X. Muller, “The manifold tangent classifer,” in Ad-  vances in Neural Information Processing Systems, 2011.  [32] D. Ciresan, U. Meier, J. Masci, and J. Schmidhuber, “Multi-column deep neural network for trafﬁc sign  classiﬁcation,” Neural Networks, vol. 32, pp. 333–338, 2012.  [33] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classﬁcation with deep convolutional neural  networks,” in Advances in Neural Information Processing Systems, 2012.  [34] R. Salakhutdinov and G. E. Hinton, “Deep boltzmann machines,” in Proceedings of the International  Conference on Artiﬁcial Intelligence and Statistics, 2009.  [35] G. E. Hinton and R. Salakhutdinov, “Reducing the dimensionality of data with neural networks,” Science,  vol. 313, no. 5786, pp. 504–507, 2006.  [36] K. Fukushima and S. Miyake, “Neocognitron: A new algorithm for pattern recognition tolerant of defor-  mations and shifts in position,” Pattern recognition, vol. 15, no. 6, pp. 455–469, 1982.  [37] D. H. Hubel and T. N. Wiesel, “Receptive ﬁeld of single neurones in the cat’s striate cortex,” The Journal  of Physiology, vol. 148, no. 3, pp. 574–591, 1959.  [38] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to docment recogni-  tion,” IEEE, vol. 86, no. 11, pp. 2278–2324, 1998.  [39] S. Risi and K. O. Stanley, “An enhanced hypercube-based encoding for evolving the placement, density  and connectivity of neurons,” Artiﬁcial Life, vol. 18, no. 4, pp. 331–363, 2012.  9  ","An important goal for the machine learning (ML) community is to createapproaches that can learn solutions with human-level capability. One domainwhere humans have held a significant advantage is visual processing. Asignificant approach to addressing this gap has been machine learningapproaches that are inspired from the natural systems, such as artificialneural networks (ANNs), evolutionary computation (EC), and generative anddevelopmental systems (GDS). Research into deep learning has demonstrated thatsuch architectures can achieve performance competitive with humans on somevisual tasks; however, these systems have been primarily trained throughsupervised and unsupervised learning algorithms. Alternatively, research isshowing that evolution may have a significant role in the development of visualsystems. Thus this paper investigates the role neuro-evolution (NE) can take indeep learning. In particular, the Hypercube-based NeuroEvolution of AugmentingTopologies is a NE approach that can effectively learn large neural structuresby training an indirect encoding that compresses the ANN weight pattern as afunction of geometry. The results show that HyperNEAT struggles with performingimage classification by itself, but can be effective in training a featureextractor that other ML approaches can learn from. Thus NeuroEvolution combinedwith other ML methods provides an intriguing area of research that canreplicate the processes in nature."
1312.5663,2014,k-Sparse Autoencoders  ,"['Alireza Makhzani', 'Brendan Frey']",https://arxiv.org/pdf/1312.5663.pdf,"k -Sparse Autoencoders  Alireza Makhzani Brendan Frey University of Toronto, 10 King’s College Rd. Toronto, Ontario M5S 3G4, Canada  makhzani@psi.utoronto.ca  frey@psi.utoronto.ca  4 1 0 2    r a     M 2 2      ]  G L . s c [      2 v 3 6 6 5  .  2 1 3 1 : v i X r a  Abstract  Recently, it has been observed that when rep- resentations are learnt in a way that encour- ages sparsity, improved performance is ob- tained on classiﬁcation tasks. These meth- ods involve combinations of activation func- tions, sampling steps and diﬀerent kinds of penalties. To investigate the eﬀectiveness of sparsity by itself, we propose the “k - sparse autoencoder”, which is an autoen- coder with linear activation function, where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we ﬁnd that this method achieves better classiﬁcation results than de- noising autoencoders, networks trained with dropout, and RBMs. k -sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse cod- ing algorithms cannot be applied.  1. Introduction  (Olshausen & Field, 1997)  Sparse feature learning algorithms range from sparse coding approaches to training neural networks with sparsity penalties (Nair & Hinton, 2009; Lee et al., 2007). These meth- ods typically comprise two steps: a learning algorithm that produces a dictionary W that sparsely represents the data {xi}N i=1, and an encoding algorithm that, given the dictionary, deﬁnes a mapping from a new input vector x to a feature vector.  A practical problem with sparse coding is that both the dictionary learning and the sparse encoding steps are computationally expensive. Dictionaries are usu- ally learnt oﬄine by iteratively recovering sparse codes  International Conference on Learning Representations, ICLR 2014  and updating the dictionary. Sparse codes are com- puted using the current dictionary W and a pursuit algorithm to solve  ˆzi = argmin  ∥xi − W z∥2  2 s.t. ∥z∥0 < k  (1)  z  where zi, i = 1, .., N are the columns of Z. Convex relaxation methods such as ℓ1 minimization or greedy methods such as OMP (Tropp & Gilbert, 2007) are used to solve the above optimization. While greedy al- gorithms are faster, they are still slow in practice. The current sparse codes are then used to update the dic- tionary, using techniques such as the method of opti- mal directions (MOD) (Engan et al., 1999) or K-SVD (Aharon et al., 2005). These methods are computa- tionally expensive; MOD requires inverting the data matrix at each step and K-SVD needs to compute a SVD in order to update every column of the dictionary.  To achieve speedups, in (Gregor & LeCun, 2010; Kavukcuoglu et al., 2010), a parameterized non-linear encoder function is trained to explicitly predict sparse codes using a soft thresholding operator. However, they assume that the dictionary is already given and do not address the oﬄine phase.  Another approach that has been taken recently is to train autoencoders in a way that encourages sparsity. However, these methods usually involve combinations of activation functions, sampling steps and diﬀerent kinds of penalties, and are sometimes not guaranteed to produce sparse representations for each input. For example, in (Lee et al., 2007; Nair & Hinton, 2009), a “lifetime sparsity” penalty function proportional to the negative of the KL divergence between the hidden unit marginals and the target sparsity probability is added to the cost function. This results in sparse acti- vation of hidden units across training points, but does not guarantee that each input has a sparse represen- tation.  The contributions of this paper are as follows. (i) We describe “k -sparse autoencoders” and show that they can be eﬃciently learnt and used for sparse coding.  k -Sparse Autoencoders  (ii) We explore how diﬀerent sparsity levels (k) im- pact representations and classiﬁcation performance. (iii) We show that by solely relying on sparsity as the regularizer and as the only nonlinearity, we can achieve much better results than the other methods, including RBMs, denoising autoencoders (Vincent et al., 2008) and dropout (Hinton et al., 2012). (iv) We demon- strate that k -sparse autoencoders are suitable for pre- training and achieve results comparable to state-of- the-art on MNIST and NORB datasets.  In this paper, Γ is an estimated support set and Γc is its complement. W † is the pseudo-inverse of W and suppk (x) is an operator that returns the indices of the k largest coeﬃcients of its input vector. zΓ is the vector obtained by restricting the elements of z to the indices of Γ and WΓ is the matrix obtained by restricting the columns of W to the indices of Γ.  2. Description of the Algorithm  2.1. The Basic Autoencoder  A shallow autoencoder maps an input vector x to a hidden representation using the function z = f(P x+b), parameterized by {P, b}. f is the activation function, e.g., linear, sigmoidal or ReLU. The hidden represen- tation is then mapped linearly to the output using ˆx = W z + b′. The parameters are optimized to mini- mize the mean square error of ∥ˆx−x∥2 2 over all training points. Often, tied weights are used, so that P = W ⊺.  2.2. The k -Sparse Autoencoder  The k-sparse autoencoder is based on an autoencoder with linear activation functions and tied weights. In the feedforward phase, after computing the hidden code z = W ⊺x+b, rather than reconstructing the input from all of the hidden units, we identify the k largest hidden units and set the others to zero. This can be done by sorting the activities or by using ReLU hid- den units with thresholds that are adaptively adjusted until the k larges activities are identiﬁed. This re- sults in a vector of activities with the support set of suppk (W ⊺x+b). Note that once the k largest activities are selected, the function computed by the network is linear. So the only non-linearity comes from the se- lection of the k largest activities. This selection step acts as a regularizer that prevents the use of an overly large number of hidden units when reconstructing the input.  Once the weights are trained, the resulting sparse rep- resentations may be used for learning to perform down- stream classiﬁcation tasks. However, it has been ob- served that often, better results are obtained when the  sparse encoding stage used for classiﬁcation does not exactly match the encoding used for dictionary train- ing (Coates & Ng, 2011). For example, while in k - means, it is natural to have a hard-assignment of the points to the nearest cluster in the encoding stage, it has been shown in (Van Gemert et al., 2008) that soft assignments result in better classiﬁcation performance. Similarly, for the k -sparse autoencoder, instead of us- ing the k largest elements of W ⊺x+b as the features, we have observed that slightly better performance is ob- tained by using the αk largest hidden units where α ≥ 1 is selected using validation data. So at the test time, we use the support set deﬁned by suppαk(W ⊺x + b). The algorithm is summarized as follows.  k -Sparse Autoencoders: Training:  1) Perform the feedforward phase and compute  z = W ⊺x + b  2) Find the k largest activations of z and set the rest to zero.  z(Γ)c = 0 where Γ = suppk (z)  3) Compute the output and the error using the sparsiﬁed z.  ˆx = W z + b′ E = ∥x − ˆx∥2  2  3) Backpropagate the error through the k largest activations deﬁned by Γ and iterate.  Sparse Encoding: Compute the features h = W ⊺x + b. Find its αk largest activations and set the rest to zero. h(Γ)c = 0 where Γ = suppαk (h)  3. Analysis of the k -Sparse  Autoencoder  In this section, we explain how the k -sparse autoen- coder can be viewed in the context of sparse coding with incoherent matrices. This perspective sheds light on why the k -sparse autoencoders work and why they achieve invariant features and consequently good clas- siﬁcation results. We ﬁrst explain a sparse recovery al- gorithm and then show that the k -sparse autoencoder iterates between an approximation of this algorithm and a dictionary update stage.  3.1. Iterative Thresholding with Inversion (ITI)  Iterative hard thresholding (Blumensath & Davies, 2009) is a class of low complexity algorithms, which has recently been proposed for the reconstruction of sparse signals. In this work, we use a variant called “iterative thresholding with inversion” (Maleki, 2009). Given a ﬁxed x and W , starting from z0 = 0, ITI iter- atively ﬁnds the sparsest solution of x = W z using the  k -Sparse Autoencoders  following steps.  1. Support Estimation Step:  Γ = suppk (zn + W ⊺(x − W zn))  (2)  WΓ to ﬁnd the non-zero values of the support set. The pseudo-inverse of the matrix WΓ is a matrix, such as PΓ, that minimizes the following cost function.  W †  Γ = arg min  ∥x − WΓzΓ∥2  2  2. Inversion Step:  PΓ  (5)  Γx = (W ⊺  Γ WΓ)−1W ⊺  Γ x  Γ  = W † zn+1 zn+1 (Γ)c = 0  (3)  PΓ  = arg min  ∥x − WΓPΓx∥2  2  Assume H = W ⊺W − I and z0 is the true sparse so- lution. The ﬁrst step of ITI estimates the support set as Γ = suppk (W ⊺x) = suppk (z0 + Hz0). If W was orthogonal, we would have Hz0 = 0 and the algo- rithm would succeed in the ﬁrst iteration. But if W is overcomplete, Hz0 behaves as a noise vector whose variance decreases after each iteration. After estimat- ing the support set of z as Γ, we restrict W to the indices included in Γ and form WΓ. We then use the pseudo-inverse of WΓ to estimate the non-zero values minimizing ∥x−WΓzΓ∥2 2. Lastly, we reﬁne the support estimation and repeat the whole process until conver- gence.  3.2. Sparse Coding with the k -Sparse  Autoencoder  Here, we show that we can derive the k -sparse autoen- coder tarining algorithm by approximating a sparse coding algorithm that uses the ITI algorithm jointly with a dictionary update stage.  The conventional approach of sparse coding is to ﬁx the sparse code matrix Z, while updating the dictio- nary. However, here, after estimating the support set in the ﬁrst step of the ITI algorithm, we jointly per- form the inversion step of ITI and the dictionary up- date step, while ﬁxing just the support set of the sparse code Z. In other words, we update the atoms of the dictionary and allow the corresponding non-zero values to change at the same time to minimize ∥X − WΓZΓ∥2 over both WΓ and ZΓ.  2  When we are performing sparse recovery with the ITI algorithm using a ﬁxed dictionary, we should perform a ﬁxed number of iterations to get the perfect recon- struction of the signal. But, in sparse coding, since we learnt a dictionary that is adapted to the signals, as shown in Section 3.3, we can ﬁnd the support set just with the ﬁrst iteration of ITI:  Γz = suppk (W ⊺x)  (4)  In the inversion step of the ITI algorithm, once we estimate the support set, we use the pseudo-inverse of  Finding the exact pseudo-inverse of WΓ is computa- tionally expensive, so instead, we perform a single step of gradient descent. The gradient with respect to PΓ is found as follows:  ∂∥x − WΓzΓ∥2  2  ∂PΓ  =  ∂∥x − WΓzΓ∥2  2  ∂zΓ  x  (6)  The ﬁrst term of the right hand side of the Equation (6) is the dictionary update stage, which is computed as follows:  ∂∥x − WΓzΓ∥2  2  ∂zΓ  = (WΓzΓ − x)z⊺  Γ  (7)  Therefore, in order to approximate the pseudo-inverse, we ﬁrst ﬁnd the dictionary derivative and then “back- propagate” it to ﬁnd the update of the pseudo-inverse.  We can view these operations in the context of an au- toencoder with linear activations where P is the en- coder weight matrix and W is the decoder weight ma- trix. At each iteration, instead of back-propagating through all the hidden units, we just back-propagate through the units with the k largest activities, deﬁned by suppk (W ⊺x), which is the ﬁrst iteration of ITI. Keeping the k largest hidden activities and ignoring the others is the same as forming WΓ by restricting W to the estimated support set. Back-propagation on the decoder weights is the same as gradient descent on the dictionary and back-propagation on the encoder weights is the same as approximating the pseudo- inverse of the corresponding WΓ.  We can perform support estimation in the feedforward phase by assuming P = W ⊺ (i.e., the autoencoder has tied weights). In this case, support estimation can be done by computing z = W ⊺x + b and picking the k largest activations; the bias just accounts for the mean and subtracts its contribution. Then the “inversion” and “dictionary update” steps are done at the same time by back-propagation through just the units with the k largest activities.  In summary, we can view k -sparse autoencoders as the approximation of a sparse coding algorithm which uses ITI in the sparse recovery stage.  k -Sparse Autoencoders  3.3. Importance of Incoherence  The coherence of a dictionary indicates the degree of similarity between diﬀerent atoms or diﬀerent collec- tions of atoms. Since the dictionary is overcomplete, we can represent each column of the dictionary as a linear combination of other columns. But what inco- herence implies is that we should not be able to repre- sent a column as a sparse linear combination of other columns and the coeﬃcients of the linear combination should be dense. For example, if two columns are ex- actly the same, then the dictionary is highly coherent since we can represent one of those columns as the sparse linear combination of the rest of the columns. A naive measure of coherence that has been proposed in the literature is the mutual coherence µ(W ) which is deﬁned as the maximum absolute inner product across all the possible pairs of the atoms of the dictionary.  µ(W ) = max i≠j  ∣⟨wi, wj⟩∣  (8)  There is a close relationship between the coherency of the dictionary and the uniqueness of the sparse solu- tion of x = W z. In (Donoho & Elad, 2003), it has been proven that if k ≤ (1 + µ−1), then the sparsest solution is unique.  We can show that if the dictionary is incoherent enough, there is going to be an attraction ball around the signal x and there is only one unique sparse lin- ear combination of the columns that can get into this attraction ball. So even if we perturb the input with a small amount of noise, translation, rotation, etc., we can still achieve perfect reconstruction of the orig- inal signal and the sparse features are always roughly conserved. Therefore, incoherency of the dictionary is a measure of invariance and stability of the fea- tures. This is related to the denoising autoencoder (Vincent et al., 2008) in which we achieve invariant features by trying to reconstruct the original signal from its noisy versions.  Here we show that if the dictionary is incoherent enough, the ﬁrst step of the ITI algorithm is suﬃcient for perfect sparse recovery.  Theorem 3.1. Assume x = W z and the columns of the dictionary have unit ℓ2-norm. Also without loss of generality, assume that the non-zero elements of z are its ﬁrst k elements and are sorted as z1 ≥ z2 ≥ ... ≥ zk . Then, if k µ ≤ zk , we can recover the support set of z 2z1 using suppk (W ⊺x).  Proof : Let us assume 0 ≤ i ≤ k and y = W ⊺x. Then, we can write:  yi = zi +  k  ∑  j=1,j≠i  ⟨wi, wj⟩zj ≥ zi − µ  k  ∑  j=1,j≠i  zj ≥ zk − k µz1  (9)  On the other hand:  max i>k  {yi} = max i>k  ⎧⎪⎪ ⎨ ⎪⎪⎩  k  ∑ j=1  ⟨wi, wj⟩zj  ⎫⎪⎪ ⎬ ⎪⎪⎭  ≤ k µz1  (10)  So if k µ ≤ zk 2z1 teed to be greater than the rest of its elements.  , all the ﬁrst k elements of y are guaran-  As we can see from Theorem 3.1, the chances of ﬁnding the true support set with the encoder part of the k - sparse autoencoder depends on the incoherency of the learnt dictionary. As the k -sparse autoencoder con- verges (i.e., the reconstruction error goes to zero), the algorithm learns a dictionary that satisﬁes x ≈ W z, so the support set of z can be estimated using the ﬁrst step of ITI. Since suppk (W ⊺x) succeeds in ﬁnding the support set when the algorithm converges, the learnt dictionary must be suﬃciently incoherent.  4. Experiments  In this section, we evaluate the performance of k - sparse autoencoders in both unsupervised learning and in shallow and deep discriminative learning tasks.  4.1. Datasets  We use the MNIST handwritten digit dataset, which consists of 60,000 training images and 10,000 test im- ages. We randomly separate the training set into 50,000 training cases and 10,000 cases for validation.  We also use the small NORB normalized-uniform dataset (LeCun et al., 2004), which contains 24,300 training examples and 24,300 test examples. This database contains images of 50 toys from 5 generic cat- egories: four-legged animals, human ﬁgures, airplanes, trucks, and cars. Each image consists of two channels, each of size 96 × 96 pixels. We take the inner 64 × 64 pixels of each channel and resize it using bicubic in- terpolation to the size of 32 × 32 pixels from which we form a vector with 2048 dimensions as the input. Data points are subtracted by the mean and divided by the standard deviation along each input dimension across the whole training set to normalize the contrast. The training set is separated into 20,000 for training and 4,300 for validation.  We also test our method on natural image patches ex- tracted from CIFAR-10 dataset. We randomly extract 1000000 patches of size 8×8 from the 50000 32×32 im-  k -Sparse Autoencoders  ages of CIFAR-10. Each patch is then locally contrast- normalized and ZCA whitened. This preprocessing pipeline is the same as the one used in (Coates et al., 2011) for feature extraction.  4.2. Training of k -Sparse Autoencoders  4.2.1. Scheduling of the Sparsity Level  When we are enforcing low sparsity levels in k -sparse autoencoders (e.g., k =15 on MNIST), one issue that might arise is that in the ﬁrst few epochs, the al- gorithm greedily assigns individual hidden units to groups of training cases, in a manner similar to k- means clustering. In subsequent epochs, these hidden units will be picked and re-enforced and other hidden units will not be adjusted. That is, too much sparsity can prevent gradient back-propagation from adjusting the weights of these other ‘dead’ hidden units. We can address this problem by scheduling the sparsity level over epochs as follows.  Suppose we are aiming for a sparsity level of k = 15. Then, we start oﬀ with a large sparsity level (e.g. k = 100) for which the k -sparse autoencoder can train all the hidden units. We then linearly decrease the sparsity level from k = 100 to k = 15 over the ﬁrst half of the epochs. This initializes the autoencoder in a good regime, for which all of the hidden units have a signiﬁcant chance of being picked. Then, we keep k = 15 for the second half of the epochs. With this scheduling, we can train all of the ﬁlters, even for low sparsity levels.  4.2.2. Training Hyper-parameters  We optimized the model parameters using stochastic gradient descent with momentum as follows.  vk+1 = mk vk − ηk ∇f(xk ) xk+1 = xk + vk  (11)  Here, vk is the velocity vector, mk is the momentum and ηk is the learning rate at the k -th iteration. We also use a Gaussian distribution with a standard devi- ation of σ for initialization of the weights. We use dif- ferent momentum values, learning rates and initializa- tions based on the task and the dataset, and validation is used to select hyperparameters. In the unsupervised MNIST task, the values were σ = 0.01 , mk = 0.9 and ηk = 0.01, for 5000 epochs. In the supervised MNIST task, training started with mk = 0.25 and ηk = 1, and then the learning rate was linearly decreased to 0.001 over 200 epochs. In the unsupervised NORB task, the values were σ = 0.01, mk = 0.9 and ηk = 0.0001, for 5000 epochs. In the supervised NORB task, training  started with mk = 0.9 and ηk = 0.01, and then the learning rate was linearly decreased to 0.001 over 200 epochs.  4.2.3. Implementations  While most of the conventional sparse coding algo- rithms require complex matrix operations such as ma- trix inversion or SVD decomposition, the k -sparse au- toencoders only need matrix multiplications and sort- ing operations in both dictionary learning stage and the sparse encoding stage. (For a parallel, distributed implementation, the sorting operation can be replaced by a method that recursively applies a threshold until k values remain.) We used an eﬃcient GPU implemen- tation obtained using the publicly available gnumpy library (Tieleman, 2010) on a single Nvidia GTX 680 GPU.  4.3. Eﬀect of Sparsity Level  In k -sparse autoencoders, we are able to tune the value of k to obtain the desirable sparsity level which makes the algorithm suitable for a wide variety of datasets. For example, one application could be pre-training a shallow or deep discriminative neural network. For large values of k (e.g., k = 100 on MNIST), the algo- rithm tends to learn very local features as is shown in Figure 1a and 2a. These features are too primitive to be used for classiﬁcation using a shallow architecture since a naive linear classiﬁer does not have enough ca- pacity to combine these features and achieve a good classiﬁcation rate. However, these features could be used for pre-training deep neural nets.  As we decrease the the sparsity level (e.g., k = 40 on MNIST), the output is reconstructed using a smaller number of hidden units and thus the features tend to be more global, as can be seen in Figure 1b,1c and 2b. For example, in the MNIST dataset, the lengths of the strokes increase when the sparsity level is decreased. These less local features are suitable for classiﬁcation using a shallow architecture. Nevertheless, forcing too much sparsity (e.g., k = 10 on MNIST), results in fea- tures that are too global and do not factor the input into parts, as depicted Figure 1d and 2c.  Fig. 3 shows the visualization of ﬁlters of the k -sparse autoencoder with 1000 hidden units and sparsity level of k = 50 learnt from random image patches extracted from CIFAR-10 dataset. We can see that the k -sparse autoencoder has learnt localized Gabor ﬁlters from natural image patches.  Fig. 4 plots histograms of the hidden unit activities for various unsupervised learning algorithms, includ-  k -Sparse Autoencoders  (a) k = 70  (b) k = 40  (c) k = 25  (d) k = 10  Figure 1. Filters of the k -sparse autoencoder for diﬀerent sparsity levels k, learnt from MNIST with 1000 hidden units.  (a) k = 200  (b) k = 150  (c) k = 50  Figure 2. Filters of the k -sparse autoencoder for diﬀerent sparsity levels k, learnt from NORB with 4000 hidden units.  k -Sparse Autoencoders  Raw Pixels RBM Dropout Autoencoder (50% hidden) Denoising Autoencoder (20% input dropout) Dropout + Denoising Autoencoder (20% input and 50% hidden) k -Sparse Autoencoder, k = 40 k -Sparse Autoencoder, k = 25 k -Sparse Autoencoder, k = 10  Error Rate 7.20% 1.81% 1.80% 1.95%  1.60%  1.54% 1.35% 2.10%  Table 1. Performance of unsupervised learning methods (without ﬁne-tuning) with 1000 hidden units on MNIST.  ing the k -sparse autoencoder (k =70 and k =15), ap- plied to the MNIST data. This ﬁgure contrasts the sparsity achieved by the k -sparse autoencoder with that of other algorithms.  Raw Pixels RBM (weight decay) Dropout Autoencoder Denoising Autoencoder (20% input dropout) k -Sparse Autoencoder, k = 200 k -Sparse Autoencoder, k = 150 k -Sparse Autoencoder, k = 75  Error Rate 23% 10.6% 10.1% 9.5%  10.4% 8.6% 9.5%  Table 2. Performance of unsupervised learning methods (without ﬁne-tuning) with 4000 hidden units on NORB.  8  7  6  5  4  3  2  1  0  0  Log histogram of hidden activities  ReLU Autoencoder Dropout Autoencoder, 50% hidden and 20% input k-Sparse Autoencoder, k=70 k-Sparse Autoencoder, k=15  1  2  3  4  5  6  Figure 3. Filters of k -sparse autoencoder with 1000 hidden units and k = 50, learnt from CIFAR-10 random patches.  4.4. Unsupervised Feature Learning Results  In order to compare the quality of the features learnt by our algorithm with those learnt by other unsuper- vised learning methods, we ﬁrst extracted features us- ing each unsupervised learning algorithm. Then we ﬁxed the features and trained a logistic regression clas- siﬁer using those features. The usefulness of the fea- tures is then evaluated by examining the error rate of the classiﬁer.  We trained a number of architectures on the MNIST and NORB datasets, including RBM, dropout autoen- coder and denoising autoencoder. In dropout, after ﬁnding the features using dropout regularization with a dropout rate of 50%, we used all of the hidden units as the features (this worked best). For the denois- ing autoencoder, after training the network by drop- ping the input pixels with a rate of 20%, we used  Figure 4. Histogram of hidden unit activities for various unsupervised learning methods.  all of the uncorrupted input pixels to ﬁnd the fea- tures for classiﬁcation (this worked best). In the k - sparse autoencoder, after training the dictionary, we used h = suppαk(W ⊺x + b) to ﬁnd the features as ex- plained in Section 2.2, where α was determined using validation data. Results for diﬀerent architectures are compared in Tables 1, 2. We can see that the perfor- mance of our k -sparse autoencoder is better than the rest of the algorithms. In our algorithm, the best re- sult is achieved by k = 25, α = 3 with 1000 hidden units on MNIST dataset and by k = 150, α = 2 with 4000 hidden units on NORB dataset.  4.5. Shallow Supervised Learning Results  In supervised learning, it is a common practice to use the encoder weights learnt by an unsuper- vised learning method to initialize the early layers of a multilayer discriminative model (Erhan et al., 2010). The back-propagation algorithm is then used  k -Sparse Autoencoders  Without Pre-Training RBM + F.T. Shallow Dropout AE + F.T. (%50 hidden) Denoising AE + F.T. (%20 input dropout) Deep Dropout AE + F.T. (Layer-wise pre-training, %50 hidden) k -Sparse AE + F.T. (k =25) Deep k -Sparse AE + F.T. (Layer-wise pre-training)  Error 1.60% 1.24% 1.05%  1.20%  0.85%  1.08%  0.97%  Table 3. Performance of supervised learning methods on MNIST. Pre-training was performed using the correspond- ing unsupervised learning algorithm with 1000 hidden units, and then the model was ﬁne-tuned.  Without Pre-Training DBN DBM third-order RBM Shallow Dropout AE + F.T. (%50 hidden) Shallow Denoising AE + F.T. (%20 input dropout) Deep Dropout AE + F.T. (Layer-wise pre-training, %50 hidden) Shallow k -Sparse AE + F.T. (k =150) Deep k -Sparse AE + F.T. (k =150, Layer-wise pre-training)  Error 12.7% 8.3% 7.2% 6.5% 8.2%  7.9%  7.0%  7.8%  7.4%  Table 4. Performance of supervised learning methods on NORB. Pre-training was performed using the corresponding unsupervised learning algorithm with 4000 hidden units, and then the model was ﬁne-tuned.  to adjust the weights of the last hidden layer and also to ﬁne-tune the weights in the previous layers. This procedure is often referred to as discriminative ﬁne-tuning. In this section, we report results us- ing unsupervised learning algorithms such as RBMs, (Salakhutdinov & Larochelle, 2010), DBMs DBNs (Salakhutdinov & Larochelle, 2010), third-order RBM (Nair & Hinton, 2009), dropout autoencoders, denois- ing autoencoders and k -sparse autoencoders to ini- tialize a shallow discriminative neural network for the MNIST and NORB datasets. We used back- propagation to ﬁne-tune the weights. The regulariza- tion method used in the ﬁne-tuning stage of diﬀerent algorithms is the same as the one used in the train- ing of the corresponding unsupervised learning task. For instance, we ﬁne-tuned the weights obtained from dropout autoencoder with dropout regularization or in denoising autoencoder, we ﬁne-tuned the discrimina- tive neural net by adding noise to the input. In a sim- ilar manner, in the ﬁne-tuning stage of the k -sparse autoencoder, we used the αk largest hidden units in the corresponding discriminative neural network, as explained in Section 2.2. Tables 3 and 4 reports the error rates obtained by diﬀerent methods.  4.6. Deep Supervised Learning Results  The k -sparse autoencoder can be used as a building block of a deep neural network, using greedy layer- wise pre-training (Bengio et al., 2007). We ﬁrst train a shallow k -sparse autoencoder and obtain the hidden codes. We then ﬁx the features and train another k -  sparse autoencoder on top of them to obtain another set of hidden codes. Then we use the parameters of these autoencoders to initialize a discriminative neural network with two hidden layers.  In the ﬁne-tuning stage of the deep neural net, we ﬁrst ﬁx the parameters of the ﬁrst and second layers and train a softmax classiﬁer on top of the second layer. We then hold the weights of the ﬁrst layer ﬁxed and train the second layer and softmax jointly using the initialization of the softmax that we found in the pre- vious step. Finally, we jointly ﬁne-tune all of the layers with the previous initialization. We have observed that this method of layer-wise ﬁne-tuning can improve the classiﬁcation performance compared to the case where we ﬁne-tune all the layers at the same time.  In all of the ﬁne-tuning steps, we keep the αk largest hidden codes, where k = 25, α = 3 in MNIST and k = 150, α = 2 in NORB in both hidden layers. Tables 3 and 4 report the classiﬁcation results of diﬀerent deep supervised learning methods.  5. Conclusion  In this work, we proposed a very fast sparse coding method called k -sparse autoencoder, which achieves exact sparsity in the hidden representation. The main message of this paper is that we can use the result- ing representations to achieve state-of-the-art classiﬁ- cation results, solely by enforcing sparsity in the hid- den units and without using any other nonlinearity or regularization. We also discussed how the k -sparse au- toencoder could be used for pre-training shallow and  k -Sparse Autoencoders  deep supervised architectures.  6. Acknowledgment  We would like to thank Andrew Delong, Babak Ali- panahi and Lei Jimmy Ba for the valuable comments.  References  Aharon, Michal, Elad, Michael, and Bruckstein, Al- fred. K-svd: Design of dictionaries for sparse repre- sentation. Proceedings of SPARS, 5:9–12, 2005.  Bengio, Yoshua, Lamblin, Pascal, Popovici, Dan, and Larochelle, Hugo. Greedy layer-wise training of deep networks. Advances in neural information process- ing systems, 19:153, 2007.  Blumensath, Thomas and Davies, Mike E.  Iterative hard thresholding for compressed sensing. Applied and Computational Harmonic Analysis, 27(3):265– 274, 2009.  Coates, Adam and Ng, Andrew. The importance of encoding versus training with sparse coding and vec- tor quantization. In Proceedings of the 28th Interna- tional Conference on Machine Learning (ICML-11), pp. 921–928, 2011.  Coates, Adam, Ng, Andrew Y, and Lee, Honglak. An analysis of single-layer networks in unsupervised fea- ture learning. In International Conference on Arti- ﬁcial Intelligence and Statistics, pp. 215–223, 2011.  Donoho, David L and Elad, Michael. Optimally sparse representation in general (nonorthogonal) dictionar- ies via 1 minimization. Proceedings of the National Academy of Sciences, 100(5):2197–2202, 2003.  Engan, Kjersti, Aase, Sven Ole, and Hakon Husoy, J. Method of optimal directions for frame design. In Acoustics, Speech, and Signal Processing, 1999. Proceedings., 1999 IEEE International Conference on, volume 5, pp. 2443–2446. IEEE, 1999.  Erhan, Dumitru, Bengio, Yoshua, Courville, Aaron, Manzagol, Pierre-Antoine, Vincent, Pascal, and Bengio, Samy. Why does unsupervised pre-training help deep learning? The Journal of Machine Learn- ing Research, 11:625–660, 2010.  Gregor, Karol and LeCun, Yann. Learning fast ap- proximations of sparse coding. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 399–406, 2010.  Hinton, Geoﬀrey E, Srivastava, Nitish, Krizhevsky, Ilya, and Salakhutdinov, Rus- Improving neural networks by preventing  Alex, Sutskever, lan R.  co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.  Kavukcuoglu, Koray, Ranzato, Marc’Aurelio, and Le- Cun, Yann. Fast inference in sparse coding al- gorithms with applications to object recognition. arXiv preprint arXiv:1010.3467, 2010.  LeCun, Yann, Huang, Fu Jie, and Bottou, Leon. Learning methods for generic object recognition with invariance to pose and lighting. In Computer Vision and Pattern Recognition, CVPR, volume 2, pp. II–97. IEEE, 2004.  Lee, Honglak, Ekanadham, Chaitanya, and Ng, An- drew. Sparse deep belief net model for visual area v2. In Advances in neural information processing systems, pp. 873–880, 2007.  Maleki, Arian. Coherence analysis of iterative thresh- olding algorithms. In Communication, Control, and Computing, 2009. Allerton 2009. 47th Annual Aller- ton Conference on, pp. 236–243. IEEE, 2009.  Nair, Vinod and Hinton, Geoﬀrey E. 3d object recog- nition with deep belief nets. In Advances in Neu- ral Information Processing Systems, pp. 1339–1347, 2009.  Olshausen, Bruno A and Field, David J. Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision research, 37(23):3311–3325, 1997.  Salakhutdinov, Ruslan and Larochelle, Hugo. Eﬃ- cient learning of deep boltzmann machines. In In- ternational Conference on Artiﬁcial Intelligence and Statistics, pp. 693–700, 2010.  Tieleman, Tijmen. Gnumpy: an easy way to use gpu boards in python. Department of Computer Science, University of Toronto, 2010.  Tropp, Joel A and Gilbert, Anna C. Signal recovery from random measurements via orthogonal match- ing pursuit. Information Theory, IEEE Transac- tions on, 53(12):4655–4666, 2007.  Van Gemert, Jan C, Geusebroek, Jan-Mark, Veenman, Cor J, and Smeulders, Arnold WM. Kernel code- books for scene categorization. In Computer Vision– ECCV 2008, pp. 696–709. Springer, 2008.  Vincent, Pascal, Larochelle, Hugo, Bengio, Yoshua, and Manzagol, Pierre-Antoine. Extracting and com- posing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pp. 1096–1103. ACM, 2008.  ","Recently, it has been observed that when representations are learnt in a waythat encourages sparsity, improved performance is obtained on classificationtasks. These methods involve combinations of activation functions, samplingsteps and different kinds of penalties. To investigate the effectiveness ofsparsity by itself, we propose the k-sparse autoencoder, which is anautoencoder with linear activation function, where in hidden layers only the khighest activities are kept. When applied to the MNIST and NORB datasets, wefind that this method achieves better classification results than denoisingautoencoders, networks trained with dropout, and RBMs. k-sparse autoencodersare simple to train and the encoding stage is very fast, making themwell-suited to large problem sizes, where conventional sparse coding algorithmscannot be applied."
1312.5714,2014,An Architecture for Distinguishing between Predictors and Inhibitors in Reinforcement Learning  ,"['Patrick C. Connor', 'Thomas P. Trappenberg']",https://arxiv.org/pdf/1312.5714.pdf,"5 1 0 2     b e F 8 1         ] I  A . s c [      2 v 4 1 7 5  .  2 1 3 1 : v i X r a  Avoiding Confusion between Predictors and Inhibitors  in Value Function Approximation  Patrick Connora,∗, Thomas Trappenberga  aFaculty of Computer Science, Dalhousie University, 6050 University Avenue,  PO BOX 15000, Halifax, Nova Scotia, Canada, B3H 4R2  Abstract  In reinforcement learning, the goal is to seek rewards and avoid punishments. A simple scalar captures the value of a state or of taking an action, where expected future rewards increase and punishments decrease this quantity. Nat- urally an agent should learn to predict this quantity to take beneﬁcial actions, and many value function approximators exist for this purpose. In the present work, however, we show how value function approximators can cause confusion between predictors of an outcome of one valence (e.g., a signal of reward) and the inhibitor of the opposite valence (e.g., a signal canceling expectation of pun- ishment). We show this to be a problem for both linear and non-linear value function approximators, especially when the amount of data (or experience) is limited. We propose and evaluate a simple resolution: to instead predict reward and punishment values separately, and rectify and add them to get the value needed for decision making. We evaluate several function approximators in this slightly diﬀerent value function approximation architecture and show that this approach is able to circumvent the confusion and thereby achieve lower value- prediction errors.  Keywords: Value Function Approximation, Conditioned Inhibition, Reinforcement Learning, Classical Conditioning  1. Introduction  Our world is full of sensory stimuli. For reinforcement learning (RL), each conﬁguration of such stimuli must be interpreted as expressing a particular world state. In simple RL problems, it is possible to enumerate all such states and employ a table to store the state values. It would seem that for most real world problems, the number of stimulus dimensions and the continuous qual- ity of sensory information begs rather for the use of a function approximation  ∗Corresponding author Email address: patrick.connor@dal.ca (Patrick Connor )  Preprint submitted to Elsevier  September 12, 2018  approach to map a value onto each state (or state-action combination). Value- function approximators [1] (VFAs) exactly fulﬁll this purpose, being based on a variety of function approximation techniques (e.g., [2, 3, 4]). For VFAs, a state is represented by a feature vector (rather than a table index), where each ele- ment reﬂects the salience of a real-world feature or stimulus in the world. Such a representation mimics the activation of cortical neurons, which specialize in responding most in the presence of speciﬁc stimuli. Also, many deep learning networks express the presence of features in terms of the activation or probabil- ity of activation of their hidden nodes. Depending on the application, a VFA’s input representation may vary, but the output representation or prediction tar- get is always the same: reward value. The reward value scale ranges from reward (positive values) through neutral (zero value) to punishment (negative values). In our natural environment, some of the stimuli making up the state corre- late directly with future rewarding or punishing events or outcomes. In animal learning or conditioning terms, such stimuli are referred to as “excitors”. “In- hibitors”, on the other hand, are deﬁned as being correlated with the absence of future events or outcomes that are otherwise expected. This distinction, in use since the days of Pavlov [5], plays out in real world circumstances. The shape and red colouring of an apple (excitor) may indicate a positive future reward, only to be canceled by the discovery of a worm’s borehole (inhibitor), or the punishment predicted by the scent of a predator (excitor) may be reduced by the perceived distance of its very faint call (inhibitor). Essentially, the role of an inhibitor is to cancel a certain excitor’s predicted outcome and its perceived future reward value (i.e., sending it toward zero).  Inhibitors and excitors of opposite valence are similar and yet diﬀerent in certain regards. A juicy apple (excitor) hanging from a tree beside which lays a predator (excitor of opposite valence) cancels the beneﬁt one might have hoped to receive, just like an inhibitor. The excitor of opposite valence alone, how- ever, encourages some action (e.g., escape from predator), whereas the inhibitor alone does not because it predicts a neutral outcome. In a classical condition- ing paradigm referred to as conditioned inhibition [5, 6], animals receive alter- nated presentations of a stimulus followed by a reinforcer, say, a reward (A+), and a combination of that stimulus and another, followed by no reward (AX-). Stimulus A becomes excitatory and stimulus X cancels or inhibits A’s reward prediction. However, the two are not viewed as entirely symmetric opposites in the classical conditioning ﬁeld. For example, the extinction of the inhibitor (i.e., eliminating its inhibitory properties) is generally more complicated than mere non-reinforced presentations [7, 8], which is eﬀective on excitatory stimuli such as A. Thus the extinction of an inhibitor and an excitor of opposite valence are generally not accomplished using the same procedure.  It is very easy to create confusion between an inhibitor and an excitor of op- posite valence in standard VFAs. We will show that when a VFA is trained on the same data as in the conditioned inhibition paradigm (i.e., [1 0](cid:59)1, [1 1](cid:59)0), it will subsequently predict a negative reinforcement or punishment when pre- sented with X alone (i.e., [0 1](cid:59)-1) instead of a zero-valued outcome, as the conditioned inhibition paradigm suggests. Indeed, it is possible to have a com-  2  bination of rewarding and punishing outcomes, which might seem to similarly balance out (i.e., [1 1](cid:59)0, where one stimulus is a predictor reward and one is a predictor of punishment). However, this double outcome is qualitatively very diﬀerent from the conditioned inhibition case, which captures the omission of an expected outcome. It would be best if there was a way to distinguish between these cases, instead of always assuming one or the other. Besides imposing an increase in prediction error, such an assumption could be read incorrectly by an agent and lead to irrational actions (e.g., fear-related responses instead of disappointment-related responses).  Here, we demonstrate this error in concrete terms and oﬀer a simple, generic solution. Instead of predicting the standard reward value, we train two function approximators: one to predict future punishment and one to predict future reward. Each prediction is also rectiﬁed (i.e., negative values are set to zero). The typical reward value is ﬁnally computed by subtracting the punishment prediction from the reward prediction. Essentially, instead of allowing positive and negative rewards to cancel each other out and go unnoticed, they are both acknowledged and used to train the model, although the ﬁnal reward signal used to make decisions does combine them in the usual way. This allows the model to make the distinction between the omission of one outcome and the prediction of the oppositely valenced outcome.  Like the standard VFA, separating the prediction of positive and negative reinforcements recognizes the importance of developing high-level representa- tions of the world as an input to reinforcement learning, though doing so is not the focus of the present work. Instead, we show that the choice of what a learner predicts, or its output representation, is equally important, and is reﬂected in the resulting eﬀect on prediction accuracy.  2. Predicting rewarding and punishing outcomes separately  Figure 1A depicts the typical VFA used in RL. The input features which ex- press the environmental state are submitted to the approximator and it predicts the total expected future reward value from that state. It is updated according to one of a variety of RL paradigms (e.g., Sarsa, Q-learning, TD(λ), etc.) and the speciﬁcs of the approximator (e.g., gradient descent for a linear approxima- tor). In contrast, Figure 1B depicts the deeper, two-staged VFA architecture that this work advocates: two supervised learning models, one that predicts fu- ture reward and one that predicts future punishment. A key diﬀerence between these two models’ prediction targets and the reward value target of the VFA is that the former has positive values only (a reward or punishment is either expected or not) whereas the latter can have both positive and negative values. This rectiﬁcation is a crucial detail of the proposed approach.  This breaking-down of the prediction into sub-predictions is not new to RL [9, 10, 11, 12, 13, 14, 15]. Although more computation is required in such an approach than for predicting reward alone, the richer predictive output has potential uses, including being able to foresee future states and allow the agent to act in anticipation of them. In Schmidhuber’s work [12], a critic (i.e., a  3  VFA) is presented that learns to predict multiple reward/punishment types and the associated prediction errors are backpropagated through a controller module to improve action selection. Applied to a pole-balancing task, the critic’s predictive targets reﬂect diﬀerent types of pain, or ways in which the task can fail. Schmidhuber found that using these multiple predictive targets allowed the controller to balance the pole with substantially fewer training episodes. In Nguyen and Widrow [9], a model is trained to predict the next state values with the ultimate goal of informing a steering controller that will minimize the ﬁnal error (diﬀerence between the ideal ﬁnal position and orientation of the truck and its location at the end of a series of backup cycles). In Schmidhuber and Huber [13], the state is represented by pixel intensity sums organized to coarsely emulate a fovea. Here, a model is trained to predict the next state, that is, the pixel sums after a simulated eye movement. Also, in this paper, the model is used to inform a controller and help direct movement eﬀectively. Neural networks such as those used in these papers to predict the future state could also be used in our proposed approach to predict rewards and punishments separately, albeit with some minor changes. In particular, the pain-predicting critic [12] learns to predict reward-related outcomes, as does our architecture, except that instead of predicting many forms of one kind of valence, we subdivide the problem into the prediction of rewards and the prediction of punishments. This will allow us to prevent confusion between excitors and inhibitors of opposite valence.  3. Function approximators  In the evaluations that follow, we consider several forms of learning models to show generally the eﬀect of the standard VFA and two-stage VFA architectures in speciﬁc scenarios. The VFA models evaluated are based on the following four supervised learning approaches: least mean squares (LMS), support-vector re- gression (SVR) [16], classiﬁcation and regression trees (CART), and multi-layer perceptron. All of these, except LMS, are capable of representing non-linear functions. With the simple task evaluated, these models required very little adjusting of parameters, which might otherwise complicate the interpretation of the results. For the two approximators in the two-stage architecture, we em- ployed a rectiﬁed version of LMS (rLMS, derived below), LMS combined with a logistic/sigmoid transfer function, and Dual Pathway Regression (DPR) [17]. All three models, in one way or another, rectify their outputs.  3.1. Standard VFA function approximators  The form of SVR used here, called epsilon-SVR, involves ﬁtting a hyperplane with margins (i.e., a hyper-rectangular prism) so that it encompasses all data while at the same time minimizing the hyperplane’s slope [16]. This is often infeasible, so that outliers are permitted but at a cost, leading to a trade- oﬀ between minimization of the slope and the acceptance of outliers. Since explanations of simulation results will not rely on the formal deﬁnition of this non-linear model, we direct the interested reader to Smola and Sch¨olkopf [16].  4  Figure 1: VFA architectures for RL using function approximators (FA). Both architectures receive a feature vector input representing the state (S1...Sn), predict future reward value (V(s)), and learn from reinforcements (R(s)). (a) The typical VFA takes features of the current state and directly predicts the total expected future reward for the agent. (b) The proposed two-stage VFA architecture: the ﬁrst stage contains two predictors, one for the expected future rewards and one for the expected future punishments, which are rectiﬁed to output positive values only. In the second stage, the ﬁnal expected reward value is calculated as the expected future reward minus the expected future punishment.  We used LIBSVM [18] in our simulations (Settings: cost = 10, epsilon = 1e− 5, radial basis function kernel with γ = 1/N o. F eatures). Although SVR will serve as the primary and most eﬀective among the VFAs, we also tested LMS, CART [19] (MATLAB Statistics toolbox) and MLP [20] (MATLAB Neural Networks toolbox) to show the generality of our results.  3.2. Two-stage VFA architecture function approximators  We will now derive the rLMS regression model used in the two-stage archi- tecture by beginning with LMS itself. Underlying LMS linear regression is the goal to maximize the likelihood of the given data being generated by a Gaussian random variable with a parameterized n-dimensional linear mean. This function can be expressed as  y = φT x + N (0, σ)  (1)  5  (2)  (3)  where y is the outcome being predicted, x is a vector of inputs, φ is the vector of model parameters, and 0 and σ represent the mean and standard deviation of the Gaussian random variable, respectively. The probability density function (PDF) becomes  p(y, x|φ) =  1√ 2πσ  e  − (y−φT x)2  2σ2  For the two-stage architecture, the reinforcer outcome being predicted cannot be less than zero,  y = G(φT x + N (0, σ))  where G() is the threshold-linear function [21] which returns its argument when- ever it is positive and returns zero otherwise. The probability function now becomes  p(y, x|φ) =  e  − (y−φT x)2 1√ 2 (1 − erf( φT x√  2πσ  2σ2  1  2σ  ),  0,  ,  f or y > 0  f or y = 0  f or y < 0  (4)    diﬀering from the PDF in Equation 2 by zeroing the probability of negative y values and instead heaping it onto the y = 0 case. That is, this probability func- tion is a density over y > 0 plus a point mass at y = 0, which is equal to 1 minus the probability of y > 0. From this function, let us derive the rLMS learning rule and, indirectly, the LMS learning rule as well. Given data generated from Equation 3, we can infer the values of φ using maximum likelihood estimation. The probability or likelihood that a certain training data set is generated from the probability function in Equation 4 is  L(φ) = p(y(1), ...y(m), x(1), ...x(m)|φ) p(y(i), x(i)|φ)  m(cid:89)  =  i=1  (5)  where m is the number of training data points. We can maximize this convex likelihood function by taking its log and ascending its gradient,  log L(φ) = log(cid:81)m  i=1 p(y(i), x(i)|φ) =(cid:80)m (cid:80) i=1 log p(y(i), x(i)|φ) i,y(i)>0(y(i) − φT x(i))2  2πσ) − 1  2σ2  √ = −my>0 log(  −my=0 log(2) +(cid:80)  i,y(i)=0 log(1 − erf( φT x(i)√  2σ  ))  (6)  where my>0 and my=0 are the number of data points when y > 0 and y = 0, respectively. Taking the gradient of this function with respect to each φj gives  ∂ log L(φ)  ∂φj  =  1 σ2  (y(i) − φT x(i))x(i)  j −  (cid:88)  i,y(i)>0  6  (cid:114) 2  (cid:88)  πσ2  i,y(i)=0  e  − (φT x(i))2 1 − erf( φT x(i)√  2σ2  2σ  x(i) j  )  (7)  M(cid:88)  i=1  The gradient can be ascended by iteratively updating the model parameters,  φj =: φj + α  ∂ log L(φ)  ∂φj  (8)  where the learning rate, α = σ2 for standard LMS (Equation 2), resulting in  n+2 . The learning rule can be similarly derived  ∂ log L(φ)  ∂φj  =  1 σ2  (y(i) − φT x(i))x(i)  j  (9)  where M is the total number of data points. The practical feature of the rectiﬁed learning rule that distinguishes it is what happens for data points where y = 0: when the weighted sum φT x is negative, learning is essentially deactivated, especially for small σ. For our simulations, we use a low noise variance of σ = 1e − 4.  The other two function approximators used in the two-stage architecture, DPR and LMS with a sigmoid transfer function, can be similarly derived. All that is diﬀerent are the PDFs that the models embody. In the case of LMS with a sigmoid transfer function,  p(y, x|φ) =  1√ 2πσ  e  − (y−  1  1+e−φT x  ))2  2σ2  and for DPR [17],  p(y, x|φ) =  1√ 2πσ  −  e  (y−2|φT  +|x(1−  1  −|φT−|x  1+e  2σ2  (10)  (11)  ))2  + and φT− are separate sets of parameters to be inferred, representing where φT the positive and negative pathways of the dual pathway approach, respectively.  4. Simulation and results  To show the diﬀerence between the two architectures, a single-step RL prob- lem is simulated. It is the combination of two simultaneous conditioned inhi- bition experiments, where there are both rewarding and punishing excitatory stimuli/reinforcements as well as inhibitory stimuli. The experiment is arranged to show how well these architectures distinguish between inhibitory features and excitatory features of the opposite valence. As a RL problem, there are no ac- tions to take; an agent is forced from one state to the next. As an aside, however, one can imagine an agent’s actions also being represented as input features, for which the process of learning to predict outcomes based on proposed actions would then be the same. The input data is comprised of 50 samples with four features apiece: two excitors, one for reward (E+) and one for punishment (E−) and two inhibitors,  7  one canceling reward (I +) and one canceling punishment (I−) which are each given real-values between 0 and 1. Each data point can be classiﬁed as either “linear” where the inhibitory feature saliences do not exceed their associated excitatory feature saliences (E+ ≥ I + and E− ≥ I−) or “non-linear”, otherwise. The expected future reward (or reward value) output is computed as R(s) = G(E+ − I +) − G(E− − I−), where G() function again is a rectiﬁer, so that an inhibitory feature alone cannot have a negative value.  The learning cycle for the standard VFA in this experiment is the following: • the 4-feature state vector is presented as input. • the reward value prediction is made • in the next time step, the actual reward value outcome (R(s)) is received  and used to update the model  The learning cycle for the two-stage VFA architecture in this experiment is  the following:  • the 4-feature state vector is presented as input. • the prediction of the positive reinforcement is made by one function ap- proximator and the prediction of the negative reinforcement is made by a second approximator  • the reward value prediction is computed as the positive reinforcer predic-  tion minus the negative reinforcer prediction  • in the next time step, the absence/presence of each reinforcer is detected  and used to update the associated function approximator.  Figure 2 shows the mean prediction error as the percentage of non-linear training samples increases among the 50 data points used, showing results for where the test data is either completely “linear” (lower panel) or completely “non-linear” (upper panel). The simulation was run 20 times for each 5% in- crement. Standard errors are left out for clarity’s sake. With low amounts of non-linear training data, non-linear test results suﬀer signiﬁcantly for all of the VFAs (blue), whereas the two-stage architecture approaches (red) fare much better, especially rLMS, which is directly modeled to solve this problem. The two-stage architecture function approximators other than rLMS do not perform as well when the test data is “linear” because the logistic and DPR functions have inherent non-linearities that prevent them from reaching zero prediction error. The CART result is better than the other VFAs when non-linear training data is slight. Its constant level of eﬀectiveness may be due to the fact that its ability to make sharply deﬁned rules allows it to roughly align with the contours of the present world model. Nevertheless, it does not achieve as good results as do the two-stage architecture models.  Figure 3 provides a more detailed picture of where the VFA errors arise. In the table to the right, there are 16 canonical data points representing all general  8  cases in which data fall, which are tested after training with diﬀerent levels of “non-linear” data, as before. The largest prediction errors occur in SVR for non-linear data points (i.e., where inhibitors play a role, long-dashed results) when there are few non-linear training data points. The next highest prediction errors occur for data points that are linear (solid lines) but contain inhibitors as well (e.g., “O”, which contains a positive inhibitor to cancel the prediction of a reward). Finally, linear data in the absence of inhibitors fare best. As expected, VFAs see the inhibitor as an excitor of opposite valence rather than as a stimulus that cancels an expected outcome. Take canonical data point ‘B’, for example. It has a single inhibitor activated (indicating the absence of a punishment) and no excitors, which ought to predict zero value. However, with low levels of non-linear data, it is seen to have substantial positive reward value. So, when trained with linear data, even a non-linear VFA will see the inhibitor as a linear excitatory feature of the opposite valence in order to “cancel out” its truly excitatory companion.  5. Discussion  As the results show, the presence of inhibitors, without much non-linear training data, causes confusion in the standard VFA. Even with half the train- ing data being non-linear, prediction errors are still generally above those when using a two-stage architecture, which maintain relatively low prediction errors regardless of the amount of non-linear data. With little non-linear data, the VFA sees the inhibitor in a linear sense, as an excitatory stimulus with opposite valence. Why does the two-stage architecture work? Recall that each function approximator predicts the presence/absence of a single reinforcer and that the prediction cannot be negative (e.g., less than zero food) because the output is rectiﬁed. Without this rectiﬁcation (i.e, using standard LMS), the two-stage architecture would give similarly poor results. In short, inhibitors are not per- mitted to make negative predictions (via rectiﬁcation), but only cancel their associated prediction. With enough training data or experience, we see that the standard VFAs are capable of learning the relationship between excitors and inhibitors. The two-stage VFA architecture, however, essentially encodes the potential of such relationships and avoids the need to explicitly learn them. This results in correct perception of the reinforcement landscape sooner, which may lead to better early decisions.  It may be that certain reinforcement learning agents or robots will rarely meet with inhibitors and thus not be given opportunity to experience the con- fusion demonstrated. Yet, it still may be appropriate to include a two-stage VFA architecture, since doing so does little to disturb the general case. When using linear training data only, prediction errors were still low for the two-stage architecture, particularly for the use of rLMS. The two-stage VFA architecture, then, insures the agent against confusion in the presence of inhibitors.  One feature of the function approximators used with the two-stage archi- tecture is that they essentially shut oﬀ learning when presented with negative  9  Figure 2: Testing standard VFAs and the two-stage VFA architecture in the dual-valenced conditioned inhibition simulation. In the upper panel, we see the two-stage results (red) perform well with low numbers of non-linear training data when the test data is non-linear relative to the VFA results (blue). The sigmoid and DPR functions do not perform as well as rLMS, however, particularly when the test data is linear (lower panel) because these models have inherent non-linearities that aﬀect them in this scenario.  predictors (i.e., inhibitors) alone and thereby not change their inhibitory qual- ities. This resonates with the animal learning literature (but see [22]), which generally ﬁnds that non-reinforced presentations of such inhibitors do not extin- guish them (i.e., they keep their inhibitory quality). This is in contrast to the presentation of excitors alone, which cause them to lose their excitatory quality and to become “neutral”, something that the two-stage function approximators also emulate.  It is possible that the ﬁrst stage of the two-stage architecture could contain multiple function approximators to predict as many types of rewards or pun- ishments as desired, much like Schmidhuber’s [12] predictor of varying forms of pain. One reason for grouping multiple forms of pain together as one, however, is that it appears that this is what biological systems may do. Bakal et al.  10  Figure 3: A breakdown of SVR (i.e., standard VFA) prediction error by canonical data points shown in the table next to the graph. SVR has trouble properly predicting the state-value of “non-linear” data points when there are few such points in the training data. Even “linear” data points that include inhibitors have higher prediction errors than those that do not. Contrast this with the two-stage VFA architecture employing rLMS, where there is essentially zero error for every canonical data point, for every percentage of non-linear training data.  [23] found in a classical conditioning blocking experiment that subjects did not associate predictors with particular punishments but rather a common negative outcome.  Apart from the rectiﬁcation, the function approximators we used to demon- strate the two-stage VFA architecture learn primarily linear relationships. How- ever, this is not to say that one could not use in their place models that learn non-linear relationships. In fact, it will be necessary to have non-linear predic- tive abilities for speciﬁc combinations of features (e.g., XOR or AND cases). Importantly, non-linear function approximators that are used in the two-stage architecture must also rectify their output to enjoy its beneﬁts. However, such rectiﬁcation is non-native to most function approximators and would have to be carefully incorporated.  The input representation that would best support the two-stage architecture is likely a hierarchical one. The best deep learning representations would seem to be those with varying-levels of feature complexity, with speciﬁc features that correlate with either rewards or punishments meaningful to the agent. For the thirsty agent, a low-level bank of edge-detectors would seem suﬃcient to identify a pool or puddle with numerous ripples. In the case of a calm pool, however, a very high-level representation that captures the mirror-like eﬀect of the water  11  would be more discriminative. A hierarchical representation aﬀords such varying levels of complexity and thus would seem to eﬀectively support the prediction of future reinforcements or omission thereof.  6. Conclusion  We have demonstrated how a two-stage VFA architecture can overcome a fundamental issue faced by standard VFAs: the inability to distinguish between predictors of reinforcement and inhibitors of reinforcement of the opposite va- lence with little training data. The prediction of reward value is replaced by the prediction of rewards as a group and punishments as a group, which themselves have a positive salience only. Such a rectiﬁed output representation allows an inhibitor (or predictor of omission) to have a negative inﬂuence when presented together with an excitor of the same valence, but no inﬂuence when it is pre- sented alone. This eliminates its confusion with predictors of reinforcement with the opposite valence. When used with the right function approximator, the two- stage VFA architecture ensures the reinforcement learning agent appropriately handles inhibitors without increasing prediction errors much in the general case (i.e., no inhibitors). This suggests that RL agents may stand to beneﬁt from incorporating the two-stage VFA architecture even if this situation is unlikely, just in case they run into such a situation.  Acknowledgments  This work was supported by funding from NSERC and Dalhousie University.  References  References  [1] R. S. Sutton, A. G. Barto, Reinforcement learning: An introduction, Cam-  bridge Univ Press, 1998.  [2] R. S. Sutton, Generalization in reinforcement learning: Successful examples using sparse coarse coding, Advances in Neural Information Processing Systems (1996) 1038–1044.  [3] S. Mahadevan, M. Maggioni, Value function approximation with diﬀusion wavelets and Laplacian eigenfunctions, in: Advances in Neural Information Processing systems, 2005, pp. 843–850.  [4] G. Konidaris, S. Osentoski, P. S. Thomas, Value function approximation  in reinforcement learning using the Fourier basis., in: AAAI, 2011.  [5] I. P. Pavlov, Conditioned reﬂexes: an investigation of the physiological activity of the cerebral cortex, Oxford University Press, Oxford, England, 1927.  12  [6] R. A. Rescorla, Pavlovian conditioned inhibition., Psychological Bulletin  72 (1969) 77–94.  [7] C. L. Zimmer-Hart, R. A. Rescorla, Extinction of Pavlovian conditioned inhibition., Journal of Comparative and Physiological Psychology 86 (1974) 837–845.  [8] D. T. Lysle, H. Fowler, Inhibition as a “slave” process: deactivation of conditioned inhibition through extinction of conditioned excitation, Journal of Experimental Psychology. Animal Behavior Processes 11 (1) (1985) 71– 94.  [9] D. Nguyen, B. Widrow, Truck backer-upper: an example of self-learning in neural networks, in: Orlando’90, 16-20 April, International Society for Optics and Photonics, 1990, pp. 596–602.  [10] R. S. Sutton, First results with Dyna, an integrated architecture for learn-  ing, planning and reacting, Neural Networks for Control (1990) 179–189.  [11] J. Schmidhuber, An on-line algorithm for dynamic reinforcement learning and planning in reactive environments, in: Neural Networks, 1990., 1990 IJCNN International Joint Conference on, IEEE, 1990, pp. 253–258.  [12] J. Schmidhuber, Networks adjusting networks,  in: Proceedings of Dis-  tributed Adaptive Neural Information Processing, St. Augustin, 1990.  [13] J. Schmidhuber, R. Huber, Learning to generate artiﬁcial fovea trajectories for target detection, International Journal of Neural Systems 2 (01n02) (1991) 125–134.  [14] R. S. Sutton, B. Tanner, Temporal-diﬀerence networks, in: Advances in  Neural Information Processing Systems, 2004, pp. 1377–1384.  [15] E. J. Rafols, M. B. Ring, R. S. Sutton, B. Tanner, Using predictive repre- sentations to improve generalization in reinforcement learning., in: IJCAI, 2005, pp. 835–840.  [16] A. J. Smola, B. Sch¨olkopf, A tutorial on support vector regression, Statistics  and Computing 14 (3) (2004) 199–222.  [17] P. Connor, T. Trappenberg, Biologically plausible feature selection through relative correlation, in: Proceedings of the 2013 International Joint Con- ference on Neural Networks (IJCNN), 2013, pp. 759–766.  [18] C.-C. Chang, C.-J. Lin, LIBSVM: A library for support vector machines, ACM Transactions on Intelligent Systems and Technology 2 (2011) 27:1– 27:27.  [19] L. Breiman, J. H. Friedman, R. A. Olshen, C. J. Stone, Classiﬁcation and Regression Trees, Statistics/Probability Series, Wadsworth Publishing Company, Belmont, California, U.S.A., 1984.  13  [20] D. E. Rumelhart, G. E. Hinton, R. J. Williams, Learning Internal Repre-  sentations by Error Propagation, MIT Press, 1986.  [21] V. Nair, G. E. Hinton, Rectiﬁed linear units improve restricted Boltzmann machines, in: Proceedings of the 27th International Conference on Machine Learning (ICML-10), 2010, pp. 807–814.  [22] I. Baetu, A. G. Baker, Extinction and blocking of conditioned inhibition in  human causal learning, Learning & Behavior 38 (2010) 394–407.  [23] C. W. Bakal, R. D. Johnson, R. A. Rescorla, The eﬀect of change in US quality on the blocking eﬀect, The Pavlovian journal of biological science: oﬃcial journal of the Pavlovian 9 (2) (1974) 97–103.  14  ","In reinforcement learning, the goal is to seek rewards and avoid punishments.A simple scalar captures the value of a state or of taking an action, whereexpected future rewards increase and punishments decrease this quantity.Naturally an agent should learn to predict this quantity to take beneficialactions, and many value function approximators exist for this purpose. In thepresent work, however, we show how value function approximators can causeconfusion between predictors of an outcome of one valence (e.g., a signal ofreward) and the inhibitor of the opposite valence (e.g., a signal cancelingexpectation of punishment). We show this to be a problem for both linear andnon-linear value function approximators, especially when the amount of data (orexperience) is limited. We propose and evaluate a simple resolution: to insteadpredict reward and punishment values separately, and rectify and add them toget the value needed for decision making. We evaluate several functionapproximators in this slightly different value function approximationarchitecture and show that this approach is able to circumvent the confusionand thereby achieve lower value-prediction errors."
1312.5479,2014,Sparse similarity-preserving hashing  ,"['Alex M. Bronstein', 'Guillermo Sapiro', 'Pablo Sprechmann', 'Jonathan Masci', 'Michael M. Bronstein']",https://arxiv.org/pdf/1312.5479.pdf,"Sparse similarity-preserving hashing  4 1 0 2     b e F 6 1         ]  V C . s c [      3 v 9 7 4 5  .  2 1 3 1 : v i X r a  Jonathan Masci Alex M. Bronstein Michael M. Bronstein Pablo Sprechmann Guillermo Sapiro  Abstract  In recent years, a lot of attention has been de- voted to eﬃcient nearest neighbor search by means of similarity-preserving hashing. One of the plights of existing hashing techniques is the intrinsic trade-oﬀ between performance and computational complexity: while longer hash codes allow for lower false positive rates, it is very diﬃcult to increase the embedding dimensionality without incurring in very high false negatives rates or prohibiting compu- tational costs. In this paper, we propose a way to overcome this limitation by enforcing the hash codes to be sparse. Sparse high- dimensional codes enjoy from the low false positive rates typical of long hashes, while keeping the false negative rates similar to those of a shorter dense hashing scheme with equal number of degrees of freedom. We use a tailored feed-forward neural network for the hashing function. Extensive experi- mental evaluation involving visual and multi- modal data shows the beneﬁts of the pro- posed method.  1. Introduction  Eﬃcient computation of similarity between entries in large-scale databases has attracted increasing in- terest, given the explosive growth of data that has to be collected, processed, stored, and searched for. This problem arises naturally in applications such as image-based retrieval, ranking, classiﬁcation, detec- tion, tracking, and registration. In all these problems, given a query object (usually represented as a feature vector), one has to determine the closest entries (near- est neighbors) in a large (or huge) database. Since  Work partially supported by NSF, NGA, ARO, NSSEFF, and ONR. A. M. and M. M. Bronstein are supported by the ERC Starting Grants nos. 335491 and 307047.  jonathan@idsia.ch bron@eng.tau.ac.il michael.bronstein@usi.ch pablo.sprechmann@duke.edu guillermo.sapiro@duke.edu  the notion of similarity of (for example) visual objects is rather elusive and cannot be measured explicitly, one often resorts to machine learning techniques that allow constructing similarity from examples of data. Such methods are generally referred to as similarity or metric learning.  Traditionally, similarity learning methods can be di- vided into unsupervised and supervised, with the for- mer relying on the data only without using any side information. PCA-type methods (Schoelkopf et al., 1997) use global structure of the data, while manifold learning techniques such as locally linear embedding (Roweis & Saul, 2000), eigenmaps (Belkin & Niyogi, 2003), and diﬀusion maps (Coifman & Lafon, 2006) consider data as a low-dimensional manifold and use its local intrinsic structure to represent similarity. Su- pervised methods assume that additional information, such as class labels (Johnson & Wichern, 2002; Mika et al., 1999; Weinberger & Saul, 2009; Xing et al., 2002): distances, similar and dissimilar pairs (Davis et al., 2007), or order relations (McFee & Lanckriet, 2009; Shen et al., 2009), is provided together with the data examples. Many similarity learning methods use some representation of the distance, e.g., in the form of a parametric embedding from the original data space to some target space. In the simplest case, such an embedding is a linear projection acting as dimension- ality reduction, and the metric of the target space is Euclidean or Mahalanobis distance (Shen et al., 2009; Weinberger & Saul, 2009).  More recently, motivated by the need for eﬃcient tech- niques for big data, there has been an increased in- terest in similarity learning methods based on em- bedding the data in spaces of binary codes with the Hamming metric (Gong et al., 2012; Gong & Lazeb- nik, 2011; Kulis & Darrell, 2009; Liu et al., 2012; Norouzi et al., 2012; Norouzi & Fleet, 2011; Wang et al., 2010). Such an embedding can be considered as a hashing function acting on the data trying to pre- serve some underlying similarity. Notable examples  Sparse similarity-preserving hashing  of the unsupervised setting of this problem include locality sensitive hashing (LSH) (Gionis et al., 1999) and spectral hashing (Weiss et al., 2008; Liu et al., 2011), which try to approximate some trusted stan- dard similarity such as the Jaccard index or the cosine distance. Similarly, Yagnik et al. (2011) proposed computing ordinal embeddings based on partial order statistics such that Hamming distance in the resulting space closely correlates with rank similarity measures. Unsupervised methods cannot be used to learn seman- tic similarities given by example data. Shakhnarovich et al. (2003) proposed to construct optimal LSH-like similarity-sensitive hashes (SSH) for data with given binary similarity function using boosting, considering each dimension of the hashing function as a weak clas- siﬁer. In the same setting, a simple method based on eigendecomposition of covariance matrices of positive and negative samples was proposed by (Strecha et al., 2012). Masci et al. (2011) posed the problem as a neural network learning. Hashing methods have been used successfully in various vision applications such large-scale retrieval (Torralba et al., 2008b), feature descriptor learning (Strecha et al., 2012; Masci et al., 2011), image matching (Korman & Avidan, 2011) and alignment (Bronstein et al., 2010).  The appealing property of such similarity-preserving hashing methods is the compactness of the represen- tation and the low complexity involved in distance computation: ﬁnding similar objects is done through determining hash collisions (i.e. looking for nearest neighbors in Hamming metric balls of radius zero), with complexity practically constant in the database size. In practice, however, most methods consider nearest neighbors lying at larger than zero radii, which then cannot be done as eﬃciently. The reason behind this is the diﬃculty of simple hash functions (typically low-dimensional linear projections) to achieve simul- taneously high precision and recall by only requiring hash collisions.  Main contributions. In this paper, we propose to introduce structure into the binary representation at the expense of its length, an idea that has been shown spectacularly powerful and led to numerous applica- tions of sparse redundant representation and com- pressed sensing techniques. We introduce a sparse similarity-preserving hashing technique, SparseHash, and show a substantial evidence of its superior recall at precision comparable to that of state-of-the-art meth- ods, on top of its intrinsic computational beneﬁts. To the best of our knowledge, this is the ﬁrst time sparse structure are employed in similarity-preserving hash- ing. We also show that the proposed sparse hashing technique can be thought of as a feed-forward neural  network, whose architecture is motivated by the iter- ative shrinkage algorithms used for sparse representa- tion pursuit (Daubechies et al., 2004). The network is trained using stochastic gradient, scalable to very large training sets. Finally, we present an extension of SparseHash to multimodal data, allowing its use in multi-modal and cross-modality retrieval tasks.  2. Background Let X ⊆ Rn be the data (or feature) space with a bi- nary similarity function sX : X × X → {−1, +1}. In some cases, the similarity function can be obtained by thresholding some trusted metric on X such as the (cid:96)2 metric; in other cases, the data form a (typically, la- tent) low-dimensional manifold, whose geodesic met- ric is more meaningful than that of the embedding Euclidean space. In yet other cases, sX represent a se- mantic rather than geometric notion of similarity, and may thus violate metric properties. It is customary to partition X × X into similar pairs of points (positives) P = {(x, x(cid:48)) : sX (x, x(cid:48)) = +1}, and dissimilar pairs of points (negatives) N = {(x, x(cid:48)) : sX (x, x(cid:48)) = −1}. Similarity-preserving hashing is the problem of rep- resenting the data from the space X in the space Hm = {±1}m of m-dimensional binary vectors with the Hamming metric dHm (a, b) = m i=1 aibi by means of an embedding ξ : X → Hm that preserves the original similarity relation, in the sense that there exist two radii, r < R such that with high probability dHm|P ≤ r and dHm|N ≥ R. In practice, the similarity sX is frequently unknown and hard to model, however, it is possible to sample it on some subset of the data. In this setting, the problem of similarity-preserving hashing boils down to ﬁnding an embedding ξ minimizing the aggregate of false positive and false negative rates,  (cid:80)m  2 − 1  2  E{dHm ◦ (ξ × ξ)|P} − E{dHm ◦ (ξ × ξ)|N}.  (1)  min  ξ  Problem (1) is highly non-linear and non-convex. We list below several methods for its optimization.  hashing  (SSH). Similarity-sensitive Shakhnarovich et al. (2003) studied a particular setting of problem (1) with embedding of the form ξ(x) = sign(Px + a), where P is an m × n projection matrix and a is an m × 1 bias vector, and proposed the SSH algorithm constructing the dimensions of ξ one-by-one using boosting. The expectations in (1) are weighted, where stronger weights are given to misclassiﬁed pairs from previous iteration.  Diﬀ-hash (DH). Strecha et al. (2012) linearized the  Sparse similarity-preserving hashing  embedding ξ(x) = sign(Px + a) to ˆξ(x) = Px + a, observing that in this case (1) can be written as  (Goemans & Williamson, 1995). Furthermore, increas- ing m slows down the retrieval.  tr(P(Σ+ − Σ−)PT),  min  PTP=I  (2)  where Σ+, Σ− are the covariance matrices of the dif- ferences of positive and negative samples, respectively. Solving (2) w.r.t. to the projection matrix P amounts to ﬁnding the smallest eigenvectors of the covariance diﬀerence matrix Σ+ − Σ−. The vector a is found separately, independently for each dimension.  Neural network hashing (NN-hash). Masci et al. (2011) realized the function ξ(x) = sign(Px + a) as a single-layer neural network with tanh(βx) ≈ sign(x) activation function, where the coeﬃcients pij, ai act as the layer weights and bias, respectively. Coupling two such networks with identical parameters in a so-called siamese architecture (Hadsell et al., 2006; Taylor et al., 2011), one can represent the loss (1) as )(cid:107)2  (cid:107)ξ(x) − ξ(x(cid:48)  L(P, a) =  2  (3)  (cid:88) (cid:88)  (x,x(cid:48))∈P  (x,x(cid:48))∈N  1 2|P| 1 2|N|  +  max{0, M − (cid:107)ξ(x) − ξ(x(cid:48)  )(cid:107)2}2  The second term in (3) is a hinge-loss providing robust- ness to outliers and producing a mapping for which negatives are pulled M -apart.  Finding the network parameters P, a minimizing loss function (3) is done using standard NN learning tech- niques, e.g. the back-propagation algorithm (LeCun, 1985). Compared to SSH and DH, the NN-hash at- tempts to solve the full non-linear problem rather than using often suboptimal solutions of the relaxed lin- earized or separable problem such as (2).  3. Sparse similarity-preserving hashing  The selection of the number of bits m and the rejec- tion Hamming radius r in a similarity-preserving hash has an important inﬂuence on the tradeoﬀ between precision and recall. The increase of m increases the precision, as a higher-dimensional embedding space al- lows representing more complicated decision bound- aries. At the same time, with the increase of m, the relative volume of the ball Br containing the positives decays exponentially fast, a phenomenon known as the curse of dimensionality, resulting in a rapid decrease of the recall. This is a well-documented phenomenon that aﬀects all hashing techniques (Grauman & Fer- gus, 2013). For instance, in the context of LSH, it can be shown that the collision probability between two points decreases exponentially with the code-length  The low recall typical to long codes can be improved by increasing the rejection radius r. However, this comes at the expense of increased query time, since the search complexity directly depends on the rejec- tion radius r. For r = 0 (collision), a look-up table (LUT) is used: the query code is fed into the LUT, containing all entries in the database having the same code. The complexity is O(m), independent of the database size N , but often with a large constant. For small r (partial collision), the search is done as for r = 0 using perturbation of the query: at most r bits of the query are changed, and then it is fed into the LUT. The ﬁnal result is the union of all the retrieved  results. Complexity in this case is O((cid:0) r  (cid:1)). Finally, for  large radii it is often cheaper in practice to use exhaus- tive search with complexity O(N ) (for typical code lengths and database sizes used in vision applications, using r > 3 is slower than brute-force search (Grau- man & Fergus, 2013)). Consequently, practical re- trieval based on similarity-preserving hashing schemes suﬀers from a fundamental limitation of the precision- recall-speed tradeoﬀ: one has to choose between fast retrieval (small r and m, resulting in low recall), high recall (large r, small m, slow retrieval), or high preci- sion (large m, small recall, and slow retrieval).  m  The key idea of this paper is to control the exploding volume of the embedding space by introducing struc- ture into the binary code. While diﬀerent types of structure can be considered in principle, we limit our attention to sparse hash codes. A number of recent studies has demonstrated that sparse over-complete representations have several theoretical and practical advantages when modeling compressible data leading to state-of-the-art results in many applications in com- puter vision and machine learning. We argue, and show experimentally in Section 5, that compared to its “dense” counterpart, an m-bit k-sparse similarity- preserving hash can enjoy from the high precision typ- ical for long hashes, while having higher recall roughly comparable to that of a dense hashing scheme with log2 ber of degrees of freedom of the m-bit sparse hash).  (cid:1) = O(k log2 m) bits (which has the same num- (cid:0)m  k  SparseHash. In order to achieve sparsity, a regu- larization needs to be incorporated into problem (1) so that the obtained embedding ξ will produce codes having only a small number of non-zero elements. In this work we employ an (cid:96)1-norm regularization, ex- tensively used in the compressed sensing literature to promote sparsity. Speciﬁcally, the loss considered in the minimization of the proposed SparseHash frame-  Sparse similarity-preserving hashing  Figure 1. Schematic representation of our ISTA-type network which we use to realize sparse hashing. Two such networks, with the same parametrization, are used.  work is given by the average of L(ξ(x), ξ(x(cid:48)  ), sX ) = sX (x, x(cid:48)  )(cid:107)ξ(x) − ξ(x(cid:48)  )(cid:107)1  (1 − sX (x, x(cid:48)  λ + 2 +α((cid:107)ξ(x)(cid:107)1 + (cid:107)ξ(x(cid:48)  )(cid:107)1),  )) max{0, M − (cid:107)ξ(x) − ξ(x(cid:48)  )(cid:107)1}2 (4)  over the training set, where sX is the groundtruth simi- larity function (1: similar, 0: dissimilar), α is a param- eter controlling the level of sparsity, λ is a weighting parameter governing the false positive and negative rate tradeoﬀ, and M is a margin.  With the new loss function given in (4), solving (1) will produce a sparse embedding that minimizes the aggregate of false positive and false negative rates for a given parametrization. Now the question is what parametrized family of embedding functions would lead to the best sparse similarity-preserving hashing codes? While there is no absolute answer to this ques- tion, recent approaches aimed at ﬁnding fast approx- imations of sparse codes have shed some light on this issue from a practical perspective (Gregor & LeCun, 2010; Sprechmann et al., 2012), and we use this same criterion for our proposed framework.  Gregor and LeCun (2010) proposed tailored feed- forward architectures capable of producing highly accurate approximations of the true sparse codes. These architectures were designed to mimic the itera- tions of successful ﬁrst order optimization algorithms such as the iterative thresholding algorithm (ISTA) (Daubechies et al., 2004). The close relation between the iterative solvers and the network architectures plays a fundamental role in the quality of the approx- imation. This particular design of the encoder archi- tecture was shown to lead to considerably better ap- proximations than other of-the-shelf feed-forward neu- ral networks (Gregor & LeCun, 2010). These ideas can be generalized to many diﬀerent uses of sparse coding, in particular, they can be very eﬀective in discrimi- native scenarios, performing similarly or better than exact algorithms at a small fraction of the computa- tional cost (Sprechmann et al., 2012). These architec-  tures are ﬂexible enough for approximating the sparse hash in (4).1  Implementation. We implement SparseHash by coupling two ISTA-type networks, sharing the same set of parameters as the ones described in (Gregor & LeCun, 2010; Sprechmann et al., 2012), and trained using the loss (4). The architecture of an ISTA-type network (Figure 1) can also be seen as a recurrent network with a soft threshold activation function. A conventional ISTA network designed to obtain sparse representations with ﬁxed complexity has continuous output units. We follow the approach of (Masci et al., 2011) to obtain binary codes by adding a tanh acti- vation function. Such smooth approximation of the binary outputs is also similar to the logistic func- tion used by KSH (Liu et al., 2012). We initialize W with a unit length normalized random subset of the training vectors, S as in the original ISTA algo- rithm considering WT as dictionary and the thresh- olds τ with zeros. The shrinkage activation is deﬁned as σ(x, τ ) = max(0,|x| − τ )sign(x). The application of the learned hash function to a new data sample involves few matrix multiplications and the computation of the element-wise soft threshold- ing, and is on par with the fastest methods available, such as SSH (Shakhnarovich et al., 2003), DH (Strecha et al., 2012), and AGH (Liu et al., 2011).  4. Multi-modal SparseHash  In modern retrieval applications, a single object is of- ten represented by more than one data modality. For example, images are frequently accompanied by tex-  1The hash codes produced by the proposed architec- ture can only be made on average k-sparse by tuning the parameter α. In order to guarantee that the codes con- tain no more than k non-zeros, one can resort to the CoD encoders, derived from the coordinate descent pursuit al- gorithm (Gregor & LeCun, 2010), wherein k is upper- bounded by the number of network layers. We stress that in our application the exact sparsity is not important, since we get the same qualitative behavior.  WxSbinzin0boutzoutbinzinbout······τ+τ=σ(bin)−bout=bin+Szoutτzoutzin()=σ(bin)−bout=bin+Szoutτzoutzin()ξ(x)στ-τtanhISTA NetworkBinarizerSparse similarity-preserving hashing  5. Experimental results  We compare SparseHash to several state-of-the-art supervised and semi-supervised hashing methods: DH (Strecha et al., 2012), SSH (Shakhnarovich et al., 2003), AGH (Liu et al., 2011), KSH (Liu et al., 2012), and NNhash (Masci et al., 2011), using codes pro- vided by the authors. For SparseHash, we use fully online training via stochastic gradient descent with an- nealed learning rate and momentum, ﬁxing the max- imum number of epochs to 250. A single layer ISTA net is used in all experiments. All dense hash methods achieve an average sparsity of about 50% per sample whereas SparseHash achieves much sparser and struc- tured codes; i.e. 6% sparsity on CIFAR10 with hash length of 128. Both sparse and dense codes are well distributed; i.e. small variance of non zero components per code.  ﬁned as mAP = (cid:80)R  Evaluation. We use several criteria to evaluate the performance of the methods: precision and recall (PR) for diﬀerent Hamming radii, and the F1 score (their harmonic average); mean average precision at R, de- n=1 P (n) · rel(n), where rel(n) is the relevance of the nth results (one if relevant and zero otherwise), and P (n) is the precision at n (percentage of relevant results in the ﬁrst n top-ranked matches); and the mean precision (MP), deﬁned as the percent- age of correct matches for a ﬁxed number of retrieved elements. For the PR curves we use the ranking in- duced by the Hamming distance between the query and the database samples. In case of r < m we con- sidered only the results falling into the Hamming ball of radius r.  CIFAR10 (Krizhevsky, 2009) is a standard set of 60K labeled images belonging to 10 diﬀerent classes, sam- pled from the 80M tiny image benchmark (Torralba et al., 2008a). The images are represented using 384- dimensional GIST descriptors. Following (Liu et al., 2012), we used a training set of 200 images for each class; for testing, we used a disjoint query set of 100 images per class and the remaining 59K images as database.  Figure 4 shows examples of nearest neighbors retrieval by SparseHash. Performance of diﬀerent methods is compared in Table 5 and Figures 5–6. In Figure 6 (left), we observe two phenomena: ﬁrst, the recall of dense hash methods drops signiﬁcantly with the in- crease of hash length (as expected from our analysis in Section 3; increasing the hash length is needed for precision performance), while the recall of SparseHash, being dependent on the number of non-zero elements rather than hash length, remains approximately un- changed. Second, SparseHash has signiﬁcantly higher  Figure 2. Five nearest neighbors retrieved by diﬀerent hashing methods for two diﬀerent queries (marked in gray) in the NUS dataset. Correct matches are marked in green. Note that in NUS dataset each image has multiple class la- bels, and our groundtruth positives are deﬁned as at least one class label in common.  tual tags, and video by an audio track. The need to search multi-modal data requires comparing ob- jects incommensurable in their original representation. Similarity-preserving hashing can address this need by mapping the modalities into a common space, thus making them comparable in terms of a single similar- ity function, such as the Hamming metric. For sim- plicity, we will henceforth limit our discussion to two modalities, though the presented ideas can be straight- forwardly generalized to any number of modalities.  We assume the data comes from two distinct data spaces X and Y , equipped with intra-modality simi- larity functions sX and sY , respectively. We further- more assume the existence of an inter-modality sim- ilarity sXY . Typically, examples of similar and dis- similar objects across modalities are more expensive to obtain compared to their intra-modality counter- parts. We construct two embeddings ξ : X → Hm and η : Y → Hm, in such way that the Hamming metric preserves the similarity relations of the modali- ties. We distinguish between a cross-modal similarity- preserving hashing, preserving only the intra-modality similarity (Bronstein et al., 2010), and the full multi- modal setting, also preserving inter-modal similarities.  Our sparse similarity-preserving hashing technique can be generalized to both settings. We construct an in- dependent SparseHash network for each modality, and train them by minimizing an aggregate loss of the form LMM = µ1L(ξ(x), ξ(x(cid:48) ), sY (y, y(cid:48) µ2L(η(y), η(y(cid:48)  ), sX (x, x(cid:48) (5) )) + L(ξ(x), η(y), sXY (x, y)),  )) +  with respect to the parameters of the networks. The parameters µ1 and µ2 control the relative importance of the intra-modality similarity, and are set to zero in the cross-modal regime. We refer to the networks constructed this way as MM-SparseHash.  DHSSHKSHNNhashSparseHashSparse similarity-preserving hashing  Table 1. Performance (in %) of diﬀerent hashing methods on the CIFAR10 dataset with diﬀerent settings of diﬀerent length m. Extended version of this table is shown in supplementary materials.  Hamming radius ≤ 2  Recall  Prec.  F1 –  Hamming radius = 0  Prec.  –  Recall  –  F1 –  L2  KSH  AGH1  AGH2  SSH  DH  NN  Sparse  Method  m 48 64 48 64 48 64 48 64 48 64 48 64 λ 0.1 0.01 0.001 0.1 0.005 0.1 0.001 0.1  m M α  48  64  16 7 11 7  mAP 17.42  31.10 32.49 14.55 14.22 15.34 14.99 15.78 17.18 13.13 13.07 30.18 34.74  23.07 21.08 23.80 21.29  –  –  18.22 10.86 15.95 6.50 17.43 7.63 9.92 1.52 3.0×10 1.0×10 32.69 22.78  −3 −3  32.69 26.03 31.74 21.41  −2 −3 −2 −3 −3 −4 −4 −5  0.86 0.13 2.8×10 4.1×10 7.1×10 7.2×10 6.6×10 3.0×10 1.0×10 1.7×10 1.45 0.28  1.81 17.00 6.87 41.68  −2 −3 −2 −2 −2 −4 −5 −5  −1  0.44 0.26 1.4×10 8.1×10 3.6×10 1.4×10 1.3×10 6.1×10 5.1×10 3.3×10 0.74 5.5×10 0.93 12.56 11.30 28.30  5.39 2.49 4.88 3.06 5.44 3.61 0.30 1.0×10 1.0×10 0.00 9.47 5.70  −3 −3  16.65 26.65 31.12 25.27  −2 5.6×10 −3 9.6×10 −3 2.2×10 −3 1.2×10 −3 3.5×10 −3 1.4×10 −5 5.1×10 −5 1.69×10 −5 1.7×10 0.00 5.2×10 8.8×10 5.0×10 3.04 0.86 10.17  −2 −3  −2  −2 −3 −3 −3 −3 −4 −5 −5  −2  0.11 1.9×10 4.4×10 2.4×10 6.9×10 2.7×10 1.0×10 3.3×10 3.4×10 0.00 0.10 1.8×10 0.10 5.46 1.70 14.50  Figure 3. Tradeoﬀ between recall/precision/query time (in sec) of diﬀerent hashing methods of length m = 48 on the CIFAR10 dataset for rejection radii r = 0 (circle), 1 (trian- gle) and 2 (plus). Retrieval for r = 0, 1 was implemented using LUT; for r = 2 brute-force search was more eﬃcient. Diﬀhash produced very low recall and is not shown.  recall at low r compared to other methods. This is also evinced in Figure 3 where we show the tradeoﬀ between precision, recall and retrieval time for hashes of length 48. We used an eﬃcient implementation of LUT-based and brute force search and took the fastest among the two; with codes of length m = 48 on CI- FAR10 dataset, for radii r = 0 and 1 LUT-based search showed signiﬁcant speedup, while for r ≥ 2 brute force search was faster. In order to further analyze this behavior, we measured the average number of codes which are mapped to the same point for each of the methods. Results are reported in Table 2.  NUS (Chua et al., 2009) is a dataset containing 270K  Figure 4. Ten nearest neighbors retrieved by SparseHash for ﬁve diﬀerent queries (marked in gray) in the CIFAR10 dataset.  Table 2. Total number of unique codes for the entire CI- FAR10 dataset and average number of retrieved results for various Hamming radii search. Hashes of length 48.  Method Unique codes  KSH AGH2 SSH DH NN Sparse  57368 55863 59733 59999 54259 9828  Avg. # of r-neighbors r = 0 r = 2 27.21 3.95 4.62 1.42 1.88 1.01 1.00 1.00 4.83 56.70  r = 1 12.38 2.33 1.12 1.00 20.12  798.47  2034.73  3249.86  annotated images from Flickr. Every images is associ- ated with one or more of the diﬀerent 81 concepts, and is described using a 500-dimensional bag-of-features. In the training and evaluation, we followed the proto- col of (Liu et al., 2011): two images were considered as neighbors if they share at least one common concept (only 21 most frequent concepts are considered). Test- ing was done on a query set of 100 images per concept; training was performed on 100K pairs of images.  10−610−410−210010−310−210−110010−1100101RecallPrecisionTimeSparseHashKSHAGH2SSHNNhashr = 012Sparse similarity-preserving hashing  Performance is shown in Table 6 and Figures 5–6; re- trieved neighbors are shown in Figure 2. We again see behavior consistent with our analysis and SparseHash signiﬁcant outperforms the other methods.  Multi-modal hashing. We repeated the experi- ment on the NUS dataset with the same indices of pos- itive and negative pairs, adding the Tags modality rep- resented as 1K-dimensional bags of words. The train- ing set contained pairs of similar and dissimilar Im- ages, Tags, and cross-modality Tags-Images pairs. We compare our MM-SparseHash to the cross-modal SSH (CM-SSH) method (Bronstein et al., 2010). Results are shown in Table 4 and in Figure 9. MM-SparseHash signiﬁcantly outperforms CM-SSH in previously re- ported state-of-the-art cross-modality (Images-Tags and Tags-Images) retrieval. Both methods outper- form the L2 baseline for intra-modal (Tags-Tags and Images-Images) retrieval; since the two modalities complement each other, we attribute the improvement to the ability of the model to pick up such correlations.  6. Conclusions  We presented a new method for learning sparse similarity-preserving hashing functions. The hashing is obtained by solving an (cid:96)1 regularized minimization of the aggregate of false positive and false negative rates. The embedding function is learned by using ISTA-type neural networks (Gregor & LeCun, 2010). These networks have a particular architecture very ef- fective for learning discriminative sparse codes. We also show that, once the similarity-preserving hashing problem is stated as training a neural network, it can be straightforwardly extended to the multi-modal set- ting. While in this work we only used networks with a single layer, more generic embeddings could be learned with this exact framework simply by considering mul- tiple layers.  A key contribution of this paper is to show that more accurate nearest neighbor retrieval can be obtained by introducing sparsity into the hashing code. Sparse- Hash can achieve signiﬁcantly higher recall at the same levels of precision than dense hashing schemes with similar number of degrees of freedom. At the same time, the sparsity in the hash codes allows retrieving partial collisions at much lower computational com- plexity than their dense counterparts in a Hamming ball with the same radius. Extensive experimental re- sults backup these claims, showing that the proposed SparseHash framework produces comparable, or supe- rior, results to some of the state-of-the-art methods.  References  Belkin, M. and Niyogi, P. Laplacian eigenmaps for di- mensionality reduction and data representation. Neural computation, 15(6):1373–1396, 2003.  Bronstein, M. M. et al. Data fusion through cross-modality In  metric learning using similarity-sensitive hashing. Proc. CVPR, 2010.  Chua, T.-S. et al. NUS-WIDE: A real-world web image database from national university of Singapore. In Proc. CIVR, 2009.  Coifman, R. R. and Lafon, S. Diﬀusion maps. App. Comp.  Harmonic Analysis, 21(1):5–30, 2006.  Daubechies, I., Defrise, M., and De Mol, C. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. Comm. Pure and App. Math., 57 (11):1413–1457, 2004.  Davis et al. Information-theoretic metric learning. In Proc.  ICML, 2007.  Gionis, A., Indyk, P., and Motwani, R. Similarity search in high dimensions via hashing. In Proc. VLDB, 1999.  Goemans, M. and Williamson, D. Improved approximation algorithms for maximum cut and satisﬁability problems using semideﬁnite programming. J. ACM, 42(6):1115– 1145, 1995.  Gong, Y. and Lazebnik, S. Iterative quantization: A pro- In Proc.  crustean approach to learning binary codes. CVPR, 2011.  Gong, Y. et al. Angular quantization-based binary codes  for fast similarity search. In Proc. NIPS, 2012.  Grauman, K. and Fergus, R. Learning binary hash codes In Machine Learning for  for large-scale image search. Computer Vision, pp. 49–87. Springer, 2013.  Gregor, K. and LeCun, Y. Learning fast approximations  of sparse coding. In ICML, 2010.  Hadsell, R., Chopra, S., and LeCun, Y. Dimensionality In Proc.  reduction by learning an invariant mapping. CVPR, 2006.  Johnson, R. A. and Wichern, D. W. Applied multivariate  statistical analysis, volume 4. Prentice Hall, 2002.  Korman, S. and Avidan, S. Coherency sensitive hashing.  In Proc. ICCV, 2011.  Krizhevsky, Alex. Learning multiple layers of features from  tiny images. Technical report, 2009.  Kulis, B. and Darrell, T. Learning to hash with binary  reconstructive embeddings. In Proc. NIPS, 2009.  LeCun, Y. Une proc´edure d’apprentissage pour r´eseau `a seuil asym´etrique. Proceedings of Cognitiva 85, Paris, pp. 599–604, 1985.  Liu, W. et al. Hashing with graphs. In Proc. ICML, 2011.  Sparse similarity-preserving hashing  Figure 5. Precision/recall characteristics of hashing methods using m = 48 bits for diﬀerent Hamming radius r = 0 (solid), 2 (dotted) and m (dashed) on CIFAR10 (left) and NUS (right) datasets. Some settings result in zero recall and the corresponding curves are not shown. While all methods show comparable performance at large r, only SparseHash performs well for small values of r.  Figure 6. Recall as function of Hamming radius r of hash codes of diﬀerent length (left: CIFAR10 dataset, solid: m = 48, dotted: m = 128; right: NUS dataset, solid: m = 64, dotted: m = 256). Note the dramatic drop in recall of dense hash methods when increasing code length m, while our proposed framework maintains performance.  Figure 7. Cross-modality retrieval on NUS dataset. Left plate: ﬁrst ﬁve Image results to a Tags query (shown in gray) obtained using CM-SSH (top) and MM-SparseHash (bottom). Correct matches are marked in green. Note how many if not most of the results from CM-SSH, considered one of the state-of-the-art techniques in this task, are basically uncorrelated to the query. Right plate: union of top ﬁve Tags results (right) to an Image query (shown in gray) obtained using CM-SSH (top) and MM-SparseHash (bottom). Tags matching ground truth tags are shown in green. Note how the proposed approach not only detects signiﬁcantly more matching words, but also the “non-matching” ones actually make a lot of sense, indicating that while they were not in the original image tags, underlying connections were learned. Extended version of this ﬁgure is shown in supplementary materials.  Liu, Wei et al. Supervised hashing with kernels. In Proc.  CVPR, 2012.  Masci, J. et al. Descriptor learning for omnidirectional im- age matching. Technical Report arXiv:1112.6291, 2011.  10−310−210−110010−410−310−210−1100RecallPrecision  CIFAR10NUSSparseHashKSHDiffHashAGH2SSHNNhash10−310−210−110010−410−310−210−1100Recalll  CIFAR10NUSSparseHashKSHDiffHashAGH2SSHNNhashHamming RadiusRecall010203040506010−510−410−310−210−1100Hamming Radius  05101520253035404510−710−610−510−410−310−210−1100sunset, tree, orange, old, abandoned, car, autumn, road, forest, fall, truck, rust, colourful, woods, antique, vehicle, halloween clouds, sunset, sea, beach, sun, ocean, summer, sand, rocks, evening, holiday, peace, happy, dunes CM-SSH:MM-SparseHash:CM-SSHMM-SparseHashpeopleportraitartTable 3. Performance (in %) of hashing methods of diﬀerent length m on the NUS dataset. Extended version of this table is shown in supplementary materials.  Sparse similarity-preserving hashing  L2  KSH  AGH1  AGH2  SSH  DH  NN  Sparse  Method  mAP@10 MP@5K Prec.  68.67  32.77  –  Hamming radius ≤ 2 F1 –  Recall  –  m 64 256 64 256 64 256 64 256 64 256 64 256 λ 0.3 0.05 1.0 0.05 0.005 0.3 1.0 0.05 0.05 1.0 0.005 0.3  m M α  64  256  7 7 16 4 4 6  72.85 73.73 69.48 73.86 68.90 73.00 72.17 73.52 71.33 70.73 76.39 78.31  74.17 74.15 74.51 74.05 74.48 71.73  42.74 45.35 47.28 46.68 47.27 47.65 44.79 47.13 41.69 39.02 59.76 61.21  56.08 51.52 55.54 60.73 59.42 54.76  83.80 84.24 69.43 75.90 68.73 74.90 60.06 84.18 84.26 84.24 75.51 83.46  71.67 69.08 79.09 78.82 81.95 78.34  −3 −3  −2  −2  −3 −3 −3  −2  6.1×10 1.4×10 0.11 1.5×10 0.14 5.3×10 0.12 1.8×10 1.4×10 1.4×10 1.59 5.8×10 1.99 0.53 1.21 3.85 1.18 6.10  −2 −3  −2  −3 −3 −3  1.2×10 2.9×10 0.22 2.9×10 0.28 0.11 0.24 3.5×10 2.9×10 2.9×10 3.11 0.11  3.98 1.06 2.42 7.34 2.33 11.30  Prec.  Hamming radius = 0 F1 –  Recall  –  –  84.21 84.24 73.35 81.64 72.82 80.45 81.73 84.24 84.24 84.24 81.24 83.94  81.11 81.67 82.76 81.82 83.24 80.85  −3 −3 −2 −3 −2 −2 −2 −3 −3 −3  −3  1.7×10 1.4×10 3.9×10 3.6×10 5.2×10 1.1×10 1.1×10 1.5×10 1.4×10 1.4×10 0.10 4.9×10 0.46 0.15 0.17 1.20 0.35 1.02  −3 −3 −2 −3  −2 −2 −3 −3 −3  −3  3.3×10 2.9×10 7.9×10 7.1×10 0.10 2.2×10 2.2×10 2.9×10 2.9×10 2.9×10 0.20 9.8×10 0.92 0.30 0.34 2.37 0.70 2.01  Table 4. Performance (in %) of CM-SSH and MM-SparseHash on the NUS multi-modal dataset with hashes of length 64.  mAp10 %  MP@5K %  Method Image-Image Tag-Tag Image-Tag Tag-Image  Image-Image Tag-Tag Image-Tag Tag-Image  L2 CM-SSH MM-Sparse  68.67 75.19 73.79  71.38 83.05 84.49  –  55.55 61.52  –  50.43 59.52  32.77 49.69 58.13  32.85 61.60 66.59  –  37.05 57.35  –  39.13 57.29  McFee, B. and Lanckriet, G. R. G. Partial order embedding  Strecha, C. et al. LDAHash:  Improved matching with  with multiple kernels. In Proc. ICML, 2009.  smaller descriptors. PAMI, 34(1):66–78, 2012.  Mika, S. et al. Fisher discriminant analysis with kernels.  In Proc. Neural Networks for Signal Processing, 1999.  Taylor, G. W. et al. Learning invariance through imitation.  In Proc. CVPR, 2011.  Norouzi, M. and Fleet, D. Minimal loss hashing for com-  pact binary codes. In Proc. ICML, 2011.  Norouzi, M., Fleet, D., and Salakhutdinov, R. Hamming  distance metric learning. In Proc. NIPS, 2012.  Roweis, S. T. and Saul, L. K. Nonlinear dimensionality re- duction by locally linear embedding. Science, 290(5500): 2323, 2000.  Schoelkopf, B., Smola, A., and Mueller, K. R. Kernel prin- cipal component analysis. Artiﬁcial Neural Networks, pp. 583–588, 1997.  Shakhnarovich, G., Viola, P., and Darrell, T. Fast pose In Proc.  estimation with parameter-sensitive hashing. CVPR, 2003.  Shen, C. et al. Positive semideﬁnite metric learning with  boosting. In Proc. NIPS, 2009.  Sprechmann, P., Bronstein, A. M., and Sapiro, G. Learning eﬃcient sparse and low rank models. Technical Report arXiv:1010.3467, 2012.  Torralba, A., Fergus, R., and Freeman, W. T. 80 million tiny images: A large data set for nonparametric object and scene recognition. PAMI, 30(11):1958–1970, 2008a.  Torralba, A., Fergus, R., and Weiss, Y. Small codes and large image databases for recognition. In Proc. CVPR, 2008b.  Wang, J., Kumar, S., and Chang, S.-F. Sequential projec- tion learning for hashing with compact codes. In Proc. ICML, 2010.  Weinberger, K. Q. and Saul, L. K. Distance metric learning for large margin nearest neighbor classiﬁcation. JMLR, 10:207–244, 2009.  Weiss, Y., Torralba, A., and Fergus, R. Spectral hashing.  In Proc. NIPS, 2008.  Xing, E. P. et al. Distance metric learning with applica- tion to clustering with side-information. In Proc. NIPS, 2002.  Yagnik, J. et al. The power of comparative reasoning. In  Proc. CVPR, 2011.  Sparse similarity-preserving hashing  Supplementary Material for Sparse similarity-preserving hashing  Figure 9. Cross-modality retrieval: ﬁrst ﬁve Image results to three Tags queries (left) obtained using CM-SSH (odd rows) and MM-SparseHash (even rows) in the multimodal NUS dataset. Note how many if not most of the results from CM-SSH, considered one of the state-of-the-art tech- niques in this task, are basically uncorrelated to the query.  CM-SSHMM-SparseHashwatercloudreﬂectionﬂowerredpeopleportraitartQuerySparse similarity-preserving hashing  Figure 10. Cross-modality retrieval: union of top ﬁve Tags results (right) to three Image queries (left) obtained using CM-SSH (odd rows) and MM-SparseHash (even rows) in the multimodal NUS dataset. Tags matching ground truth tags showed in green. Note how the proposed approach not only detects signiﬁcantly more matching words, but also the “non-matching” ones actually make a lot of sense, indicating that while they were not in the original image tags, underlying connections were learned.  nature, sky, water, landscape, sunset, light, white, trees, color, reﬂection, black, animal, tree, sun, orange, winter, snow, beautiful, river, wildlife, photography, lake, bird, dark, forest, birds, ice, reﬂections, wood, ﬂying, evening, outdoors, photographer, dusk  nature, sky, water, clouds, green, explore, sunset, people, sea, art, beach, ocean, asia, sand, rocks, airplane, aircraft, boats, ﬂying, plane, rural, waves, ﬂight, aviation, breathtaking, bush, thailand, vivid, twilight, glow, cliff, landscapes, airplanessunset, tree, orange, old, abandoned, car, autumn, road, forest, fall, truck, rust, colourful, woods, antique, vehicle, halloween clouds, sunset, sea, beach, sun, ocean, summer, sand, rocks, evening, holiday, peace, happy, dunes QueryCM-SSH:MM-SparseHash:england, italy, island, ship, italia, hawaii, interesting, cow, islands, elephants, mauinature, sky, blue, water, clouds, red, sea, yellow, beach, california, winter, ocean, building, old, sand, sunrise, spain, cloud, wall, coast, sepia, stone, eaves, mist, perspective, fence, school, ﬂy, oregon, jump, monument, perfect, surf, alleySparse similarity-preserving hashing  Figure 8. Precision as function of Hamming radius r of hash codes of diﬀerent length (left: CIFAR10 dataset, solid: m = 48, dotted: m = 128; right: NUS dataset, solid: m = 64, dotted: m = 256).  Table 5. Performance (in %) of diﬀerent hashing methods on the CIFAR10 dataset with diﬀerent settings of diﬀerent length m.  Hamming radius ≤ 2  Recall  Prec.  F1 –  Hamming radius = 0  Prec.  –  Recall  –  F1 –  L2  KSH  AGH1  AGH2  SSH  DH  NN  Sparse  Method  m M α  48  64  128  16 7 11 7 16  0.01 0.001 0.005 0.001  0  mAP 17.42  31.10 32.49 33.50 14.55 14.22 13.53 15.34 14.99 14.38 15.78 17.18 17.20 13.13 13.07 13.12 30.18 34.74 37.89  23.07 21.08 23.80 21.29 21.97  m 48 64 128 48 64 128 48 64 128 48 64 128 48 64 128 48 64 128 λ 0.1 0.1 0.1 0.1 0.1  –  –  18.22 10.86 2.91 15.95 6.50 2.89 17.43 7.63 3.78 9.92 1.52 0.30 3.0×10 1.0×10 0.00 32.69 22.78 5.38  −3 −3  32.69 26.03 31.74 21.41 25.94  −3 −2 −3 −3 −2 −3 −3 −3 −4 −5 −5 −5  −2  0.44 0.13 3.3×10 1.4×10 4.1×10 1.1×10 3.6×10 7.2×10 1.6×10 6.6×10 3.1×10 5.1×10 5.1×10 1.7×10 0.00 0.74 0.28 2.9×10 0.93 12.56 6.87 41.68 18.11  −2 −2 −3 −3 −2 −2 −3 −2 −4 −4 −4 −5  −1 −2  0.86 0.26 6.5×10 2.8×10 8.1×10 2.2×10 7.1×10 1.4×10 3.2×10 1.3×10 6.1×10 1.0×10 1.0×10 3.3×10 0.00 1.45 5.5×10 5.7×10 1.81 17.00 11.30 28.30 21.30  5.39 2.49 0.67 4.88 3.06 1.58 5.44 3.61 1.43 0.30 1.0×10 0.10 1.0×10 0.00 0.00 9.47 5.70 1.39  −3  −3  16.65 26.65 31.12 25.27 27.99  −2 −3 −3 −3 −3 −4 −3 −3 −4 −5 −5 −5 −5  −2 −3 −3  −2  5.6×10 9.6×10 4.5×10 2.2×10 1.2×10 3.4×10 3.5×10 1.4×10 3.9×10 5.1×10 1.7×10 1.7×10 1.7×10 0.00 0.00 5.2×10 8.8×10 2.2×10 5.0×10 3.04 0.86 10.17 3.81  −2 −3 −3 −3 −4 −3 −3 −4 −4 −5 −5 −5  −2 −3  0.11 1.9×10 8.9×10 4.4×10 2.4×10 6.8×10 6.9×10 2.7×10 7.8×10 1.0×10 3.3×10 3.4×10 3.4×10 0.00 0.00 0.10 1.8×10 4.4×10 0.10 5.46 1.70 14.50 6.71  010203040506010−0.410−0.310−0.210−0.1Hamming Radius  SparseHashKSHDiffHashAGH2SSHNNhashCIFAR10NUS01020304010−310−210−1100Hamming RadiusPrecision  Sparse similarity-preserving hashing  Table 6. Performance (in %) of hashing methods of diﬀerent length m on the NUS dataset. Comparable degrees of freedom are 256-bit SparseHash and 80-bit dense hashes.  Method  mAP@10 MP@5K Prec.  Hamming radius ≤ 2 F1  Recall  Hamming radius = 0 F1  Recall  Prec.  L2  KSH  AGH1  AGH2  SSH  DH  NN  Sparse  m 64 80 256 64 80 256 64 80 256 64 80 256 64 80 256 64 80 256 λ 0.3 0.05 1.0 0.05 0.005 0.3 0.05 1.0 1.0 0.05 0.005 0.3  m M α  64  256  7 7 16 4 4 6  68.67  32.77  72.85 72.76 73.73 69.48 69.62 73.86 68.90 69.73 73.00 72.17 72.58 73.52 71.33 70.34 70.73 76.39 75.51 78.31  74.17 74.15 74.51 74.05 74.48 71.73  42.74 43.32 45.35 47.28 47.23 46.68 47.27 47.32 47.65 44.79 46.96 47.13 41.69 37.75 39.02 59.76 59.59 61.21  56.08 51.52 55.54 60.73 59.42 54.76  83.80 84.21 84.24 69.43 71.15 75.90 68.73 70.57 74.90 60.06 83.96 84.18 84.26 84.24 84.24 75.51 77.17 83.46  71.67 69.08 79.09 78.82 81.95 78.34  −3 −3 −3  −2 −2  −2  −3 −3 −3 −3 −3  −2  6.1×10 1.8×10 1.4×10 0.11 7.5×10 1.5×10 0.14 0.12 5.3×10 0.12 1.9×10 1.8×10 1.4×10 4.9×10 1.4×10 1.59 2.02 5.8×10 1.99 0.53 1.21 3.85 1.18 6.10  −2 −3 −3  −2  −3 −3 −3 −3 −3  1.2×10 3.6×10 2.9×10 0.22 0.15 2.9×10 0.28 0.24 0.11 0.24 3.9×10 3.5×10 2.9×10 9.8×10 2.9×10 3.11 3.94 0.11  3.98 1.06 2.42 7.34 2.33 11.30  84.21 84.23 84.24 73.35 74.14 81.64 72.82 73.85 80.45 81.73 80.91 84.24 84.24 84.24 84.24 81.24 81.89 83.94  81.11 81.67 82.76 81.82 83.24 80.85  −3 −3 −3 −2 −3 −3 −2 −2 −2 −2 −2 −3 −3 −3 −3  −3  1.7×10 1.4×10 1.4×10 3.9×10 2.5×10 3.6×10 5.2×10 4.2×10 1.1×10 1.1×10 1.3×10 1.5×10 1.4×10 4.9×10 1.4×10 0.10 0.24 4.9×10 0.46 0.15 0.17 1.20 0.35 1.02  −3 −3 −3 −2 −2 −3  −2 −2 −2 −2 −3 −3 −3 −3  −3  3.3×10 2.9×10 2.9×10 7.9×10 5.1×10 7.1×10 0.10 8.3×10 2.2×10 2.2×10 2.6×10 2.9×10 2.9×10 9.8×10 2.9×10 0.20 0.48 9.8×10 0.92 0.30 0.34 2.37 0.70 2.01  ","In recent years, a lot of attention has been devoted to efficient nearestneighbor search by means of similarity-preserving hashing. One of the plightsof existing hashing techniques is the intrinsic trade-off between performanceand computational complexity: while longer hash codes allow for lower falsepositive rates, it is very difficult to increase the embedding dimensionalitywithout incurring in very high false negatives rates or prohibitingcomputational costs. In this paper, we propose a way to overcome thislimitation by enforcing the hash codes to be sparse. Sparse high-dimensionalcodes enjoy from the low false positive rates typical of long hashes, whilekeeping the false negative rates similar to those of a shorter dense hashingscheme with equal number of degrees of freedom. We use a tailored feed-forwardneural network for the hashing function. Extensive experimental evaluationinvolving visual and multi-modal data shows the benefits of the proposedmethod."
1311.6091,2014,A Primal-Dual Method for Training Recurrent Neural Networks Constrained by the Echo-State Property  ,"['Jianshu Chen', 'Li Deng']",https://arxiv.org/pdf/1311.6091.pdf,"4 1 0 2    r a  M 6         ]  G L . s c [      3 v 1 9 0 6  .  1 1 3 1 : v i X r a  A Primal-Dual Method for Training Recurrent Neural Networks Constrained by the Echo-State  Property  Jianshu Chen  Department of Electrical Engineering  University of California  Los Angeles, CA 90034, USA  cjs09@ucla.edu  Li Deng  Machine Learning Group  Microsoft Research  Redmond, WA 98052, USA deng@microsoft.com  Abstract  We present an architecture of a recurrent neural network (RNN) with a fully- connected deep neural network (DNN) as its feature extractor. The RNN is equipped with both causal temporal prediction and non-causal look-ahead, via auto-regression (AR) and moving-average (MA), respectively. The focus of this paper is a primal-dual training method that formulates the learning of the RNN as a formal optimization problem with an inequality constraint that provides a suf- ﬁcient condition for the stability of the network dynamics. Experimental results demonstrate the effectiveness of this new method, which achieves 18.86% phone recognition error on the TIMIT benchmark for the core test set. The result ap- proaches the best result of 17.7%, which was obtained by using RNN with long short-term memory (LSTM). The results also show that the proposed primal-dual training method produces lower recognition errors than the popular RNN methods developed earlier based on the carefully tuned threshold parameter that heuristi- cally prevents the gradient from exploding.  1  Introduction  Considerable impact in speech recognition has been created in recent years using fully-connected deep neural networks (DNN) that drastically cut errors in large vocabulary speech recognition (Yu et al., 2010; Dahl et al., 2011; Seide et al., 2011; Dahl et al., 2012; Kingsbury et al., 2012; Hinton et al., 2012; Deng et al., 2013b,c; Yu et al., 2013; Dahl et al., 2013; Sainath et al., 2013). However, the well-known problem of the previous state-of-the-art approach based on Gaussian Mixture Model (GMM)-HMMs has not been addressed by the DNNs in a principled way: missing temporal cor- relation structure that is prevalent in the speech sequence data. Recurrent neural networks (RNN) have shown their potential to address this problem (Robinson, 1994; Graves et al., 2013), but the difﬁculty of learning RNNs due to vanishing or exploding gradients (Pascanu et al., 2013) or the complexity of the LSTM (long short-term memory) structure in RNNs (Graves et al., 2013) have so far slowed down the research progress in using RNNs to improve speech recognition and other sequence processing tasks. In this paper, we propose an architecture of RNNs for supervised learning, where the input sequence to the RNN is computed by an independent feature extractor using a fully-connected DNN receiving its input from raw data sequences. We have formulated both autoregressive (AR) and autoregres- sive and moving-average (ARMA) versions of this RNN. A new learning method is developed in this work, which is successfully applied to both AR and ARMA versions of the RNN. The new method frames the RNN learning problem as a constrained optimization one, where cross entropy is maximized subject to having the inﬁnity norm of the recurrent matrix of the RNN to be less than  1  a ﬁxed value that provides a sufﬁcient condition for the stability of RNN dynamics. A primal-dual technique is devised to solve the resulting constrained optimization problem in a principled way. Experimental results on phone recognition demonstrate: 1) the primal-dual technique is effective in learning RNNs, with satisfactory performance on the TIMIT benchmark; 2) The ARMA version of the RNN produces higher recognition accuracy than the traditional AR version; 3) The use of a DNN to compute high-level features of speech data to feed into the RNN gives much higher accuracy than without using the DNN; and 4) The accuracy drops progressively as the DNN features are extracted from higher to lower hidden layers of the DNN.  2 Related Work  The use of recurrent or temporally predictive forms of neural networks for speech recognition dated back to early 1990’s (Robinson, 1994; Deng et al., 1994), with relatively low accuracy. Since deep learning became popular in recent years, much more research has been devoted to the RNN (Graves et al., 2006; Graves, 2012; Maas et al., 2012; Mikolov, 2012; Vinyals et al., 2012; Graves et al., 2013). Most work on RNNs made use of the method of Back Propagation Through Time (BPTT) to train the RNNs, and empirical tricks need to be exploited in order to make the training effective. The most notable trick is to truncate gradients when they become too large (Mikolov et al., 2011; Pascanu et al., 2013; Bengio et al., 2013). Likewise, another empirical method, based on a regularization term that represents a “preference for parameter values,” was introduced to handle the gradient vanishing problem (Pascanu et al., 2013). The method we propose for training RNN in this paper is based on optimization principles, capi- talizing on the cause of difﬁculties in BPTT analyzed in (Pascanu et al., 2013). Rather than using empirical ways to scale up or scale down the gradients, we formulate the RNN learning problem as an optimization one with inequality constraints and we call for a natural, primal-dual method to solve the constrained optimization problem in a principled way (Polyak, 1987). Our work also dif- fers from the earlier work reported in (Sutskever, 2013; Martens & Sutskever, 2011), which adopted a special way of initialization using echo state networks (Jaeger, 2001a) when carrying out the stan- dard BPTT procedure, without using a formal constrained optimization framework. This work is originally motivated by the echo state network (Jaeger, 2001a), which carefully hand- designed the recurrent weights making use the the echo-state property. The weights are then ﬁxed and not learned, due to the difﬁculty in learning them. Instead, in this work, we devise a method of learning these recurrent weights by a formal optimization framework which naturally incorporates the constraint derived from the echo-state property. The echo-state property proposed in (Jaeger, 2001a) provides a sufﬁcient condition for avoiding the exploding gradient problem. Although such a sufﬁcient constraint looks restrictive, it allows the RNN to be trained in a relatively easy and principled way, and the trained RNN achieves satisfactory performance on the TIMIT benchmark. As another main contribution of this paper, we build a “deep” version of the RNN by using an independent DNN to extract high-level features from the raw speech data, capitalizing on the well established evidence for the power of DNNs in carrying out automatic feature extraction (Hinton et al., 2012; Dahl et al., 2011; LeCun, 2012). Having the DNN as the feature extractor and the RNN as the classiﬁer trained separately helps reduce overﬁtting in the training. This strategy of building a deep version of RNNs contrasts with other strategies in the literature, where the same RNN model is stacking up on top of each other and all layers of RNNs are trained jointly. Aside from using weight noise for regularization, the overﬁtting problem caused by joint training of many RNNs has been circumvented mainly by exploiting elaborate structures such as LSTM (Graves et al., 2013), making the complexity of the overall model higher and the results more difﬁcult to reproduce than our simpler approach to building the deep version of the RNN reported in this paper. Finally, our method exploits the DNN features in a very different way than the “tandem” architecture discussed in (T¨uske et al., 2012), where posterior outputs at the top layer of the DNN is concatenated with the acoustic features as new inputs to an HMM system. We take the hidden layer of the DNN as the features, which are shown experimentally in our work to be much better than the top-layer posterior outputs as in the tandem method. Further, rather than using the GMM-HMM as a separate sequence classiﬁer in (T¨uske et al., 2012), we use the RNN as the sequence classiﬁer.  2  Figure 1: Architecture of the recurrent neural network with DNN features, where one of the hidden layers of the DNN will be used as input for the RNN. Arrows of different colors from the DNN to the RNN indicate alternative ways of providing the inputs to RNN.  3 The Recurrent Neural Network  3.1 Basic architecture We consider a deep-RNN shown in Fig. 1. y(cid:48) t denotes the output of the DNN that is trained over randomly permuted input sequence, and yt denotes the output of RNN corresponding to the input sequence of the original order. The network consists of two parts: (i) a lower-level DNN, and (ii) an upper-level RNN. The major task of the lower DNN is to extract useful features from the raw data and feed them to the upper RNN component, which replaces HMM as a sequence classiﬁer. The RNN in the upper level captures the temporal contextual information in its hidden states and generates the outputs that predict the labels of the input data in the supervised sequential prediction task such as recognizing phones or words embedded in a sentence.  3.2 Mathematical formulation of the RNN  In this section, we focus on the RNN part of the deep network, given the DNN outputs that provide input vectors vt’s to the RNN. A standard RNN is described by the following equations:  ht = f (Wht−1 + WI vt + b) yt = g (Uht)  (1) (2) where ht ∈ RN , vt ∈ RNI and yt ∈ RNo are vectors at discrete time t that represent the hidden states, the inputs, and the outputs, respectively, and the matrices W ∈ RN×N , WI ∈ RN×NI , U ∈ RNo×N , and vector b ∈ RN collect the recurrent weights, input weights, the output weights and the bias. The function f (x) represents the nonlinearities that are applied to each entry of the vector x, i.e., the nonlinear operation that is applied to the vector component-by-component:  f (x) = [f (x1)  ···  f (xN )]T  where xk denotes the k-entry of the vector x, and each f (xk) can be sigmoid, tanh or rectiﬁed. (We will use sigmoid nonlinearity through out this work as an example.) And g(x) represents the operation applied at the output units. Typical choices are linear function g(x) = x or soft-max operation. While the standard RNN model described by (1) updates the hidden states ht from its summary of the history ht−1 and the current input vt, which we call the autoregressive (AR) version, we can have a more general ARMA version described by  (cid:32)  ∆2(cid:88)  τ =−∆1  (cid:33)  ht = f  Wht−1 +  WI,τ vt−τ + b  (3)  where ∆1 and ∆2 denote the number of inputs that the network looks forward and backwards. If ∆1 = 0, then it only looks backwards into the past history and if ∆2 = 0, it only looks into future.  3  DNN RNN vt=h2,tvt=h1,t(cid:80)∆2  The models described by (1) and (3) are parallel to the vector AR and ARMA models, respectively, in time series analysis except that the classical AR and ARMA models are linear:  [AR] : ht = Wht−1 + WI vt  [ARMA] : ht = Wht−1 +  WI,τ vt−τ  (4)  (5)  ∆2(cid:88)  τ =−∆1  Then, (3) can be written in the following equivalent form:  In the context of ARMA version of the RNN, Wht−1 is called AR part, and the term τ =−∆1 WI,τ vt−τ is called MA part. Depending on the nature of the time sequence, some tasks are easier to be modeled by AR, and some others by MA, or jointly ARMA. Just as AR model is a special case of ARMA model (∆1 = ∆2 = 0), model (1) is a special case of (3). Due to its generality, the ARMA version of the RNN (3) is expected to be more powerful than the AR version (1). Indeed, our experimental results presented in Section 5 have conﬁrmed this expectation. A key observation we make is that in order to develop a uniﬁed learning method, (3) can be converted back to the form of (1) by deﬁning some extra augmented variables. Let vt and WI be deﬁned as (6) (7)  t+∆1  t−∆2 WI (cid:44) [WI,−∆2  vt (cid:44)(cid:2)vT (cid:3)T ··· vT ··· WI,∆1] ht = f(cid:0)Wht−1 + WI vt + b(cid:1)  (8) In other words, the ARMA version of the RNN model in (3) can be implemented in an equivalent manner by having a context window that slides through several input samples and deliver them as an augmented input sample. From now on, we will proceed with the simple AR model (1)–(2) since the general ARMA version can be reduced to the AR version by the above conversion and hence the learning method will stay unchanged from the AR version to the ARMA version. Note that another approach to letting the model look into future is the bi-directional recurrent neural network(Graves et al., 2013), which uses information from future by letting its hidden layers depend on the hidden states at future. On the other hand, the ARMA is made to depend on future by letting its input include future. So these two methods uses information from future in two different ways and our ARMA method is relatively easier to implement and also effectively capture the future information. Furthermore, by incorporating past and future information into the inputs, the ARMA-RNN also provides a relative easy way of enhancing the amount of memory in the RNN.  3.3 Learning the RNN and its difﬁculties  The standard BPTT method for learning RNNs “unfolds” the network in time and propagates error signals backwards through time. Let J(Θ) denote the cost function that measures how good the RNN predicts the target, where Θ (cid:44) {W, WI , U, b} is the set of all the parameters to be trained. In general, the cost can be written as an average of costs over different time instants:  T(cid:88)  t=1  J(Θ) =  1 T  Jt(yt, dt)  (9)  where dt denotes the desired signal (also called target) at time t, and Jt(yt, dt) characterizes the cost at time t. The cost Jt(yt, dt) depends on the model parameters Θ through yt, which, as shown in (1)–(2), further depends on {W, WI , U, b}. Typical choices of Jt(yt, dt) includes squared- error and cross entropy, etc. The gradient formula for J(Θ) with respect to Θ (cid:44) {W, WI , U, b} can be computed by back propagation through time (BPTT), which are given in sections B–C of the supplementary material. It is well known that training RNNs is difﬁcult because of the exploding and vanishing gradient problems (Pascanu et al., 2013). Let  γ (cid:44) (cid:107)Df(cid:107)  where Df is a diagonal matrix constructed from the element-wise derivative of f (x):  Df (cid:44) diag{f  (cid:48)  (pt)}  4  (10)  (11)  and (cid:107)X(cid:107) denotes the 2−norm (the largest singular value) of the matrix X. For sigmoid function f (x), γ = 1/4, and for tanh, γ = 1. The argument in (Pascanu et al., 2013) pointed out that a sufﬁcient condition for vanishing gradient problem to occur is  and a necessary condition for exploding gradient to occur is that  (cid:107)W(cid:107) = (cid:107)WT(cid:107) <  1 γ  (cid:107)W(cid:107) = (cid:107)WT(cid:107) >  1 γ  (12)  (13)  Therefore, the properties of the recurrent matrix W is essential for the performance of the RNN. In previous work (Pascanu et al., 2013; Mikolov et al., 2011), the way to solve the exploding gradient problem is to empirically clip the gradient if the norm of the gradient exceeds a certain threshold, and the way to avoid the vanishing gradient is also empirical — either to add a regularization term to push up the gradient or exploit the information about the curvature of the objective function (Maas et al., 2012). Below, we describe an alternative primal-dual approach for training RNN, which is based on the echo-state property, a sufﬁcient condition for avoiding the exploding gradient problem.  4 A New Algorithm for Learning the RNN  In this section, we ﬁrst discuss an essential property for the model of the kind in (1) — the echo- state property, and provide a sufﬁcient condition for it. Then, we formulate the problem of training the RNN that preserves the echo-state property as a constrained optimization problem. Finally, we develop a primal-dual method to solve the problem.  4.1 The echo-state property  t(cid:107) < (cid:15)t, where ht [resp. h(cid:48)  We now show that conditions of the form (12)–(13) are closely related to whether the model (1)–(2) satisﬁes the echo-state property (Jaeger, 2002), which states that “if the network has been run for a very long time [from minus inﬁnity time in the deﬁnition], the current network state is uniquely determined by the history of the input and the (teacher-forced) output”. It is also shown in (Jaeger, 2001b) that this echo-state property is equivalent to the “state contracting” property. Since we do not consider the case with feedback from output in the model (1), we here describe the “state contracting” property slightly different from (Jaeger, 2001b): Deﬁnition 1 (State contracting). The network is state contracting if for all right-inﬁnite input se- quences {vt}, where t = 0, 1, 2, . . ., there exists a null sequence ((cid:15)t)t≥0 such that for all starting 0 and for all t > 0 it holds that (cid:107)ht − h(cid:48) states h0 and h(cid:48) t] is the hidden state vector at time t obtained when the network is driven by vt up to time t after having been stated in v0, [resp. in h(cid:48) 0]. It is shown in (Jaeger, 2001a) that a sufﬁcient condition for the non-existence, i.e., a necessary condition for the existence, of the echo-state property is that the spectral radius of the recurrent matrix W is greater than one (resp. four) when tanh (resp. sigmoid) units are used. And it is further conjectured that for a weight matrix W that is randomly generated by sampling the weights over uniform distribution over [−1, 1] and normalized such that ρ(W) = 1 − δ would satisfy the echo-state property with high probability, where δ is a small positive number. In echo-state networks, the reservoir or the recurrent weights are randomly generated and normalized according to the rule above and will be ﬁxed over time in the training. The input weights are ﬁxed as well. Instead, in this paper, we learn the recurrent weights together with the input and output weights subject to the constraint that the network satisﬁes the echo-state property. To this end, we propose the following sufﬁcient condition for the echo-state property, which will be shown to be easily handled in the training procedure. The proof of the following proposition can be found in Appendix A. Proposition 1 (Echo-state: a sufﬁcient condition). Let γ (cid:44) maxx |f(cid:48)(x)|. The network model (1) satisﬁes the echo-state property if  (cid:107)W(cid:107)∞ <  1 γ  5  (14)  where (cid:107)W(cid:107)∞ denote the ∞-norm of the matrix W (maximum absolute row sum), and γ = 1 for tanh units, and γ = 1/4 for sigmoid units.  The echo state property is typically assumed to capture a short-term memory in the data. However, it is reasonable for many applications, such as speech recognition, using RNNs since each phone typically lasts for fewer than 60 frames, requiring relatively short memory. Suppose we have two RNNs that run on different input sequences up to some time t so that they have different hidden states at time t. Afterwards, if the input acoustic sequences to the two RNNs become identical, the echo state property requires that the hidden states of these two RNNs be close to each other in order to produce the same predicted labels. Another important consequence of condition (14) is that it also provides a sufﬁcient condition to avoid the exploding gradient problem in a more principled manner. This can be shown by following the same argument as (Pascanu et al., 2013) except that the 2-norm is replaced by ∞-norm. Thus, if the condition (14) can be enforced in the training process, there is no need to clip the gradient. We will show in next section that the ∞-norm constraint on W is convenient to handle in optimization. Therefore, our proposed method below provides a relatively easy way to train RNN in a principled manner, especially on tasks that requires short memory. Learning RNN under reﬁned characterization of its dynamics, such as (Manjunath & Jaeger, 2013), is left as future work.  4.2 Formal formulation of the learning problem  Based on the previous discussion, we can formulate the problem of training the RNN that preserves the echo-state property as the following constrained optimization problem:  J(Θ) = J(W, WI , U, b)  Θ  min s.t. (cid:107)W(cid:107)∞ ≤ 1 γ  (15)  (16)  In other words, we need to ﬁnd the set of model parameters that best predict the target while pre- serving the echo-state property. Recall that (cid:107)W(cid:107)∞ is deﬁned as the maximum absolute row sum. Therefore, the above optimization problem is equivalent to the following constrained optimization problem (since max{x1, . . . , xN} ≤ 1/γ is equivalent to xi ≤ 1/γ, i = 1, . . . , N):  min  Θ  s.t.  N(cid:88)  j=1  J(Θ) = J(W, WI , U, b)  |Wij| ≤ 1 γ  ,  i = 1, . . . , N  where Wij denotes the (i, j)-th entry of the matrix W. Next, we will proceed to derive the learning algorithm that can achieve this objective.  4.3 Primal-dual method for optimizing RNN parameters  We solve the constrained optimization problem (17)–(18) by primal-dual method. The Lagrangian of the problem can be written as  L(Θ, λ) = J(W, WI , U, b)+  λi  N(cid:88)   N(cid:88)  i=1  j=1    |Wij|− 1 γ  where λi denotes the ith entry of the Lagrange vector λ (or dual variable) and is required to be non-negative. Let the dual function q(λ) be deﬁned as the following unconstrained optimization problem:  q(λ) = min Θ  L(Θ, λ)  (20)  It is shown that the dual function q(λ) obtained from the above unconstrained optimization problem is always concave, even when the original cost J(Θ) is not convex (Boyd & Vandenberghe, 2004). And the dual function is always a lower bound of the original constrained optimization problem (17)–(18): q(λ) ≤ J(Θ(cid:63)). Maximizing q(λ) subject to the constraint λi ≥ 0, i = 1, . . . , N will  6  (17)  (18)  (19)  be the best lower bound that can be obtained from the dual function (Boyd & Vandenberghe, 2004). This new problem is called the dual problem of the original optimization problem (17)–(18):  (21)  λ  q(λ)  max s.t. λi ≥ 0,  (22) which is a convex optimization problem since we are maximizing a concave objective with linear inequality constraints. After solving λ(cid:63) from (21)–(22), we can substitute the corresponding λ(cid:63) into the Lagrangian (19) and then solve the correponding set of parameters Θo = {Wo, Wo I , Uo, bo} that minimizes L(Θ, λ) for this given λ(cid:63):  i = 1, . . . , N  (23) Θo = arg min Θ Then, the obtained Θo = {Wo, Wo I , Uo, bo} will be an approximation to the optimal solutions. In convex optimization problems, this approximate solution will be the same global optimal solution under some mild conditions (Boyd & Vandenberghe, 2004). This property is called strong duality. However, in general non-convex problems, it will not be the exact solution. But since ﬁnding the globally optimal solution to the original problem (17)–(18) is not realistic, it would be satisfactory if it can provide a good approximation. Back to the problem (21)–(22), we are indeed solving the following problem  L(Θ, λ(cid:63))  Θ  min  max λ(cid:23)0  (24) where the notation λ (cid:23) 0 means that each entry of the vector λ is greater than or equal to zero. Now, to solve the problem, what we need to do is to minimize the Lagrangian L(Θ, λ) with respect to Θ, and in the mean time, maximize the dual variable λ subjected to the constraint that λ (cid:23) 0. Therefore, as we will see soon, updating the RNN parameters consists of two steps: primal update (minimization of L with respect to Θ) and dual update (maximization of L with respect to λ). First, we provide the primal update rule. To minimize the Lagrangian L(Θ, λ) with respect to Θ, we may apply gradient descent to L(Θ, λ) with respect to Θ. However, note that L(Θ, λ) consists of two parts: J(Θ) that measures the prediction quality, and the part that penalizes the violation of the constraint (18). Indeed, the second part is a sum of many (cid:96)1 regularization terms on the rows of the matrix W:  L(Θ, λ)  |Wij| = (cid:107)wi(cid:107)1  (25)  N(cid:88)  j=1  where wi denotes the ith row vector of the matrix W. With such observations, the Lagrangian in (19) can be written in the following equivalent form: L(Θ, λ) = J(W, WI , U, b) +  (26) To Minimize L(Θ, λ) of the above structure with respect to Θ = {W, WI , U, b}, we can apply an argument similar to the one made in (Beck & Teboulle, 2009) to derive the following iterative soft-thresholding algorithm for the primal update of W:  (cid:107)wi(cid:107)1 − 1 γ  N(cid:88)  (cid:18)  (cid:19)  λi  i=1  Wk = Tλµk  Wk−1−µk  (27) where Tλµk (X) denote a component-wise shrinkage (soft-thresholding) operator on a matrix X, deﬁned as  ∂W  ∂J(Wk−1, WI,k−1, Uk−1, bk−1)  (cid:26)  (cid:27)  Xij − λiµk Xij ≥ λiµk  Xij + λiµk Xij ≤ −λiµk 0  otherwise  [Tλµk (X)]ij =  (28)  Note that the primal update for W is implemented by a standard (stochastic) gradient descent fol- lowed by a shrinkage operator. On the other hand, the primal updates for WI, U and b follow the standard (stochastic) gradient descent rule: WI,k = WI,k−1 − µk  ∂J(Wk−1, WI,k−1, Uk−1, bk−1)  (29)  ∂WI  7  Table 1: Phone recognition error (percent) on the TIMIT core test set using DNN-top feature se- quences as the input to the RNNs. The results are shown as a function of two hyperparameters: the size of the RNN’s hidden layer and the moving-average order of the RNN.  HiddenSz Order=0 Order=2 Order =4 Order=6 Order=8 Order=10 Order=12  100 200 300 500 1000 2000  19.85 19.86 20.02 20.00 20.44 20.70  19.83 19.72 19.72 19.73 19.83 20.34  19.80 19.65 19.60 19.56 19.60 20.10  19.65 19.60 19.56 19.44 19.49 19.65  19.50 19.45 19.40 19.34 19.24 19.45  19.44 19.35 19.23 19.06 19.10 19.30  19.42 19.31 19.16 18.91 18.98 19.12  Table 2: Phone recognition error as a function of the clipping threshold ((Mikolov et al., 2011); (Pascanu et al., 2013)). The lowest error 19.05% around threshold 1.0 is nevertheless higher than 18.91% obtained by the new primal-dual method without tuning parameters.  Threshold  1  2  Phone error (%)  19.65  19.50  0.5 19.25  9  19.10  1.0 19.05  1.1 19.08  1.5 19.15  2  19.54  10 20.5  Uk = Uk−1 − µk bk = bk−1 − µk  ∂J(Wk−1, WI,k−1, Uk−1, bk−1)  ∂J(Wk−1, WI,k−1, Uk−1, bk−1)  ∂U  ∂b  (30)  (31)  In order to accelerate the convergence of the algorithm, we can, for example, add momentum or use Nesterov method (Sutskever et al., 2013) to replace the gradient descent steps in (27)–(30). In our experiments reported in Section 5, we adopted Nesterov method to accelerate the training process. Next, we describe the dual update, which aims to maximize L with respect to λ subject to the constraint that λ (cid:23) 0. To this end, we use the following rule of gradient ascent with projection, which increases the function value of L while enforcing the constraint:  (cid:107)wi,k−1(cid:107)1 − 1 γ  +  λi,k =  λi,k−1 + µk  (32) where [x]+ (cid:44) max{0, x}. Note that λi is indeed a regularization factor in L(Θ, λ) that penalizes the violation of constraint (18) for the ith row of W. The dual update can be interpreted as a rule to adjust the regularization factor in an adaptive manner. When the sum of the absolute values of the ith row of W exceeds 1/γ, i.e., violating the constraint, the recursion (32) will increase the regularization factor λi,k on the ith row in (19). On the other hand, if the constraint (18) for a certain i is not violated, i.e., (cid:107)wi(cid:107)1 < 1/γ, then the dual update (32) will decrease the value of the corresponding λi so that (cid:107)wi(cid:107)1−1/γ is less penalized in (19). This process will repeat itself until the constraints (18) are satisﬁed. The projection operator [x]+ makes sure that once the regularization factor λi is decreased below zero, it will be set to zero and the constraint for the ith row in (22) will not be penalized in (19). An alternative choice to enhance the constraint is to apply projection operator to W after each stochastic gradient descent update (29)–(31).  (cid:20)  (cid:18)  (cid:19)(cid:21)  5 Experiments and Results  We use the TIMIT phone recognition task to evaluate the deep-RNN architecture and the primal-dual optimization method for training the RNN part of the network. The standard 462-speaker training set is used and all SA sentences are removed conforming to the standard protocol (Lee & Hon, 1989; Hinton et al., 2012; Deng et al., 2013a). A separate development set of 50 speakers is used for tuning all hyper parameters. Results are reported using the 24-speaker core test set, which has no overlap with the development set. In our experiments, standard signal processing techniques are used for the raw speech waveforms, and 183 target class labels are used with three states for each of 61 phones. After decoding, the original 61 phone classes are mapped to a set of 39 classes for ﬁnal scoring according to the standard  8  Table 3: Phone recognition error (percent) on the TIMIT core test set using DNN-middle and DNN- bottom features, as well as the raw ﬁlter-bank features.  Features  Order0 Order4 Order8 Order12 20.70 19.65 DNN-Middle 21.50 DNN-Bottom 23.10 Filter-Banks 30.50 28.15  20.30 22.65 30.15  19.96 22.00 29.40  evaluation protocol. In our experiments, a bi-gram language model over phones, estimated from the training set, is used in decoding. To prepare the DNN and RNN targets, a high-quality tri-phone HMM model is trained on the training data set, which is then used to generate state-level labels based on HMM forced alignment. The DNN as part of the baseline described in (Deng et al., 2013a) is used in this work to extract the high-level features from raw speech features. That is, the DNN features are discriminatively learned. In our experiments, we use a DNN with three hidden layers, each having 2000 units. Each hidden layer’s output vector, with a dimensionality of 2000, can be utilized as the DNN features. Thus, we have three sets of high-level features: DNN-top, DNN-middle, and DNN-bottom, indicated in Figure 1 with three separate colors of arrows pointing from DNN to RNN. We ﬁrst report the phone recognition error performance of the deep-RNN using the DNN-top fea- tures. In Table 1, the percent error results are shown as a function of the RNN hidden layer’s size, varying from 100 to 2000, and also as a function of the moving average (MA) order in the RNN. Note when MA order is zero, the ARMA version of the RNN reverts to the traditional AR version. The above results are obtained using the ﬁxed insertion penalty of zero and the ﬁxed bi-phone “language model” weight of one. When the language model’s weight is tuned slightly over the development set, the core test set error further drops to 18.86%, which is close to the best numbers reported recently on this benchmark task in (Deng et al., 2013a) using deep convolutional neural networks with special design of the pooling strategy (18.70%) and in (Graves et al., 2013) using a bi-directional RNN with special design of the memory structure (17.7%). No bi-directionality and no special design on any structure are used in the RNN reported in this paper. The confusion matrix of this best result is shown in Section D of the supplementary document. We next compare the new primal-dual training method with the classical BPTT using gradient clip- ping as described in (Pascanu et al., 2013). Table 2 shows the the phone recognition error of the classical BPTT with gradient clipping on the TIMIT benchmark. We found that the error rate is sensitive to the threshold value. The best phone error rate on the test set is found to be between 19.05%-20.5% over a wide range of the threshold values where the best tuned clipping threshold is around 1.0 which corresponds to the error rate of 19.05%. This is higher than the 18.91% from our primal-dual method (without tuning the language model weights). Thus, using the new method pre- sented in the paper, we do not need to tune the hyper-parameter of clipping threshold while obtaining lower errors. We ﬁnally show the phone recognition error (percent) results for the features of DNN-middle, DNN- bottom, and of raw ﬁlter-bank data. The size of the RNN’s hidden layer is ﬁxed at 500. And the results for four different MA orders are shown in Table 3. Comparisons among the results in Tables 1 and 3 on phone recognition provide strong evidence that the high-level features extracted by DNNs are extremely useful for lowering recognition errors by the RNN. Further, the higher hidden layers in the DNN are more useful than the lower ones, given the same dimensionality of the features (ﬁxed at 2000 as reported in this paper but we have the same conclusion for all other dimensions we have experimented). In addition, the introduction of MA components in the traditional AR version of the RNN also contributes signiﬁcantly to reducing recognition errors.  6 Discussion and Conclusion  The main contribution of the work described in this paper is a formulation of the RNN that lends it- self to effective learning using a formal optimization framework with a natural inequality constraint  9  that provides a sufﬁcient condition to guarantee the stability of the RNN dynamics during learning. This new learning method overcomes the challenge of the earlier echo-state networks that typically ﬁxed the recurrent weights due to the well-known difﬁculty in learning them. During the devel- opment of our new method, we propose a sufﬁcient condition for the echo-state property, which is shown to be easily incorporated in the training procedure. We also make contributions to a new deep-RNN architecture, where the MA part is added to the original AR-only version of the RNN. The learning of both AR and ARMA versions of the RNN is uniﬁed after re-formulation of the model, and hence the same learning method developed can be applied to both versions. The experimental contributions of this work are of four folds. First, we successfully apply the new learning method for the RNN to achieve close-to-record low error rates in phone recognition in the TIMIT benchmark. Second, we demonstrate the effectiveness of using DNNs to extract high-level features from the speech signal for providing the inputs to the RNN. With a combination of the DNN and RNN, we form a novel architecture of the deep-RNN, which, when trained separately, mitigates the overﬁtting problem. Third, on the same TIMIT benchmark task, we demonstrate clear superiority of the ARMA version of the RNN over the traditional AR version. The same efﬁcient learning procedure is applied to both versions, with the only difference in the additional need to window the DNN outputs in the ARMA version of the RNN. Fourth, we show experimentally that the new training method motivated by optimization methodology achieves satisfactory performance as a sequence classiﬁer on TIMIT benchmark task. Compared to the previous methods (Mikolov et al., 2011; Pascanu et al., 2013) of learning RNNs using heuristic rules of truncating gradients during the BPTT procedure, our new method reports slightly lower phone recognition errors on the TIMIT benchmark and no longer needs to tune the threshold parameter as in the previous methods.  References Beck, A. and Teboulle, M. A fast iterative shrinkage-thresholding algorithm for linear inverse prob-  lems. SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.  Bengio, Y., Boulanger-Lewandowski, N., and Pascanu, R. Advances in optimizing recurrent net-  works. In Proc. ICASSP, Vancouver, Canada, May 2013.  Boyd, S. P. and Vandenberghe, L. Convex Optimization. Cambridge university press, 2004. Dahl, G., Yu, D., Deng, L., and Acero, A. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. IEEE Trans. on Audio, Speech and Language Processing, 20(1):30–42, jan 2012.  Dahl, G. E., Yu, D., Deng, L., and Acero, A. Large vocabulary continuous speech recognition with context-dependent DBN-HMMs. In Proc. IEEE ICASSP, pp. 4688–4691, Prague, Czech, May 2011.  Dahl, G. E., Sainath, T. N., and Hinton, G. E.  Improving deep neural networks for lvcsr using rectiﬁed linear units and dropout. In Proc. ICASSP, pp. 8609–8613, Vancouver, Canada, May 2013. IEEE.  Deng, L., Hassanein, K., and Elmasry, M. Analysis of the correlation structure for a neural predictive  model with application to speech recognition. Neural Networks, 7(2):331–339, 1994.  Deng, L., Abdel-Hamid, O., and Yu, D. A deep convolutional neural network using heterogeneous pooling for trading acoustic invariance with phonetic confusion. In Proc. IEEE ICASSP, Vancou- ver, Canada, May 2013a.  Deng, L., Hinton, G., and Kingsbury, B. New types of deep neural network learning for speech recognition and related applications: An overview. In Proc. IEEE ICASSP, Vancouver, Canada, May 2013b.  Deng, L., Li, J., Huang, J.-T., Yao, K., Yu, D., Seide, F., Seltzer, M., Zweig, G., He, X., Williams, J., Gong, Y., and Acero, A. Recent advances in deep learning for speech research at microsoft. In Proc. ICASSP, Vancouver, Canada, 2013c.  Graves, A. Sequence transduction with recurrent neural networks.  Workshp, ICML, 2012.  In Representation Learning  Graves, A., Fern´andez, S., Gomez, F., and Schmidhuber, J. Connectionist temporal classiﬁcation: labelling unsegmented sequence data with recurrent neural networks. In Proc. ICML, pp. 369– 376, Pittsburgh, PA, June 2006. ACM.  10  Graves, A., Mohamed, A., and Hinton, G. Speech recognition with deep recurrent neural networks.  In Proc. ICASSP, Vancouver, Canada, May 2013.  Hinton, G., Deng, L., Yu, D., Dahl, G. E., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T. N., and Kingsbury, B. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82–97, November 2012.  Jaeger, H. The “echo state” approach to analysing and training recurrent neural networks. GMD  Report 148, GMD - German National Research Institute for Computer Science, 2001a.  Jaeger, H. Short term memory in echo state networks. GMD Report 152, GMD - German National  Research Institute for Computer Science, 2001b.  Jaeger, H. Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the “echo state network” approach. GMD Report 159, GMD - German National Research Institute for Computer Science, 2002.  Kingsbury, B., Sainath, T. N., and Soltau, H. Scalable minimum bayes risk training of deep neural network acoustic models using distributed hessian-free optimization. In Proc. INTERSPEECH, Portland, OR, September 2012.  LeCun, Yann. Learning invariant feature hierarchies. In Proc. ECCV, pp. 496–505, Firenze, Italy,  October 2012. Springer.  Lee, K.-F. and Hon, H.-W. Speaker-independent phone recognition using hidden Markov models. IEEE Transactions on Acoustics, Speech and Signal Processing,, 37(11):1641–1648, November 1989.  Maas, A. L., Le, Q., O’Neil, T. M., Vinyals, O., Nguyen, P., and Ng, A. Y. Recurrent Neural Net- works for Noise Reduction in Robust ASR. In Proc. INTERSPEECH, Portland, OR, September 2012.  Manjunath, G. and Jaeger, H. Echo state property linked to an input: Exploring a fundamental  characteristic of recurrent neural networks. Neural computation, 25(3):671–696, 2013.  Martens, J. and Sutskever, I. Learning recurrent neural networks with hessian-free optimization. In  Proc. ICML, pp. 1033–1040, Bellevue, WA, June 2011.  Mikolov, T. Statistical Language Models Based on Neural Networks. PhD thesis, Ph. D. thesis,  Brno University of Technology, 2012.  Mikolov, T., Deoras, A., Povey, D., Burget, L., and Cernocky, J. Strategies for training large scale neural network language models. In Proc. IEEE ASRU, pp. 196–201, Honolulu, HI, December 2011. IEEE.  Pascanu, R., Mikolov, T., and Bengio, Y. On the difﬁculty of training recurrent neural networks. In  Proc. ICML, Atlanta, GA, June 2013.  Polyak, B. Introduction to Optimization. Optimization Software, NY, 1987. Robinson, A. J. An application of recurrent nets to phone probability estimation. IEEE Transactions  on Neural Networks, 5(2):298–305, August 1994.  Sainath, T.N., Kingsbury, B., Soltau, H., and Ramabhadran, B. Optimization techniques to improve IEEE Transactions on Audio,  training speed of deep neural networks for large speech tasks. Speech, and Language Processing, 21(11):2267–2276, November 2013.  Seide, F., Li, G., and Yu, D. Conversational speech transcription using context-dependent deep  neural networks. In Proc. INTERSPEECH, pp. 437–440, Florence, Italy, August 2011.  Sutskever, I. Training Recurrent Neural Networks. PhD thesis, Ph. D. thesis, University of Toronto,  2013.  Sutskever, I., Martens, J., Dahl, G., and Hinton, G. E. On the importance of initialization and  momentum in deep learning. In Proc. ICML, Atlanta, GA, June 2013.  T¨uske, Z., Sundermeyer, M., Schl¨uter, R., and Ney, H. Context-Dependent MLPs for LVCSR:  TANDEM, Hybrid or Both? In Proc. Interspeech, Portland, OR, September 2012.  Vinyals, Oriol, Ravuri, Suman V, and Povey, Daniel. Revisiting recurrent neural networks for robust  ASR. In Proc. ICASSP, pp. 4085–4088, Kyoto, Japan, March 2012. IEEE.  11  Yu, D., Deng, L., and Dahl, G. Roles of pre-training and ﬁne-tuning in context-dependent DBN- HMMs for real-world speech recognition. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2010.  Yu, D., Deng, L., and Seide, F. The deep tensor neural network with applications to large vocabulary speech recognition. IEEE Trans. on Audio, Speech and Language Processing, 21(2):388 –396, 2013.  12  Supplementary Material A Proof of a sufﬁcient condition for echo-state property  First, we prove the following property regarding f (x):  (33) where γ (cid:44) max f(cid:48)(x). Let xk and yk denote the k-th entries of the N × 1 vectors x and y, respectively. Then,  (cid:107)f (x) − f (y)(cid:107)∞ ≤ γ · (cid:107)x − y(cid:107)∞  (cid:107)f (x) − f (y)(cid:107)∞ = max 1≤k≤N  |f (xk) − f (yk)|  By mean value theorem, we have  f (xk) = f (yk) +  f(cid:48)(cid:0)yk + t(xk − yk)(cid:1)dt(xk − yk)  (cid:90) 1  0  so that  (cid:12)(cid:12)(cid:12)(cid:12)(cid:90) 1 (cid:12)(cid:12)(cid:12)(cid:12) f(cid:48)(cid:0)yk + t(xk − yk)(cid:1)dt(xk − yk) |f (xk) − f (yk)| (cid:90) 1 ≤ (cid:12)(cid:12)f(cid:48)(cid:0)yk + t(xk − yk)(cid:1)(cid:12)(cid:12) dt · |xk − yk| (cid:90) 1 ≤ ≤ γ · |xk − yk| Substituting the above result into (34), we get  γ · dt · |xk − yk|  ≤  0  0  0  (cid:107)f (x) − f (y)(cid:107)∞ ≤ γ · max 1≤k≤N  |xk − yk|  (37) From the expressions of tanh and sigmoid functions, we can easily verify that γ = 1 for tanh function and γ = 1/4 for sigmoid function. Let ht and h(cid:48) tions h0 and h(cid:48)  t denote the hidden states of the recurrent neural network (1) that starts at initial condi-  0, respectively:  = γ · (cid:107)x − y(cid:107)∞  Subtracting (39) from (38), we obtain ht − h (cid:48) t = f (Wht−1 + WI vt + b)  Taking ∞-norm of both sides of the above expression and using (37), we get  (34)  (35)  (36)  (38) (39)  (40)  (cid:48)  (cid:48)  ht = f (Wht−1 + WI vt + b) (cid:48) h  t−1 + WI vt + b(cid:1) t = f(cid:0)Wh t−1 + WI vt + b(cid:1) − f(cid:0)Wh =(cid:13)(cid:13)f (Wht−1 + WI vt + b) t−1 + WI vt + b(cid:1)(cid:13)(cid:13)∞ − f(cid:0)Wh ≤ γ ·(cid:13)(cid:13)W(ht−1 − h (cid:13)(cid:13)∞ ≤(cid:0)γ · (cid:107)W(cid:107)∞(cid:1)t · (cid:107)h0 − h  (cid:48) t−1) ≤ γ · (cid:107)W(cid:107)∞ · (cid:107)ht−1 − h t−1(cid:107)∞ (cid:48) 0(cid:107)∞ (cid:48)  (cid:48)  (cid:107)ht − h t(cid:107)∞ (cid:48)  13  (41) Let (cid:15)t (cid:44) (γ · (cid:107)W(cid:107)∞)t · (cid:107)h0 − h(cid:48) 0(cid:107)∞. Obviously, as long as γ · (cid:107)W(cid:107)∞ < 1 or, equivalently, (cid:107)W(cid:107)∞ < 1/γ, we have (cid:15)t → 0 and we can conclude the network (1) is state contracting and thus has echo-state property.  B Gradient formula for BPTT Speciﬁcally, let pt be a N × 1 vector that collects the input to the hidden units:  pt (cid:44) Wht + WI vt + b  Let δt denote the error signal to be propagated:  δt (cid:44) T · ∂J ∂pt  =  ∂ ∂pt  T(cid:88)  t=1  Jt(yt, dt)  (42)  (43)  Then, by chain rule (see Section C of the supplementary document for details), δt is propagated through time according to the following backward recursion:  δt = Df WT · δt+1 +  ∂Jt ∂pt  ,  t = T, T − 1, . . . , 0  (44)  with initialization δT +1 = 0, where Df is a diagonal matrix constructed from the element-wise derivative of f (x):  (45) depends on the choice of cost functions Jt and the output units g(x). For example,  Df (cid:44) diag{f  (cid:48)  (pt)}  The form of ∂Jt ∂pt for linear output units and square-error cost, i.e.,  the expression for ∂Jt ∂pt  is given by  g(x) = x  Jt(yt, dt) = (cid:107)yt − dt(cid:107)2  = −2UT (dt − yt)  ∂Jt ∂pt  On the other hand, if soft-max output units and cross-entropy cost are used, i.e.,  g(x)n =  exn(cid:80)No Jt(yt, dt) = − No(cid:88)  n=1 exn  dn,t ln yn,t  (46) (47)  (48)  (49)  (50)  where dn,t and yn,t denote the nth entry of the vectors dt and yt, respectively, the expression for ∂Jt ∂pt  will be given by  n=1  = −(dt − yt)  ∂Jt ∂pt  (51)  And the gradients of J(θ) with respect to the parameters W, WI, U and b are given by the follow- ing expressions:  t=1  T(cid:88) T(cid:88) T(cid:88) T(cid:88)  t=1  t=1  t=1  δthT  t−1  δtvT t  ∂Jt ∂U  δt  ∂J ∂W  ∂J ∂WI  ∂J ∂U  ∂J ∂b  =  =  =  =  1 T  1 T  1 T  1 T  (52)  (53)  (54)  (55)  (56)  For soft-max output units with cross-entropy cost, which we use in this work, ∂Jt  ∂U is given by  ∂Jt ∂U  = −(dt − yt)hT  t  14  C Derivation of the back propagation recursion  From the deﬁnition of the error signal in (43), we can write δt as  In RNN, only Ju with u ≥ t depend on pt, so we have ∂Ju  ∂pt  = 0 for u < t and hence  Ju  T(cid:88)  u=1  ∂Ju ∂pt  ∂ ∂pt  T(cid:88)  δt =  =  u=1  ∂Ju ∂pt  u=t  T(cid:88) T(cid:88) T(cid:88)  u=t+1  u=t+1  ∂pT t ∂pt+1  ∂pT t ∂pt+1  ∂pT t ∂pt+1  ∂J ∂pt  =  =  =  =  =  =  ∂Ju ∂pt  +  ∂Jt ∂pt  ∂pT t ∂pt+1  T(cid:88) · T(cid:88)  ·  u=1  u=t+1  · ∂Ju ∂pt+1  ∂Ju ∂pt+1  +  +  ∂Jt ∂pt  ∂Jt ∂pt  ∂Ju ∂pt+1  +  ∂Jt ∂pt  · δt+1 +  ∂Jt ∂pt  Now we evaluate ∂pT ∂pt+1  t  , which by chain rule, can be written as  ∂pT t ∂pt+1  =  ∂hT t ∂pt  · ∂pT t+1 ∂ht  By the expression of pt in (42), we have  ∂pT t+1 ∂ht  =  ∂ ∂ht = WT  {Wht + WI vt + b}T  and by (1), we have  ∂hT t ∂pt  =  =  = f = Df Substituting (60)–(61) into (59), we get  (pt)  {f (Wht−1 + WI vt + b)}T {f (pt)}T  ∂ ∂pt ∂ ∂pt (cid:48)  ∂pT t ∂pt+1  = Df WT  (cid:33)  Ju  (cid:32) T(cid:88)  u=1  15  ∂J ∂Wij  =  ·  1 T  ∂  ∂Wij  Substituting (62) into (58), we conclude our proof of (44). Next, we derive (52)–(54). We will only show the proof of (52). By chain rule, we have  (57)  (58)  (59)  (60)  (61)  (62)  (cid:33)  Ju  (cid:32) T(cid:88)  u=1  ∂pT t ∂Wij  · ∂ ∂pt  =  t=1  1 T  · T(cid:88) T(cid:88) (cid:21) (cid:20) ∂pT  1 T  t=1  =  t ∂Wij  =  n  ∂pT t ∂Wij  δt  0  n (cid:54)= i  (cid:26)[ht]j n = i T(cid:88) T(cid:88)  [ht]j[δt]i  t=1  δthT t  t=1  ∂J ∂W  =  1 T  By (42), we have  where the notatin [x]i denotes the ith entry of the vector. Therefore,  ∂J ∂Wij  =  1 T  which implies that, in matrix form,  (63)  (64)  (65)  (66)  D Confusion matrix of phone recognition  See the ﬁgure in next page.  16  17  ","We present an architecture of a recurrent neural network (RNN) with afully-connected deep neural network (DNN) as its feature extractor. The RNN isequipped with both causal temporal prediction and non-causal look-ahead, viaauto-regression (AR) and moving-average (MA), respectively. The focus of thispaper is a primal-dual training method that formulates the learning of the RNNas a formal optimization problem with an inequality constraint that provides asufficient condition for the stability of the network dynamics. Experimentalresults demonstrate the effectiveness of this new method, which achieves 18.86%phone recognition error on the TIMIT benchmark for the core test set. Theresult approaches the best result of 17.7%, which was obtained by using RNNwith long short-term memory (LSTM). The results also show that the proposedprimal-dual training method produces lower recognition errors than the popularRNN methods developed earlier based on the carefully tuned threshold parameterthat heuristically prevents the gradient from exploding."
1312.5412,2014,Approximated Infomax Early Stopping: Revisiting Gaussian RBMs on Natural Images  ,"['Taichi Kiwaki', 'Takaki Makino', 'Kazuyuki Aihara']",https://arxiv.org/pdf/1312.5412.pdf,"4 1 0 2     n a J    6      ] L M  . t a t s [      3 v 2 1 4 5  .  2 1 3 1 : v i X r a  Approximated Infomax Early Stopping:  Revisiting Gaussian RBMs on Natural Images  Taichi Kiwaki  The University of Tokyo  kiwaki@sat.t.u-tokyo.ac.jp  Takaki Makino  The University of Tokyo  mak@sat.t.u-tokyo.ac.jp  Kazuyuki Aihara  The University of Tokyo  aihara@sat.t.u-tokyo.ac.jp  Abstract  We pursue an early stopping technique that helps Gaussian Restricted Boltzmann Machines (GRBMs) to gain good natural image representations in terms of over- completeness and data ﬁtting. GRBMs are widely considered as an unsuitable model for natural images because they gain non-overcomplete representations which include uniform ﬁlters that do not represent useful image features. We have recently found that GRBMs once gain and subsequently lose useful ﬁlters during their training, contrary to this common perspective. We attribute this phe- nomenon to a tradeoff between overcompleteness of GRBM representations and data ﬁtting. To gain GRBM representations that are overcomplete and ﬁt data well, we propose a measure for GRBM representation quality, approximated mutual in- formation, and an early stopping technique based on this measure. The proposed method boosts performance of classiﬁers trained on GRBM representations.  1  Introduction  While the Restricted Boltzmann Machines (RBMs) have been demonstrated to be effective in var- ious tasks [1, 2, 3, 4], they often fail to learn overcomplete representations from continuous data, especially from natural images [5, 6]. For example, RBMs trained on natural images learn a large number of uniform ﬁlters that do not represent sharp edges. The failure of RBMs in the application for continuous data has been attributed to the deﬁciency of Gaussian RBMs (GRBMs), a variant of RBMs for continuous data, in capturing the covariances between data variables [6]. This perspective has led people to several model extensions of RBMs that can model data covariances [5, 6, 7]. It is, however, not widely known that GRBMs once gain useful ﬁlters and subsequently lose many of them during the training [8]. Figure 1 (a) to (d) show ﬁlters learned by a GRBM from CIFAR-10 in different stages of the training. As is reported [5, 6], the GRBM gains a lot of uniform ﬁlters at the end of the training (after 300 sweeps over training data, or epochs) in Fig. 1 (d). However, Fig. 1 (b) and (c) suggest us that these uniform ﬁlters are actually degenerated from meaningful ﬁlters that represented edges or color gradients in the middle stages of the training (after 30 and 100 epochs). As apparent, we cannot trace back the GRBM training process for a useful representation into the early stages of the training where all the ﬁlters were nearly noisy initial states as in Fig. 1 (a). The change in the quality of GRBM feature representations impacts on the performance of a clas- siﬁer trained on the representations. Here, we consider a stack of the GRBM and an L2-SVM with the linear kernel, and see classiﬁcation performance on CIFAR-10. Figure 1 (e) and (f) illustrate the relationship between duration of the GRBM training and classiﬁcation performance of the stacked system. The percentage accuracy on both test and training data increases in the early stages of the  1  (a)  (b)  (e)  (f)  (g)  (c)  (d)  Figure 1: from (a) to (d): Development of feature representations. Shown are 20 randomly selected ﬁlters of a GRBM after (a) 10 epochs, (b) 30 epochs, (c) 100 epochs, and (d) 300 epochs of training. (e) and (f): Duration of RBM training and classiﬁcation performance. (g) Development of FED. We used GRBM training setting (d) and SVM parameters for ﬁg. 4.  training (1–30 epochs) and then declines in the late stages of the training (30–300 epochs), corre- sponding to the quality of GRBM feature representations. These phenomena cannot be simply explained that the GRBM overﬁts the training data, owing to two reasons. First, decline in the percentage accuracy is observed on both the test and training data. However, GRBM overﬁtting would result in decline only in the test data accuracy and increase in the training data accuracy. Second, the difference between the averaged free energy of test and train- ing data (hereafter, free energy difference, or FED), a common overﬁtting measure for RBMs [9], remains small throughout the GRBM training as in Fig. 1 (g). If GRBM overﬁtting were responsible for the phenomena, FED would show a signiﬁcant increase as the GRBM loses its useful ﬁlters. Compared with our previous study [8], there are two contributions to report. First, to better under- stand the phenomena, we claim a detailed mechanism on how GRBMs once gain and eventually lose useful ﬁlters during the training (Section 3). We there argue that GRBMs lose overcomplete representations because the maximum likelihood estimation of GRBMs can penalize such represen- tations. This argument leads us an idea of a tradeoff between overcompleteness of GRBM represen- tations and ﬁtting of GRBMs to training data, which suggests us that better ﬁtting of a GRBM does not always result in better representations even if the GRBM does not overﬁt. The tradeoff between overcompleteness and ﬁtting motivates us to perform early stopping training for GRBMs. Second, we propose an efﬁcient early stopping technique for GRBM representations that are over- complete and ﬁt data well. In Section 4, We describe our proposed method, dubbed a-infomax early stopping for GRBMs, that maximizes approximated mutual information (AMI), which is the sum of mutual information between each hidden unit and data. A-infomax early stopping is efﬁcient while alternative strategies, such as directly using test or validation data accuracy at each candi- date of stopping time, can incur us huge computational costs in performing supervised training and convolutional feature extraction. Moreover, parameter tuning for a-infomax early stopping is easier than our previous approaches [8]. In Section 5, we present experiments to explore the beneﬁts from a-infomax early stopping.  2 Gaussian RBMs  RBMs are a Markov random ﬁeld of a bipartite graph that consists of two layers of variables: vis- ible layer representing data, and hidden layer representing latent variables. GRBMs are one of the  2  75767778Test DataAccuracy[%]868890Training DataAccuracy[%]05010015020025030000.5EpochsFree EnergyDifference [nat]variants of RBMs with real-valued data and binary latent variables, whose joint probabilities are  p(H = h, V = v) =  1 Z  exp  vj σj  wijhi +   M(cid:88)  N(cid:88)  i=1  j=1  M(cid:88)  aihi − N(cid:88)  i=1  j=1  (vj − bj)2  σ2 j   ,  (1)  where M and N are the number of hidden and visible variables, H and V are a vector notation of hidden and visible variables, i.e., H = (H1, . . . , HM )T and V = (V1, . . . , VN )T, Z is the partition function, wij are connection weights between Hi and Vj, ai and bj are biases for hidden and visible variables, σj are parameters that control the variances of visible variables, and h and v are arbitrary realizations of H and V with hi and vj being their elements, i.e., h = {0, 1}M and v ∈ RN . A GRBM models a data distribution as a Gaussian mixture distribution, in which the number of the mixing components is exponential in the number of the hidden variables.  2.1 Training RBMs  GRBMs are trained by adjusting their parameters wij, ai, and bj so that the log-likelihood of data is maximized. The weight parameter update rule for GRBMs can be derived by taking the gradient of the log-likelihood:  ∂ log p ∂wij  ∝ EpD [HiVj] − E[HiVj],  (2) where EpD [·] denotes an expectation by an empirical distribution pD(V = v). The ﬁrst term of the r.h.s. of Eq. 2 is called the positive gradient and the second term is called the negative gradient. Be- cause exact inference of the negative gradient is intractable, practical applications use approximated inference based on log-likelihood approximation, such as contrastive divergence [10] or persistent contrastive divergence [11]. Weights are often updated by a gradient that is averaged over a set of several training cases, which is called a batch. We deﬁne an epoch as a unit duration of RBM training in which all the training cases in a dataset are used once for weight updates. Sparsity helps unsupervised learning systems learn suitable data representations [12, 13, 14]. GRBMs can obtain sparse representations by sparse regularization with two control parameters: the regularization strength λ and the sparsity target ρ. This regularization constrains activation prob- abilities of GRBM hidden units to be close to ρ [12].  2.2 GRBMs as Data Encoders  There are several ways to obtain data representations using GRBMs. The natural choice is the conditional expectation of hidden units which can be efﬁciently computed, but this is not necessarily the best strategy [15]. We can gain alternative representations based on various encoding schemes. Particularly when using SVMs in supervised learning, as Coates and Ng [15] report, an encoding scheme called “soft thresholding” is effective for achieving high classiﬁcation accuracy.  3 Mechanisms of GRBMs Losing Useful Representations As we have seen in Section 1, a GRBM gains a lot of meaningful ﬁlters after a certain period of training, but then loses a large number of them if the training continues. Such representations gained after prolonged training are widely regarded as a poor representation because they are not overcomplete [5, 6]. Representations are said overcomplete if there are more ﬁlters (of course, it is assumed in literature that the ﬁlters are in mutually different directions, and uniform ﬁlters are therefore excluded) than the effective dimensionality of data [16]. We view that the lost of GRBM overcompleteness arises from adjustment of the number of non- uniform GRBM ﬁlters according to the effective data dimensionality. What we assume here is that a data distribution on a low dimensional manifold is close to the family of distributions that GRBMs with a small number of hidden units — as small as the manifold dimensionality — can express. In maximum likelihood learning of a GRBM with a large number of hidden units, excess ﬁlters are thus attenuated to reduce negative effects that those ﬁlter can have on the likelihood. To verify that GRBM ﬁlter number adjustment is responsible for the lost of overcompleteness, we performed a toy data experiment on a GRBM with four ﬁlters. Data was generated from a two  3  A  B  (a)  (b)  (c)  Figure 2: A: GRBM ﬁlter number adjustment. State of the GRBM (a) after 10 epochs, (b) after 140 epochs, (c) after 2000 epochs. Points represent the data where colors indicate components of the mixture, white arrows represent GRBM ﬁlters, black lines represent planes that the activation probabilities of GRBM hidden units are at 0.5, and gray shadows represent the GRBM components (area within 2σ) where opacity corresponds to their proportion in the mixture. The GRBM was trained using the true likelihood gradient. B Tradeoff between overcompleteness and ﬁtting.  dimensional Gaussian mixture with four components: one dense component and three sparse com- ponents that are the same distance apart from the dense one. This distribution can be well approxi- mated, but cannot be completely expressed by a GRBM with two ﬁlters. The GRBM develops as follows. The GRBM initially gains a representation that is overcomplete, but does not ﬁt data well as in Fig. 2 A (a). After 130 more epochs of training, the GRBM better ﬁts the data and GRBM components roughly spread over the data points as in Fig. 2 A (b). After 2000 epochs of training, the GRBM further ﬁts the data and concisely assigns its components to the four mixture components of the data distribution as in Fig. 2 A (c). However, overcompleteness is damaged because two ﬁlters are attenuated to zero. One important point in this experiment is that the GRBM does not overﬁt the training data; the averaged log-likelihood of another data generated from the data distribution will remain close from that of the training data. The balance between data ﬁtting and overcompleteness dominates the performance of a linear clas- siﬁer (e.g., an SVM) trained on the GRBM representation for a component classiﬁcation problem. After 10 epochs of training, the classiﬁcation performance is expected to be low because planes that separate regions in which a hidden unit is likely 0 or 1 do not clearly divide the data compo- nents (in Fig. 2 A (a)). After 140 epochs of training, the classiﬁcation performance will become higher because two separation planes (denoted by thick lines) divide all the data components, and the classiﬁer can assign data labels based on the conﬁguration of two hidden units that correspond to those planes. After 2000 epochs of training, however, the performance will be degraded because two effective separation planes fail to divide one data component (in purple) from the others. Description above suggest us that two important aspects of the quality of representations, overcom- pleteness and ﬁtting, are in a tradeoff relation in GRBM training, except for early stages of training where all the ﬁlters are nearly zero. Figure 2 B sketches this concept. GRBM overcompleteness (the blue curve) rapidly grows in the early stages of training, but eventually becomes damaged as ﬁtting of a GRBM to training data (the green curve) improves. The quality of GRBM representations is ﬁrst improved and then damaged according to this balance between ﬁtting and overcompleteness. This perspective also tells us that higher data likelihood of GRBMs does not always result in better representations. We believe that our perspective is related to reports that maximum likelihood train- ing of RBMs for lower layers of deep networks is suboptimal [17, 18], and therefore foresee that the tradeoff can be observed in maximum likelihood training of other RBM variants.  4 Approximated Mutual Infomax Early Stopping for Better GRBM  Representations  The perspective that we have described so far motivates us to perform early stopping in GRBM training for better representations in terms of ﬁtting and overcompleteness. Because the standard early stopping is a method to prevent overﬁtting [19], we here explore a new early stopping criterion.  4  OvercompletenessThe Quality of RepresentationsFittingGRBM DevelopmentAMI(H; D) (cid:44) M(cid:88)  i=1  We now present two candidate criteria. First candidate is validation data accuracy, the same crite- rion for the standard method. It is straightforward to use validation data accuracy for early stopping because it is exactly what we want to optimize. However, validation data accuracy is computation- ally expensive because of supervised training phase where we compute feature maps by convolving GRBM ﬁlters and input images, and then train SVMs on a huge set of feature maps [5, 15]. Second candidate is the information of data held by GRBM representations, which directly mea- sures the GRBM representation quality. We here propose an infomax approach [20] with a novel information measure, approximated mutual information (AMI) deﬁned as:  I(Hi; D).  (3)  tion, i.e., I(H; D) ≤ AMI(H; D) by simply considering SD(H) ≤(cid:80)N  This quantity approximates the mutual information between data and whole hidden variables I(H; D). Actually, we can verify that AMI of GRBMs is an upper bound of the mutual informa- i SD(Hi) where S denotes entropy. The idea is to perform early stopping at the timing where AMI reaches its maximum. Thus, we call our method approximated-infomax early stopping, more shortly, a-infomax early stopping. AMI can be efﬁciently computed for GRBMs. To begin with, the mutual information between each hidden unit and data can be decomposed as I(Hi; D) = SD(Hi) − S(Hi|D), where SD(Hi) and S(Hi|D) are entropy and conditional entropy of hidden units when they are used to encode data. Both of these entropies can be efﬁciently computed. First, SD(Hi) can be directly computed from the activation probabilities of hidden units when they encode data, p(Hi = hi|V = d)  pD(Hi = hi) =  (cid:88)  (4)  1 |D| .  Second, S(Hi|D) can be computed as  d∈D  (cid:88)  (cid:88)  d∈D  hi∈{0,1}  S(Hi|D) =  1 |D|  p(Hi = hi|V = d) log  1  p(Hi = hi|V = d)  ,  (5)  where conditional probabilities are efﬁciently computed because RBM hidden units are condition- ally independent given a data vector. For further technical details on a-infomax early stopping, see the appendix. Note that AMI is computed within time linear in data size without sampling. AMI does not require supervised training phase with convolutional feature extraction and SVM training. There are two intuitive interpretations of a-infomax early stopping. The ﬁrst interpretation is that the mutual information between data and each hidden variable is a good indicator for non-uniformity of the corresponding ﬁlter. Suppose a GRBM ﬁlter for a sharp edge. The hidden variable correspond- ing to this ﬁlter shows strong correlation between the input image patches to the GRBM visible layer. The mutual information therefore become greater than zero. On the other hand, the mutual information becomes nearly zero with uniform ﬁlters, because the corresponding hidden units are almost statistically independent of the input images. AMI, the sum of the mutual information over the hidden variables, thus captures the sharpness of the ﬁlters throughout the whole representation. The second interpretation is that AMI roughly measures overcompleteness of representations. Let us consider an extreme example where we add an extra hidden unit ˆH to a perfect data representation H, such that D = f (H) with f (·) being a function. Introduction of this new variable helps better overcompleteness of the resulting representation {H, ˆH} if that variable reﬂects information about the data, I( ˆH; D) > 0. AMI is sensitive to this improvement and increases as AMI({H, ˆH}; D) = AMI(H; D) + I( ˆH; D). On the other hand, mutual information does not reﬂect the improvement in overcompleteness and remains the same, I({H, ˆH}; D) = I(H; D). Therefore, AMI can be considered as a corrected mutual information for measuring overcompleteness.  5 Experiments and Discussion  5.1 AMI and The Quality of GRBM Representations  In this section, we investigate the relationship between AMI and the quality of GRBM represen- tations under various settings of GRBM training: the weight update algorithms and GRBM hyper  5  A (a) (b) (c) (d) (e)  B  Figure 3: A: GRBM representations under various training settings. All ﬁlters were gained after 80 epochs of training. The hidden unit index i was put in ascending order of the mutual information. 20 ﬁlters with i/M = 0.025, 0.075, 0.125, . . . , 0.975 are shown for parameter settings (a) {1600, 432, 0.003, PCD}, (b) {1600, 192, 0.003, PCD}, (c) {1600, 108, 0.003, PCD}, (d) {800, 108, 0.003, PCD}, and (e) {400, 108, 0.003, PCD}. B: Mutual information between each hidden unit and data. Vertical lines indicate the points where the plots of mutual information in the corresponding color exceeds 0.02[nat].  parameters. For the algorithms, we examined persistent contrastive divergence (PCD) [11] and con- trastive divergence (CD) [10]. For hyper parameters, there are four major GRBM parameters to be considered: the learning rates, the batch size, and the number of hidden and visible units. However, we omit batch size from our investigation because increasing the batch size has similar effects as lowering the learning rate for sufﬁciently large batch sizes. Therefore, we here focus on the number of hidden and visible units, and the learning rate. We trained GRBMs by PCD on 100,000 image patches from CIFAR-10 [21], which were pre- processed by contrast normalization and whitening. We ﬁrst performed 5-fold cross validation to determine the optimal parameters for GRBM sparsity, feature encoders, and L2-SVMs. L2-SVMs were trained on GRBM representations pooled over quadrants of CIFAR images to compute vali- dation data accuracy. After ﬁnding the best parameters, we trained and evaluated GRBMs on the standard training and test data of CIFAR-10. GRBMs were trained with different numbers of hidden and visible units (or the size of receptive ﬁelds), the learning rate, and the weight update algorithm. To describe the GRBM training setting, we use a tuple {M, N, δ, algorithm} where δ denotes the learning rate. We here examined seven GRBM training settings from (a) to (g): from (a) to (c), we varied the receptive ﬁeld size, and from (c) to (e), we varied the number of hidden units. In (f), we tried a lower learning rate. In (g), we used CD instead of PCD. We trained GRBMs for 80 epochs in all the trials except for (f) where the training duration was 240 epochs. To begin with, let us discuss how mutual information between data and each hidden variable relates to the sharpness of the corresponding ﬁlter, and how AMI relates to the overcompleteness of a whole representation. Figure 3 A (a) to (e) show uniformly taken samples of GRBM ﬁlters which are in ascending order of the mutual information I(Hi; D). From the vertical lines in Figure 3, it is clearly seen that hidden units with sharp edge ﬁlters hold relatively large values of I(Hi; D) (larger than 0.02[nat]). Finally, let us turn to whole representations from individual ﬁlters. Figure 3 B shows plots of I(Hi; D) where each curve corresponds to the training settings from (a) to (e). AMI of a representation is the product of the number of hidden units and the area between the corresponding curve and the horizontal axis in Fig. 3 B. As is obvious, overcomplete representations with a large number of non-uniform ﬁlters show a large value of AMI.  6  00.10.20.30.40.50.60.70.80.9100.050.10.150.20.25Normalizedunitindexi/MI(Hi;D)[nat]  (a)(b)(c)(d)(e)A  B  Figure 4: Performance and AMI (See ap- pendix for the deﬁnition). The parameter settings (a) to (e) are the same as in Fig. 3. Here, we also show settings (f) {1600, 108, 0.001, PCD} and (g) {1600, 108, 0.003, CD} . Series of points in a color correspond to each run of GRBM training and lines in- dicate the temporal order.  θ  Figure 5: Performance and early stopping. Error bars indicate the standard deviation over 20 executions of training. Training settings from (c) to (e), and {3200, 108, 0.003, PCD} were examined. A: A-infomax early stopping. B: Without early stopping. GRBMs trained for 80 epochs were used.  Moreover, two interesting trends can be seen. First, the larger the number of hidden units becomes, the worse overcompleteness becomes. This can be explained as the number of excess ﬁlters in terms of likelihood maximization increases as the total number of hidden units becomes large. Second, the larger the number of visible units becomes, the better overcompleteness becomes. This can be attributed to a large effective data dimensionality. Large number of visible dimensions helps data variations to lie in orthogonal directions. This results in a large effective data dimensionality, and therefore inhibits the lost of overcompleteness. We can actually use AMI as a useful measure for GRBM representation quality. Plots (c) to (g) in Fig. 4 for small receptive ﬁelds show that ﬁne correlations between AMI and the test data accuracy are maintained regardless of the number of hidden units, the learning rate, and the weight update algorithm (Note that AMI = −AMI + Const. See appendix for more details). Plots (a) and (b) for larger receptive ﬁelds, however, do not show prominent correlations, reﬂecting that overcomplete- ness is not largely damaged. Even in these cases, drop in AMI is also observed. This phenomenon suggests that AMI is not a perfect measure for representation quality; AMI only serves as a good measure when the visible dimension is small as 100 in our experiments. Nevertheless, the high per- formance are often achieved with small receptive ﬁelds, i.e., small visible dimensions [14]. AMI, therefore, practically persists to be useful.  5.2 Performance Improvements by Early Stopping  We next verify how a-infomax early stopping improves classiﬁcation performance of stacked sys- tems. We here only varied the number of hidden units and ﬁxed the other training settings. We per- formed 20 runs of GRBM training for each of the settings. The threshold parameter for a-infomax early stopping (see appendix for more details) was selected as θ ∈ {−1.5,−0.7, 0, 0.7, 1.5}. The GRBM training was terminated after 80 epochs, even if the early stopping condition did not hold. Figure 5 shows that a-infomax early stopping enables performance boosts in all the training settings shown. Moreover, there are two interesting trends to note. First, the larger the number of GRBM hidden units, the larger the performance gain by early stopping. Second, the larger the number of GRBM hidden units, the closer the timings at which performance and AMI reach their maximum values (this corresponds to performance peaks observed at θ ≈ 0 in Figure 5 A). We can explain these trends that a larger gap between the total number of hidden units and the optimal number of effective ﬁlters in terms of likelihood maximization leads a severer and more obvious impact on performance that the lost of overcompleteness has. These two trends are practically important, ﬁrst because a larger number of hidden units generally results in higher classiﬁcation accuracy [14], and second because we can know that the optimal a-infomax early stopping thresholding parameter for  7  7274767880051015TestDataAccuracy[%]AMI[nat]  (a)(b)(c)(d)(e)(f)(g)  −1.5−0.700.71.574757677787980Test Data Accuracy [%]M=3200 M=1600 M=800 M=400Table 1: CIFAR-10 test data classiﬁcation accuracy by SVMs and unsupervised algorithms with the same number of hidden units. 20 GRBMs were trained under {1600, 108, 0.003, CD}.  Unsupervised Learning Algorithm GRBMs without early stopping GRBMs with a-infomax early stopping Sparse Coding OMP-1 OMP-10  (θ = 0) (Coates and Ng [15]) (Coates and Ng [15]) (Coates and Ng [15])  Test Acc. 77.0 ± 0.2 79.7 ± 0.2  78.9 79.4 80.1  GRBMs with a sufﬁciently large number of hidden units is θ ≈ 0, without tuning θ. The second property actually is the main advantage of a-infomax early stopping over our previous study where we needed to tune a thresholding parameter for GRBM reconstruction errors by cross validation [8].  5.2.1 Comparison with Other Approaches  A-infomax early stopping provides an approach for improving GRBM representations without ex- tending the model. It has been proved that model extensions are effective for RBMs to gain over- complete representations [5, 6]. However, these model extensions introduces some problems, such as conditional dependency of visible variables [6] or the need for a special treatment for the contin- uous hidden variables for stacking [5, 17]. A-infomax early stopping is free from such problems. Our approach thus will be directly applied to well-established deep learning models, such as deep Boltzmann machines [22] or deep belief networks [23]. Our ﬁndings may also help us to a deeper understanding of the mechanism by which such model extensions improve representation quality. The score achieved by GRBMs with a-infomax early stopping is comparable to the state-of-the-art score by single layer models that were reported to largely outperform GRBMs [14, 15] (in Table 1). This striking result also demonstrates the impact of a-infomax early stopping and potential ability of GRBMs to model natural images.  6 Conclusions  We proposed a-infomax early stopping to enhance GRBM representations in terms of overcomplete- ness and data ﬁtting. We reviewed a recently found phenomenon where a GRBM once gains and eventually loses sharp edge ﬁlters as the training proceeds. We attributed this phenomenon to a tradeoff between overcompleteness and ﬁtting of GRBMs. Along this line, we developed a-infomax early stopping that enables GRBMs to gain representations that are overcomplete and ﬁt data well. We performed experiments on stacks of a GRBM and an SVM to verify the classiﬁcation perfor- mance improvement by a-infomax early stopping. We found huge performance boosts by a-infomax early stopping, and that performance can compete with the state-of-the-art performance by other single layer algorithms.  Acknowledgments  This research is supported by the Aihara Innovative Mathematical Modelling Project, the Japan Society for the Promotion of Science (JSPS) through the Funding Program for World-Leading Inno- vative R&D on Science and Technology (FIRST Program), initiated by the Council for Science and Technology Policy (CSTP), and by JSPS Grant-in-Aid for JSPS Fellows (135500000216). We thank Mr. Sainbayar Sukhbaatar and Dr. Naoya Fujiwara for valuable discussion and helpful comments.  References [1] Hugo Larochelle and Yoshua Bengio. Classiﬁcation using discriminative restricted Boltzmann machines. In Proceedings of the 25th International Conference on Machine Learning, pages 536–543. ACM, 2008.  [2] Vinod Nair and Geoffrey E Hinton. Implicit mixtures of restricted Boltzmann machines. In  NIPS ’08, pages 1145–1152, 2008.  8  [3] Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. Restricted Boltzmann machines for collaborative ﬁltering. In Proceedings of the 24th International Conference on Machine Learning, pages 791–798. ACM, 2007.  [4] Graham W Taylor, Geoffrey E Hinton, and Sam Roweis. Modeling Human Motion Using In Advances in Neural Information Processing Systems 19, pages  Binary Latent Variables. 1345–1352, May 2007.  [5] Aaron Courville, James Bergstra, and Yoshua Bengio. A spike and slab restricted Boltzmann  machine. Journal of Machine Learning Research, 15:233–241, 2011.  [6] Marc’Aurelio Ranzato and Geoffrey E Hinton. Modeling pixel means and covariances using factorized third-order Boltzmann machines. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pages 2551–2558. IEEE, 2010.  [7] George E Dahl, Marc’Aurelio Ranzato, Abdel-rahman Mohamed, and Geoffrey E Hinton. Phone recognition with the mean-covariance restricted Boltzmann machine. Advances in neu- ral information processing systems, 23:469–477, 2010.  In submission to International Journal of Pattern Recognition and Artiﬁcial  [8] The Authors. Intelligence.  [9] Geoffrey E Hinton. A practical guide to training restricted Boltzmann machines. Technical  Report UTML TR 2010-003, University of Toronto, 2012.  [10] Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural  Computation, 14(8):1771–1800, August 2002.  [11] Tijmen Tieleman. Training restricted Boltzmann machines using approximations to the like- lihood gradient. In Proceedings of the 25th International Conference on Machine Learning, pages 1064–1071. ACM, July 2008.  [12] Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th International Conference on Machine Learning, pages 609–616. ACM, 2009.  [13] Honglak Lee, Peter T Pham, Yan Largman, and Andrew Y Ng. Unsupervised feature learn- ing for audio classiﬁcation using convolutional deep belief networks. In Advances in Neural Information Processing Systems 22, pages 1096–1104, 2009.  [14] Adam Coates, Honglak Lee, and Andrew Y Ng. An analysis of single-layer networks in  unsupervised feature learning. Journal of Machine Learning Research, 15:215–223, 2011.  [15] Adam Coates and Andrew Y Ng. The importance of encoding versus training with sparse cod- ing and vector quantization. In Proceedings of the 28th international conference on Machine learning, pages 921–928, 2011.  [16] Bruno A Olshausen and David J Field. Sparse Coding with an Overcomplete Basis Set: A  Strategy Employed by V1. Vision Research, 37:1–15, May 1997.  [17] Heng Luo, Pierre Luc Carrier, Aaron Courville, and Yoshua Bengio. Texture modeling with convolutional spike-and-slab RBMs and deep extensions. In Proceedings of the 16th Interna- tional Conference on Artiﬁcial Intelligence and Statistics, 2013.  [18] Nicolas Le Roux and Yoshua Bengio. Representational power of restricted boltzmann ma-  chines and deep belief networks. Neural Computation, 20(6):1631–1649, 2008.  [19] Shun-ichi Amari, Noboru Murata, Klaus-Robert M¨uller, Michael Finke, and Howard H Yang. Asymptotic statistical theory of overtraining and cross-validation. Neural Networks, IEEE Transactions on, 8(5):985–996, 1997.  [20] Anthony J Bell and Terrence J Sejnowski. An information-maximization approach to blind  separation and blind deconvolution. Neural Computation, 7(6):1129–1159, 1995.  [21] Alex Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis,  University of Toronto, 2009.  [22] Ruslan Salakhutdinov and Geoffrey E Hinton. Using deep belief bets to learn covariance kernels for Gaussian processes. In Advances in Neural Information Processing Systems 20, pages 1249–1256, 2007.  [23] Geoffrey E Hinton, Simon Osindero, and Yee Whye Teh. A fast learning algorithm for deep  belief nets. Neural Computation, 18(7):1527–1554, July 2006.  9  A Techinical Details on A-infomax Early Stopping  To detect the peak timing of AMI, we use following procedure. During RBM training, we check the loss of AMI relative to its peak value at time T : AMI (T ) = maxT (cid:48)AMI (T (cid:48)) − AMI (T ). Because AMI become unimodal curve except for small ﬂuctuations, simple thresholding of AMI (T ) results in two stopping time, one before and the other after the AMI peak timing. To avoid this difﬁculty, we use the following conversion (cid:93)AMI (T ) = AMI (T ) Sign(T − arg minT (cid:48) AMI (T (cid:48))), where Sign(·) is a sign function; (cid:93)AMI (T ) now shows a monotonically increasing shape. We perform a-infomax early stopping by thresholding (cid:93)AMI (T ),  (6) where θ is a threshold. As with REs, we ﬁrst compute maxT (cid:48)AMI (T (cid:48)) and then determine the values of RBM parameters for supervised training.  arg min  T  (cid:12)(cid:12)(cid:12)(cid:93)AMI (T ) − θ  (cid:12)(cid:12)(cid:12) ,  B Details of The Cross Validation  GRBMs were trained by PCD with the mini batch size being set to 100, the learning rate being set as δ = 0.003, and the number of hidden and visible units being set to 1,600 and 108, respectively. We select the parameters as follows: the sparsity target ρ of GRBMs was selected from 0.01 to 0.06 uniformly separated by 0.01, and the sparsity strength λ was selected from 0.1 to 0.5 separated by 0.1. The SVM parameter C was selected from 35, 75, 150, 300 and the soft thresholding bias was selected from 0.1 to 0.7 separated by 0.1.  10  ","We pursue an early stopping technique that helps Gaussian RestrictedBoltzmann Machines (GRBMs) to gain good natural image representations in termsof overcompleteness and data fitting. GRBMs are widely considered as anunsuitable model for natural images because they gain non-overcompleterepresentations which include uniform filters that do not represent usefulimage features. We have recently found that GRBMs once gain and subsequentlylose useful filters during their training, contrary to this common perspective.We attribute this phenomenon to a tradeoff between overcompleteness of GRBMrepresentations and data fitting. To gain GRBM representations that areovercomplete and fit data well, we propose a measure for GRBM representationquality, approximated mutual information, and an early stopping technique basedon this measure. The proposed method boosts performance of classifiers trainedon GRBM representations."
1312.5402,2014,Some Improvements on Deep Convolutional Neural Network Based Image Classification  ,['Andrew Howard'],https://arxiv.org/pdf/1312.5402.pdf,"     Some Improvements on Deep Convolutional  Neural Netw ork Based Image Classification           Andrew G. Howard   Andrew Howard Consulting   Ventura, CA 93003   andrewgeraldhoward@gmail.com   Abstract   We investigate multiple techniques to improve upon the current state of the  art  deep  convolutional  neural  network  based  image  classification  pipeline.  The  techniques  include  adding  more  image  transformations  to  the  training  data, adding  more  transformations to  generate additional predictions at test  time and using complementary  models applied to higher resolution images.  This  paper  summarizes  our  entry  in  the  Imagenet  Large  Scale  Vis ual  Recognition  Challenge  2013.  Our  system  achieved  a  top  5  classification  error  rate  of  13.55%  using  no  external  data  which  is  over  a  20%  relative  improvement on the previous year’s winner.    1    I n t ro d u c t i o n    Deep  convolutional  neural  networks  have  recently  been  substantially  improving  upon  the  state  of  the  art  in  image  classification  and  other  recognition  tasks  [2,6,10].  Since  their  introduction  in  the  early  1990s  [7],  convolutional  neural  networks  have  consistently  been  competitive  with  other  techniques  for  image  classification  and  recognition.  Recently,  they  have  pulled  away  from  competing  methods  due  the  availability  of  larger  data  sets,  better  models  and  training  algorithms  and  the  availability  of  GPU  computing  to  enable  investigation of larger and deeper models.   The  Imagenet  Large  Scale  Visual  Recognition  Challenge  (ILSVRC)  [8]  is  a  venue  for  evaluating  what  the  current  state  of  the  art  for  image  classification  and  recognition  is.  It  is  large  dataset  of  1.2  million  images  with  1000  classes  that  are  a  subset  of  the  Imagenet  dataset [3]. The 2012 competition demonstrated a large step forward for convolutional neural  networks  where a convnet based system [6] soundly beat the competing  methodology based  on  Fisher  Vectors  [4]  by  roughly  10%  in  absolute  terms.  The  convolution  neural  network  based system was made up of an ensemble of deep, eight layer networks. It also incorporated  important  features  such  as  pooling  and  normalizing  layers,  data  transformations  to  expand  the training data size, data transformations at test time, and drop out [5] to avoid over fitting.       We  investigate  useful  additions  to  the  winning  system  from  2012  [6]  and  improve  upon  its  results  by  20%  in  relative  terms.  This  paper  summarizes  our  entry  in  the  2013  ILSVRC  which achieved a 13.55% top 5 error rate compared to the previous year’s 16.4% top  5 error  rate.  The  winning  entry,  Clarifai,  which  is  partially  detailed  in  [10],  achieved  a  top  5  error  rate  of  11.74%.  The  methods  outlined  in  this  paper  should  be  able  to  improve  upon  it  and  other convolutional neural network based systems.    Our  models  are  based  on  the  2012  winning  system  [6]  and  use  the  code  provided  at  http://code.google.com/p/cuda-convnet  as  a  starting  point.  Our  model  structure  is  identical  with  the  exception  that  the  fully  connected  layers  are  twice  as  big.  It  turns  out,  that  this  change  does  not  improve  the  top  5  error  rate.  We  use  the  same  training  methodology  of  training  the  net  until  the  validation  error  plateaus  and  reducing  the  step  size  by  10   at  each   plateau. Additional changes are detailed in the follow sections.    The paper is organized as follows. Section 2 describes the additional transformations used to  increase  the  effective  number  of  training  examples.  Section  3  describes  the  additional  transformations  used  at  test  time  to  improve  prediction  and  a  method  to  reduce  the  total  number  of  predictions  to  a  manageable  size.  Section  4  describes  complementary  high  resolution  models.  Section  5  reports  the  results  of  our  system  and  we  conclude  with  a  summary and discussion.     2    A d d i t i o n a l   D a t a  Tra n s f o r ma t i o n s   f o r Tr a i n i n g    E x t e n d i n g   I m a g e   C r o p s   i n t o   E x t r a   P i x e l s    Deep  neural  networks  are  greatly  improved  with  the  addition  of  more  training  data.  When  more training data is not available, transformations to the existing training data which reflect  the variation found in images can synthetically increase the training s et size. In the previous  Imagenet  classification  system  [6],  three  types  of  image  transformations  were  used  to   augment  the  training  set.  The  first  was  to  take  a  randomly  located  crop  of  224x224  pixels  from  a  256x256  pixel  image  capturing  some  translation  i nvariance.  The  second  was  to  flip  the image horizontally to capture the reflection invariance. The final data transformation was  to  add  randomly  generated  lighting  which  tries  to  capture  invariance  to  the  change  in  lighting  and  minor  color  variation.  We  add  additional  transformations  that  extend  the  translation invariance and color invariance.    2 . 1   Previously [6], translated image crops of 224x224 pixels were selected from a training image  of 256x256. The 256x256 image was generated by rescaling the largest image dimension to  256 and then cropping the other side to be 256. This results in a loss of information by  not  considering roughly 30% of the pixels. While the cropped pixels are most likely less  informative than the middle pixels we found that making use of these additional pixels  improved the model.    To use the whole image, we first scale the smallest  side to 256 leaving us with a 256xN or  Nx256 sized image. We then select a random crop of 224x224 as a training im age. This  yields a large number of additional training exa mples and helps the net learn more extensive  translation invariance. Figure 1 shows a comparison of a square cropped image versus using  the full image. The square training image of the cat will neve r generate training examples  with a tail in it or with both ears compared to selecting crops from the full image.      Figure 1: Even well centered images when cropped lose information like the cat’s ear and   tail compared to the full image on the right. We select training patches from the full image to   avoid loss of information.                 2 . 2     A d d i t i o n a l   C o l o r   M a n i p u l a t i o n s    In addition to the random lighting  noise that has been used in previous pipelines [6], we also  add  additional  color  manipulations.  We  randomly  manipulate  the  contrast,  brightness  and  color using the python image library (PIL). This helps generate training examples that cover  the  span  of  image  variations  helping  the  neural  network  to  learn  invariance  to  changes  in  these properties. We randomly choose an order for the  three manipulations and then choose a  number between 0.5 and 1.5 for the amount of enhancement ( a setting of 1 leaves the image   unchanged). After  manipulating  the  contrast,  brightness  and  color,  we  then  add  the  random  lighting noise similar to [6].     3    A d d i t i o n a l   D a t a  Tra n s f o r ma t i o n s   f o r Te s t i n g    Previous  methods  combined  the  predictions  of  10  image  transformations  into  a  final  prediction. They  used  the  central  crop  and  the  four  corners  as  well  as  the  horizontal  flip  of  these  five.  We  found  that  predicting  at  three  different  scales  improved  the  joint  prediction.  We also made predictions on three different views of the data captur ing the extra pixels that  are previously cropped out. The combination of 5 translations, 2 flips, 3 scales, and 3 views  leads  to  90  predictions  which  slow  predictions  down  by  almost  an  order  of  magnitude.  To  rectify  this,  we  show  that  a  simple  greedy  algorithm  can  choose  a  subset  of  10  transforms  that predicts almost as well as all 90 and a subset of 15 that predicts slightly better.      3 . 1    P r e d i c t i o n s   a t   M u l t i p l e   S c a l e s    Images  contain  useful  predictive  elements  at  different  scales.  To  capture  this  we  make  predictions  at  three  different  scales.  We  use  the  original  256  scale  as  well  as  228  and  284.  Note  that  when  scaling an image  up, it is  important to  use a  good interpolation  method like  bicubic  scaling  and  not  to  use  anti  aliasing  filters  designed  for  scaling   images  down. When  scaling  down,  anti  aliasing  seems  to  help  a  little  bit  but  in  practice  we  used  bicubic  scaling  for up scaling and down scaling.     3 . 2    P r e d i c t i o n s   w i t h   M u l t i p l e   Vi e w s    In order to make use of all of the pixels in the image when making predi ctions, we generate  three different square image views. For an 256xN (Nx256) image, we generate a left (upper),  center, and right (lower) view of 256x256 pixels and then apply all crops, flips and scales to  each of these views. Figure 2 demonstrates how the three views are constructed.   Table  1  shows  the  effect  of  using  the  new  training  and  testing  transforms  compared  to  the  previous  baseline  result.  It  also  shows  that  the  new  model  architecture  with  doubled  fully  connected layers does not improve the top 5 error rate.          Figure 2: We generate predictions based on three different square views of the image to   incorporate all of the pixels and to take into account differing image sizes.          Krizhevsky et al Single Convnet [6]  New Training, Test Transforms  Double FC + New Training, Test Transforms   Val Top-1   Val Top-5   40.7%  37.5%  37.0%   18.2%  15.9%  15.8%   Table 1: Results for new training and testing transforms using the architecture of Krizhevsky  et al and the same model with double the size for all fully  connected layers. The larger fully   connected layers do not improve the top 5 validation error substantively.    3 . 3    R e d u c i n g   t h e   N u m b e r   o f   P r e d i c t i o n s       Our  final  combination  of  5  crops,  2  flips,  3  scales  and  3  views  yields  a  combination  of  90  predictions  per  model.  This  is  almost  an  order  of  magnitude  larger  than  the  10  predictions   that  were  used  previously  and  may  cause  an  unacceptable  delay  in  real  world  applications.  We  used  a  simple  greedy  algorithm  to  reduce  the  number  of  predictions  t o  an  acceptable  number.    The  simple  greedy  algorithm  starts  with  the  best  prediction  and  at  each  step  adds  another  prediction  until  there  is  no  additional  improvement.  This  algorithm  finds  that  the  first  10  predictions  are  almost  as  accurate  as  using  all  90  which  would  have  t he  same  run  time  as  previous methods that also only use 10 predictions. It is also able to find a slightly improved  combination  using  15  predictions  that  improves  on  the  90  prediction  baseline.  Figure  2  shows a plot of accuracy as more predictions are added.   This  simple  greedy  algorithm  is  easy  to  implement,  quick  to  run  and  has  no  parameters  to  tune.  It  avoids  some  of  the  pitfalls  inherent  in  various  weight  based  learning  methods   for  combining  predictions.  Because  most  of  the  predictions  are  very  similar  (almost  collinear),  methods such as stacking [9] or similar algorithms tend to have trouble and can create large  weights of opposing sign. Penalization based method (l1, l2 etc) can help but not completely  mitigate this effect. Table 2 shows the effect of the  new  test transformations and the results  of  the  greedy  algorithm.  Figure  3  shows  the  progression  of  the  greedy  algorithm  as  it  adds  models until convergence.       Val Top-1   Val Top-5   10 Predictions: 5 Crops, 2 Flips   30 Predictions: 5 Crops, 2 Flips, 3 Scales   30 Predictions: 5 Crops, 2 Flips, 3 Views   90 Predictions: 5 Crops, 2 Flips, 3 Scales, 3 Views   10 Predictions: Greedy  15 Predictions: Greedy   39.1%   38.3%   37.7%   37.1%   37.2%  37.1%   17.4%   16.7%   16.4%   15.9%   16.0%  15.9%   Table 2: This table shows results for additional test time transformations and combinations   with a greedy algorithm.               e t a R r o r r E 5 p o T        10 Predictions 90 Predictions Greedy Combined Predictions  19%  18%  17%  16%  15%  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  Number of Predictions      Figure 3: This figure shows the accuracy of the greedy selection algorithm as it adds more   predictions compared to the baseline 10 predictions and the full 90 predictions .     4    H i g h e r  R e s o l u t i o n   M o d e l s    Objects  in  images  can  exist  at  different  scales.  In  the  previous  section  we  run  our  trained  network  on  scaled  up  and  scaled  down  versions  of  the  image.  However,  once  the  image  is  scaled  up  too  much,  the  trained  network  no  longer  performs  well.  To  search  fo r  objects  in  images in scaled up higher resolution images we need to retrain the network at that scale.  In  practice, previously trained models can be used to initialize higher resolution models and cut  training time down substantially to 30 epochs from 90  epochs. Higher resolution models are  complementary  to  base  models  and  a  single  high  resolution  model  is  as  valuable  as  four  additional  base  models  as  shown  in Table  3.  Complementary  models  have  been  found  to  be  valuable in other large scale data contests such as the Netflix Prize [1].    4 . 1   M o d e l   D e t a i l s    In previous sections, models are trained on 224x224 patches cropped from 256xN (Nx256)  images. For the higher resolution models, we wish to train on larger images using the same  model structure. We build models on 448xN (Nx448) images using crops of 224x224. This  ensures that at test time, the four corner crops do not overlap.  In practice it may not be  practical to store images at multiple resolutions or they may not be available. So to reuse  256xN (Nx256) training images we take 128x128 sized patches and scale them up to be  224x224. This simulates using 224x224 patches in a 448xN (Nx448) image without needing  access to separate training images.    We use an identical model structure and training methods as the lower resolution models.  Because the model is identical in structure, we can initialize the higher resolution models  with fully trained low resolution models. This allows us to train the new model in 30 epochs  rather than the previous 90 epochs. We start with a step size of 0.001 and divide the step size  by 10 once the validation score plateaus. We reduce the step size twice before convergence.  Because there are effectively more training patches due to t he smaller patch size relative to  image size, drop out is not as important. In practice we use drop out for the initial training  and the first step size reduction to 0.0001, we then turn off drop out to finish the training at  0.0001. Then the model is finished training with drop out turned off and the step size  reduced to 0.00001. This gives better results than training with drop out alone or without  drop out.      When  making  predictions  on  new  images,  we  can  make  use  of  more  image  patches.  Previously  5  crops  (or  translations)  are  used  when  making  predictions.  Because  these  crops   have less overlap for the higher resolution model, we find it useful to increase this to 9 crops  by adding the middle top, middle bottom, left middle and right middle. This brings th e total  number  of  predictions  per  model  up  to  162  (9  crops,  2  flips,  3  scales,  3  views)  making  the  greedy selection algorithm from section 3.3 very important.           One Base Net   Two Base Nets   One High Resolution Net   One Base Net + One High Resolution Net   Five Base Nets   Val Top-1   Val Top-5   37.0%   35.9%   36.8%   34.9%   35.2%   15.8%   15.1%   16.2%   14.5%   14.5%   Table 3: The combination of base model and high resolution model is  better than five base   models for top-1 classification and equivalent for top-5 classification.      5    R e s u l t s    The final image classification system submitted to ILSVRC2013 was composed of 10 neural  networks  made  up  of  5  base  models  and  5  high  resolution  models  and  had  a  test  set  top  5  error  rate  of  13.6%.  This  is  an  improvement  on  the  previous  state  of  the  art  of  16.4%  but  short  of  the  best  result  of  11.7%.  The  methods  described  in  this  paper  should  be  able  improve on the current state of the art of 11.7%. Results are summarized in Table 4.            Val Top-1   Val Top-5   Test Top-5   Krizhevsky et al [] Five Nets   Five Base Nets   Five High Resolution Nets   Five Base + Five High Resolution Nets   Clarifai   38.1%   35.2%   35.3%   33.7%   -   16.4%   14.5%   15.1%   13.7%   -   16.4%   -   -   13.6%   11.7%   Table 4: This table shows a comparison of the proposed methods to the previous year’s   winner Krizhevsky et al [6] and current year’s winner Clarifai. The improvements proposed   could also improve on the Clarifai system.     6    C o n c l u s i o n    In  this  paper  we  showed  a  number  of  ways  to  improve  neural  network  based  image  classification  systems.  We  first  showed  some  new  useful  image  transformations  to  increase  the effective  size of the training set. These were based on using more of the image to  select  training  crops  and  additional  color  manipulations.  We  also  showed  useful  image  transformations  for  generating  testing  predictions.  We  made  predictions  at  different  scales  and  generated  predictions  on  different  views  of  the  image. These  additional  pr edictions  can  slow  down  the  system  so  we  showed  a  simple  greedy  algorithm  that  reduces  the  number  of  predictions  needed.  Finally,  we  showed  an  efficient  way  to  train  higher  resolution  models  that  generate  useful  complementary  predictions.  A  single  base  model  and  a  single  high  resolution  model  are  as  good  as  5  base  models.  These  improvements  to  the  image  classification  pipeline  are  easy  to  implement  and  should  be  able  to  improve  other  convolutional neural network based image classification systems.    R e f e r e n c e s    [1]  R.M.  Bell,  Y.  Koren.  Lessons  from  the  Netflix  prize  challenge.  ACM  SIGKDD  Explorations  Newsletter, 9(2):75-79, 2007.   [2]  D.C.  Ciresan,  J.  Meier,  and  J.  Schmidhuber.  Multi-column  deep  neural  networks  for  image  classification. CVPR, 2012.   [3] J. Deng, W. Dong, R. Socher, L.J. Li, and L. Fei-Fei. Imagenet: A Large-Scale Hierarchical Image  Database. CVPR, 2009.   [4] N. Gunji, T. Higuchi, K. Yasumoto, H. Muraoka, Y. Ushiku, T. Harada, and Y. Kuniyoshi .  Classifier entry. ILSVRC-2012, 2012.   [5] G.E. Hinton, N. Srivastave, A. Sutskever, I. Sutskever, and R.R. Salakhutdinov. Improving neural  networks by preventing co-adaptation of feature detectors. arXiv:1207.0580, 2012.   [6] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural  networks. Advances in Neural Information Processing Systems 25, 2012.   [7]  Y.  LeCun,  B.  Boser,  J.S.  Denker,  D.  Henderson,  R.E.  Howard,  W.  Hubbard,  and  L.D.  Jackel.  Backpropagation  applied  to  handwritten  zip  code  recognition .  Neural  Computation,  1(4):541-551,  1989.   [8] O. Russakovsky,  J. Deng, J. Krause, A. Berg, F.  Li.  ILSVRC-2013, 2013, URL  http://www.image- net.org/challenges/LSVRC/2013/.   [9] D. Wolpert. Stacked generalization. Neural Networks, 5:2 241-260, 1992.   [10]  M.  Zeiler,  R.  Fergus.  Visualizing  arXiv:1311.2901v3, 2013.   and  Understanding  Convolutional  Networks.               ","We investigate multiple techniques to improve upon the current state of theart deep convolutional neural network based image classification pipeline. Thetechiques include adding more image transformations to training data, addingmore transformations to generate additional predictions at test time and usingcomplementary models applied to higher resolution images. This paper summarizesour entry in the Imagenet Large Scale Visual Recognition Challenge 2013. Oursystem achieved a top 5 classification error rate of 13.55% using no externaldata which is over a 20% relative improvement on the previous year's winner."
1312.5604,2014,Learning Transformations for Classification Forests  ,"['Qiang Qiu', 'Guillermo Sapiro']",https://arxiv.org/pdf/1312.5604.pdf,"Learning Transformations for Classiﬁcation Forests  4 1 0 2     b e F 6         ]  V C . s c [      2 v 4 0 6 5  .  2 1 3 1 : v i X r a  Qiang Qiu Department of Electrical and Computer Engineering, Duke University, Durham, NC 27708, USA Guillermo Sapiro Department of Electrical and Computer Engineering, Department of Computer Science, Department of Biomedical Engineering, Duke University, Durham, NC 27708, USA  Abstract  This work introduces a transformation-based learner model for classiﬁcation forests. The weak learner at each split node plays a crucial role in a classiﬁcation tree. We propose to optimize the splitting objective by learning a linear transfor- mation on subspaces using nuclear norm as the optimization criteria. The learned linear trans- formation restores a low-rank structure for data from the same class, and, at the same time, max- imizes the separation between different classes, thereby improving the performance of the split function. Theoretical and experimental results support the proposed framework.  1. Introduction Classiﬁcation Forests (Breiman, 2001; Criminisi & Shot- ton, 2013) have recently shown great success for a large va- riety of classiﬁcation tasks, such as pose estimation (Shot- ton et al., 2012), data clustering (Moosmann et al., 2007), and object recognition (Gall & Lempitsky, 2009). A classi- ﬁcation forest is an ensemble of randomized classiﬁcation trees. A classiﬁcation tree is a set of hierarchically con- nected tree nodes, i.e., split (internal) nodes and leaf (ter- minal) nodes. Each split node is associated with a different weak learner with binary outputs (here we focus on binary trees). The splitting objective at each node is optimized us- ing the training set. During testing, a split node evaluates each arriving data point and sends it to the left or right child based on the weak learner output. The weak learner associated with each split node plays a crucial role in a classiﬁcation tree. An analysis of  QIANG.QIU@DUKE.EDU  GUILLERMO.SAPIRO@DUKE.EDU  the effect of various popular weak learner models can be found in (Criminisi & Shotton, 2013), including decision stumps, general oriented hyperplane learner, and conic sec- tion learner. In general, even for high-dimensional data, we usually seek for low-dimensional weak learners that sepa- rate different classes as much as possible. High-dimensional data often have a small intrinsic dimen- sion. For example, in the area of computer vision, face im- ages of a subject (Basri & Jacobs, 2003), (Wright et al., 2009), handwritten images of a digit (Hastie & Simard, 1998), and trajectories of a moving object (Tomasi & Kanade, 1992), can all be well-approximated by a low- dimensional subspace of the high-dimensional ambient space. Thus, multiple class data often lie in a union of low- dimensional subspaces. These theoretical low-dimensional intrinsic structures are often violated for real-world data. For example, under the assumption of Lambertian re- ﬂectance, (Basri & Jacobs, 2003) show that face images of a subject obtained under a wide variety of lighting condi- tions can be accurately approximated with a 9-dimensional linear subspace. However, real-world face images are often captured under additional pose variations; in addition, faces are not perfectly Lambertian, and exhibit cast shadows and specularities (Cand`es et al., 2011). When data from the same low-dimensional subspace are arranged as columns of a single matrix, the matrix should be approximately low-rank. Thus, a promising way to han- dle corrupted underlying structures of realistic data, and as such, deviations from ideal subspaces, is to restore such low-rank structure. Recent efforts have been invested in seeking transformations such that the transformed data can be decomposed as the sum of a low-rank matrix compo- nent and a sparse error one (Peng et al., 2010; Shen & Wu, 2012; Zhang et al., 2011). (Peng et al., 2010) and  Learning Transformations for Classiﬁcation Forests  (Zhang et al., 2011) are proposed for image alignment (see (Kuybeda et al., 2013) for the extension to multiple-classes with applications in cryo-tomograhy), and (Shen & Wu, 2012) is discussed in the context of salient object detection. All these methods build on recent theoretical and computa- tional advances in rank minimization. In this paper, we present a new formulation for random forests, and propose to learn a linear discriminative trans- formation at each split node in each tree to improve the class separation capability of weak learners. We optimize the data splitting objective using matrix rank, via its nu- clear norm convex surrogate, as the learning criteria. We show that the learned discriminative transformation recov- ers a low-rank structure for data from the same class, and, at the same time, maximize the subspace angles between different classes. Intuitively, the proposed method shares some of the attributes of the Linear Discriminant Analy- sis (LDA) method, but with a signiﬁcantly different metric. Similar to LDA, our method reduces intra-class variations and increases inter-class separations to achieve improved data splitting. However, we adopt the matrix nuclear norm as the key criterion to learn a transformation, being this appropriate for data expected to be in (the union of) sub- spaces. As shown later, our method signiﬁcantly outper- forms the LDA method, as well as state-of-the-art learners in classiﬁcation forests. The learned transformations help in other classiﬁcation task as well, e.g., subspace based methods (Qiu & Sapiro, 2013).  2. Transformation Forests A classiﬁcation forest is an ensemble of binary classiﬁca- tion trees, where each tree consists of hierarchically con- nected split (internal) nodes and leaf (terminal) nodes. Each split node corresponds to a weak learner, and evalu- ates each arriving data point and sends it to the left or right child based on the weak learner binary outputs. Each leaf node stores the statistics of the data points that arrived dur- ing training. During testing, each classiﬁcation tree returns a class posterior probability for a test sample, and the forest output is often deﬁned as the average of tree posteriors. In this section, we introduce transformation learning at each split node to dramatically improve the class separation ca- pability of a weak learner. Such learned transformation is virtually computationally free at the testing time.  2.1. Learning Transformation Learners Consider two-class data points Y = {yi}N i=1 ⊆ Rd, with each data point yi in one of the C low-dimensional sub- spaces of Rd, and the data arranged as columns of Y. We assume the class labels are known beforehand for training purposes. Y+ and Y− denote the set of points in each of the two classes respectively, points again arranged as  columns of the corresponding matrix. We propose to learn a d × d linear transformation T,1 min||TY+||∗ + ||TY−||∗ − ||T[Y+, Y−]||∗, s.t.||T||2 = 1,  arg T  (1)  where [Y+, Y−] denotes the concatenation of Y+ and Y−, and ||·||∗ denotes the matrix nuclear norm, i.e., the sum of the singular values of a matrix. The nuclear norm is the convex envelop of the rank function over the unit ball of matrices (Fazel, 2002). As the nuclear norm can be op- timized efﬁciently, it is often adopted as the best convex approximation of the rank function in the literature on rank optimization (see, e.g., (Cand`es et al., 2011) and (Recht et al., 2010)). The normalization condition ||T||2 = 1 pre- vents the trivial solution T = 0; however, the effects of different normalizations is interesting and is the subject of future research. Throughout this paper we keep this partic- ular form of the normalization which was already proven to lead to excellent results. As shown later, such linear transformation restores a low- rank structure for data from the same class, and, at the same time, maximizes the subspace angles between classes. In this way, we reduce the intra-class variation and introduce inter-class separations to improve the class separation ca- pability of a weak learner.  2.2. Theoretical Analysis  One fundamental factor that affects the performance of weak learners in a classiﬁcation tree is the separation be- tween different class subspaces. An important notion to quantify the separation between two subspaces Si and Sj is the smallest principal angle θij (Miao & Ben-Israel, 1992), (Elhamifar & Vidal, 2013), deﬁned as  u(cid:48)v  (2)  arccos  θij = min  ||u||2||v||2  .  u∈Si,v∈Sj Note that θij ∈ [0, π 2 ]. We show next that the learned trans- formation T using the objective function (1) maximizes the angle between subspaces of different classes, leading to im- proved data splitting in a tree node. We start by presenting some basic norm relationships for matrices and their corre- sponding concatenations. Theorem 1. Let A and B be matrices of the same row dimensions, and [A, B] be the concatenation of A and B, we have  ||[A, B]||∗ ≤ ||A||∗ + ||B||∗,  with equality obtained if the column spaces of A and B are orthogonal.  1We can also consider learning a s × d matrix, s < d, and  simultaneously reducing the data dimension.  Learning Transformations for Classiﬁcation Forests  (a)(cid:2)θAB = 0.79,  θBC = 1.05(cid:3)  Y+ = {A(green), B(blue)}, Y− = {C(red)}.  θAC = 0.79,   0.42 (cid:2)θAB = 0.006,  (b) T =  0.39 −0.17 −0.14  0.33 −0.13 0.32 −0.16 0.81 θAC = 1.53,  ; θBC = 1.53(cid:3).  (cid:20)θAB = 1.05,  θBC = 1.32,  θAC = 1.05, (c) θBD = 1.39, Y+ = {A(blue), B(light blue)}, Y− = {C(yellow), D(red)}.  θAD = 1.05, θCD = 0.53  (cid:21)  ,  0.08 −0.03 0.04 −0.16 0.98  0.18 −0.03 −0.01  ;   0.48 (cid:20)θAB = 0.03,  θBC = 1.41,  (d) T =  θAC = 1.41, θBD = 1.41,  θAD = 1.40, θCD = 0.01  (cid:21)  .  Figure 1. Learning transformation T using (1). We denote the angle between subspaces A and B as θAB (and analogous for the other pairs of subspaces). As indicated in (a) and (c), we assign subspaces to different classes Y+ and Y−. Using (1), we transform subspaces in (a),(c) to (b),(d) respectively. We observe that the learned transformation T increases the inter-class subspace angle towards the maximum π  2 , and reduces intra-class subspace angle towards the minimum 0.  Proof. See Appendix A.  Based on this result we have that  ||TY+||∗ + ||TY−||∗ − ||T[Y+, Y−]||∗ ≥ 0,  (3)  and the proposed objective function (1) reaches the min- imum 0 if the column spaces of two classes are orthogo- nal after applying the learned transformation T; or equiva- lently, (1) reaches the minimum 0 when the angle between subspaces of two classes is maximized after transforma- tion, i.e., the smallest principal angle between subspaces 2 . equals π We now discuss the advantages of adopting the nuclear norm in (1) over the rank function and other (popular) ma- trix norms, e.g., the induced 2-norm and the Frobenius norm. When we replace the nuclear norm in (1) with the  rank function, the objective function reaches the minimum when subspaces are disjoint, but not necessarily maximally distant. If we replace the nuclear norm in (1) with the in- duced 2-norm norm or the Frobenius norm, as shown in Appendix B, the objective function is minimized at the triv- ial solution T = 0, which is prevented by the normalization condition ||T||2 = 1. Thus, we adopt the nuclear norm in (1) for two major ad- vantages that are not so favorable in the rank function or other (popular) matrix norms: (a) The nuclear norm is the best convex approximation of the rank function (Fazel, 2002), which helps to reduce the variation within classes (ﬁrst term in (1)); (b) The objective function (1) is in gen- eral optimized when the distance between subspaces of different classes is maximized after transformation, which helps to introduce separations between the classes.  00.20.40.60.8100.20.40.60.800.10.20.30.40.50.60.70.8Original subspaces00.020.040.060.080.100.050.1−0.04−0.0200.020.040.060.08Transformed subspaces00.20.40.60.81−0.500.5100.20.40.60.81Original subspaces00.020.040.060.08−0.0100.010.020.03−0.0200.020.040.060.080.10.12Transformed subspacesLearning Transformations for Classiﬁcation Forests  2.3. Synthetic Examples  We now illustrate the properties of the above mentioned learned transformation T using synthetic examples in Fig. 1 (real-world examples are presented in Section 3). We adopt a simple gradient descent optimization method (though other modern nuclear norm optimization tech- niques could be considered) to search for the transforma- tion matrix T that minimizes (1). As shown in Fig. 1, the learned transformation T via (1) increases the inter-class 2 , and reduces intra- subspace angle towards the maximum π class subspace angle towards the minimum 0.  2.4. Transformation Learner Model for a Classiﬁcation  Tree  i  i  i or D−  i and D−  i and Y−  that gives the smaller reconstruction error.  During training, at the i-th split node, we denote the ar- riving training samples as Y+ i . When more than two classes are present at a node, we randomly di- vide classes into two categories. This step is to purposely introduce node randomness to avoid duplicated trees as dis- cussed later. We then learn a transformation matrix Ti us- i and TiY− ing (1), and represent the subspaces of TiY+ i as D+ respectively. The weak learner model at i , D− the i-th split node is now deﬁned as θi = (Ti, D+ i ). During both training and testing, at the i-th split node, each arriving sample y uses Tiy as the feature, and is assigned to D+ Various techniques are available to perform the above implementation, we obtain D+ evaluation. and D− i using the K-SVD method (Aharon et al., 2006) and denote a transformation learner as θi = i )†, D− i )†), where D† = (DTD)−1DT. (Ti, D+ i (D+ |Tiy − The split evaluation of a test sample y, i )†Tiy|, only involves matrix multiplication, D+ which is of low computational complexity at the testing time. Given a data point y ⊆ Rd, in this paper, we considered a square linear transformation T of size d × d. Note that, if we learn a “fat” linear transformation T of size r × d, where (r < d), we enable dimension reduction along with transformation to handle very high-dimensional data.  i (D−  i (D+  In our  i  each split node into two categories (given more than two classes), to obtain the training sets Y+ and Y−. In (1), we learn a transformation optimized for a two-class problem. This randomly class dividing strategy reduces a multi-class problem into a two-class problem at each node for transfor- mation learning; furthermore, it introduces node random- ness to avoid generating duplicated trees. Note that (1) is non-convex and the employed gradient descent method converges to a local minimum. Initializing the transforma- tion T with different random matrices might lead to differ- ent local minimum solutions. The identity matrix initial- ization of T in this paper leads to excellent performance, however, understanding the node randomness introduced by adopting different initializations of T is the subject of future research.  3. Experimental Evaluation This section presents experimental evaluations using pub- lic datasets: the MNIST handwritten digit dataset, the Ex- tended YaleB face dataset, and the 15-Scenes natural scene dataset. The MNIST dataset consists of 8-bit grayscale handwritten digit images of “0” through “9” and 7000 ex- amples for each class. The Extended YaleB face dataset contains 38 subjects with near frontal pose under 64 light- ing conditions (Fig. 2). All the images are resized to 16×16 for the MNIST and the Extended YaleB datasets, which gives a 256-dimensional feature. The 15-Scenes dataset contains 4485 images falling into 15 natural scene cate- gories (Fig. 3). The 15 categories include images of living rooms, kitchens, streets, industrials, etc. We also present results for 3D data from the Kinect datatset in (Denil et al., 2013). We ﬁrst compare many learners in a tree context for accuracy and testing time; then we compare with learners that are common for random forests.  3.1. Illustrative Examples  2.5. Randomness in the Model: Transformation Forest  During the training phase, we introduce randomness into the forests through a combination of random training set sampling and randomized node optimization. We train each classiﬁcation tree on a different randomly selected training set. As discussed in (Breiman, 2001; Criminisi & Shotton, 2013), this reduces possible overﬁtting and improves the generalization of classiﬁcation forests, also signiﬁcantly re- ducing the training time. The randomized node optimiza- tion is achieved by randomly dividing classes arriving at  Figure 2. Example illumination in the extended YaleB dataset.  We construct classiﬁcation trees on the extended YaleB face dataset to compare different learners. We split the dataset into two halves by randomly selecting 32 light- ing conditions for training, and the other half for testing. Fig. 4 illustrates the proposed transformation learner model in a classiﬁcation tree constructed on faces of all 38 sub- jects. The third column shows that transformation learn- ers at each split node enforce separation between two ran- domly selected categories, and clearly demonstrates how data in each class is concentrated while the different classes  Learning Transformations for Classiﬁcation Forests  (a) .  (b) Original.  (c) Transformed.  (d) .  (e) .  (f) Original.  (g) Transformed.  (h) .  (i) .  (j) Original.  (k) Transformed.  (l) .  Figure 4. Transformation learners in a classiﬁcation tree constructed on faces of 38 subjects. The root split node is shown in the ﬁrst row and its two child nodes are in the 2nd and 3rd rows. The ﬁrst column denotes training samples in the original subspaces, with different classes (subjects) in different colors. For visualization, the data are plotted with the dimension reduced to 3 using Laplacian Eigenmaps (Belkin & Niyogi, 2003). As shown in the second column, we randomly divide arriving classes into two categories and learn a discriminative transformation using (1). The transformed samples are shown in the third column, clearly demonstrating how data in each class is concentrated while the different classes are separated. The fourth column shows the ﬁrst dimension of transformed samples in the third column.  a node. In Table 1, we construct classiﬁcation trees with a max- imum depth of 9 using different learners ( no maximum depth is deﬁned for the C4.5 tree.). For reference pur- pose, we also include the performance of several subspace learning methods, which provide state-of-the-art classiﬁ- cation accuracies on this dataset. Using a single classi- ﬁcation tree, the proposed transformation learner already signiﬁcantly outperforms the popular weak learners deci- sion stump and conic section (Criminisi & Shotton, 2013), where 100 trees are used (30 tries are adopted here). We ob- serve that the proposed learner also outperforms more com- plex split functions SVM and LDA. The identity learner denotes the proposed framework but replacing the learned transformation with the identity matrix. Using a single tree, the proposed approach already outperforms state-of-the-art results reported on this dataset. As shown later, with ran- domness introduced, the performance in general increases further by employing more trees. While our learner has higher complexity compared to weak learners like decision stump, the performance for random forests is judged by the accuracy and test time. Increasing the number of trees (sublinearly) increases accuracy, at the  Figure 3. 15-Scenes natural scene dataset.  are separated. A maximum tree depth is typically speciﬁed for random forests to limit the size of a tree (Criminisi & Shotton, 2013), which is different from algorithms like C4.5 (Quin- lan, 1993) that grow the tree only relying on termination criterion. The tree depth in this paper is the maximum tree depth. To avoid under/over-ﬁtting, we choose the maxi- mum tree depth through a validation process. We also im- plement additional termination criteria to prevent further training of a branch, e.g., the number of samples arriving at  −0.02−0.015−0.01−0.00500.0050.01−0.02−0.0100.010.02−0.02−0.0100.010.020.030.04−0.02−0.015−0.01−0.00500.0050.01−0.02−0.0100.010.02−0.02−0.0100.010.020.030.04−0.02−0.0100.010.020.030.04−0.0500.050.10.15−0.1−0.08−0.06−0.04−0.0200.020100200300400500600−0.015−0.01−0.00500.0050.010.0150.020.0250.030.035−0.0100.010.020.03−0.02−0.0100.010.020.03−0.02−0.0100.010.020.030.040.050.06−0.0100.010.020.03−0.02−0.0100.010.020.03−0.02−0.0100.010.020.030.040.050.06−0.03−0.02−0.0100.010.020.03−0.0500.05−0.04−0.0200.020.040.060.08050100150200250300350−0.025−0.02−0.015−0.01−0.00500.0050.010.0150.020.025−0.02−0.0100.010.020.03−0.02−0.0100.010.020.03−0.0200.020.040.060.08−0.02−0.0100.010.020.03−0.02−0.0100.010.020.03−0.0200.020.040.060.08−0.06−0.04−0.0200.02−0.1−0.0500.050.1−0.08−0.06−0.04−0.0200.020.04050100150200250300350−0.06−0.05−0.04−0.03−0.02−0.0100.010.02Learning Transformations for Classiﬁcation Forests  (a) MNIST.  (b) 15-Scenes.  (c) Kinect.  Figure 5. Classiﬁcation accuracy using transformation learner forests.  Table 1. Classiﬁcation accuracies (%) and testing time for the Extended YaleB dataset using classiﬁcation trees with different learners.  Method  Accuracy (%)  Testing time (s)  Non-tree based methods D-KSVD (Zhang & Li, 2010) LC-KSVD (Jiang et al., 2011) SRC (Wright et al., 2009) Classiﬁcation trees Decision stump (1 tree) Decision stump (100 trees) Conic section (1 tree) Conic section (100 trees) C4.5 (1 tree) (Quinlan, 1993) LDA (1 tree) LDA (100 trees) SVM (1 tree) Identity learner (1 tree) Transformation learner (1 tree)  94.10 96.70 97.20  28.37 91.77 8.55 78.20 39.14 38.32 94.98 95.23 84.95 98.77  - - -  0.09 13.62 0.05 5.04 0.21 0.12 7.01 1.62 0.29 0.15  cost of (linearly) increased test time (Criminisi & Shotton, 2013). As shown in Table 1, our learner exhibits similar test time as other weaker learners, but with signiﬁcantly im- proved accuracy. By increasing the number of trees, other learners may approach our accuracy but at the cost of or- ders of magnitude more test time. Thus, the fact that 1-2 orders of magnitude less trees with our learned matrix out- performs standard random forests illustrates the importance of the proposed general transform learning framework.  3.2. Randomized Trees  We now evaluate the effect of random training set sam- pling using the MNIST dataset. The MNIST dataset has a training set of 60000 examples, and a test set of 10000 examples. We train 20 classiﬁcation trees with a depth of 9, each using only 10% randomly selected training sam-  ples (In this paper, we select the random training selection rate to provide each tree about 5000 training samples). As shown in Fig. 5a, the classiﬁcation accuracy increases from 93.74% to 97.30% by increasing the number of trees to 20. Fig. 6 illustrates in detail the proposed transformation learner model in one of the trees. As discussed, increas- ing the number of trees (sublinearly) increases accuracy, at the cost of (linearly) increased test time. Though report- ing a better accuracy with hundreds of trees is an option (with limited pedagogical value), a few (∼20) trees are suf- ﬁcient to illustrate the trade-off between accuracy and per- formance. Using the 15-Scenes dataset in Fig. 3, we further evaluate the effect of randomness introduced by randomly dividing classes arriving at each split node into two categories. We randomly use 100 images per class for training and used the remaining data for testing. We train 20 classiﬁcation trees with a depth of 5, each using all training samples. As shown in Fig. 5b, the classiﬁcation accuracy increases from 66.23% to 79.06% by increasing the number of trees to 20. We notice that, with only 20 trees, the accuracy is already comparable to state-of-the-art results reported on this dataset shown in Table 2. We in general expect the performance increases further by employing more trees.  Table 2. Classiﬁcation accuracies (%) for the 15-Scenes dataset.  Method ScSPM (Yang et al., 2009) KSPM (Lazebnik et al., 2006) KC (Gemert et al., 2008) LSPM (Yang et al., 2009) Transformation forests  Accuracy (%) 80.28 76.73 76.67 65.32 79.06  3.3. Microsoft Kinect  We ﬁnally evaluate the proposed transformation learner in the task of predicting human body part labels from a depth  51015200.20.40.60.8The number of trees (depth=9)Classification accuracy  Transformation learnerDecision stumpConic section51015200.20.40.60.8The number of trees (depth=5)Classification accuracy  Transformation learnerDecision stumpConic section510152025300.20.40.60.8The number of trees (depth=9)Classification accuracy  Transformation learnerDecision stumpConic sectionLearning Transformations for Classiﬁcation Forests  (a) .  (b) Original.  (c) Transformed.  (d) .  (e) .  (f) Original.  (g) Transformed.  (h) .  (i) .  (j) Original.  (k) Transformed.  (l) .  Figure 6. Transformation-based learners in a classiﬁcation tree constructed on the MNIST dataset. The root split node is shown in the ﬁrst row and its two child nodes are in the 2nd and 3rd rows. The ﬁrst column denotes training samples in the original subspaces, with different classes in different colors. For visualization, the data are plotted with the dimension reduced to 3 using Laplacian Eigenmaps (Belkin & Niyogi, 2003). As shown in the second column, we randomly divide arriving classes into two categories and learn a discriminative transformation using (1). The transformed samples are shown in the third column, clearly demonstrating how data in each class is concentrated while the different classes are separated. The fourth column shows the ﬁrst dimension of transformed samples in the third column.  mization criteria, we learn a transformation at each split node that reduces variations/noises within the classes, and increases separations between the classes. The ﬁnal classi- ﬁcation results combines multiple random trees. Thereby we expect the proposed framework to be very robust to noise. We demonstrated the effectiveness of the proposed learner for classiﬁcation forests, and provided theoretical support to these experimental results reported for very di- verse datasets.  image. We adopt the Kinect datatset provided in (Denil et al., 2013), where pairs of 640 × 480 resolution depth and body part images are rendered from the CMU mocap dataset. The 19 body parts and one background class are represented by 20 unique color identiﬁers in the body part image. For this experiment, we only use the 500 testing poses from this dataset. We use the ﬁrst 450 poses for train- ing and remaining 50 poses for testing. During training, we sample 10 pixels for each body part in each pose and pro- duce 190 data points for each depth image. Each pixel is represented using depth difference from its 96 neighbors with radius 8, 32 and 64 respectively, forming a 288-dim descriptor. We train 30 classiﬁcation trees with a depth of 9, each using 5% randomly selected training samples. As shown in Fig. 5c, the classiﬁcation accuracy increases from 55.48% to 73.12% by increasing the number of trees to 30. Fig. 7 shows an example input depth image, the groud truth body parts, and the prediction using the proposed method.  4. Conclusion We introduced a transformation-based learner model for classiﬁcation forests. Using the nuclear norm as opti-  (a) Depth.  (b) Groundtruth.  (c) Prediction.  Figure 7. Body parts prediction from a depth image using trans- formation forests.  −0.02−0.0100.010.02−0.02−0.0100.010.02−0.03−0.02−0.0100.010.02−0.02−0.0100.010.02−0.02−0.0100.010.02−0.03−0.02−0.0100.010.02−0.03−0.02−0.0100.01−0.02−0.0100.01−0.03−0.02−0.0100.010.0202004006008001000120014001600−0.01−0.00500.0050.010.0150.020.025−0.01−0.00500.0050.010.0150.02−0.02−0.0100.01−0.03−0.02−0.0100.010.020.03−0.01−0.00500.0050.010.0150.02−0.02−0.015−0.01−0.00500.0050.01−0.03−0.02−0.0100.010.020.03−0.02−0.015−0.01−0.00500.0050.010.0150.02−0.01−0.00500.0050.010.0150.02−0.04−0.0200.020.040100200300400500600700800900−0.02−0.015−0.01−0.00500.0050.010.0150.02−0.02−0.015−0.01−0.00500.0050.01−0.04−0.0200.02−0.02−0.015−0.01−0.00500.0050.010.0150.02−0.02−0.015−0.01−0.00500.0050.01−0.04−0.0200.02−0.02−0.015−0.01−0.00500.0050.010.0150.02−0.02−0.015−0.01−0.00500.0050.01−0.02−0.0100.010.02−0.015−0.01−0.00500.0050.010.0150.020.025020040060080010001200−0.02−0.015−0.01−0.00500.0050.01Learning Transformations for Classiﬁcation Forests  A. Proof of Theorem 1 Proof. We know that ((Srebro et al., 2005)) F + ||V||2 F ).  (||U||2  ||A||∗ = min A=UV(cid:48)  U,V  1 2  We denote UA and VA the matrices that achieve the mini- mum; same for B, UB and VB; and same for the concate- nation [A, B], U[A,B] and V[A,B]. We then have  ||A||∗ = ||B||∗ =  1 2 1 2  (||UA||2 (||UB||2  F + ||VA||2 F ), F + ||VB||2 F ).  The matrices [UA, UB] and [VA, VB] obtained by con- catenating the matrices that achieve the minimum for A and B when computing their nuclear norm, are not neces- sarily the ones that achieve the corresponding minimum in the nuclear norm computation of the concatenation matrix [A, B]. It is easy to show that  ||[A, B]||2  F = ||A||2  F + ||B||2 F ,  F + ||V[A,B]||2 F )  where ||·||F denotes the Frobenius norm. Thus, we have ||[A, B]||∗ = 1 2 ≤ 1 2 1 2 1 2 1 2  (||U[A,B]||2 (||[UA, UB]||2 (||UA||2 (||UA||2 (||UB||2  F + ||UB||2 F + ||VA||2 F ) F + ||VB||2 F )  F + ||[VA, VB]||2 F ) F + ||VA||2 F + ||VB||2 F )  =  =  + = ||A||∗ + ||B||∗.  We now show the equality condition. We perform the sin- gular value decomposition of A and B as  (cid:20)ΣA 0 (cid:21) (cid:20)ΣB 0 (cid:21)  0  0  0  0  [VA1VA2](cid:48),  [VB1VB2](cid:48),  A = [UA1UA2]  B = [UB1UB2]  where the diagonal entries of ΣA and ΣB contain non-zero singular values. We have AA(cid:48) = [UA1UA2]  [UA1UA2](cid:48),  2  BB(cid:48) = [UB1UB2]  [UB1UB2](cid:48).  (cid:20)ΣA (cid:20)ΣB  0  0  2  (cid:21) (cid:21)  0 0  0 0  The column spaces of A and B are considered to be or- (cid:48)UB1 = 0. The above can be written thogonal, i.e., UA1 as  AA(cid:48) = [UA1UB1]  BB(cid:48) = [UA1UB1]  (cid:21) (cid:21)  0 0  (cid:20)ΣA (cid:20)0  0  2  0 0 ΣB  2  [UA1UB1](cid:48),  [UA1UB1](cid:48).  Then, we have [A, B][A, B](cid:48) = AA(cid:48) + BB(cid:48)  = [UA1UB1]  (cid:20)ΣA  0  (cid:21)  2  0 ΣB  2  [UA1UB1](cid:48).  The nuclear norm ||A||∗ is the sum of the square root of the singular values of AA(cid:48). Thus, ||[A, B]||∗ = ||A||∗ + ||B||∗.  B. Basic Propositions Proposition 2. Let A and B be matrices of the same row dimensions, and [A, B] be the concatenation of A and B, we have  ||[A, B]||2 ≤ ||A||2 + ||B||2,  with equality if at least one of the two matrices is zero. Proposition 3. Let A and B be matrices of the same row dimensions, and [A, B] be the concatenation of A and B, we have  ||[A, B]||F ≤ ||A||F + ||B||F ,  with equality if and only if at least one of the two matrices is zero.  References Aharon, M., Elad, M., and Bruckstein, A. K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation. IEEE Trans. on Signal Process- ing, 54(11):4311–4322, Nov. 2006.  Basri, R. and Jacobs, D. W. Lambertian reﬂectance and linear subspaces. IEEE Trans. on Patt. Anal. and Mach. Intell., 25(2):218–233, 2003.  Belkin, M. and Niyogi, P. Laplacian eigenmaps for dimen- sionality reduction and data representation. Neural Com- putation, 15:1373–1396, 2003.  Breiman, L. Random forests. Machine Learning, 45(1):  5–32, 2001.  Learning Transformations for Classiﬁcation Forests  Cand`es, E. J., Li, X., Ma, Y., and Wright, J. Robust prin- cipal component analysis? J. ACM, 58(3):11:1–11:37, June 2011.  Criminisi, A. and Shotton, J. Decision Forests for Com- puter Vision and Medical Image Analysis. Springer, 2013.  Denil, M., Matheson, D., and Nando, D. F. Consistency of online random forests. In International Conference on Machine Learning, 2013.  Elhamifar, E. and Vidal, R. Sparse subspace clustering: Al- gorithm, theory, and applications. IEEE Trans. on Patt. Anal. and Mach. Intell., 2013. To appear.  Fazel, M. Matrix Rank Minimization with Applications.  PhD thesis, Stanford University, 2002.  Gall, J. and Lempitsky, V. Class-speciﬁc hough forests for object detection. In Proc. IEEE Computer Society Conf. on Computer Vision and Patt. Recn., 2009.  Gemert, J. C., Geusebroek, J., Veenman, C. J., and Smeul- ders, A. W. Kernel codebooks for scene categoriza- tion. In Proc. European Conference on Computer Vision, 2008.  Hastie, T. and Simard, P. Y. Metrics and models for hand- written character recognition. Statistical Science, 13(1): 54–65, 1998.  Jiang, Z., Lin, Z., and Davis, L. S. Learning a discrimi- native dictionary for sparse coding via label consistent In Proc. IEEE Computer Society Conf. on K-SVD. Computer Vision and Patt. Recn., Colorado springs, CO, 2011.  Kuybeda, O., Frank, G. A., Bartesaghi, A., Borgnia, M., Subramaniam, S., and Sapiro, G. A collaborative frame- work for 3D alignment and classiﬁcation of heteroge- neous subvolumes in cryo-electron tomography. Journal of Structural Biology, 181:116–127, 2013.  Lazebnik, S., Schmid, C., and Ponce, J. Beyond bags of features: Spatial pyramid matching for recognizing nat- ural scene categories. In Proc. IEEE Computer Society Conf. on Computer Vision and Patt. Recn., 2006.  Miao, J. and Ben-Israel, A. On principal angles between subspaces in rn. Linear Algebra and its Applications, 171(0):81 – 98, 1992.  Moosmann, F., Triggs, B., and Jurie, F. Fast discriminative visual codebooks using randomized clustering forests. In Advances in Neural Information Processing Systems, 2007.  Peng, Y., Ganesh, A., Wright, J., Xu, W., and Ma, Y. RASL: Robust alignment by sparse and low-rank de- In Proc. composition for linearly correlated images. IEEE Computer Society Conf. on Computer Vision and Patt. Recn., San Francisco, USA, 2010.  Qiu, Q. and Sapiro, G. Learning transformations for clus-  tering and classiﬁcation. CoRR, abs/1309.2074, 2013.  Quinlan, J. Ross. C4.5: Programs for Machine Learning.  Morgan Kaufmann Publishers Inc., 1993.  Recht, B., Fazel, M., and Parrilo, P. A. Guaranteed min- imum rank solutions to linear matrix equations via nu- clear norm minimization. SIAM Review, 52(3):471–501, 2010.  Shen, X. and Wu, Y. A uniﬁed approach to salient object detection via low rank matrix recovery. In Proc. IEEE Computer Society Conf. on Computer Vision and Patt. Recn., Rhode Island, USA, 2012.  Shotton, J., Girshick, R., Fitzgibbon, A., Sharp, T., Cook, M., Finocchio, M., Moore, R., Kohli, P., Criminisi, A., Kipman, A., and Blake, A. Efﬁcient human pose esti- mation from single depth images. IEEE Trans. on Patt. Anal. and Mach. Intell., 99(PrePrints), 2012.  Srebro, N., Rennie, J., and Jaakkola, T. Maximum margin matrix factorization. In Advances in Neural Information Processing Systems, Vancouver, Canada, 2005.  Tomasi, C. and Kanade, T. Shape and motion from im- age streams under orthography: a factorization method. International Journal of Computer Vision, 9:137–154, 1992.  Wright, J., Yang, M., Ganesh, A., Sastry, S., and Ma, Y. Robust face recognition via sparse representation. IEEE Trans. on Patt. Anal. and Mach. Intell., 31(2):210–227, 2009.  Yang, J., Yu, K., Gong, Y., and Huang, T. Linear spatial pyramid matching using sparse coding for image classi- ﬁcation. In Proc. IEEE Computer Society Conf. on Com- puter Vision and Patt. Recn., 2009.  Zhang, Q. and Li, B. Discriminative k-SVD for dictionary In Proc. IEEE Computer learning in face recognition. Society Conf. on Computer Vision and Patt. Recn., San Francisco, CA, 2010.  Zhang, Z., Liang, X., Ganesh, A., and Ma, Y. TILT: trans- In Proc. Asian con- form invariant low-rank textures. ference on Computer vision, Queenstown, New Zealand, 2011.  ","This work introduces a transformation-based learner model for classificationforests. The weak learner at each split node plays a crucial role in aclassification tree. We propose to optimize the splitting objective by learninga linear transformation on subspaces using nuclear norm as the optimizationcriteria. The learned linear transformation restores a low-rank structure fordata from the same class, and, at the same time, maximizes the separationbetween different classes, thereby improving the performance of the splitfunction. Theoretical and experimental results support the proposed framework."
1312.5398,2014,Continuous Learning: Engineering Super Features With Feature Algebras  ,['Michael Tetelman'],https://arxiv.org/pdf/1312.5398.pdf,"4 1 0 2     b e F 7 1         ]  G L . s c [      2 v 8 9 3 5  .  2 1 3 1 : v i X r a  Continuous Learning: Engineering Super Features  With Feature Algebras  Michael Tetelman  Advanced Magic Technologies  Los Gatos, CA 95032, USA  michael.tetelman@gmail.com  Abstract  In this paper we consider a problem of searching a space of predictive models for a given training data set. We propose an iterative procedure for deriving a sequence of improving models and a corresponding sequence of sets of non-linear features on the original input space. After a ﬁnite number of iterations N , the non-linear features become 2N -degree polynomials on the original space. We show that in a limit of an inﬁnite number of iterations derived non-linear features must form an associative algebra: a product of two features is equal to a linear combination of features from the same feature space for any given input point. Because each iteration consists of solving a series of convex problems that contain all previous solutions, the likelihood of the models in the sequence is increasing with each iteration while the dimension of the model parameter space is set to a limited controlled value.  1 Introduction  This paper proposes a method of ﬁnding a sequence of improving models for a given ﬁxed data set that we call Continuous Learning. The method is based on iterative exploration of a space of mod- els that have a speciﬁc limited number of parameters, which correspond to non-linear polynomial features of an input space. The feature set is evolving with each iteration, so the most important features are selected from the current feature set then the reduced feature set is expanded to include higher degree polynomials while the dimension of the expanded feature space is limited or ﬁxed. Resulting features are computed recursively from iteration to iteration with different parameters of the recursions found for each execution of the iteration algorithm.  The paper is organized as follows:  To search a model space we need to compare different models and solutions. We use Bayesian framework that provides a natural way for model comparison [1]. Continuous Learning consists of a sequence of iteration cycles. Each iteration cycle has a number of steps. In ﬁrst of these steps, we use bootstrap method to obtain a set of sampled solutions [4] and parameter-feature duality as a method for exploring the feature space. We construct a model of a solution distribution and use Principal Component Analysis to select a subspace of the most important solutions and reduce dimensions of the parameter space. Then we use non-linear expansion of the feature space by adding tensor- features that are products of the principal features selected in the previous step. That concludes a deﬁnition of one iteration cycle.  Each iteration is recursively redeﬁning features that become non-linear functions on the original feature space. We analyzed a stationary solution of the iteration cycle and found that, in a limit of the inﬁnite number of iterations, features form a Feature Algebra. Different solutions of Feature Algebra deﬁne non-linear feature representations.  1  For the purpose of this work, we will consider a prediction problem setup. The goal is to ﬁnd a probability of a class label y for a given input x: P rob(y|x). The model class family is deﬁned by a conditional probability function P (y|x, w), where w is a pa- rameter vector. For a set of independent training data samples (yt, xt), t = 1 . . . tmax, the probability of labels for given inputs is deﬁned by the Bayesian integrals  B(y, {yt}|x, {xt}) =Z P (y|x, w)Yt  P (yt|xt, w)P0(w|r)dw,  B({yt}|{xt}) =Z Yt  P (yt|xt, w)P0(w|r)dw,  P rob(y|x) = B(y, {yt}|x, {xt})/B({yt}|{xt}),  (1)  (2)  (3)  where P0(w|r) is a prior probability of parameters w that guarantees existence and convergence of the Bayesian integrals in Equations 1 and 2 and it is normalized as follows:  Z P0(w|r)dw = 1.  (4)  Then the Bayesian integral in the Equation 2 is equal to a probability of labels {yt} for the given input vectors {xt} of the training set. The prior distribution P0(w|r) itself depends on some parameters r (hyper-parameters or regulariza- tion parameters). Then the Bayesian integrals in the Equations 1 and 2 also depend on regularization parameters r. Optimal values of the regularization parameters could be found by maximizing the probability of the training data given by the Bayesian integral in Equation 2. This is possible because due to normalization of the prior probability in the Equation 4, the Bayesian integrals include the contribution of the normalization factor that depends only on regularization parameters r. The regu- larization solution found by maximizing the probability of training data is equivalent to the solution for regularization parameters found by cross-validation.  For the purpose of this paper, it is sufﬁcient to estimate values of the Bayesian integrals in the Equations 1 and 2 using maximum likelihood approximation (MLA) by ﬁnding a solution wm that maximizes log-likelihood of the training set that includes the prior probability of parameters w:  wm = arg max  w  L(w), L(w) =(ln(P0(w)) +Xt  ln(P (yt|xt, w))) .  Then the Bayesian integral can be approximated by the maximum likelihood as follows  B({yt}|{xt}) ≈ exp(L(wm)).  2 Training and sampling noise  (5)  (6)  The training data are always a limited-size set or a selection from a limited-size set. For that reason it contains a sampling noise. The sampling noise affects solutions. This is easy to see by sub-sampling the training data: for each sampled training set the MLA solution is different in the Equation 5 as well as the value of the Bayesian integral B in the Equation 6. Our goal is to ﬁnd a solution that is the least dependent on the sampling noise and better represents an actual statistics of a source.  To achieve that, we can sample the training data to create a set of {Ts, s = 1 . . . 2tmax} and then ﬁnd an MLA solution ws {yt, xt, t ∈ Ts}.  the training sets for each sampled training set  2  ws = arg max  w (ln(P0(w)) + Xt∈Ts  ln(P (yt|xt, w))) , Bs ≈ exp(L(ws)).  (7)  Now we have a set of solutions {ws} which is a noisy representation of a source statistics. The probability of each solution is given by the value of the Bayesian integral on the solution and is equal to  P rob(ws) = eL(ws)/Xs′  eL(ws  ′ ).  (8)  The solution distribution in the Equation 8 is based on sampling of the original training data set. It is a variant of a bootstrap technique [4]. This type of methods is actively used in different forms to improve training and to ﬁnd a robust solution, that is less dependent on a sampling noise in the original training set. For example, see the dropout method that randomly omits hidden features during training [5] or the method of adding artiﬁcial noise or a corruption to a training data [2].  Instead of trying to ﬁnd a single best solution we use the bootstrap method here to obtain a distribu- tion of solutions.  3 Distribution of solutions  Let’s consider the set of solutions in the Equation 7 as samples from unknown distribution of solu- tions, where each solution ws has a weight exp (L (ws)) and the probability of the solution is given by the Equation 8. Then we can model this distribution of solutions by proposing a probability function P rob (ws |z ) with parameters z, which we can ﬁnd by maximizing by z the following log-likelihood  Xs  eL(ws) ln (P rob (ws |z )) /Xs  eL(ws).  Up until now we did not specify the model class distribution P (y |x, w ). For the following con- sideration, we will use logistic regression for model class distribution with binary label y = 0, 1 as follows  P (y |x, w ) =  exp (yw · f (x))  1 + exp (w · f (x))  ,  (9)  1xi is a product of the parameter vector w with the feature vector  where w · f (x) = w0 +Pi wi  f (x) which includes a bias feature 1. The use of the logistic regression here is not a limitation on the possible models. It is selected only for certainty and to avoid an unnecessary complication of the consideration. As we will see the following approach is applicable to any model class distribution that is a function of a scalar product of a feature vector and a parameter vector. Also it could be used for a model class distribution that is a function of multiple products of a feature vector and parameter vectors.  Using the Equation 9 we will ﬁnd a set of solutions deﬁned in the Equation 7 for each corresponding training set Ts. To model the distribution of solutions we will start by considering Gaussian model for the distribu- tion of solutions P rob (ws |z ). Then the model is deﬁned by mean  hwi =Xs  wseL(ws)/Xs  eL(ws)  3  (10)  and covariance matrix  cov (w) = h(w − hwi) ⊗ (w − hwi)i =  Xs  (ws − hwi) ⊗ (ws − hwi) eL(ws)/Xs  eL(ws).  (11)  We will use Principal Component Analysis (PCA) to separate important solutions from noise. That leads to the following representation of the parameter vector w:  w = V0 +Xα  V α 1  Uα,  (12)  where V0 = hwi and Uα are selected eigenvectors of the covariance matrix cov (w) indexed by α. The coordinates V α 1 span over the principal-component subspace that is deﬁned by selected eigenvectors. The selected eigenvectors correspond to a high-variance subspace, where eigenvalues of the covariance matrix cov (w) are larger than a certain threshold. The value of the threshold for selecting the principal components is a hyper-parameter that controls the dimension of the principal component space, which in practice is constrained by the available memory.  4 Iterating over sequence of models  The important property of the model class probability distribution is a parameter - feature duality: the parameters w for the model class distribution P (y |x, w ) are used only in a product form  w · f (x) = w0 +Xi  wi  1xi,  (13)  where f (x) are the original features. By considering solutions that are limited to the principal component space we can ﬁnd that the product is given by the following Equation  w · f (x) = (V0 · f (x)) +Xα  V α 1 (Uα · f (x)) .  (14)  We will have exactly the same product form here as in the Equation 13 when we will deﬁne new features F0(x), Fα(x) via original features f (x) as follows  F0 (x) = (V0 · f (x)) /|V0|, Fα (x) = (Uα · f (x)) ,  so the parameter-feature product will look like this  w · f (x) = V0F0 (x) +Xα  V α 1 Fα (x) ,  (15)  (16)  1 are new parameters for the model class distribution with re-deﬁned super-  where now V0 and V α features F0(x), Fα (x) from the Equation 15. The result of this step is that using PCA and redeﬁning features we reduced the original parameter space to a new smaller space.  Let’s now extend the parameter-feature space by adding products of the super-features  w · f (x) → V0F0(x) +Xα  1 Fα(x) + V 00 V α  2 F0(x)2 +Xα  V 0α  2 F0(x)Fα(x) +Xα,β  V αβ 2 Fα(x)Fβ (x).  (17)  4  By extending the feature space in the Equation 17, we increased the dimension of the parameter space by adding new parameters V2 and creating new features as non-linear (quadratic) functions of the previous features.  Now we can repeat the iteration cycle, which consists of the steps in the Table 1.  - - - - -  sample data get solution set select principal components redeﬁne features extend feature set by adding products of features.  Table 1: Iteration cycle  It is important to emphasize that  − At each iteration we have a model that is deﬁned on a new feature space and has a limited  deﬁned number of dimensions in its parameter space.  − At each iteration the feature space is a non-linear map of the original feature space. − Each iteration makes new super-features to be higher degree polynomials of the original  basic features.  − After N iterations new features are 2N -degree polynomials of the original features.  The expansions of the feature set by adding products of features were used in recently proposed sum-product networks [6] and Neural Tensor Networks [3].  5 Feature Algebra  To simplify notations, let’s allow the feature indices α, β to include value 0. Then the Equation 17 will look like this  w · f (x) →Xα  V α  1 Fα(x) +Xα,β  V αβ 2 Fα(x)Fβ (x).  (18)  The iterations will converge when the product of super-features Fα (x) in the Equation 18 could be expressed only as a linear combination of the super-features  Fα (x) Fβ (x) =Xγ  C γ  αβ Fγ (x) ,  where for α = 0 the super-feature Fα(x) is the bias super-feature F0(x). The Equation 19 deﬁnes a feature algebra with structure constants C γ αβ.  The feature algebra has following important properties:  1. It must be associative:  Fα (x) (Fβ (x) Fγ (x)) = (Fα (x) Fβ (x)) Fγ (x) ,  that property leads to major equations for structure constants:  Xµ  C µ  αβC ν  µγ =Xµ  C ν  αµC µ  βγ;  5  (19)  (20)  (21)  2. The super-feature space with the feature algebra is a complete linear vector space: due to the algebra, any function g(F ) on the super-feature space representable by power series is equal to a linear combination of the super-features with computable coefﬁcients Aα:  g (F (x)) =Xα  AαFα (x) .  (22)  The feature algebra deﬁned by the Equation 19 is not limited to polynomial functions, it could be any function set that satisﬁes the algebra Equation 19 with structure constants that are a solution of the Equation 21.  Simple examples of algebras that are deﬁned by Equations 19 and 21 are complex numbers and quaternions. Less trivial examples of such algebras are operator algebras that were successfully used in Statistical Physics of Phase Transitions and Quantum Field Theory.  6 Conclusions  We proposed an iterative procedure for generating non-linear features (super-features) that are high- degree polynomials on the original feature space after a ﬁnite number of iterations.  For a ﬁnite number of iterations, the non-linear super-features are deﬁned by sets of principal com- ponents selected at each iteration.  By selecting a small set of principal components, the dimensionality of a feature space is limited at each iteration while resulting super-features are highly non-linear (as polynomials of exponen- tially high with number of iterations degree). That contrasts with an approach when high-degree polynomials are used as the original features - which requires to ﬁnd a solution for an exponentially high-dimensional model.  In the limit of inﬁnite iterations, the super-features form a linear vector space with an associative algebra.  Acknowledgments  I am grateful to my wife Yelena for support.  References  [1] David Barber, Bayesian Reasoning and Machine Learning, Cambridge University Press, 2012. [2] Yoshua Bengio, Li Yao, Guillaume Alain, and Pascal Vincent, Generalized denoising auto-encoders as  generative models, CoRR abs/1305.6663 (2013).  [3] Danqi Chen, Richard Socher, Christopher D. Manning, and Andrew Y. Ng, Learning new facts from knowl-  edge bases with neural tensor networks and semantic word vectors, CoRR abs/1301.3618 (2013).  [4] Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The elements of statistical learning: data mining,  inference and prediction, 2 ed., Springer, 2009.  [5] Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov, Im-  proving neural networks by preventing co-adaptation of feature detectors, CoRR abs/1207.0580 (2012).  [6] Hoifung Poon and Pedro Domingos, Sum-product networks: A new deep architecture, CoRR  abs/1202.3732 (2012).  6  ","In this paper we consider a problem of searching a space of predictive modelsfor a given training data set. We propose an iterative procedure for deriving asequence of improving models and a corresponding sequence of sets of non-linearfeatures on the original input space. After a finite number of iterations N,the non-linear features become 2^N -degree polynomials on the original space.We show that in a limit of an infinite number of iterations derived non-linearfeatures must form an associative algebra: a product of two features is equalto a linear combination of features from the same feature space for any giveninput point. Because each iteration consists of solving a series of convexproblems that contain all previous solutions, the likelihood of the models inthe sequence is increasing with each iteration while the dimension of the modelparameter space is set to a limited controlled value."
1312.4400,2014,Network In Network  ,"['Min Lin', 'Qiang Chen', 'Shuicheng Yan']",https://arxiv.org/pdf/1312.4400.pdf,"4 1 0 2    r a  M 4         ] E N . s c [      3 v 0 0 4 4  .  2 1 3 1 : v i X r a  Network In Network  Min Lin1,2, Qiang Chen2, Shuicheng Yan2  1Graduate School for Integrative Sciences and Engineering  2Department of Electronic & Computer Engineering {linmin,chenqiang,eleyans}@nus.edu.sg  National University of Singapore, Singapore  Abstract  We propose a novel deep network structure called “Network In Network”(NIN) to enhance model discriminability for local patches within the receptive ﬁeld. The conventional convolutional layer uses linear ﬁlters followed by a nonlinear acti- vation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive ﬁeld. We in- stantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro net- works over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to uti- lize global average pooling over feature maps in the classiﬁcation layer, which is easier to interpret and less prone to overﬁtting than traditional fully connected lay- ers. We demonstrated the state-of-the-art classiﬁcation performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.  1  Introduction  Convolutional neural networks (CNNs) [1] consist of alternating convolutional layers and pooling layers. Convolution layers take inner product of the linear ﬁlter and the underlying receptive ﬁeld followed by a nonlinear activation function at every local portion of the input. The resulting outputs are called feature maps. The convolution ﬁlter in CNN is a generalized linear model (GLM) for the underlying data patch, and we argue that the level of abstraction is low with GLM. By abstraction we mean that the fea- ture is invariant to the variants of the same concept [2]. Replacing the GLM with a more potent nonlinear function approximator can enhance the abstraction ability of the local model. GLM can achieve a good extent of abstraction when the samples of the latent concepts are linearly separable, i.e. the variants of the concepts all live on one side of the separation plane deﬁned by the GLM. Thus conventional CNN implicitly makes the assumption that the latent concepts are linearly separable. However, the data for the same concept often live on a nonlinear manifold, therefore the represen- tations that capture these concepts are generally highly nonlinear function of the input. In NIN, the GLM is replaced with a ”micro network” structure which is a general nonlinear function approxi- mator. In this work, we choose multilayer perceptron [3] as the instantiation of the micro network, which is a universal function approximator and a neural network trainable by back-propagation. The resulting structure which we call an mlpconv layer is compared with CNN in Figure 1. Both the linear convolutional layer and the mlpconv layer map the local receptive ﬁeld to an output feature vector. The mlpconv maps the input local patch to the output feature vector with a multilayer percep- tron (MLP) consisting of multiple fully connected layers with nonlinear activation functions. The MLP is shared among all local receptive ﬁelds. The feature maps are obtained by sliding the MLP  1  (a) Linear convolution layer  (b) Mlpconv layer  Figure 1: Comparison of linear convolution layer and mlpconv layer. The linear convolution layer includes a linear ﬁlter while the mlpconv layer includes a micro network (we choose the multilayer perceptron in this paper). Both layers map the local receptive ﬁeld to a conﬁdence value of the latent concept.  over the input in a similar manner as CNN and are then fed into the next layer. The overall structure of the NIN is the stacking of multiple mlpconv layers. It is called “Network In Network” (NIN) as we have micro networks (MLP), which are composing elements of the overall deep network, within mlpconv layers, Instead of adopting the traditional fully connected layers for classiﬁcation in CNN, we directly output the spatial average of the feature maps from the last mlpconv layer as the conﬁdence of categories via a global average pooling layer, and then the resulting vector is fed into the softmax layer. In traditional CNN, it is difﬁcult to interpret how the category level information from the objective cost layer is passed back to the previous convolution layer due to the fully connected layers which act as a black box in between. In contrast, global average pooling is more meaningful and interpretable as it enforces correspondance between feature maps and categories, which is made possible by a stronger local modeling using the micro network. Furthermore, the fully connected layers are prone to overﬁtting and heavily depend on dropout regularization [4] [5], while global average pooling is itself a structural regularizer, which natively prevents overﬁtting for the overall structure.  2 Convolutional Neural Networks  Classic convolutional neuron networks [1] consist of alternatively stacked convolutional layers and spatial pooling layers. The convolutional layers generate feature maps by linear convolutional ﬁlters followed by nonlinear activation functions (rectiﬁer, sigmoid, tanh, etc.). Using the linear rectiﬁer as an example, the feature map can be calculated as follows:  fi,j,k = max(wT  k xi,j, 0).  (1)  Here (i, j) is the pixel index in the feature map, xij stands for the input patch centered at location (i, j), and k is used to index the channels of the feature map. This linear convolution is sufﬁcient for abstraction when the instances of the latent concepts are linearly separable. However, representations that achieve good abstraction are generally highly non- linear functions of the input data. In conventional CNN, this might be compensated by utilizing an over-complete set of ﬁlters [6] to cover all variations of the latent concepts. Namely, individual linear ﬁlters can be learned to detect different variations of a same concept. However, having too many ﬁlters for a single concept imposes extra burden on the next layer, which needs to consider all combinations of variations from the previous layer [7]. As in CNN, ﬁlters from higher layers map to larger regions in the original input. It generates a higher level concept by combining the lower level concepts from the layer below. Therefore, we argue that it would be beneﬁcial to do a better abstraction on each local patch, before combining them into higher level concepts. In the recent maxout network [8], the number of feature maps is reduced by maximum pooling over afﬁne feature maps (afﬁne feature maps are the direct results from linear convolution without  2   . . . . . .applying the activation function). Maximization over linear functions makes a piecewise linear approximator which is capable of approximating any convex functions. Compared to conventional convolutional layers which perform linear separation, the maxout network is more potent as it can separate concepts that lie within convex sets. This improvement endows the maxout network with the best performances on several benchmark datasets. However, maxout network imposes the prior that instances of a latent concept lie within a convex set in the input space, which does not necessarily hold. It would be necessary to employ a more general function approximator when the distributions of the latent concepts are more complex. We seek to achieve this by introducing the novel “Network In Network” structure, in which a micro network is introduced within each convolutional layer to compute more abstract features for local patches. Sliding a micro network over the input has been proposed in several previous works. For example, the Structured Multilayer Perceptron (SMLP) [9] applies a shared multilayer perceptron on different patches of the input image; in another work, a neural network based ﬁlter is trained for face detection [10]. However, they are both designed for speciﬁc problems and both contain only one layer of the sliding network structure. NIN is proposed from a more general perspective, the micro network is integrated into CNN structure in persuit of better abstractions for all levels of features.  3 Network In Network  We ﬁrst highlight the key components of our proposed “Network In Network” structure: the MLP convolutional layer and the global averaging pooling layer in Sec. 3.1 and Sec. 3.2 respectively. Then we detail the overall NIN in Sec. 3.3.  3.1 MLP Convolution Layers  Given no priors about the distributions of the latent concepts, it is desirable to use a universal func- tion approximator for feature extraction of the local patches, as it is capable of approximating more abstract representations of the latent concepts. Radial basis network and multilayer perceptron are two well known universal function approximators. We choose multilayer perceptron in this work for two reasons. First, multilayer perceptron is compatible with the structure of convolutional neural networks, which is trained using back-propagation. Second, multilayer perceptron can be a deep model itself, which is consistent with the spirit of feature re-use [2]. This new type of layer is called mlpconv in this paper, in which MLP replaces the GLM to convolve over the input. Figure 1 illustrates the difference between linear convolutional layer and mlpconv layer. The calculation performed by mlpconv layer is shown as follows:  T  xi,j + bk1, 0).  f 1 i,j,k1  f n i,j,kn  = max(w1 k1 ... = max(wn kn  T f n−1  i,j + bkn , 0).  (2)  Here n is the number of layers in the multilayer perceptron. Rectiﬁed linear unit is used as the activation function in the multilayer perceptron. From cross channel (cross feature map) pooling point of view, Equation 2 is equivalent to cas- caded cross channel parametric pooling on a normal convolution layer. Each pooling layer performs weighted linear recombination on the input feature maps, which then go through a rectiﬁer linear unit. The cross channel pooled feature maps are cross channel pooled again and again in the next layers. This cascaded cross channel parameteric pooling structure allows complex and learnable interactions of cross channel information. The cross channel parametric pooling layer is also equivalent to a convolution layer with 1x1 con- volution kernel. This interpretation makes it straightforawrd to understand the structure of NIN.  3  Figure 2: The overall structure of Network In Network. In this paper the NINs include the stacking of three mlpconv layers and one global average pooling layer.  Comparison to maxout layers: the maxout layers in the maxout network performs max pooling across multiple afﬁne feature maps [8]. The feature maps of maxout layers are calculated as follows:  fi,j,k = max  m  (wT km  xi,j).  (3)  Maxout over linear functions forms a piecewise linear function which is capable of modeling any convex function. For a convex function, samples with function values below a speciﬁc threshold form a convex set. Therefore, by approximating convex functions of the local patch, maxout has the capability of forming separation hyperplanes for concepts whose samples are within a convex set (i.e. l2 balls, convex cones). Mlpconv layer differs from maxout layer in that the convex func- tion approximator is replaced by a universal function approximator, which has greater capability in modeling various distributions of latent concepts.  3.2 Global Average Pooling  Conventional convolutional neural networks perform convolution in the lower layers of the network. For classiﬁcation, the feature maps of the last convolutional layer are vectorized and fed into fully connected layers followed by a softmax logistic regression layer [4] [8] [11]. This structure bridges the convolutional structure with traditional neural network classiﬁers. It treats the convolutional layers as feature extractors, and the resulting feature is classiﬁed in a traditional way. However, the fully connected layers are prone to overﬁtting, thus hampering the generalization abil- ity of the overall network. Dropout is proposed by Hinton et al. [5] as a regularizer which randomly sets half of the activations to the fully connected layers to zero during training. It has improved the generalization ability and largely prevents overﬁtting [4]. In this paper, we propose another strategy called global average pooling to replace the traditional fully connected layers in CNN. The idea is to generate one feature map for each corresponding category of the classiﬁcation task in the last mlpconv layer. Instead of adding fully connected layers on top of the feature maps, we take the average of each feature map, and the resulting vector is fed directly into the softmax layer. One advantage of global average pooling over the fully connected layers is that it is more native to the convolution structure by enforcing correspondences between feature maps and categories. Thus the feature maps can be easily interpreted as categories conﬁdence maps. Another advantage is that there is no parameter to optimize in the global average pooling thus overﬁtting is avoided at this layer. Futhermore, global average pooling sums out the spatial information, thus it is more robust to spatial translations of the input. We can see global average pooling as a structural regularizer that explicitly enforces feature maps to be conﬁdence maps of concepts (categories). This is made possible by the mlpconv layers, as they makes better approximation to the conﬁdence maps than GLMs.  3.3 Network In Network Structure  The overall structure of NIN is a stack of mlpconv layers, on top of which lie the global average pooling and the objective cost layer. Sub-sampling layers can be added in between the mlpconv  4   . . . . . . . . . . . . . . . . . . . . .....layers as in CNN and maxout networks. Figure 2 shows an NIN with three mlpconv layers. Within each mlpconv layer, there is a three-layer perceptron. The number of layers in both NIN and the micro networks is ﬂexible and can be tuned for speciﬁc tasks.  4 Experiments  4.1 Overview  We evaluate NIN on four benchmark datasets: CIFAR-10 [12], CIFAR-100 [12], SVHN [13] and MNIST [1]. The networks used for the datasets all consist of three stacked mlpconv layers, and the mlpconv layers in all the experiments are followed by a spatial max pooling layer which down- samples the input image by a factor of two. As a regularizer, dropout is applied on the outputs of all but the last mlpconv layers. Unless stated speciﬁcally, all the networks used in the experiment sec- tion use global average pooling instead of fully connected layers at the top of the network. Another regularizer applied is weight decay as used by Krizhevsky et al. [4]. Figure 2 illustrates the overall structure of NIN network used in this section. The detailed settings of the parameters are provided in the supplementary materials. We implement our network on the super fast cuda-convnet code developed by Alex Krizhevsky [4]. Preprocessing of the datasets, splitting of training and validation sets all follow Goodfellow et al. [8]. We adopt the training procedure used by Krizhevsky et al. [4]. Namely, we manually set proper initializations for the weights and the learning rates. The network is trained using mini-batches of size 128. The training process starts from the initial weights and learning rates, and it continues until the accuracy on the training set stops improving, and then the learning rate is lowered by a scale of 10. This procedure is repeated once such that the ﬁnal learning rate is one percent of the initial value.  4.2 CIFAR-10  The CIFAR-10 dataset [12] is composed of 10 classes of natural images with 50,000 training images in total, and 10,000 testing images. Each image is an RGB image of size 32x32. For this dataset, we apply the same global contrast normalization and ZCA whitening as was used by Goodfellow et al. in the maxout network [8]. We use the last 10,000 images of the training set as validation data. The number of feature maps for each mlpconv layer in this experiment is set to the same number as in the corresponding maxout network. Two hyper-parameters are tuned using the validation set, i.e. the local receptive ﬁeld size and the weight decay. After that the hyper-parameters are ﬁxed and we re-train the network from scratch with both the training set and the validation set. The resulting model is used for testing. We obtain a test error of 10.41% on this dataset, which improves more than one percent compared to the state-of-the-art. A comparison with previous methods is shown in Table 1.  Table 1: Test set error rates for CIFAR-10 of various methods.  Method Stochastic Pooling [11] CNN + Spearmint [14] Conv. maxout + Dropout [8] NIN + Dropout CNN + Spearmint + Data Augmentation [14] Conv. maxout + Dropout + Data Augmentation [8] DropConnect + 12 networks + Data Augmentation [15] NIN + Dropout + Data Augmentation  Test Error 15.13% 14.98% 11.68% 10.41% 9.50% 9.38% 9.32% 8.81%  It turns out in our experiment that using dropout in between the mlpconv layers in NIN boosts the performance of the network by improving the generalization ability of the model. As is shown in Figure 3, introducing dropout layers in between the mlpconv layers reduced the test error by more than 20%. This observation is consistant with Goodfellow et al. [8]. Thus dropout is added  5  in between the mlpconv layers to all the models used in this paper. The model without dropout regularizer achieves an error rate of 14.51% for the CIFAR-10 dataset, which already surpasses many previous state-of-the-arts with regularizer (except maxout). Since performance of maxout without dropout is not available, only dropout regularized version are compared in this paper.  Figure 3: The regularization effect of dropout in between mlpconv layers. Training and testing error of NIN with and without dropout in the ﬁrst 200 epochs of training is shown.  To be consistent with previous works, we also evaluate our method on the CIFAR-10 dataset with translation and horizontal ﬂipping augmentation. We are able to achieve a test error of 8.81%, which sets the new state-of-the-art performance.  4.3 CIFAR-100  The CIFAR-100 dataset [12] is the same in size and format as the CIFAR-10 dataset, but it contains 100 classes. Thus the number of images in each class is only one tenth of the CIFAR-10 dataset. For CIFAR-100 we do not tune the hyper-parameters, but use the same setting as the CIFAR-10 dataset. The only difference is that the last mlpconv layer outputs 100 feature maps. A test error of 35.68% is obtained for CIFAR-100 which surpasses the current best performance without data augmentation by more than one percent. Details of the performance comparison are shown in Table 2.  Table 2: Test set error rates for CIFAR-100 of various methods.  Method Learned Pooling [16] Stochastic Pooling [11] Conv. maxout + Dropout [8] Tree based priors [17] NIN + Dropout  Test Error 43.71% 42.51% 38.57% 36.85% 35.68%  4.4 Street View House Numbers  The SVHN dataset [13] is composed of 630,420 32x32 color images, divided into training set, testing set and an extra set. The task of this data set is to classify the digit located at the center of each image. The training and testing procedure follow Goodfellow et al. [8]. Namely 400 samples per class selected from the training set and 200 samples per class from the extra set are used for validation. The remainder of the training set and the extra set are used for training. The validation set is only used as a guidance for hyper-parameter selection, but never used for training the model. Preprocessing of the dataset again follows Goodfellow et al. [8], which was a local contrast normal- ization. The structure and parameters used in SVHN are similar to those used for CIFAR-10, which consist of three mlpconv layers followed by global average pooling. For this dataset, we obtain a  6  02040608010012014016018020000.10.20.30.40.50.60.70.80.9Number of epochsError rate  training error w/o dropouttraining error w/ dropouttesting error w/o dropouttesting error w/ dropoutTable 3: Test set error rates for SVHN of various methods.  Method Stochastic Pooling [11] Rectiﬁer + Dropout [18] Rectiﬁer + Dropout + Synthetic Translation [18] Conv. maxout + Dropout [8] NIN + Dropout Multi-digit Number Recognition [19] DropConnect [15]  Test Error  2.80% 2.78% 2.68% 2.47% 2.35% 2.16% 1.94%  test error rate of 2.35%. We compare our result with methods that did not augment the data, and the comparison is shown in Table 3.  4.5 MNIST  The MNIST [1] dataset consists of hand written digits 0-9 which are 28x28 in size. There are 60,000 training images and 10,000 testing images in total. For this dataset, the same network structure as used for CIFAR-10 is adopted. But the numbers of feature maps generated from each mlpconv layer are reduced. Because MNIST is a simpler dataset compared with CIFAR-10; fewer parameters are needed. We test our method on this dataset without data augmentation. The result is compared with previous works that adopted convolutional structures, and are shown in Table 4.  Table 4: Test set error rates for MNIST of various methods.  Method 2-Layer CNN + 2-Layer NN [11] Stochastic Pooling [11] NIN + Dropout Conv. maxout + Dropout [8]  Test Error  0.53% 0.47% 0.47% 0.45%  We achieve comparable but not better performance (0.47%) than the current best (0.45%) since MNIST has been tuned to a very low error rate.  4.6 Global Average Pooling as a Regularizer  Global average pooling layer is similar to the fully connected layer in that they both perform linear transformations of the vectorized feature maps. The difference lies in the transformation matrix. For global average pooling, the transformation matrix is preﬁxed and it is non-zero only on block diag- onal elements which share the same value. Fully connected layers can have dense transformation matrices and the values are subject to back-propagation optimization. To study the regularization effect of global average pooling, we replace the global average pooling layer with a fully connected layer, while the other parts of the model remain the same. We evaluated this model with and without dropout before the fully connected linear layer. Both models are tested on the CIFAR-10 dataset, and a comparison of the performances is shown in Table 5.  Table 5: Global average pooling compared to fully connected layer.  Method mlpconv + Fully Connected mlpconv + Fully Connected + Dropout mlpconv + Global Average Pooling  Testing Error  11.59% 10.88% 10.41%  As is shown in Table 5, the fully connected layer without dropout regularization gave the worst performance (11.59%). This is expected as the fully connected layer overﬁts to the training data if  7  no regularizer is applied. Adding dropout before the fully connected layer reduced the testing error (10.88%). Global average pooling has achieved the lowest testing error (10.41%) among the three. We then explore whether the global average pooling has the same regularization effect for conven- tional CNNs. We instantiate a conventional CNN as described by Hinton et al. [5], which consists of three convolutional layers and one local connection layer. The local connection layer generates 16 feature maps which are fed to a fully connected layer with dropout. To make the comparison fair, we reduce the number of feature map of the local connection layer from 16 to 10, since only one feature map is allowed for each category in the global average pooling scheme. An equivalent network with global average pooling is then created by replacing the dropout + fully connected layer with global average pooling. The performances were tested on the CIFAR-10 dataset. This CNN model with fully connected layer can only achieve the error rate of 17.56%. When dropout is added we achieve a similar performance (15.99%) as reported by Hinton et al. [5]. By replacing the fully connected layer with global average pooling in this model, we obtain the error rate of 16.46%, which is one percent improvement compared with the CNN without dropout. It again veriﬁes the effectiveness of the global average pooling layer as a regularizer. Although it is slightly worse than the dropout regularizer result, we argue that the global average pooling might be too demanding for linear convolution layers as it requires the linear ﬁlter with rectiﬁed activation to model the conﬁdence maps of the categories.  4.7 Visualization of NIN  We explicitly enforce feature maps in the last mlpconv layer of NIN to be conﬁdence maps of the categories by means of global average pooling, which is possible only with stronger local receptive ﬁeld modeling, e.g. mlpconv in NIN. To understand how much this purpose is accomplished, we extract and directly visualize the feature maps from the last mlpconv layer of the trained model for CIFAR-10. Figure 4 shows some examplar images and their corresponding feature maps for each of the ten categories selected from CIFAR-10 test set. It is expected that the largest activations are observed in the feature map corresponding to the ground truth category of the input image, which is explicitly enforced by global average pooling. Within the feature map of the ground truth category, it can be observed that the strongest activations appear roughly at the same region of the object in the original image. It is especially true for structured objects, such as the car in the second row of Figure 4. Note that the feature maps for the categories are trained with only category information. Better results are expected if bounding boxes of the objects are used for ﬁne grained labels. The visualization again demonstrates the effectiveness of NIN. It is achieved via a stronger local re- ceptive ﬁeld modeling using mlpconv layers. The global average pooling then enforces the learning of category level feature maps. Further exploration can be made towards general object detection. Detection results can be achieved based on the category level feature maps in the same ﬂavor as in the scene labeling work of Farabet et al. [20].  5 Conclusions  We proposed a novel deep network called “Network In Network” (NIN) for classiﬁcation tasks. This new structure consists of mlpconv layers which use multilayer perceptrons to convolve the input and a global average pooling layer as a replacement for the fully connected layers in conventional CNN. Mlpconv layers model the local patches better, and global average pooling acts as a structural regularizer that prevents overﬁtting globally. With these two components of NIN we demonstrated state-of-the-art performance on CIFAR-10, CIFAR-100 and SVHN datasets. Through visualization of the feature maps, we demonstrated that feature maps from the last mlpconv layer of NIN were conﬁdence maps of the categories, and this motivates the possibility of performing object detection via NIN.  References [1] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning  applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.  8  Figure 4: Visualization of the feature maps from the last mlpconv layer. Only top 10% activations in the feature maps are shown. The categories corresponding to the feature maps are: 1. airplane, 2. automobile, 3. bird, 4. cat, 5. deer, 6. dog, 7. frog, 8. horse, 9. ship, 10. truck. Feature maps corresponding to the ground truth of the input images are highlighted. The left panel and right panel are just different examplars.  [2] Y Bengio, A Courville, and P Vincent. Representation learning: A review and new perspec- tives. IEEE transactions on pattern analysis and machine intelligence, 35:1798–1828, 2013. [3] Frank Rosenblatt. Principles of neurodynamics. perceptrons and the theory of brain mecha-  nisms. Technical report, DTIC Document, 1961.  [4] Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classiﬁcation with deep con- volutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1106–1114, 2012.  [5] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhut- Improving neural networks by preventing co-adaptation of feature detectors. arXiv  dinov. preprint arXiv:1207.0580, 2012.  [6] Quoc V Le, Alexandre Karpenko, Jiquan Ngiam, and Andrew Ng. Ica with reconstruction cost In Advances in Neural Information Processing  for efﬁcient overcomplete feature learning. Systems, pages 1017–1025, 2011.  [7] Ian J Goodfellow. Piecewise linear multilayer perceptrons and dropout.  arXiv:1301.5088, 2013.  arXiv preprint  [8] Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio.  Maxout networks. arXiv preprint arXiv:1302.4389, 2013.  [9] C¸ a˘glar G¨ulc¸ehre and Yoshua Bengio. Knowledge matters: Importance of prior information for  optimization. arXiv preprint arXiv:1301.4083, 2013.  [10] Henry A Rowley, Shumeet Baluja, Takeo Kanade, et al. Human face detection in visual scenes.  School of Computer Science, Carnegie Mellon University Pittsburgh, PA, 1995.  [11] Matthew D Zeiler and Rob Fergus. Stochastic pooling for regularization of deep convolutional  neural networks. arXiv preprint arXiv:1301.3557, 2013.  [12] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.  Master’s thesis, Department of Computer Science, University of Toronto, 2009.  [13] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, volume 2011, 2011.  [14] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of ma-  chine learning algorithms. arXiv preprint arXiv:1206.2944, 2012.  9  1        2         3         4         5         6        7         8         9       10     1        2         3         4         5         6        7         8         9       10     [15] Li Wan, Matthew Zeiler, Sixin Zhang, Yann L Cun, and Rob Fergus. Regularization of neural networks using dropconnect. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 1058–1066, 2013.  [16] Mateusz Malinowski and Mario Fritz. Learnable pooling regions for image classiﬁcation.  arXiv preprint arXiv:1301.3516, 2013.  [17] Nitish Srivastava and Ruslan Salakhutdinov. Discriminative transfer learning with tree-based  priors. In Advances in Neural Information Processing Systems, pages 2094–2102, 2013.  [18] Nitish Srivastava. Improving neural networks with dropout. PhD thesis, University of Toronto,  2013.  [19] Ian J Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, and Vinay Shet. Multi-digit number recognition from street view imagery using deep convolutional neural networks. arXiv preprint arXiv:1312.6082, 2013.  [20] Cl´ement Farabet, Camille Couprie, Laurent Najman, Yann Lecun, et al. Learning hierarchical features for scene labeling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35:1915–1929, 2013.  10  ","We propose a novel deep network structure called ""Network In Network"" (NIN)to enhance model discriminability for local patches within the receptive field.The conventional convolutional layer uses linear filters followed by anonlinear activation function to scan the input. Instead, we build micro neuralnetworks with more complex structures to abstract the data within the receptivefield. We instantiate the micro neural network with a multilayer perceptron,which is a potent function approximator. The feature maps are obtained bysliding the micro networks over the input in a similar manner as CNN; they arethen fed into the next layer. Deep NIN can be implemented by stacking mutipleof the above described structure. With enhanced local modeling via the micronetwork, we are able to utilize global average pooling over feature maps in theclassification layer, which is easier to interpret and less prone tooverfitting than traditional fully connected layers. We demonstrated thestate-of-the-art classification performances with NIN on CIFAR-10 andCIFAR-100, and reasonable performances on SVHN and MNIST datasets."
1312.4461,2014,Low-Rank Approximations for Conditional Feedforward Computation in Deep Neural Networks  ,"['Andrew Davis', 'Itamar Arel']",https://arxiv.org/pdf/1312.4461.pdf,"4 1 0 2     n a J    8 2      ]  G L . s c [      4 v 1 6 4 4  .  2 1 3 1 : v i X r a  Low-Rank Approximations for Conditional  Feedforward Computation in Deep Neural Networks  Department of Electrical Engineering and Computer Science  Andrew S. Davis  University of Tennessee  andrew.davis@utk.edu  Department of Electrical Engineering and Computer Science  Itamar Arel  University of Tennessee itamar@ieee.org  Abstract  Scalability properties of deep neural networks raise key research questions, par- ticularly as the problems considered become larger and more challenging. This paper expands on the idea of conditional computation introduced in [2], where the nodes of a deep network are augmented by a set of gating units that deter- mine when a node should be calculated. By factorizing the weight matrix into a low-rank approximation, an estimation of the sign of the pre-nonlinearity ac- tivation can be efﬁciently obtained. For networks using rectiﬁed-linear hidden units, this implies that the computation of a hidden unit with an estimated nega- tive pre-nonlinearity can be omitted altogether, as its value will become zero when nonlinearity is applied. For sparse neural networks, this can result in considerable speed gains. Experimental results using the MNIST and SVHN data sets with a fully-connected deep neural network demonstrate the performance robustness of the proposed scheme with respect to the error introduced by the conditional computation process.  1 Introduction  In recent years, deep neural networks have redeﬁned state-of-the-art in many application domains, notably in computer vision [11] and speech processing [14]. In order to scale to more challenging problems, however, neural networks must become larger, which implies an increase in computa- tional resources. Shifting computation to highly parallel platforms such as GPUs has enabled the training of massive neural networks that would otherwise train too slowly on conventional CPUs. While the extremely high computational power used for the experiment performed in [12] (16,000 cores training for many days) was greatly reduced in [4] (3 servers training for many days), special- ized high-performance platforms still require several machines and several days of processing time. However, there may exist more fundamental changes to the algorithms involved which can greatly assist in scaling neural networks.  Many of these state-of-the-art networks have several common properties: the use of rectiﬁed-linear activation functions in the hidden neurons, and a high level of sparsity induced by dropout regular- ization or a sparsity-inducing penalty term on the loss function. Given that many of the activations are effectively zero, due to the combination of sparsity and the hard thresholding of rectiﬁed linear units, a large amount of computation is wasted on calculating values that are eventually truncated to zero and provide no contribution to the network outputs or error components. Here we focus on this  1  Gating Units  0  1  0  1  0  V  Layer l+1  U  W  Layer l  Figure 1: An illustration of an activation estimator. U and V represent the factorization of the low- rank matrix and W denotes the full-rank matrix. In this case, the activation estimator recommends that only the 2nd and the nth neuron be computed for layer l + 1.  key observation in devising a scheme that can predict the zero-valued activations in a computation- ally cost-efﬁcient manner.  2 Conditional Computation in Deep Neural Networks  2.1 Exploiting Redundancy in Deep Architectures  In [5], the authors made the observation that deep models tend to have a high degree of redundancy in their weight parameterization. The authors exploit this redundancy in order to train as few as 5% of the weights in a neural network while estimating the other 95% with the use of carefully constructed low-rank decompositions of the weight matrices. Such a reduction in the number of active training parameters can render optimization easier by reducing the number of variables to optimize over. Moreover, it can help address the problem of scalability by greatly reducing the communication overhead in a distributed system.  Assuming there is a considerable amount of redundancy in the weight parameterization, a similar level of redundancy is likely found in the activation patterns of individual neurons. Therefore, given an input sample, the set of redundant activations in the network may be approximated. If a sufﬁciently accurate approximation can be obtained using low computational resources, activations for a subset of neurons in the network’s hidden layers need not be calculated.  In [2] and [3], the authors propose the idea of conditional computation in neural networks, where the network is augmented by a gating model that turns activations on or off depending on the state of the network. If this gating model is able to reliably estimate which neurons need to be calculated for a particular input, great improvements in computational efﬁciency may be obtainable if the network is sufﬁciently sparse. Figure 1 illustrates a conditional computation unit augmenting a layer of a neural net by using some function f (U, V, al) to determine which hidden unit activations al+1 should be computed given the activations al of layer l.  2.2 Sparse Representations, Activation Functions, and Prediction  In some situations, sparse representations may be superior to dense representations, particularly in the context of deep architectures [7]. However, sparse representations learned by neural networks with sigmoidal activations are not truly “sparse”, as activations only approach zero in the limit to- wards negative inﬁnity. A conditional computation model estimating the sparsity of a sigmoidal network would thus have to impose some threshold, beyond which the neuron is considered in- active. So-called “hard-threshold” activation functions such as rectiﬁed-linear units, on the other hand, produce true zeros which can be used by conditional computation models without imposing additional hyperparameters.  2  Comparison of Low−Rank Feedforward with Activation Estimation Feedforward     Mean ||s (aW) − s (aUV)|| 2 2  Mean ||s (aW) − s (aW) .* S|| 2 2  20  40  60  80  100  Rank of UV  120  140  160  180  200  250  200  E S M  150  100  50  0     Figure 2: The low-rank approximation U V can be substituted in for W , and can approximate the matrix W with a relatively low rank. However, if we use the output of the activation estimator S, as deﬁned in Eq. (5), with the full-rank feedforward, σ (aW ) · S, a lower-rank approximation can be utilized. The activations and weights are from the ﬁrst layer of a neural network trained on MNIST, and the factorization U V is obtained via SVD.  3 Problem Formulation  3.1 Estimating Activation Sign via Low-Rank Approximation  Given the activation al of layer l of a neural network, the activation al+1 of layer l + 1 is given by:  al+1 = σ(alWl)  (1)  where σ(·) denotes the function deﬁning the neuron’s non-linearity, al ∈ Rn×h1, al+1 ∈ Rn×h2, Wl ∈ Rh1×h2. If the weight matrix is highly redundant, as in [5], it can be well-approximated using a low-rank representation and we may rewrite (1) as  al+1 = σ(alUlVl)  (2)  where UlVl is the low-rank approximation of Wl, Ul ∈ Rh1×k, Vl ∈ Rk×h2, k ≪ min(h1, h2). So long as k < h1h2 , the low-rank multiplication alUlVl requires fewer arithmetic operations than h1+h2 the full-rank multiplication alWl, assuming the multiplication alUl occurs ﬁrst. When σ(·) is the rectiﬁed-linear function,  σ(x) = max(0, x)  (3)  such that all negative elements of the linear transform alWl become zero, one only needs to estimate the sign of the elements of the linear transform in order to predict the zero-valued elements. Assum- ing the weights in a deep neural network can be well-approximated using a low-rank estimation, the small error in the low-rank estimation is of marginal relevance in the context of recovering the sign of the operation. Given a low-rank approximation Wl ≈ UlVl = ˆWl, the estimated sign of al+1 is given by  sgn(al+1) ≈ sgn(al ˆWl)  (4)  l  . If sgn(al ˆW (j)  Each element (al+1)i,j is given by a dot product between the row vector a(i) and the column vector W (j) ) = −1, then the true activation (al+1)i,j is likely negative, and will likely become zero after the rectiﬁed-linear function is applied. Considerable speed gains are possible if we skip those dot products based on the prediction; such gains are especially substantial when the network is very sparse. The overall activation for a hidden layer l augmented by the activation  l  l  3  estimator is given by σ (alWl)·Sl, where · denotes the element-wise product and Sl denotes a matrix of zeros and ones, where  (Sl)i,j=  0,   1,   sgn(cid:16)(alUlVl)i,j(cid:17) = −1 sgn(cid:16)(alUlVl)i,j(cid:17) = +1  (5)  Figure 2 illustrates the error proﬁle of a neural network using the low-rank estimation U V in place of W compared with a neural network augmented with an activation sign estimator as the rank is varied from one to full-rank. One can see that the error of the activation sign estimator diminishes far more quickly than the error of the low-rank activation, implying that the sign estimator can do well with a relatively low-rank approximation of W .  3.2 SVD as a Low-Rank Approximation  The Singular Value Decomposition (SVD) is a common matrix decomposition technique that factor- izes a matrix A ∈ Rm×n into A = U ΣV T , U ∈ Rm×m, Σ ∈ Rm×n, V ∈ Rn×n. By [6], the matrix A can be approximated using a low rank matrix ˆAr corresponding to the solution of the constrained optimization of  ˆAr kA − ˆArkF min  (6)  where k · kF is the Frobenius norm, and ˆAr is constrained to be of rank r < rank(A). The minimizer ˆAr is given by taking the ﬁrst r columns of U , the ﬁrst r diagonal entries of Σ, and the ﬁrst r columns of V . The resulting matrices Ur, Σr, and Vr are multiplied, yielding ˆAr = UrΣrV T r . The low-rank approximation ˆW = U V is then deﬁned such that ˆW = Ur(ΣrV T r ), where U = Ur and V = ΣrV T r . Unfortunately, calculating the SVD is an expensive operation, on the order of O(mn2), so recal- culating the SVD upon the completion of every minibatch adds signiﬁcant overhead to the training procedure. Given that we are uniquely interested in estimating in the sign of al+1 = alWl, we can opt to calculate the SVD less frequently than once per minibatch, assuming that the weights Wl do not change signiﬁcantly over the course of a single epoch so as to corrupt the sign estimation.  3.3 Encouraging Neural Network Sparsity  To overcome the additional overhead imposed by the conditional computation architecture, the neu- ral network must have sparse activations. Without encouragement to settle on weights that result in sparse activations, such as penalties on the loss function, a neural network will not necessarily be- come sparse enough to be useful in the context of conditional computation. Therefore, an ℓ1 penalty for the activation vector of each layer is applied to the overall loss function, such that  J(W, λ) = L(W ) + λ  L  X  l=1  kalk1  (7)  Such a penalty is commonly used in sparse dictionary learning algorithms and tends to push elements of al towards zero [13]. Dropout regularization [9] is another technique known to sparsify the hidden activations in a neural network. Dropout ﬁrst sets the hidden activations al to zero with probability p. During training, the number of active neurons is likely less than p for each minibatch. When the regularized network is running in the inference mode, dropout has been observed to have a sparsifying effect on the hidden activations [17]. The adaptive dropout method [1] can further decrease the number of active neurons without degrading the performance of the network.  3.4 Theoretical Speed Gain  For every input example, a standard neural network computes σ (aW ), where a ∈ RN ×d and W ∈ Rd×h, where N = 1 for a fully-connected network, or N is the number of convolutions for a convolutional network. Assuming additions and multiplications are constant-time operations,  4  the matrix multiplication requires N (2d − 1) h ﬂoating point operations (we need to compute N h dot products, where each dot product consists of d multiplications and d − 1 additions), and the activation function requires N h ﬂoating point operations, yielding N (2d − 1) h + N h operations. The activation estimator σ (aU V ), U ∈ Rd×k, V ∈ Rk×h requires N (2d − 1) k + N (2k − 1) h ﬂoating point operations for the low-rank multiplication followed by N h operations for the sgn (·) activation function, yielding N (2d − 1) k + N (2k − 1) h + N h. However, given a sparsity coef- ﬁcient α ∈ [0, 1] (where α = 0 implies no activations are active, and α = 1 implies all activations are active), a conditional matrix multiplication would require αN (2d − 1) h + αN h operations. The SVD calculation to obtain the activation estimation weights is βO (nd min (n, d)), where β is the ratio of feed-forwards to SVD updates (eg., with a minibatch size of 250, a training set size of 50,000, and once-per-epoch SVD updates, β = 250 Altogether, the number of ﬂoating point operations for calculating the feed-forward in a layer in a standard neural network is  50000 = 0.005).  and the number of ﬂoating point operations for the activation estimation network with conditional computation is  Fnn = N (2d − 1) h + N h  (8)  Fae = N (2d − 1) k + N (2k − 1) h + N h + αh (N (2d − 1) h + N h) + βO (nd min (n, d)) (9) The relative reduction of ﬂoating point operations for a layer can be represented as Fnn , and is Fae simpliﬁed as  2dh  k (2d + 2h − 1) + 2αdh + βO (nd min (n, d))  For a neural network with many layers, the relative speedup is given by  L  X  i=1  L  X  i=1  F (l) nn  F (l) ae  (10)  (11)  nn is the number of ﬂoating point operations for the lth layer of the full network, and F (l)  where F (l) ae is the number of ﬂoating point operations for the lth layer of the network augmented by the activation estimation network. The overall speedup is greatly dependent on the sparsity of the network and the overhead of the activation estimator.  3.5 Implementation Details  The neural network is built using Rasmus Berg Palm’s Deep Learning Toolbox [16]. All hidden units are rectiﬁed-linear, and the output units are softmax trained with a negative log-likelihood loss function. The weights, w, are initialized as w ∼ N (cid:0)0, σ2(cid:1) and biases b are set to 1 in order to encourage the neurons to operate in their non-saturated region once training begins, as suggested in [11]. In all experiments, the dropout probability p is ﬁxed to 0.5 for the hidden layers. The learning rate γ is scheduled such that γn = γ0λn where γn is the learning rate for the nth epoch, γ0 is the initial learning rate, and λ is a decay term slightly less than 1, eg., 0.995. The momentum term ν is scheduled such that νn = max (νmax, ν0βn) where νn is the momentum for the nth epoch, νmax is the maximum allowed momentum, ν0 is the initial momentum, and β is an incremental term slightly greater than 1, eg., 1.05.  To simplify prototyping, the feed-forward is calculated for a layer, and the activation estimator is immediately applied before the next layer activations are used. This is equivalent to bypassing the calculations for activations that are likely to produce zeros. In practice, re-calculating the SVD once per epoch for the activation estimator seems to be a decent tradeoff between activation estimation accuracy and computational efﬁciency, but this may not necessarily be true for other datasets.  5  Architecture Weight Init  Init Learning Rate  Learning Rate Scaling Maximum Momentum Momentum Increment  Maximum Norm  ℓ1 Activation Penalty  SVHN  MNIST  1024-1500-700-400-200-10 w ∼ N (0, 0.01); b = 1  784-1000-600-400-10 w ∼ N (0, 0.05); b = 1  0.25 0.99 0.8 1.05 25  0.15 0.99 0.8 1.01 25 0 –  ℓ2Weight Penalty  1 × 10−5 5 × 10−5 Table 1: Hyperparameters for SVHN and MNIST experiments.  r o r r     E n o i t a c i f i s s a C  l  SVHN Classification Error, Validation Set    25−25−25−25 50−35−25−25 75−50−40−30 100−75−50−25 200−100−75−15 control 200−100−75−40  100  200 Epoch  300  400  0.12  0.1  0.08  0.06  0.04     Figure 3: Classiﬁcation error of the validation set for SVHN on seven conﬁgurations of the activation estimator for each hidden layer. The ’control’ network has no activation estimator and is used as a baseline of comparison for the other networks.  4 Experimental Results  4.1 SVHN  Street View House Numbers (SVHN) [15] is a large image dataset containing over 600,000 labeled examples of digits taken from street signs. Each example is an RGB 32 × 32 (3072-dimensional) image. To pre-process the dataset, each image is transformed into the YUV colorspace. Next, local contrast normalization [10] followed by a histogram equalization is applied to the Y channel. The U and V channels are discarded, resulting in a 1024-dimensional vector per example. The dataset is then normalized for the neural network by subtracting out the mean and dividing by the square root of the variance for each variable. To select the hyperparameters, the training data was split into 590,000 samples for the training set and 14,388 samples for the validation set. The architecture was held ﬁxed while the other hyperparameters were chosen randomly over 30 runs using a network with no activation estimation. The hyperparameters of the neural network with the lowest resulting validation error were then used for all experiments.  To evaluate the sensitivity of the activation estimator, several parameterizations for the activation estimator are evaluated. Each network is trained with the hyperparameters in Table 1, and the results of seven parameterizations are shown in Figure 3. Each parameterization is described by the rank of each approximation, eg., ‘75-50-40-30’ describes a network with an activation estimator using a 75-rank approximation for W1, a 50-rank approximation for W2, a 40-rank approximation for W3, and a 30-rank approximation for W4. Note that a low-rank approximation is not necessary for W5 (the weights connecting the last hidden layer to the output layer), as we do not want to approximate the activations for the output layer.  Some runs, speciﬁcally 25-25-25-25 and 50-35-25-25 in Figure 3 exhibit an initial decrease in clas- siﬁcation error, followed by a gradual increase in classiﬁcation error as training progresses. In the initial epochs, the hidden layer activations are mostly positive because the weights are relatively small and the biases are very large. As a consequence, the activation estimation is a much simpler  6  Error of Estimator, 200−100−75−50    0.3  0.25  0.2  0.15  0.1  0.05  r o r r  E  0     100  200 Epoch  300  400  y c a r u c c A  0.3  0.25  0.2  0.15  0.1  0.05  0  Error of Estimator, 25−25−25−25  100  200 Epoch  300  400  W 1  W 2  W 3  W 4  Figure 4: A comparison of a low-rank activation estimator and a higher-rank activation estimator. In this instance, a 25-25-25-25 activation estimator is too coarse to adequately capture the structure of the weight matrices.  Network Control  200-100-75-15 100-75-50-25 100-75-50-15 75-50-40-30 50-40-40-35 25-25-15-15  Error 9.31% 9.67% 9.96% 10.01% 10.72% 12.16% 19.40%  Table 2: SVHN test set error for seven networks.  task for the initial epochs. However, as the pattern of the activation signs diversiﬁes as the network continues to train, the lower-rank approximations begin to fail, as illustrated in Figure 4. Table 2 summarizes the test set error for the control and activation estimation networks. W1 appears to be most sensitive, quickly reducing the test set error from 10.72% to 12.16% when the rank of ˆW1 is lowered from 75 to 50. The rank of ˆW4 appears to be the least sensitive, reducing the test set error from 9.96% to 10.01% as the rank is lowered from 25 to 15.  4.2 MNIST  MNIST is a well-known dataset of hand-written digits containing 70,000 28 × 28 labeled images, and is generally split into 60,000 training and 10,000 testing examples. Very little pre-processing is required to achieve good results - each feature is transformed by xt = x√σ2 max − 0.5, where x is the input feature, σ2 max is the maximum variance of all features, and 0.5 is a constant term to roughly center each feature. To select the hyperparameters, the training data was split into 50,000 samples for the training set and 10,000 samples for the validation set. The architecture was held ﬁxed while the other hyperparameters were chosen randomly over 30 runs using a network with no activation estimation. The hyperparameters of the neural network with the lowest resulting validation error were then used for all experiments. Several parameterizations for the activation estimator are evalu- ated for a neural network trained with the hyperparameters listed in Table 1 using the same approach as the SVHN experiment above. The results for the validation set plotted against the epoch number are shown in Figure 5, and the ﬁnal test set accuracy is reported in Table 3.  A neural network with a very low-rank weight matrix in the activation estimation can train sur- prisingly well on MNIST. Lowering the rank from 784-600-400 to 50-35-25 impacts performance negligibly. Ranks as low as 25-25-25 does not lessen performance too greatly, and ranks as low as 10-10-5 yield a classiﬁer capable of 2.28% error.  7  r o r r  E   n o i t a c i f i s s a C  l  0.05  0.045  0.04  0.035  0.03  0.025  0.02  0.015  0.01     MNIST Classification Error, Validation Set     10−10−5 15−10−5 25−25−25 50−35−25 control  50  100  150 Epoch  200  250  300  Figure 5: Classiﬁcation error of the validation set for MNIST on ﬁve conﬁgurations of the activation estimator for each hidden layer.  Network Error 1.40% Control 50-35-25 1.43% 1.60% 25-25-25 1.85% 15-10-5 10-10-5 2.28%  Table 3: MNIST test set error for ﬁve networks.  5 Discussion and Further Work  Low-rank estimations of weight matrices of a neural network obtained via once-per-epoch SVD work very well as efﬁcient estimators of the sign of the activation for the next hidden layer. In the context of rectiﬁed-linear hidden units, computation time can be reduced greatly if this estimation is reliable and the hidden activations are sufﬁciently sparse. This approach is applicable to any hard- thresholding activation function, such as the functions investigated in [8], and can be easily extended to be used with convolutional neural networks.  While the activation estimation error does not tend to deviate too greatly inbetween minibatches over an epoch, as illustrated in Figure 6, this is not guaranteed. An online approach to the low-rank approximation would therefore be preferable to a once-per-epoch calculation. In addition, while the low-rank approximation given by SVD minimizes the objective function kA − ˆArkF , this is not necessarily the best objective function for an activation estimator, where we seek to minimize  r o r r  E  0.22  0.2  0.18  0.16  0.14  0.12  0.1  0.08     Activation Estimator Error     W 1 W 2 W 3  18  20  22  24  26 Epoch  28  30  32  34  Figure 6: Because the SVD is calculated at the beginning of each epoch, each subsequent gradient update in each minibatch moves the weight matrix further from low-rank factorization, resulting in an increasing error until the SVD is recalculated at the beginning of the next epoch. Different layers are negatively impacted in differing degrees.  8  kσ (aW ) − σ (aW · S)k, which is a much more difﬁcult and non-convex objective function. Also, setting the hyperparameters for the activation estimator can be a tedious process involving expensive cross-validation when an adaptive algorithm could instead choose the rank based on the spectrum of the singular values. Therefore, developing a more suitable low-rank approximation algorithm could provide a promising future direction of research.  In [1], the authors propose a method called “adaptive dropout” by which the dropout probabilities are chosen by a function optimized by gradient descent instead of ﬁxed to some value. This approach bears some resemblance to this paper, but with the key difference that the approach in [1] is moti- vated by improved regularization and this paper’s method is motivated by computational efﬁciency. However, the authors introduce a biasing term that allows for greater sparsity that could be intro- duced into this paper’s methodology. By modifying the conditional computation unit to compute sgn (aU V − b), where b is some bias, we can introduce a parameter that can tune the sparsity of the network, allowing for a more powerful trade-off between accuracy and computational efﬁciency.  Acknowledgments  This work was partially supported by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-13-2-0016.  References  [1] Jimmy Ba and Brendan Frey. Adaptive dropout for training deep neural networks. In Advances  in Neural Information Processing Systems, pages 3084–3092, 2013. 3.3, 5  [2] Yoshua Bengio. Deep learning of representations: Looking forward. In Adrian-Horia Dediu, Carlos Martin-Vide, Ruslan Mitkov, and Bianca Truthe, editors, Statistical Language and Speech Processing, volume 7978 of Lecture Notes in Computer Science, pages 1–37. Springer Berlin Heidelberg, 2013. (document), 2.1  [3] Yoshua Bengio, Nicholas Léonard, and Aaron C. Courville. Estimating or propagating gra- dients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. 2.1  [4] Adam Coates, Brody Huval, Tao Wang, David Wu, Bryan Catanzaro, and Ng Andrew. Deep learning with cots hpc systems. In Proceedings of the 30th International Conference on Ma- chine Learning (ICML-13), pages 1337–1345, 2013. 1  [5] Misha Denil, Babak Shakibi, Laurent Dinh, Marc’Aurelio Ranzato, and Nando de Freitas.  Predicting parameters in deep learning. arXiv preprint arXiv:1306.0543, 2013. 2.1, 3.1  [6] Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank.  Psychometrika, 1(3):211–218, 1936. 3.2  [7] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectiﬁer networks. In Pro- ceedings of the 14th International Conference on Artiﬁcial Intelligence and Statistics. JMLR W&CP Volume, volume 15, pages 315–323, 2011. 2.2  [8] Rostislav Goroshin and Yann LeCun.  arXiv:1301.3577, 2013. 5  Saturating auto-encoder.  arXiv preprint  [9] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhut- Improving neural networks by preventing co-adaptation of feature detectors. arXiv  dinov. preprint arXiv:1207.0580, 2012. 3.3  [10] Kevin Jarrett, Koray Kavukcuoglu, Marc’Aurelio Ranzato, and Yann LeCun. What is the In Computer Vision, 2009 IEEE 12th  best multi-stage architecture for object recognition? International Conference on, pages 2146–2153. IEEE, 2009. 4.1  [11] Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classiﬁcation with deep con- volutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1106–1114, 2012. 1, 3.5  [12] Quoc Le, Marc’Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg Corrado, Jeff Dean, and Andrew Ng. Building high-level features using large scale unsupervised learning. In International Conference in Machine Learning, 2012. 1  9  [13] Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Ng. Efﬁcient sparse coding algorithms.  In Advances in neural information processing systems, pages 801–808, 2006. 3.3  [14] A Mohamed, Tara N Sainath, George Dahl, Bhuvana Ramabhadran, Geoffrey E Hinton, and Michael A Picheny. Deep belief networks using discriminative features for phone recognition. In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, pages 5060–5063. IEEE, 2011. 1  [15] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng.  Reading digits in natural images with unsupervised feature learning. 4.1  [16] R. B. Palm. Prediction as a candidate for learning deep hierarchical models of data, 2012. 3.5 [17] Nitish Srivastava. Improving neural networks with dropout. PhD thesis, University of Toronto,  2013. 3.3  10  ","Scalability properties of deep neural networks raise key research questions,particularly as the problems considered become larger and more challenging.This paper expands on the idea of conditional computation introduced by Bengio,et. al., where the nodes of a deep network are augmented by a set of gatingunits that determine when a node should be calculated. By factorizing theweight matrix into a low-rank approximation, an estimation of the sign of thepre-nonlinearity activation can be efficiently obtained. For networks usingrectified-linear hidden units, this implies that the computation of a hiddenunit with an estimated negative pre-nonlinearity can be ommitted altogether, asits value will become zero when nonlinearity is applied. For sparse neuralnetworks, this can result in considerable speed gains. Experimental resultsusing the MNIST and SVHN data sets with a fully-connected deep neural networkdemonstrate the performance robustness of the proposed scheme with respect tothe error introduced by the conditional computation process."
1311.6184,2014,Bounding the Test Log-Likelihood of Generative Models  ,"['Yoshua Bengio', 'Li Yao', 'KyungHyun Cho']",https://arxiv.org/pdf/1311.6184.pdf,"4 1 0 2     y a M 9         ]  G L . s c [      4 v 4 8 1 6  .  1 1 3 1 : v i X r a  Bounding the Test Log-Likelihood of Generative  Models  Yoshua Bengio∗1,2, Li Yao†2, and Kyunghyun Cho‡3  2D´epartement d’Informatique et de Recherche Op´erationelle ,  3Department of Information and Computer Science , Aalto University  1CIFAR Senior Fellow  Universit´e de Montr´eal  School of Science  May 13, 2014  Abstract  Several interesting generative learning algorithms involve a complex probabil- ity distribution over many random variables, involving intractable normalization constants or latent variable marginalization. Some of them may not have even an analytic expression for the unnormalized probability function and no tractable ap- proximation. This makes it difﬁcult to estimate the quality of these models, once they have been trained, or to monitor their quality (e.g. for early stopping) while training. A previously proposed method is based on constructing a non-parametric density estimator of the model’s probability function from samples generated by the model. We revisit this idea, propose a more efﬁcient estimator, and prove that it provides a lower bound on the true test log-likelihood and an unbiased estimator as the number of generated samples goes to inﬁnity, although one that incorporates the effect of poor mixing. We further propose a biased variant of the estimator that can be used reliably with a ﬁnite number of samples for the purpose of model comparison.  1 Motivating Sampling-Based Estimators of Generative  Models’ Quality  Since researchers have begun considering more and more powerful models of data distributions, they have been facing the difﬁculty of estimating the quality of these models.  ∗yoshua.bengio@umontreal.ca †li.yao@umontreal.ca ‡kyunghyun.cho@aalto.ﬁ  1  In some cases, the probability distribution of a model involves many latent vari- ables, and it is intractable to marginalize over those latent variables or to compute the normalization constant (partition function). There exist approximation algorithms that were proposed to overcome these intractabilities. One such example is Annealed Im- portance Sampling (AIS, Neal, 2001; Salakhutdinov and Murray, 2008; Murray and Salakhutdinov, 2009; Salakhutdinov and Hinton, 2009). AIS, however, tends to pro- vide optimistic estimates most of the time, just like any estimator based on importance sampling. This optimistic estimation happens when the samples from the AIS pro- posal distribution miss many important modes of the distribution. This is problematic because when we want to compare learning algorithms, we often prefer a conserva- tive estimate of performance than an optimistic estimate that tends to over-estimate the value of the model. This issue can be particularly troubling when the amount of over-estimation depends on the model, which makes model comparisons based on an optimistic estimator dangerous.  In other cases, one has a generative model but there is no explicit formulat cor- responding to the probability function estimated by the model. That includes Herd- ing Welling (2009), the non-zero temperature version of Herding (Breuleux et al., 2011), and the recently proposed generative procedures for contractive auto-encoders (CAE, Rifai et al., 2012a), denoising auto-encoders (DAE, Bengio et al., 2013b), and genera- tive stochastic networks (GSN, Bengio et al., 2014).  For this reason, in this paper, we discuss a way to assess the quality of a generative model simply by considering the samples it generates. In the next section, we dis- cuss the general idea of estimating a probability function (or a density function) from the samples generated by a generative model. We ﬁrst review a previously proposed estimator that aimed to solve this goal. We show that the estimate by this estimator, in expectation over the generated samples from a generative model, is a lower bound on the true test log-likelihood and unbiased asymptotically. We then propose a more efﬁcient variant of this estimator that has lower variance.  2 Previous Work As far as we know, Breuleux et al. (2010) and Breuleux et al. (2011) ﬁrst proposed this kind of estimator. The estimator computes the estimate of a geneartive model by the following three steps:  1. Generate a set of samples S from the model.  2. Construct a non-parametric estimator ˆf of the probability distribution f of the  model distribution.  3. Compute the log-likelihood of test data under ˆf.  by  In the case where the data are continuous, a Parzen density estimator is constructed  ˆf (x) = meanx(cid:48)∈SN (x; µ = x(cid:48), σI),  (1)  2  where S is a set of samples from the model collected by a Markov chain, and N (x; µ, Σ) is the probability density of x under a Gaussian distribution with mean µ and covari- ance Σ. In this case, the bandwidth hyper-parameter σ must be tuned according to, for instance, the log-likelihood of a validation set.  This estimator was recently used to assess the qualities of the generative models such as stacked CAE (Rifai et al., 2012b), restricted Boltzmann machines (RBM, Des- jardins et al., 2010), deep belief networks (Bengio et al., 2013a) as well as DAEs (Ben- gio et al., 2013c) and GSNs (Bengio et al., 2014).  As noted in Breuleux et al. (2010), such an estimator measures not only the quality of the model but also that of the generative procedure. Any variant of this estimator will tend to estimate the log-likelihood to be lower than the true one when the generative procedure used to collecte samples from a model does not mix well.  Another way to evaluate the quality of a generative model whose probability is nei- ther tractable nor easily approximated is to use a non-parametric two-sample test (Gret- ton et al., 2012). Unlike the approach by Breuleux et al. (2010), this approach com- pares the (smoothed) distribution of the generated samples to test samples using an L2 measure (the squared error in estimated probability). Since in this paper we are more interested in the case of using Kullback-Leibler divergence as a measure, we do not discuss this approach of two-sample test any further.  3 Conservative Sampling-based Likelihood Estimator  We propose in this section a new estimator of the log-likelihood of a generative model, called Conservative Sampling-based Log-likelihood (CSL) estimator. The proposed CSL estimator does not require tuning a non-parametric estimator to ﬁt samples gen- erated from a model. It only requires that a Markov chain is deﬁned for the model and used to collect samples from the generative model. Furthermore, we assume that the Markov chain alternatively samples from latent variables and observed variables such that the conditional distribution P (x|h) is well deﬁned.  This assumption holds for many widely used generative models. An RBM using a block Gibbs sampling is one, and a generalized denoising autoencoder whose sampling procedure was proposed recently by Bengio et al. (2013d) is another. Multi-layered generative models such as DBNs and deep Boltzmann machines (DBM, Salakhutdinov and Hinton, 2009). Given the conditional probability P (x|h) of a model and a set S of samples h(cid:48) of the latent variables collected from a Markov chain, the CSL estimate is computed by  log ˆf (x) = log meanh(cid:48)∈SP (x|h(cid:48)).  (2)  The overall procedure of the CSL estimator is presented in Alg. 1. Unlike the original estimator described in Sec. 2, the CSL estimator utilizes the conditional probability P (x|h(cid:48)) rather than the actual sample x(cid:48) of observed variables generated from the Markov chain. This has the effect of considerably reducing the variance of the CSL estimator. In the case of a Gaussian conditional P (x|h(cid:48)), whose mean µ is a function of h(cid:48), for instance, this has the consequence of centering the Gaussian components of the Parzen density estimator on the mean µ(cid:48) rather than on the  3  Algorithm 1 CSL requires a set S of samples of the latent variables h(cid:48) from a Markov chain, a conditional distribution P (x|h), and a set X of test samples. 1: LL = 0 2: for x in X do 3: 4: 5: 6: 7:  r = 0 for h(cid:48) in S do  r ← r + P (x|h(cid:48))  end for ˆfS(x) = r|S| LL ← LL + log ˆfS(x)  8: 9: end for 10: Return LL/|X|  actual sample x(cid:48). Since each mean µ(cid:48) can “summarize” a very large number of potential samples x(cid:48) (one could have obtained by ﬁxing h(cid:48) and considering many draws from P (x|h(cid:48))), the CSL estimator is a much more efﬁcient estimator with lower variance than other estimators obtained purely from the generated samples, such as the one described in previous section.  The other important consequence of using the conditional distribution of the ob- served variables is that it allows us to get rid of the bandwidth hyper-parameter. In- deed, a natural choice of bandwidth (in the case of Gaussian conditional P (x|h(cid:48))) is precisely the standard deviation of the Gaussian conditional distribution. This allows us to prove that the CSL estimator is asymptotically consistent and conservative in average later.  4 Asymptotically Unbiased, Conservative Estimator  We ﬁrst prove that the CSL estimator log ˆfS(x) is asymptotically unbiased, i.e., that as the number of generated samples increases, it approaches the ground truth log- likelihood log f (x) associated with the stationary distribution of a generating Markov chain. Proposition 1. If the samples in S are taken from chains of length L → ∞, the CSL estimator log ˆfS(x) (Algorithm 1, Eq. 2) converges to the ground truth probability f (x) as the number of samples |S| → ∞, i.e.,  lim|S|→∞ log ˆfS(x) = log f (x)  (3)  (cid:82) P (h(cid:48))P (x|h(cid:48))dh(cid:48) is the marginal distribution over x associated with this stationary  Proof. According to the hypothesis on the samples in S, we have that Monte-Carlo estimates obtained from S converge to their expectation, i.e., the distribution of h(cid:48) converges to P (h(cid:48)) under the stationary distribution of the Markov chain. Since f (x) = distribution, its Monte-Carlo estimator ˆf (x) = meanh(cid:48) inSP (x|h(cid:48)) converges in the  4  limit of |S| → ∞ to its expectation under P (h(cid:48)), i.e., f (x). Finally, note that the log of the limit equals the limit of the log.  We then prove that in the ﬁnite sample case, the CSL estimator log ˆfS(x) tends to  underestimate the ground truth log-probability log f (x). Proposition 2. The expected value of the log of the CSL estimator log ˆfS(x) (Algo- rithm 1) over samples S from the generative model is a lower bound on the true log- likelihood log f (x), i.e.,  ES[log ˆfS(x)] ≤ log f (x)  (4)  Proof. We simply take advantage of the concavity of the log and Jensen’s inequality:  ES[log ˆfS(x)] ≤ log ES[ ˆfS(x)]  = log EH [P (x|H)] = log f (x)  Another interesting question is the rate of convergence of the CSL estimator to its asymptotic (ground truth) value. That rate is governed by the variance of the estimator, which decreases linearly with the number of samples in S, up to a factor which cor- responds to the effective sample size associated with the Markov chain, just like any other Monte-Carlo average associated with the chain.  5 Empirical Validation  In this section, we empirically evaluate the CSL estimator on a real dataset to investi- gate the rate at which the estimator converges.  We report here the experimental result on denoising auto-encoders (DAE), gener- ative stochastic networks (GSN), restricted Boltzmann machines (RBM), deep Boltz- mann machines (DBM), and deep belief nets (DBNs). DAEs and GSNs themselves deﬁne generative (sampling) procedures, and for RBMs, DBMs and DBNs we used block Gibbs sampling to generate samples of latent variables.  One interesting aspect of these experiments is that they highlight the dependency of the estimator on the effective sample size of the Markov chain, i.e., on the mixing rate of the chain. For a ﬁxed number of samples |S|, chains that mix faster provide an estimator that is closer to its asymptote. In particular, these results conﬁrm the previ- ously reported observations of poor mixing rate of block Gibbs sampling for RBMs, DBMs and DBNs that are very well trained. Indeed, these models are able to capture a sharper estimated distribution than their less-well trained counterparts. However, Gibbs sampling on these less-well trained models tends to mix better (Bengio et al., 2013a), because the major modes of the learned distribution are not separated by vast zones of tiny probability.  All models in these experiments were trained on the binarized MNIST data (thresh- olding at 0.5). The CSL estimates of the test set on the following models were eval- uated. For each model, every 100-th sample from a Markov chain was collected to  5  compute the CSL estimate. For more details on the architecture and training procedure of each model, see Appendix A.  Note that although on the RBM/DBN/DBM the testset log-likelihood is estimated by AIS (or its lower bound), there is no such AIS estimator for DAEs and GSNs, which is where the CSL estimator may become more useful. The number of generated samples was varied between 10,000 and 150,000. The resulting CSL estimates are presented in Table 1.  Table 1: The CSL estimates obtained using different numbers of samples of latent variables. Note that samples are collected once every 100 sampling steps of the Markov chain. Where available, an AIS-based estimate is also shown.  # samples GSN-1 GSN-2 DBN-2 DBM-2 RBM 10k -233 -192 50k -177 100k -170 150k AIS -64.1  -173 -144 -135 -132 -76.5  -446 -370 -340 -325 -57  -108 -101 -98 -97  -142 -126 -120 -117  In the following experiment, we trained an RBM with only 5 hidden units on MNIST, for which the exact log-likelihood can be computed easily. In this case, we observed that the CSL estimate matched the AIS and true likelihood closely as the num- ber of samples grew. The CSL estimates for varying numbers of generated samples are shown in Table 2.  Table 2: The CSL estimate converges to the true loglikelihood on a small RBM with only 5 hidden units.  # of Samples Log-likelihood -188.49 1k 2k -186.18 -182.26 5k -181.58 10k -180.65 20k 30k -180.71 -180.24 exact AIS -180.22  6 Biased CSL Estimator for Model Comparison  Although the CSL estimator is unbiased asymptotically as shown in Sec. 4, it may be desirable in practice to obtain a biased, but readily available, estimator. Hence, in this section, we describe an algorithm, called biased CSL, that works with a ﬁnite number of samples. This algorithm is biased, but we show at the end of this section, that the  6  Figure 1: The estimates of the log-probabilities of the test samples for 12 RBMs with varying numbers of latent variables. The curves represent the log-probabilities esti- mated using AIS (blue), the biased CSL with a single step of 10 parallel Markov chains (red), the biased CSL with 300 steps of 10 parallel Markov chains (cyan) and the true log-probabilities (only for the small models, green).  estimate correlate well with the exact log-likelihood or the AIS-based estimate and that it may be used for model comparison.  The biased CSL aims at estimating the log-probability of a single, test sample x at a time. As with the original algorithm in Algorithm 1, this estimator requires only that there are a computable conditional distribution P (x|h) and a Markov chain from where the latent variable of a model can be sampled. Unlike the unbiased CSL estimator, the biased CSL collected a small set Sx of consecutive latent samples h(cid:48) from a Markov chain that starts from the test sample x. This procedure ensures that the set Sx will always include at least a few samples that correspond to the neighborhood of the test samples x. Furthermore, by collecting consecutive samples, we ensure that the samples do not deviate too far away from the starting point x.  Although the locality and correlatedness of the consecutive samples Sx starting from the test sample induce a bias, we ﬁnd this to be beneﬁcial in the case of ﬁnite samples, since the lack of any latent sample that is close to the test sample x makes the estimate highly unreliable. The biased CSL ensures that the estimate of the probability of x will be reliable and have less variance. One consequence of the induced bias is that the biased CSL estimator is not anymore conservative, but tends to over-estimate the probability of the test samples.  Fig. 5 shows how well the biased CSL estimates correlate with either the true log- probabilities or the AIS-based estimates. We computed the biased CSL estimate by  7  running 10 parallel Gibbs sampling chains per test sample for either a single step or 30 steps.  It is clear from the ﬁgure that the biased CSL estimates correlate very well with the true or AIS-based estimated log-probabilities. As expected we see that the biased CSL estimator tends to overestimate the log-probabilities of the test samples. Nevertheless, we can see that the biased CSL estimator correctly orders the model performances with only a very small amount of samples.  This result suggests that in practice the biased CSL estimator, which requires only a few samples per test sample, may safely be used for the purpose of model comparison. This is especially useful when a model does not have an explicit probability function, such as DAEs and GSNs. We leave more in-depth investigation on how the biased CSL estimator works with those models that do not have an explicit probability function for the future.  7 Conclusion  We have proposed a novel sample-based estimator for estimating the probability that a trained model assigns to an example, called conservative sampling-based log-likelihood (CSL) estimator. We have justiﬁed its theoretical consistency and empirically val- idated it on recently popular generative models including restricted Boltzmann ma- chines (RBM), deep Boltzmann machines (DBM), deep belief networks (DBN), de- noising autoencoders (DAE) and generative stochastic networks (GSN).  The proposed CSL estimator uses only a set of samples of latent variables gener- ated from a model by a Markov chain. This make the estimator useful for generative models that do not have an explicit probability distribution but only deﬁne a generative procedure. Also, this property of using only samples from a model makes the estimator reﬂect, not only the generative performance of the mode, but also the mixing property of the generative procedure used to generate samples from the model. We observed this interesting phenomenon empirically by computing the CSL estimates on well-trained RBMs, DBNs and DBMs by generating samples using Gibbs sampling which is known to have a poor mixing behavior in these models.  In addition to the unbiased CSL estimator, we also proposed a biased variant of the estimator that requires only a few consecutive samples to approximate the proba- bility of a single test sample, called biased CSL estimator. The empirically evidence suggested that the biased CSL estimator can be used to compare models of varying complexities correctly, which makes the CSL estimator more useful for those models without an explicit probability function, such as GSNs, DAEs and contractive autoen- coders (CAE).  In the future, more systematic study of how the proposed CSL estimator, both unbi- ased and biased, behaves with different generative models. Especially, more empirical investigation of applying the CSL estimator to those models without an explicit proba- bility distribution but only with a generative procedure will be required.  8  Acknowledgements We would like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012), as well NSERC, CIFAR, Compute Canada, and Calcul Qu´ebec for funding.  References  Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A., Bouchard, N., and Bengio, Y. (2012). Theano: new features and speed improve- ments. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.  Bengio, Y. (2013). Estimating or propagating gradients through stochastic neurons.  Technical Report arXiv:1305.2982, Universite de Montreal.  Bengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. (2013a). Better mixing via deep  representations. In ICML’13.  Bengio, Y., Yao, L., Alain, G., and Vincent, P. (2013b). Generalized denoising auto-  encoders as generative models. In NIPS26. Nips Foundation.  Bengio, Y., Li, Y., Alain, G., and Vincent, P. (2013c). Generalized denoising auto- encoders as generative models. Technical Report arXiv:1305.6663v1, Universite de Montreal.  Bengio, Y., Yao, L., Alain, G., and Vincent, P. (2013d). Generalized denoising auto- encoders as generative models. In Advances in Neural Information Processing Sys- tems 26 (NIPS’13).  Bengio, Y., Thibodeau-Laufer, E., and Yosinski, J. (2014). Deep generative stochastic  networks trainable by backprop. Technical Report arXiv:1306.1091.  Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-Farley, D., and Bengio, Y. (2010). Theano: a CPU and GPU math expres- sion compiler. In Proceedings of the Python for Scientiﬁc Computing Conference (SciPy). Oral Presentation.  Breuleux, O., Bengio, Y., and Vincent, P. (2010). Unlearning for better mixing. Tech-  nical Report 1349, Universit´e de Montr´eal/DIRO.  Breuleux, O., Bengio, Y., and Vincent, P. (2011). Quickly generating representative  samples from an RBM-derived process. Neural Computation, 23(8), 2053–2073.  Cho, K., Raiko, T., and Ilin, A. (2013). Enhanced gradient for training restricted boltz-  mann machines. Neural computation, 25(3), 805–831.  Desjardins, G., Courville, A., Bengio, Y., Vincent, P., and Delalleau, O. (2010). Tem- pered Markov chain Monte Carlo for training of restricted Boltzmann machine. In AISTATS’2010, volume 9, pages 145–152.  9  Gretton, A., Borgwardt, K., Rasch, M., Schoelkopf, B., and Smola, A. (2012). A kernel  two-sample test. J. Mach. Learning Res., 13, 723–773.  Murray, I. and Salakhutdinov, R. (2009).  Evaluating probabilities under high-  dimensional latent variable models. In NIPS’08, volume 21, pages 1137–1144.  Neal, R. M. (2001). Annealed importance sampling. Statistics and Computing, 11(2),  125–139.  Rifai, S., Bengio, Y., Dauphin, Y., and Vincent, P. (2012a). A generative process for  sampling contractive auto-encoders. In ICML’12.  Rifai, S., Bengio, Y., Dauphin, Y., and Vincent, P. (2012b). A generative process for  sampling contractive auto-encoders. In ICML’2012.  Salakhutdinov, R. and Hinton, G. E. (2009). Deep Boltzmann machines.  TATS’2009, volume 5, pages 448–455.  In AIS-  Salakhutdinov, R. and Murray, I. (2008). On the quantitative analysis of deep belief networks. In W. W. Cohen, A. McCallum, and S. T. Roweis, editors, ICML 2008, volume 25, pages 872–879. ACM.  Welling, M. (2009). Herding dynamic weights for partially observed random ﬁeld  models. In UAI’09. Morgan Kaufmann.  A Model Descriptions  Here we describe the architecture and training procedure of each model:  cost, 200 epochs  GSN-1 (DAE): • Architecture: 784 (input) - 2000 (tanh) • Noise: (input) 0.28 salt-and-pepper, (hidden) no noise • Learning: 9-step walkback (Bengio, 2013), learning rate 0.05, cross-entropy • Early-stop: visual inspection of generated samples GSN-2: • Architecture: 784 (input) - 1500 (tanh) - 1500 (tanh) • Noise: (input) 0.4 salt-and-pepper, (hidden 1) no noise, (hidden 2) white Gaus- • Learning: learning rate 0.1, cross-entropy cost, 300 epochs • Early-stop: visual inspection of generated samples DBN-2: • Architecture: 784 (input) - 4000 (sigmoid) - 1000 (sigmoid) • Learning: (1st layer) RBM from (Cho et al., 2013) (2nd layer) RBM with PCD-9  sian noise with std. 2.0  10  DBM-2: • Architecture: 784 (input) - 500 (sigmoid) - 1000 (sigmoid) • Learning: procedure from (Salakhutdinov and Hinton, 2009) RBM: • Architecture: 784 (input) - 4000 (sigmoid) • Learning: procedure from (Cho et al., 2013) (enhanced gradient, adaptive learn-  ing rate and parallel tempering)  11  ","Several interesting generative learning algorithms involve a complexprobability distribution over many random variables, involving intractablenormalization constants or latent variable normalization. Some of them may evennot have an analytic expression for the unnormalized probability function andno tractable approximation. This makes it difficult to estimate the quality ofthese models, once they have been trained, or to monitor their quality (e.g.for early stopping) while training. A previously proposed method is based onconstructing a non-parametric density estimator of the model's probabilityfunction from samples generated by the model. We revisit this idea, propose amore efficient estimator, and prove that it provides a lower bound on the truetest log-likelihood, and an unbiased estimator as the number of generatedsamples goes to infinity, although one that incorporates the effect of poormixing. We further propose a biased variant of the estimator that can be usedreliably with a finite number of samples for the purpose of model comparison."
1308.3818,2014,Reference Distance Estimator  ,['Yanpeng Li'],https://arxiv.org/pdf/1308.3818.pdf,"     Reference Distance Estimator         Yanpeng Li   Department of Computer Science   Dalian University Technology   Dalian, China 116024   liyanpeng.lyp@gmail.com   Abstract   Abstract:  A  theoretical  study  is  presented  for  a  simple  linear  classifier  called reference distance estimator (RDE), which assigns the weight of each  feature  j  as  P(r|j)-P(r),  where  r  is  a  reference  feature  relevant  to  the  target  class  y.  The  analysis  shows  that  if  r  performs  better  than  random  guess  in  predicting  y  and  is  conditionally  independent  with  each  feature  j,  the  RDE  will  have  the  same  classification  performance  as  that  from  P(y|j)-P(y),  a  classifier trained with the gold standard y. Since the estimation of P(r|j)-P(r)  does  not  require  labeled  data,  under  the  assumption  above,  RDE  trained  with  a  large  number  of  unlabeled  examples  would  be  close  to  that  trained  with  infinite  labeled  examples.  For  the  case  the  assumption  does  not  hold,  we theoretically analyze the factors that influence the closeness of the RDE  to  the  perfect one under  the  assumption, and  present  an algorithm to select  reference  features  and  combine  multiple  RDEs  from  different  reference  features using both labeled and unlabeled data. The experimental results on  10  text  classification  tasks  show  that  the  semi-supervised  learning  method  improves  supervised methods using 5,000  labeled  examples  and 13 million  unlabeled  ones,  and  in  many  tasks,  its  performance  is  even  close  to  a  classifier trained with 13 million labeled examples. In addition, the bounds  in  the  theorems  provide  good  estimation  of  the  classification  performance  and can be useful for new algorithm design.           1    I n t ro d u c t i o n    Semi-supervised  learning  [1][2]  trains  classifiers  using  both  labeled  and  unlabeled  data  in  order  to  enhance  the  classifiers  from  labeled  data  only.  In  many  real-world  applications,  there  are  much  more  unlabeled  examples  available  than  labeled  ones,  so  the  potential  development  space  of  semi-supervised  learning  is  large.  Various  algorithms  have  been  investigated during the past 20 years, such as self-training [3], co-training [4], TSVM [5], EM  algorithm  [6] and graph-based regularization  [7].  Although  there are  successful  applications  reported,  there  is  still  big  challenge  in  this  area  [1][2].  For  example,  it  is  difficult  for  large-scale  semi-supervised  learning  to  work  on  the  current  “big  data”,  and  there  is  little  theory that can drive new algorithm design.     Why  do  people  study  semi-supervised  learning?  One  intuition  is  that  they  hope  to  move  towards a perfect upper bound where all the unlabeled examples are correctly labeled, but it  is usually an unreachable goal in practice. We call this model semi-perfect classifier, which is  trained  by  all  the  gold  standard  labels  in  both  labeled  and  unlabeled  data.  Although  it  may  not  be  a  perfect  classifier  with  100%  accuracy  due  to  the  impact  of  feature  engineering,  classification  algorithm  and  the  nature  of  data,  it  can  be  viewed  as  an  ultimate  goal  for  semi-supervised learning, because if  all the unlabeled data is correctly labeled, usually there  is  no  need  of  semi-supervised  learning.  Therefore,  the  theoretical  analysis  of  the  distance   between certain learning algorithm and its semi-perfect classifier would be  important,  since  from it we can see the potential space for the current method to be further improved.     Consider a simple linear classifier, which assigns the weight of each feature  j as P(r|j)- P(r),  where r is a Boolean feature. We call this classifier  reference distance estimator (RDE) and  the  feature  r  reference  feature.  Consider  three  different  types  of  RDEs:  1)  if  r  is  the  gold  standard  label  y  in  labeled  data,  it  can  be  viewed  as  a  supervised  classifier   trained  on  the  labeled  data;  2)  when  r  is  the  gold  standard  of  all  the  labeled  and  unlabeled  data,  it  is  a  semi-perfect classifier described above; 3) once r is not the gold standard label but a feature  learned from both labeled and unlabeled data, the RDE works in a semi-supervised setting. In  the following sections, we show in theory the impact of some characteristics of the reference  features  on  the  distance  between  a  semi-supervised  RDE  (Case  3)  and  a  semi-perfect  RDE  (Case  2).  Based  on  the  theoretical  result,  we  design  an  algorithm  for  constructing  good  reference features and combining multiple RDEs so as to improve the prediction performance.  Experiments of 10 text classification tasks are designed to examine the theory as well as the  algorithm based on RDE.       2    R e f e re n c e   D i s t a n c e   E s t i m a t o r    Let the input data be the matrix           , where               indicates the occurrence of  feature  j  in  the  example  i  (         for  “Yes”  and           for  “No”).  The  row  vector                    indicates  an  example,  and  the  column  vector                       denotes  the  occurrence  of  feature  j  on  all  the  examples.  Let                  be  the  gold  standard  class  labels,  where               (only  the  binary  case  is  considered  in  this  work).  We  define  reference distance estimator as a linear combination of features as follows:                                                                                        (1)   where  r  is  called  reference  feature,  and  the  same  for  all  the  examples;          is  the  probability of the feature  j conditioned on r, and        is the marginal probability of r. The  probability  difference                 as  the  weight  of  feature  j  measurers  the  change  of        after  seeing  .  If  r  is  highly  correlated  to  the  class  label  y,  intuitively  the  weight  of  feature j indicates the correlation information between j and y to some extent, and the linear  combination           measures a type of distance between the example     and the class label  y.  Since  the  class  label  is  a  special  Boolean  feature,  if  r  is  one  of  the  gold  standard  labels  (e.g.,  y  or    )  over  all  the  examples,  RDE  becomes  a  linear  classifier  trained  as  if  all  the  examples  are  correctly  labeled,  exactly  the  semi-perfect  classifier  described  in  Section  1,  denoted  by          or          ,  which  is  intuitively  one  of  the  best  RDEs  derived  from  different reference features. Although it is not justified by theory,  the experimental results in  Section 3 demonstrate that           estimated from 13 million labeled examples outperforms  a Naïve Bayes classifier  trained with the same data. However, in a real world application, it  is almost impossible to get the correct class labels for the all the examples directly, and what  will the performance be if  r is not the gold standard, and  how far is it from the semi-perfect  RDE? We are trying to give answers in the following theoretical analysis.    2 . 1    T h e o r e t i c a l  A n a l y s i s    2.1.1  The case under assumption   Before the introduction of the theorems, we give some new definitions related to the theory.  For a feature j, feature imbalance coefficient is defined as:                                                                                               (2)   where               ,           and            are  the  joint  probabilities  of  j  and  class  labels.  The  metric        measures  the  imbalance  degree  of  feature  j  over  the  positive  ( )  and  negative  (  )  classes.  Obviously,  for  any  j,  there  is              .  In  the  extreme  cases,            indicates  a  feature  cannot  distinguish  positive  and  negative  classes,  which  is  equivalent to random guess.        equals 1 or -α when j appears only in positive or  negative  examples respectively, that is,                          .     Theorem  1.    For  a  reference  feature  r,  if   j,                           and                               , then   i                                                                                                                              (3)   Proof:                                                                                                               (from Formula (1))   Using the assumption                                                               , we have                                                                                                                                                                                                                                                                                                                      (according to Formula (2))   The theorem implies that if r is conditionally independent with each feature j on both classes  y  and    ,  the  decision  function  can  be  written  as  the  product  of  two  parts              and           ,  where  the  impact  of  reference  feature  and  original  features  can  be  described  separately. From Theorem 1, we can get an interesting corollary as below:      Corollary 1.    For a reference feature r, if     1)            and   2)   j,                                                               , then     1)      ,                                                                                                                                       (4)   2)      ,  the  classifier            yields  the  same  ROC  curve  as  one  of            or             on the task of predicting    .     Proof: For the semi-perfect RDE           we have                                                     ,                                       and                                   (conditional  independence).     Hence using Theorem 1, we have:                                                                                                     (5)   Similarly, for          , we have:                                                                                              (6)   If          , combining Formula (3), (5) and (6), we have:                                                                                         . This concludes the proof of the first part.     For  the  second  part,  if          ,          if                     ,  using  Formula  (4),we  have                     ,  so           and           yield  the  same  ranking  order  for  each  example,  thus  leading  to  the  same  ROC  curve.  Similarly,  if          ,           and            have  the  same ROC curve. The proof is completed.     In  other  words,  if  r  performs  better  than  random  guess  and  conditionally  independent  with  each  feature  j  on  both  classes,  the  RDE            will  have  the  same  classification  performance with one of the semi-perfect RDEs           and          , since given the proper  threshold  the  same  ROC  curve  always  means  the  same  result  on  almost  all  the  evaluation  measures  for  classification  such  as  Precision,  Recall,  Accuracy,  F-score  and  AUC.  Using  Formula (4), we have                     , so the two semi-perfect classifiers tend to yield the  same  result  in  practice,  since  usually  completely  opposite  decision  functions  can  be  easily   integrated to the same one using training data.        2.1.2  The real-world case   In  the  real-world  application,  sometimes  it  is  difficult  to  find  a  reference  feature  exactly  under the assumption of Corollary 1, and how the performance will be when the assumption   does  not  hold?  Since                       and                     are  equivalent  when  the  assumption  holds,  for   the real-world case we investigate the distance between them defined as bellow:                                                                                                         (7)      It is the expectation of the absolute value of the difference between a RDE and a semi-perfect  RDE. Keeping it small tends to make a RDE close to the semi-perfect classifier.     For any feature j, r, and l, we define conditional dependence coefficient as:                                                                                                     (8)                                   It  describes  the  degree  of  conditional  dependence  between  two  features  j  and  r.  The  two  features are conditionally independent when               . This measure was found to play a  key role in bounding the distance in Formula (7).     Lemma 1. For        , there is:                                                                                                                                       (9)   Proof: if                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             If            , then              ,                       ,                           , and           .     Therefore,                                                                                                                                                                                                                                                                                                       Similarly,  we  can  obtain  that  if  any  of                                           equals  zero,  Formula (9) will be satisfied. This concludes the proof of Lemma 1.     This  lemma  implies  that  the  difference  between  the  weights  of  a  RDE  and  a  Semi -perfect  RDE  (the  left  side  of  Formula  (9))  can  be  represented  as  a  linear  combination  of             and              (the right side of Formula (9)). Based on it we can obtain the relation between  the  distance  in  Formula  (7)  and  conditional  dependence  coefficients  described  in  the  following theorems.     Theorem 2. Given a RDE          and a semi-perfect RDE         , we have:                                                                                                                                         (10)                                                                                                   (11)   Proof:    According to Formula (4) and (7),                                                                                                                                                                                                                                                                                                                                                                      (Lemma 1)                                                                                                                                                                                                                                                                                                                                                             ,                                                                                                  . (Proof of Formula (10)   Since               , for any a, b                                                          1 . Therefore, let  = +   1+   , |  and  =       , | , using (10) we have:                                                                                                                                                                                                                      . (Proof for Formula (11))   This theorem indicates that the distance between a RDE and a semi-perfect RDE in Formula  (7)  can  be  bounded  by  Inequity  (10)  or  (11),  which  is  determined  by  the  following  factors  related to the reference feature, i.e.,      ,               and              .  The theorem  leads to  the  same  result  as  Corollary  1  when           and                              (conditionally  independent). If r and j are not conditionally independent, a large        , a small               and              can also lead to a small  upper bound of distance. The bound in  Formula (11)  is  looser  than  Formula  (10),  but  there  is  no  need  to  estimate  each      ,  which  is  usually  difficult  to  learn  accurately  from  limited  labeled  training  data.  Note  that  the  estimation  of               and                also  requires  the  joint  probabilities  concerned  with  the  class  labels, which could be inaccurate due to the limited size of labeled data. Intuitively, there is  connection  between  conditional  independence  and  non-conditional  independence,  so  we  derive another bound including only the measure of non-conditional independence.     Theorem 3: Given a RDE          and a semi-perfect RDE         ,     if                              , then:                                                                                                             (12)   where                                                                                                                   (13)   Proof: combining Formula (10) and                         ,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Here  in  order  to  make  the  following  analysis  easier  we  assume  that  the  signs  of             and              are  equivalent,  which  is  a  much  more  relaxed  assumption  than  that  in  Theorem  1.  The  bound  avoids  the  estimation  of                or               ,  while  it  introduces a new term M. Using Formula (2) and (13), we can bound M as follow:                                                                                                                     (14)   In  the  experiment  we found  M  changed slightly  with  different  r  and was  much smaller than  the first part. In this case, we can use only the first part of Formula (12) for reference feature  selection,  which  suggests  selecting  the  reference  feature  with  high  precision  in  individual  performance  (a  big         )  and  high  independence  with  other  features  (a  small                          ).                     2 . 2     A l g o r i t h m   D e s i g n    The theory can be used for algorithm design in several aspects: 1) the bounds in Theorem 2  and  3  can  be  used  for  reference  feature  selection.  2)  Even  if  the  performance  of  individual  RDE  is  inferior,  combining  multiple  RDEs  derived  from  different  reference  features  may  lead to a higher performance.  3) Prune the original features that yield a high distance. Note  that  different  from  Method  1,  Method  3  is  to  select  the  original  feature  j  rather  than  the  reference  feature  r.  For  example,  if  one  of  the  original  features  is  the  same  as  or  highly  correlated  to  the  reference  feature,  removing  it  from  feature  set  X  tends  to  lower  the  upper  bounds  in  Theorem  2  and 3.  In  the  following, we  present  a  simple  algorithm  that  addresses  the strategies:     1)  Rank candidate reference features by   and select top k reference features.                                                         in ascending order   2)  Construct k RDEs with the selected reference features in Step 1.     3)  For each RDE remove any original feature j with                                   4)  Build a classifier using the decision score of each pruned RDE as a feature, and train   the classifier with labeled examples.   Here the candidate reference features can be the existing features in labeled data, and          can  be  estimated  from  labeled  data,  and              from  unlabeled  data.  Since                                we assume that features are Boolean types, all the probabilities can be estimated based on the  frequencies in labeled or unlabeled data:                        ,                         ,                           ,                                       where A and B refer to any features,         is count of A in unlabeled data,            is the  co-occurrence count of A and B in unlabeled data, and      is the total number of examples in  unlabeled data. Similarly,      means labeled data. Note that labeled data is used only for the  estimation of      .     In the algorithm, there are two parameters, i.e., k (the number of reference feature) and t (the  threshold  for  feature  selection)  required  to  be  selected  in  practice.  The  parameters  can  be  tuned in the labeled training data and also depends on the specific classifier used in Step (4).     This  algorithm  can  be  viewed  as  an  extension  of  our  previous  work  feature  coupling  generalization  (FCG)  [8],  a  framework  for  generating  new  features  from  co-occurrence  in  unlabeled  data.  It  achieved  state-of-the-art  performance  in  several  challenging  datasets.  However, FCG is more of a general strategy rather than a specific algorithm, that is, there is   a lot of heuristics in the design of FCG, and no theoretical  analysis  to justify why it works,  which  seriously  limit  its  applications.  The  theoretical  study  here  shed  light  on  why  FCG  works  in  a  special  case  where  the  feature  co-occurrence  measure  is               ,  and  the  algorithm presented here is much easier to be applied to various real-world tasks.       3    E x p e r i m e n t s    The dataset consists of 13 million MEDLINE abstracts. The task is to determine whether an  article  belongs  to  one  of  the  10  classes,  the  first  10  main  headings  in  MeSH  tree  (see  also  Table  1).  The  reason  we  used  the  10  binary  text  classification  tasks  is  that  the  dataset  has  13M  labeled  examples,  so  it  is  able  to  estimate a  more accurate  semi-perfect classifier than  other smaller datasets, which is crucial in this work, and the MEDLINE data is clean and free  available,  so  the  experiment  can  be  followed  easily.  Here  we  randomly  selected  5,000  examples from the 13M ones as labeled data, other 5,000 examples for testing and the rest of  them as unlabeled data. Sampling was done 5 times, and the mean and standard deviation of  AUCs  were  reported  in  Table  1  for  each  task.  AUC  was  used  for  evaluation  because  some  classes are highly imbalanced where the AUC can be a more stable measure than F-score and  Accuracy, and it avoids the impact of tuning thresholds.  Two feature sets  were  examined in  this work: unigrams and bigrams of words. Three types of RDEs  were investigated: SuRDE  is supervised RDE that used the label y as reference feature (Case 1 in Section 1). SeRDE is  the  ensemble  of  semi-supervised  RDEs  (the  algorithm  in  Section  2.2).  PerfRDE  is  SuRDE  trained  with  the  13M  labeled  examples,  which  was  treated  as  an  approximation  of  semi-perfect  RDE.  For  SeRDE,  k  was  set  at  100,  t  was  20  and  the  candidate  reference  features were words with the count over 20 in each training set, since the estimation of        from 5k labeled examples could be inaccurate for low frequency words. Logistic regression  in  Weka  [9]  (with  “ridge”  parameter  set  at  40  for  all  the  tasks)  was  used  to  integrate  the  RDE-based  features  in  the  SeRDE.  Naïve  Bayes  classifier  with  Laplace  smoothing  and  toolkits for large scale  SVM [10] and Logistic regression  [11] were used as baselines. Their  parameters were tuned to be the optimal on the test set of each sampling.           C U A  0.9   0.8   0.7   0.6      C U A  0.9   0.8   0.7   0.6   NB   SeRDE   SVM   LR   SuRDE   NB   SeRDE   SVM   LR   SuRDE   Number of labeled examples   Number of labeled examples         (a) Unigram feature   (b) Bigram feature   Figure 1: Average performance varied with the size of labeled training data   In Figure 1, we compare the average performance of classifiers trained on different sizes of  labeled data, and it shows that  given more labeled examples, the performances increase and  the discrepancy between different classifiers become small. In these tasks, SVM and Logistic  regression were difficult to work on more than 200K training examples due to the complexity  of optimization algorithms. The best AUCs from RDEs on the two feature sets are 86% and  87.1% respectively, which are  treated as  the approximation of semi-perfect RDEs, the goals  we  aim  at.  It  is  encouraging  to  see  that  using  5,000  labeled  examples,  the  performance  of  SeRDE  (84.9%  and  84.1%)  is  better  than  supervised  methods  just  as  moving  towards  the  semi-perfect classifiers. A much larger improvement for bigrams than unigrams indicates that  RDE  can  be  especially  effective  in  solving  the  data  sparseness  problem  caused  by  bigrams.  When the training data is small, unigrams perform better than bigrams, and vice versa, which  implies  that  semi-supervised  learning  provides  more  opportunities  for  feature  engineering,   since  some  poor  features  in  supervised  learning  can  become  good  features  using  unlabeled  data.  Also  it  is  notable  that  supervised  RDE  from  5,000  labeled  examples  performs  much  better any other  supervised classifiers, while SVM performs best on 200K labeled examples  on unigram features.          ) t s i D ( g o L  8   6   4   2   0     ) t s i D ( g o L  8   6   4   2   0   Dist   Bound   Dist   Bound   0.5   0.7   AUC   0.9   0.5   0.7   AUC   0.9                (a) Chemicals (500K labeled examples)                  (b) Chemicals (5K labeled examples)     ) t s i D  (   g o  l  8   6   4   2   0   0.5   0.55   0.6   0.65   AUC     ) t s i D ( g o  l  8   6   4   2   0   Dist   Bound      Dist   Bound   0.5   0.55   0.6   0.65   AUC         (c) Disciplines (500K labeled examples)                  (d) Disciplines (5K labeled examples)   Figure 2: Relation between AUC of RDE, the distance (RDE vs. Semi-perfect RDE) and the   bound of the distance.     Dist – the distance in Formula (7); Bound – the first part in Formula (12). The X axis is AUC and Y  axis is the log of each distance based on 10. Each point relates to a RDE with a word-based reference  feature. The size of labeled data used to estimate the bound is given in bracket. The best and worst  performing classes (‘Chemicals’ and ‘Disciplines’) were selected for observation.      From Figure 2, we can see clearly that the AUC achieved by RDE is highly correlated to the  distance  as  well  as  bound  in  the  Theorem  3  especially  when  the  AUC  is  high,  which  convinces  our  effort  to  improve  the  performance  by  minimizing  the  distance.  In  addition,  Figure  2  shows  that  the  bound  and  real  distance  have  the  similar  trend,  and  the  bound  estimated  from  5K  labeled  data  and  500K  unlabeled  data  have  similar  shapes  to  that  from  500K  labeled  data.  This  demonstrates  that  the  bound  can  be  useful  for  algorithm  design  in  practice,  which  was  also  convinced  by  the  good  performance  of  SeRDE.  In  addition,  a  surprising  finding  is  that  in  some  task,  e.g.,  the  “Chemicals”  class,  an  RDE  from  only  one  word-based  reference  feature  can  lead  to  a  high  AUC  over  85%,  since  every  data  point  in  Figure 2 is concerned with one reference feature. Therefore, in some applications, if we don’t  have  labeled  examples  but  know  some  good  reference  feature,  for  example,  with  high  imbalance  degree  in  Formula  (2)  based  on  our  prior  knowledge,  a  high  classification  performance can be also obtained via RDE without any labeled training data.     For  each  task  we  compare  the  performance  of  several  semi-supervised  learning  algorithms  including Co-training, TSVM and SeRDE in Table 1. Here the base classifier for co-training  is  supervised  RDE,  since  it  is  the  best  supervised  classifier  in  the  experiment  and  scalable   well. For co-training, we split the feature set into two views: the first half of the  article and  the  rest  half.  We  found  that  there  was  no  significant  change  after  co-training,  and  the  improvement in ‘Anthropology’ was just because of the divide-and-conquer of the two views but  not the introduction of unlabeled data. TSVM in SVM-light [5] was used in the experiment. It  was also not able to improve the performance over supervised SVM . One possible reason is  that the size of both labeled and unlabeled data are  big in this work, while in most previous  works  these  two  methods  were  reported  to  be  effective  on  very  small  size  of  data,  e.g.,  several labeled examples and thousands of unlabeled ones [2]. It is also notable that in many  of  the  tasks  SeRDE  outperformed  SVM  by  5%  -  10%,  and  performed  as  well  as  the  semi-perfect  RDE,  while  SVM  has  been  considered  as  one  of  the  most  effective  class ifiers  for text classification.  It also justified the  strategy in Section 2.2 that the  ensemble of RDEs  can move closer to the semi-perfect RDE, although the performance of individual RDE may  be inferior. We also tried TFIDF weighting scheme for SVM as well as other classifiers, but  the results were not as good as binary features, so they were not reported in this paper.        Table 1: Comparison of AUC (%) on each task.     The size of labeled data used is in bracket. The features are unigram bag-of-words. There are  13M labeled examples used in “PerfRDE”, and 5K used in the other methods. The best results  from 5K labeled data were bolded.        Task   Anatomy    Organisms    Diseases  Chemicals  Analytical  Psychiatry  Phenomena  Disciplines  Anthropology  Technology  Average   SVM    (5K)  82.6±0.4  82.1±1.7  83.7±0.7  87.6±0.4  70.2±0.9  83.8±0.4  80.1±0.2  65.8±0.7  76±0.5  74.7±1.7  78.7±0.2        5    C o n c l u s i o n s    SuRDE    (5K)  82.3±0.6  87.1±1.4  85.1±0.5  88.6±0.2  71.1±0.8  88.9±0.6  80.4±0.3  70.2±1.5  84.1±0.4  82.8±0.8  82.1±0.1   Co-training  (5K)  82.2±0.6  87.6±1.2  85.9±0.2  88.2±0.3  71±0.7  89.1±0.8  80.6±0.5  70.5±2.3  85.7±0.3  80.8±2.2  82.2±0.2   SeRDE  (5K)   TSVM  (5K)  81.6±0.4  85.8±0.8  79.3±1.5  90.2±1.2  80.3±0.8  87.7±0.5  86.1±0.5  91.8±0.2  69.4±0.8  74.4±0.8  81.6±0.9  91±1.0  78.8±0.5  82.7±0.5  63.6±1.0  74.2±1.6  74.6±0.6  86.3±0.7  72.5±1.9  85.4±1.1  76.8±0.4  84.9±0.2   PerfRDE    (13M )  84.9±0.5  92.8±0.8  87.9±0.2  91.2±0.2  73.8±0.8  91.8±0.5  82.6±0.2  78.9±1.6  88.3±0.8  88±1.5  86±0.3   In this work, we found an interesting theory behind RDE, a simple classifier with pretty good  performance  both  in  supervised  and  semi-supervised  settings.  The  theory  as  well  as  algorithm provides  a  powerful  tool  for  exploiting  the  “big data”. There are  many  directions  that can be addressed in the future:     1)  Applying  it  to  various  machine  learning  tasks  with  a  lot  of  unlabeled  data.  For  binary  features,  it  can  be  used  in  a  straightforward  way  as  the  experiment  in  this  work.  For  real  value  features,  one  strategy  is  to  use  discretization  technique  or  binary  classifier  to  convert  the  features  into  binary  ones,  and  another  way  is  to  give  density  estimation  for  the  probabilities in RDE like generative models.     2)  Developing  better  algorithms  based  on  the  theory,  for  example,  learning  a  reference  feature  by  optimizing  the  bounds,  and  jointly  selecting  original  features  and  reference  features, which will lead to many novel machine learning algorithms.     3)  Exploiting  potentially  useful  features  for  RDE  using  feature  conjunction,  such  as  the  bigram  features  in  the  experiment,  which  were  not  good  features  in  5K  labeled  data  but  worked well in 13M data. It implies that the effectiveness of features even in the same task is  not  constant  but  varies  with  the  size  of  both  labeled  and  unlabeled  data.  For  the  tasks  that   have  already  reached  the  semi-perfect  RDE  (e.g.,  some  tasks  in  Table  1),  the  only  way  to  improve  the  performance  further  is  to  build  another  semi-perfect  RDE  with  better  performance  using  feature  conjunction  (e.g.,  high  order  n-grams),  and  find  another  set  of  reference features that fit the new feature set. Therefore, machine learning can be  described  as  a  process  that  we  first  establish  a  feature  set  that  could  potentially  lead  to  a  good  semi-perfect  classifier,  and  then  find  good  reference  features  by  learning  from  as  much  unlabeled  data  as  possible  so  as  to  move  close  to  this  goal.  There  would  be  a  lot  of  theoretical as well as experimental work to do in this new area.        A c k n o w l e d g m e n t s    This  work  was  supported  in  part  by  grants  from  “the  Fundamental  Research  Funds  for  the  Central  Universities  No.  DUT11RC(3)85”  and  “China  Postdoctoral  Science  Foundation  funded project No. 2012M511147.”      A. Blum and T. Mitchell, “Combining labeled and unlabeled data with co-training,” in Proceedings   O. Chapelle, B. Schölkopf, A. Zien, and others, Semi-supervised learning, vol. 2. MIT press   T. Joachims, “Transductive inference for text classification using support vector machines,” in   X. Zhu, “Semi-supervised learning literature survey,” 2005.  D. Yarowsky, “Unsupervised word sense disambiguation rivaling supervised methods,” in   R e f e r e n c e s   [1]  Cambridge, 2006.  [2]  [3]  Proceedings of the 33rd annual meeting on Association for Computational Linguistics, 1995, pp. 189–196.  [4]  of the eleventh annual conference on Computational learning theory, 1998, pp. 92–100.  [5]  MACHINE LEARNING-INTERNATIONAL WORKSHOP THEN CONFERENCE-, 1999, pp. 200–209.  [6]  K. Nigam, A. K. McCallum, S. Thrun, and T. Mitchell, “Text classification from labeled and  unlabeled documents using EM,” Machine learning, vol. 39, no. 2–3, pp. 103–134, 2000.  [7]  harmonic functions,” in MACHINE LEARNING-INTERNATIONAL WORKSHOP THEN CONFERENCE-,  2003, vol. 20, p. 912.  [8]  applications in biomedical literature mining,” IEEE/ACM Transactions on Computational Biology and  Bioinformatics (TCBB), vol. 8, no. 2, pp. 294–307, 2011.  [9]  mining software: an update,” ACM SIGKDD Explorations Newsletter, vol. 11, no. 1, pp. 10–18, 2009.  [10]  [11]  categorization,” Technometrics, vol. 49, no. 3, pp. 291–304, 2007.   T. Joachims, “Making large scale SVM learning practical,” 1999.  A. Genkin, D. D. Lewis, and D. Madigan, “Large-scale Bayesian logistic regression for text   M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten, “The WEKA data   Y. Li, X. Hu, H. Lin, and Z. Yang, “A framework for semisupervised feature generation and its   X. Zhu, Z. Ghahramani, and J. Lafferty, “Semi-supervised learning using gaussian fields and         ","A theoretical study is presented for a simple linear classifier calledreference distance estimator (RDE), which assigns the weight of each feature jas P(r|j)-P(r), where r is a reference feature relevant to the target class y.The analysis shows that if r performs better than random guess in predicting yand is conditionally independent with each feature j, the RDE will have thesame classification performance as that from P(y|j)-P(y), a classifier trainedwith the gold standard y. Since the estimation of P(r|j)-P(r) does not requirelabeled data, under the assumption above, RDE trained with a large number ofunlabeled examples would be close to that trained with infinite labeledexamples. For the case the assumption does not hold, we theoretically analyzethe factors that influence the closeness of the RDE to the perfect one underthe assumption, and present an algorithm to select reference features andcombine multiple RDEs from different reference features using both labeled andunlabeled data. The experimental results on 10 text classification tasks showthat the semi-supervised learning method improves supervised methods using5,000 labeled examples and 13 million unlabeled ones, and in many tasks, itsperformance is even close to a classifier trained with 13 million labeledexamples. In addition, the bounds in the theorems provide good estimation ofthe classification performance and can be useful for new algorithm design."
1311.0701,2014,On Fast Dropout and its Applicability to Recurrent Networks  ,"['Justin Bayer', 'Christian Osendorfer', 'Sebastian Urban', 'Nutan Chen', 'Daniela Korhammer', 'Patrick van der Smagt']",https://arxiv.org/pdf/1311.0701.pdf,"4 1 0 2    r a  M 5         ] L M  . t a t s [      7 v 1 0 7 0  .  1 1 3 1 : v i X r a  On Fast Dropout and its Applicability to Recurrent  Networks  Justin Bayer, Christian Osendorfer, Daniela Korhammer, Nutan Chen, Sebastian Urban and Patrick van der Smagt  Lehrstuhl f¨ur Robotik und Echtzeitsysteme  Fakult¨at f¨ur Informatik  Technische Universit¨at M¨unchen  bayer.justin@googlemail.com, osendorf@in.tum.de, korhammd@in.tum.de,  ntchen86@gmail.com, surban@tum.de, smagt@brml.org  Abstract  Recurrent Neural Networks (RNNs) are rich models for the processing of sequen- tial data. Recent work on advancing the state of the art has been focused on the optimization or modelling of RNNs, mostly motivated by adressing the problems of the vanishing and exploding gradients. The control of overﬁtting has seen con- siderably less attention. This paper contributes to that by analyzing fast dropout, a recent regularization method for generalized linear models and neural networks from a back-propagation inspired perspective. We show that fast dropout imple- ments a quadratic form of an adaptive, per-parameter regularizer, which rewards large weights in the light of underﬁtting, penalizes them for overconﬁdent predic- tions and vanishes at minima of an unregularized training loss. The derivatives of that regularizer are exclusively based on the training error signal. One conse- quence of this is the absence of a global weight attractor, which is particularly appealing for RNNs, since the dynamics are not biased towards a certain regime. We positively test the hypothesis that this improves the performance of RNNs on four musical data sets.  1  Introduction  Recurrent Neural Networks are among the most powerful models for sequential data. The capa- bilty of representing any measurable sequence to sequence mapping to arbitrary accuracy (Hammer, 2000) makes them universal approximators. Nevertheless they were given only little attention in the last two decades due to the problems of vanishing and exploding gradients (Hochreiter, 1991; Bengio et al., 1994; Pascanu et al., 2012). Error signals either blowing up or decaying exponentially for events many time steps apart rendered them largely impractical for the exact problems they were supposed to solve. This made successful training impossible on many tasks up until recently without resorting to special architectures or abandoning gradient-based optimization. Successful application on tasks with long-range dependencies has thus relied on one of those two paradigms. The former ist to make use of long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997). These approaches are among the best methods for the modelling of speech and handwriting (Graves et al., 2013, 2008; Graves, 2013). The latter is to to rely on sensible initializations leading to echo-state networks (J¨ager et al., 2003). The publication of (Martens and Sutskever, 2011) can nowadays be considered a landmark, since it was shown that even standard RNNs can be trained with the right optimization method. While a sophisticated Hessian-free optimizer was employed initially, further research (Sutskever et al., 2013; Bengio et al., 2012) has shown that carefully designed ﬁrst-order methods can ﬁnd optima of similar quality.  1  After all, the problem of underﬁtting standard RNNs can be dealt with to the extent that RNNs are practical in many areas, e.g., language modelling (Sutskever et al., 2011; Mikolov et al., 2010). In contrast, the problem of overﬁtting in standard RNNs has (due to the lack of necessity) been tackled only by few. As noted in (Pascanu et al., 2012), using priors with a single optima on the parameters may have detrimental effects on the representation capability of RNNs: a global attractor is constructed in parameter space. In the case of a prior with a mode at zero (e.g. an L2 regularizer) this biases the network towards solutions which lets information die out exponentially fast in time, making it impossible to memorize events for an indeﬁnite amount of time. Graves (2011) proposes to stochastically and adaptively distort the weights of LSTM-based RNNs, which is justiﬁed from the perspective of variational Bayes and the minimum description length principle. Overﬁtting is practically non-existent in the experiments conducted. It is untested whether this approach works well for standard RNNs–along the lines of the observations of Pachitariu and Sahani (2013) one might hypothesize that the injected noise disturbs the dynamics of RNNs too much and leads to divergence during training. The deep neural network community has recently embraced a regularization method called dropout (Hinton et al., 2012). The gist is to randomly discard units from the network during training, leading to less interdependent feature detectors in the intermediary layers. Here, dropping out merely means to set the output of that unit to zero. An equivalent view is to set the complete outgoing weight vector to zero of which it is questionable whether a straight transfer of dropout to RNNs is possible. The resulting changes to the dynamics of an RNN during every forward pass are quite dramatic. This is the reason why Pachitariu and Sahani (2013) only use dropout on those parts of the RNN which are not dynamic., i.e. the connections feeding from the hidden into the output layer. Our contribution is to show that using a recent smooth approximation to dropout (Wang and Man- ning, 2013) regularizes RNNs effectively. Since the approximation is deterministic, we may assert that all dynamic parts of the network operate in reasonable regimes. We show that fast dropout does not keep RNNs from reaching rich dynamics during training, which is not obvious due to the relation of classic dropout to L2 regularization (Wager et al., 2013). The structure of the paper is as follows. We will ﬁrst review RNNs and fast dropout (FD) (Wang and Manning, 2013). A novel analysis of the derivatives of fast dropout leads to an interpretation where we can perform a decomposition into a loss based on the average output of a network’s units and a regularizer based on its variance. We will discuss why this is a form that is well suited to RNNs and consequently conduct experiments that conﬁrm our hypothesis.  2 Methods  In this section we will ﬁrst review RNNs and fast dropout. We will then introduce a novel interpre- tation of what the fast dropout loss constitutes in section 2.2.2 and show relationships to two other regularizers.  2.1 Recurrent Neural Networks  We will deﬁne RNNs in terms of two components. For one, we are ultimately interested in an output y, which we can calculate given the parameters θ of a network and some input x. Secondly, we want to learn the parameters, which is done by the design and optimization of a function of the parameters L(θ), commonly dubbed loss, cost, or error function. Calculating the Output of an RNN Given an input sequence x = (x1, . . . , xT ), xt ∈ Rκ we produce an output y = (y1, . . . , yT ), yt ∈ Rω which is done via an intermediary representation called the hidden state layer h = (h1, . . . , hT ), ht ∈ Rγ. κ, ω, and γ are the dimensionalities of the inputs, outputs, and hidden state at each time step. Each component of the layers is sometimes referred to as a unit or a “neuron”. Depending on the associated layer, these are then input, hidden, or output units. We will also denote the set of units which feed into some unit i as the incoming units of i. The units into which a unit i feeds are called the outgoing units of i. For a recurrent network  2  with a single hidden layer, this is done via iteration of the following equations from t = 1 to T :  ht = fh(xtWin + ht−1Wrec + bh), yt = fy(htWout + by),  where {Win, Wout, Wrec} are weight matrices and {bh, by} bias vectors. These form the set of pa- rameters θ together with initial hidden state h0. The dimensionalities of all weight matrices, bias vectors, and initial hidden states are determined by the dimensionalities of the input sequences as well as desired hidden layer and output layer sizes. The functions fh and fy are so-called transfer functions and mostly coordinate-wise applied nonlinearities. We will call the activations of units pre-synaptic before the application of f and post-synaptic afterwards. Typical choices include the logistic sigmoid f (ξ) = 1+exp(−ξ), tangent hyperbolicus and, more recently, the rectiﬁed linear f (ξ) = max(ξ, 0) (Zeiler et al., 2013). If we set the recurrent weight matrix Wrec to zero, we recover a standard neural network Bishop (1995) applied independently to each time step.  1  Loss Function and Adaption of Parameters We will restrict ourselves to RNNs for the su- pervised case, where we are given a data set D = {(xi, zi)}N i=1 consisting of N pairs with xi ∈ RT×κ and zi ∈ RT×ω. Here T refers to the sequence length, which we assume to be constant over the data set. We are interested to adapt the parameters of the network θ in a way to let each of its outputs yi ∈ RT×O be close to zi. Closeness is typically formulated as a loss function, e.g. 2 or the binary cross entropy i z(i) log y(i) +(1−z(i)) log(1−y(i)). If a loss L is locally differentiable, ﬁnding good parameters can be performed by gradient-based optimization, such as nonlinear conjugate gradients or stochastic gradient descent. The gradients can be calculated efﬁciently via back-propagation through time (BPTT) (Rumelhart et al., 1986).  the mean squared error Lmse(θ) = (cid:80)  Lbce(θ) =(cid:80)  i ||z(i) − y(i)||2  2.2 Fast Dropout  In fast dropout (Wang and Manning, 2013), each unit in the network is assumed to be a random variable. To assure tractability, only the ﬁrst and second moments of those random variables are kept, which sufﬁces for a very good approximation. Since the pre-synaptic activation of each unit is a weighted sum of its incoming units (of which each is dropped out with a certain probability) we can safely assume Gaussianity for those inputs due to the central limit theorem. As we will see, this is sufﬁcient to ﬁnd efﬁcient ways to propagate the mean and variance through the non-linearity f.  2.2.1 Forward propagation  We will now inspect the forward propagation for a layer into a single unit, that is  a = (d ◦ x)T w y =  f (a),  where ◦ denotes the element-wise product and f is a non-linear transfer function as before. Let the input layer x to the unit be Gaussian distributed with diagonal covariance by assumption: x ∼ x). Furthermore, we have Bernoulli distributed variables indicating whether an incoming N (µx, σ2 unit is not being dropped out organized in a vector d with di ∼ B(p), p being the complementary drop out rate. The weight vector w is assumed to be constant. A neural network will in practice consist of many such nodes, with some of them, the output units, directly contributing to the loss function L. Others, the input units, will not stem from calculation but come from the data set. Each component of x represents an incoming unit, which might be an external input to the network or a hidden unit. In general, y will have a complex distribution depending highly on the nature of f. Given that the input to a function f is Gaussian distributed, we obtain the mean and variance of the output as follows:  (cid:90) (cid:90)  E[y] = fµ(a) =  V[y] = fσ(a) =  f (x)N (x|E[a], V[a])dx, (f (x) − fµ(a))2N (x|E[a], V[a])dx.  3  Forward propagation through the non-linearity f for calculation of the post-synaptic activation can be approximated very well in the case of the logistic sigmoid and the tangent hyperbolicus and done exactly in case of the rectiﬁer (for details, see Wang and Manning (2013)). While the rectiﬁer has been previously reported to be a useful ingredient in RNNs (Bengio et al., 2012) we found that it leads to unstable learning behaviour in preliminary experiments and thus neglected it in this study, solely focusing on the tangent hyperbolicus. Other popular transfer functions, such as the softmax, need to be approximated either via sampling or an unscented transform (Julier and Uhlmann, 1997). To obtain a Gaussian approximation for a = (d ◦ x)T w, we will use ˆa ∼ N (E[a], V[a]). The mean and variance of a can be obtained as follows. Since d and x are independent it follows that (1) For independent random variables A and B, V[AB] = V[A]E[B]2 + E[A]2V[B] + V[A]V[B]. If we assume the components of x to be independent, we can write 1 x)T w2.  (2) Furthermore the independency assumption is necessary such that the Lyapunov condition is satisﬁed (Lehmann, 1999) for the the central limit theorem to hold, ensuring that a is approximately Gaussian. Propagating the mean and the variance through f via fµ and fσ sufﬁces for determining the pre- synaptic moments of the outgoing units. At the output y of the whole model, we will simplify matters and ignore the variance. Some loss functions take the variance into account (e.g., a Gaussian log-likelihood as done in (Bayer et al., 2013)). Sampling can be a viable alternative as well.  E[a] = E[(x ◦ d)T w] = (E[x] ◦ E[d])T w.  V[a] = (p(1 − p)µ2  x + pσ2  Fast Dropout for RNNs The extension of fast dropout to recurrent networks is straightforward from a technical perspective. First, we note that we can concatenate the input vector at time step t, xt and the hidden state at the previous layer ht−1 into a single vector: ct = [ht−1, xt]. We obtain a corresponding weight matrix by concatenation of the input to hidden and recurrent weight matrices Win and Wrec: Wc = [Wrec, Win]. We can thus reduce the computation to the step from above.  2.2.2 Beyond the Backward Pass: A Regularization Term  Given the forward pass, we used automatic differentiation with Theano (Bergstra et al., 2010) to calculate the gradients. Nevertheless, we will contribute a close inspection of the derivatives. This will prove useful since it makes it possible to interpret fast dropout as an additional regularization term independent of the exact choice of loss function. Consider a loss L(D; θ) which is a function of the data D and parameters θ 2. In machine learning, we wish this loss to be minimal under unseen data D although we only have access to a training set Dtrain. A typical approach is to optimize another loss J (Dtrain; θ) as a proxy in the hope that a good minimum of it will correspond to a good minimum of L for unseen data. Learning is often done by the optimization of J = L + R, where R is called a regularizer. A common example of a regular- izer is to place a prior on the parameters, in which case it is a function of θ and corresponds to the log-likelihood of the parameters. For weight decay, this is a spherical Gaussian with inverse scale 2. Regularizers can be more sophisticated, e.g. Rifai et al. (2011) determine λ, i.e. Rwd(θ) = λ||θ||2 directions in input space to which a model’s outputs should be invariant. More recently, dropout (i.e. non-fast dropout) for generalized linear models has been intepreted as a semi-supervised regulariza- tion term encouraging conﬁdent predictions by Wager et al. (2013). While it seems difﬁcult to bring the objective function Jfd(D; θ) of fast dropout into the form of L + R, it is possible with the derivatives of each node. For this, we perform back-propagation like calculations. Let a = (d◦ x)T w and y = f (a) be the pre- and post-synaptic activations of a component of a layer in the network. First note that ∂J /∂wi = ∂J /∂a· ∂a/∂wi according to the chain rule. Since a is a random variable, it will be described in one of two forms. In the case of a Gaussian approximation, we will summarize it in terms of its mean and variance; this approach is used if propagation through f is possible in closed form. In the case of sampling, we will have a single instantiation ˆa of the random variable, which we can propagate through f. An analysis of both cases is as follows.  V[a] = p(1 − p)µ2T  1In contrast to Wang and Manning (2013), we do not drop the variance not related to dropout. In their case, 2We will frequently omit the explicit dependency on D and θ where clear from context.  x w2 was used instead of Equation (2).  4  Gaussian approximation We ﬁnd the derivative of J with respect to one of its incoming weights wi to be  ∂J ∂wi  =  ∂J ∂E[a]  ∂E[a] ∂wi  +  ∂J ∂V[a]  ∂V[a] ∂wi  .  We know that E[a] = (x◦d)T w and thus ∂E[a]/∂wi = xidi. This can be recognized as the standard back-propagation term if we consider the dropout variable di as ﬁxed. We will thus deﬁne  ∂La ∂wi  :=  ∂J ∂E[a]  ∂E[a] ∂wi  ,  (3)  and subsequently refer to it as the local derivative of the training loss. The second term can be analysed similarly. We apply the chain-rule once more which yields  ∂J ∂V[a]  ∂V[a] ∂wi  =  ∂V[a] ∂w2 i  ∂w2 i ∂wi  .  ∂J ∂V[a]  (cid:124) (cid:123)(cid:122) (cid:125)  :=δa  for which any further simpliﬁcation of δa depends on the exact form of J . The remaining two factors can be written down explicitly, i.e.  ∂V[a] ∂w2 i ∂w2 i ∂wi  = p(1 − p)E[xi]2 + pV[xi],  = 2wi.  Setting  we conclude that  ηa i :=  = |δa|p(cid:2)(1 − p)E[xi]2 + V[xi](cid:3)  |δa| ∂V[a]  ∂w2 i  >  0  ∂J ∂V[a]  ∂V[a] ∂wi  = 2 sgn(δa)ηawi  =:  ∂Ra approx ∂wi  .  In alignment with Equation (3) this lets us arrive at  ∂J ∂wi  =  ∂La ∂wi  +  ∂Ra approx ∂wi  ,  and offers an interpretation of fast dropout as an additive regularization term. An important and limiting aspect of this decomposition is that it only holds locally at a. We note that depending on the sign of the error signal δa, fast dropout can take on three different behaviours:  • δa = 0 The error signal is zero and thus the variance of the unit considered to be optimal for the loss. The fast dropout term vanishes; this is especially true at optima of the overall loss.  • δa < 0 The unit should increase its variance. The exact interpretation of this depends on the loss, but in many cases this is related to the expectation of the unit being quite erroneous and leads to an increase of scatter of the output. The fast dropout term encourages a quadratic growth of the weights.  • δa > 0 The unit should decrease its variance. As before, this depends on the exact loss function but will mostly be related to the expectation of the unit being quite right which makes a reduction of scatter desirable. The fast dropout term encourages a quadratic shrink- age of the weights.  5  Figure 1: Visualizations of the behaviour of δa for a single unit. The axes correspond to the pre- synaptic mean E[a] and variance V[a] feeding into a unit y = f (a). A loss measuring the divergence from the target value 0.2 is then applied and indices the color on a logarithmic scale. The gradients of the loss are shown as a vector ﬁeld plot. Squared error is shown on the left, Gaussian log-likelihood in the middle and Bernoulli log-likelihood on the right. For the ﬁrst two plots, the optimium is in the middle, for the last it is a little to the left.  This behaviour can be illustrated for output units by numerically inspecting the values and gradients of the pre-synaptic moments given a loss. For that we consider a single unit y = f (a) and a loss d(y, z) measuring the divergence of its output to a target value z. The pre-synaptic variance V[a] can enter the loss not at all or in one of two ways, respected by either the loss (see (Bayer et al., 2013)) or the transfer function. Three examples for this are  1. Squared loss on the mean, i.e. d(y, z) = (E[y] − t)2 with y = a, 2. Gaussian log-likelihood on the moments, i.e. d(y, z) ∝ (E[y]−t)2 3. Negative Bernoulli cross entropy, i.e. d(y, z) = z log E[y] + (1 − z) log(1 − E[y]) with  2V[y] + log(cid:112)2πV[y] with  y = a,  1  y =  1+exp(−a).  We visualize the pre-synaptic mean and variance, their gradients and their respective loss values in Figure 1. For the two latter cases, erroneous units ﬁrst increase the variance, then move towards the correct mean and subsequently reduce the variance.  Sampling An already mentioned alternative to the forward propagation of E[a] and V[a] through  f is to incarnate ˆa via sampling and calculate f (ˆa). Let s ∼ N (0, 1) and ˆa = E[a] + s(cid:112)V[a]. We  can then use ˆa explicitly and it follows that  (cid:20) ∂ˆa  ∂E[a]  ∂J ∂ˆa  ∂ˆa ∂wi  =  ∂J ∂ˆa  ∂E[a] ∂wi  +  ∂ˆa  ∂V[a]  ∂V[a] ∂wi  (cid:21)  .  Again, we recognize ∂J /∂ˆa · ∂ˆa/∂E[a] · ∂E[a]/∂wi = ∂J /∂ˆa xidi as the standard back- propagation formula with dropout variables. The variance term can be written as ∂V[a] ∂wi  ∂J ∂V[a]  ∂J ∂ˆa  ∂J ∂ˆa  ∂ˆa  =  which, making use of results from earlier in the section is equivalent to  ∂(cid:112)V[a] ∂(cid:112)V[a] (cid:112)p(1 − p)E[xi]2 + pV[xi]s.  ∂V[a] ∂w2 i  ∂w2 i ∂wi  ∂J ∂ˆa  ∂V[a]  ∂J ∂ˆa  ∂J ∂V[a]  ∂V[a] ∂wi  =  (4)  The value of this is a zero-centred Gaussian random variable, since s is Gaussian. The scale is independent of the current weight value and only determined by the post-synaptic moments of the incoming unit, the dropout rate and the error signal. We conclude, that also in this case, we can write  ∂J ∂wi  =  ∂La ∂wi  +  draw  ∂Ra ∂wi  ,  6  −2.0−1.5−1.0−0.50.00.51.01.52.0E[a]0.51.01.52.0V[a]−7.2−6.0−4.8−3.6−2.4−1.20.01.2−2.0−1.5−1.0−0.50.00.51.01.52.0E[a]0.51.01.52.0V[a]−6.0−4.5−3.0−1.50.01.53.04.5−3−2−10123E[a]0.51.01.52.0V[a]−7.2−6.0−4.8−3.6−2.4−1.20.0draw/∂wi is deﬁned as in Equation (4) and essentially an adaptive noise term.  where ∂Ra We want to stress the fact that in the approximation as well as in the sampling case the regularization term vanishes at any optima of the training loss. A consequence of this is that no global attractor is formed, which makes the method theoretically useful for RNNs. One might argue that fast dropout should not have a regularizing effect all. Yet, regularization is not only inﬂuencing the ﬁnal solution but also the optimization, leading to different optima.  Relationship to Weight Decay As already mentioned, imposing a Gaussian distribution centred at the origin with precision λ as a prior on the weights leads to a method called weight decay. It is not only probabilistically sound but also works well empirically, see e.g. (Bishop et al., 2006). Recalling that the derivative of weight decay is of the form ∂Rwd/∂wi = 2λwi we can reinterpret Rapprox as a weight decay term where the coefﬁcient λ is weight-wise, dependent on the current activations and possibly negative. Weight decay will always be slightly wrong on the training set, since the derivative of the weight decay term has to match the one of the unregularized loss. In order for ∂L/∂θ + ∂Rwd/∂θ to be minimal, L cannot be minimal unless so is R. Relationship to Adaptive Weight Noise In this method (Graves, 2011) not the units but the weights are stochastic, which is in pratice implemented by performing Monte Carlo sampling. We can use a similar technique to FD to ﬁnd a closed form approximation. In this case, a layer is y = f (xT w), where we have no dropout variables and the weights are Gaussian distributed with w), with covariance diagonal and organized into a vector. We assume Gaussian w ∼ N (µw, σ2 density for a = xT w. Using similar algebra as above, we ﬁnd that  E[a] = µT V[a] = σ2T  x µw, x µ2  w + σ2T  x σ2  w + µ2T  x σ2 w.  (5) (6)  It is desirable to determine whether fast dropout and “fast adaptive weight noise” are special cases of each other. Showing that the aproaches are different can be done by equating equations (1) and (5) and solving for µw. This shows that rescaling by 1 − p sufﬁces in the case of the expectation. It is however not as simple as that for the variance, i.e. for equations (2) and (6), where the solution depends on µx and σ2 x and thus is not independent of the input to the network. Yet, both methods share the property that no global attractor is present in the loss: the “prior” is part of the optimization and not ﬁxed.  2.3 Bag of Tricks  Throughout the experiments we will resort to several “tricks” that have been introduced recently for more stable and efﬁcient optimization of neural networks and RNNs especially. First, we make use of rmsprop (Tieleman and Hinton, 2012), an optimizer which divides the gradient by an exponential moving average of its squares. This approach is similar to Adagrad (Duchi et al., 2011), which uses a window based average. We found that enhancing rmsprop with Nesterov’s accelerated gradient (Sutskever, 2013) greatly reduces the training time in preliminary experiments. To initialize the RNNs to stable dynamics we followed the initialization protocol of (Sutskever et al., 2013) of setting the spectral radius ρ to a speciﬁc value and the maximum amount of in- coming connections of a unit to ν; we did not ﬁnd it necessary to centre the inputs and outputs. The effect of not only using the recurrent weight matrix for propagating the states through time but also its element-wise square for advancing the variances can be quantiﬁed. The stability of a network is coupled to the spectral radius of the recurrent weight matrix ρ(Wrec); thus, the stabil- ity of forward propagating the variance is related to the spectral radius of its element-wise square rec). Since ρ(AB) ≤ ρ(A)ρ(B) for non-negative matrices and non-singular matrices A and ρ(W 2 B (Horn and Johnson, 2012), setting Wrec to full rank and its spectral radius to ρ(Wrec) assures rec) = ρ(|Wrec|2) ≤ ρ(Wrec)2, where | · | denotes taking the absolute value element-wise. We ρ(W 2 also use the gradient clipping method introduced in (Pascanu et al., 2012), with a ﬁxed threshold of 225. Since the hidden-to-hidden connections and the hidden-to-output connections in an RNN can make use of hidden units in quite distinct ways, we found it beneﬁcial to separate the dropout rates. Speciﬁcally, a hidden unit may have a different probability to be dropped out when feeding into  7  Table 1: Results on the midi data sets. All numbers are average negative log-likelihoods on the test set, where “FD” represents our work; “plain” and “RNN-NADE” results are from (Bengio et al., 2012) while “Deep RNN“ shows the best results from (Pascanu et al., 2013). Note that “RNN-NADE” and “Deep RNN“ employ various extensions of the model structure of this work, i.e. structured outputs and various forms of depths. Our results are the best for the shallow RNN model considered in this work.  Data set Piano-midi.de Nottingham MuseData JSBChorales  FD plain 7.39 7.58 3.43 3.09 6.99 6.75 8.01 8.58  RNN-NADE Deep RNN – 2.95 6.59 7.92  7.05 2.31 5.60 5.19  the hidden layer at the next time step than when feeding into the output layer. Taking this one step further, we also consider networks in which we completely neglect fast dropout for the hidden-to- output connections; an ordinary forward pass is used instead. Note that this is not the same as setting the dropout rate to zero, since the variance of the incoming units is completely neglected. Whether this is done is treated as another hyperparameter for the experiment.  3 Experiments and Results  3.1 Musical Data  All experiments were done by performing a random search (Bergstra and Bengio, 2012) over the hyper parameters (see Table 2 in the Appendix for an overview), where 32 runs were performed for each data set. We report the test loss of the model with the lowest validation error over all training runs, using the same split as in (Bengio et al., 2012). To improve speed, we organize sequences into minibatches by ﬁrst splitting all sequences of the training and validation set into chunks of length of 100. Zeros are prepended to those sequences which have less than 100 time steps. The test error is reported on the unsplitted sequences. Training RNNs to generatively model polyphonic music is a valuable benchmark for RNNs due to its high dimensionality and the presense of long as well as short term dependencies. This data set has been evaluated previously by Bengio et al. (2012) where the model achieving the best results, RNN-NADE (Boulanger-Lewandowski et al., 2013), makes speciﬁc assumptions about the data (i.e. binary observables). RNNs do not attach any assumptions to the inputs.  3.1.1 Setup  The data consists of four distinct data sets, namely Piano-midi.de (classical piano music), Notting- ham (folk music), MuseData (orchestral) and JSBChorales (chorales by Johann Sebastian Bach). Each has a dimensionality of 88 per time step organized into different piano rolls which are se- quences of binary vectors; each component of these vectors indicates whether a note is occuring at the given time step. We use the RNN’s output to model the sufﬁcient statistics of a Bernoulli random variable, i.e.  p(xt,i|x1:t−1) = yt,i,  which describes the probability that note i is present at time step t. The output non-linearity fy of the network is a sigmoid which projects the points to the interval (0, 1). We perform learning by the minimization of the average negative log-likelihood (NLL); in this case, this is the average binary cross-entropy  (cid:88)  i,t,k  L(θ) =  1  T − 1  1 N  x(k) t,i log y(k)  t−1,i + (1 − x(k)  t,i ) log(1 − y(k)  t−1,i),  where k indices the training sample, i the component of the target and t the time step.  8  Figure 2: Spectral radius over the training process. It increases ﬁrst and then slowly decreases until a certain value is reached. We did not observe this behaviour when training plain RNNs.  3.1.2 Results  Although a common metric for evaluating the performance of such benchmarks is that of accuracy (Bay et al., 2009) we restrict ourselves to that of the NLL–the measure of accuracy is not what is optimized and to which the NLL is merely a proxy. We present the results of FD-RNNs compared with the various other methods in Table 1. Our method is only surpassed by methods which either incorporate more speciﬁc assumptions of the data or employ various forms of depth (Boulanger- Lewandowski et al., 2013; Pascanu et al., 2013). We want to stress that we performed only 32 runs for each data set once more. This shows the relative ease to obtain good results despite of the huge space of potential hyper parameters. One additional observation is the range of the Eigenvalues of the recurrent weight matrix Wrec during training. We performed an additional experiment on JSBChorales where we inspected the Eigenval- ues and and the test loss. We found that the spectral radius ﬁrst increases sharply to a rather high value and then decreases slowly to settle to a speciﬁc value. We tried to replicate this behaviour in plain RNNs, but found that RNNs never exceeded a certain spectral radius at which they stuck. This stands in line with the observation from Section 2.2.2 that weights are encouraged to grow when the error is high and shrink during convergence to the optimum. See Figure 2 for a plot of the spectral radius ρ(Wrec) over the training process stages.  4 Conclusion  We have contributed to the ﬁeld of neural networks in two ways. First, we have analysed a fast approximation of the dropout regularization method by bringing its derivative into the same form as that of a loss regularized with an additive term. We have used this form to gain further insights upon the behaviour of fast dropout for neural networks in general and shown that this objective function does not bias the solutions to those which perform suboptimal on the unreguarized loss. Second, we have hypothesized that this is beneﬁcial especially for RNNs We conﬁrmed this hypothesis by conducting quantitative experiments on an already established benchmark used in the context of learning recurrent networks.  References Bay, M., Ehmann, A. F., and Downie, J. S. (2009). Evaluation of multiple-f0 estimation and tracking  systems. In ISMIR, pages 315–320.  Bayer, J., Osendorfer, C., Urban, S., et al. (2013). Training neural networks with implicit variance. In Proceedings of the 20th International Conference on Neural Information Processing , ICONIP- 2013.  Bengio, Y., Boulanger-Lewandowski, N., and Pascanu, R. (2012). Advances in optimizing recurrent  networks. arXiv preprint arXiv:1212.0901.  Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient  descent is difﬁcult. Neural Networks, IEEE Transactions on, 5(2):157–166.  9  050100150200Number of epochs0102030405060|ρ(Wrec)|ρ(Wrec) during training on JSBChoralesSpectral radiusTest loss8.08.28.48.68.89.0Test lossBergstra, J. and Bengio, Y. (2012). Random search for hyper-parameter optimization. The Journal  of Machine Learning Research, 13:281–305.  Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde- In  Farley, D., and Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. Proceedings of the Python for Scientiﬁc Computing Conference (SciPy). Oral Presentation.  Bishop, C. M. (1995). Neural networks for pattern recognition. Oxford university press. Bishop, C. M. et al. (2006). Pattern recognition and machine learning, volume 1. springer New  York.  Boulanger-Lewandowski, N., Bengio, Y., and Vincent, P. (2013). High-dimensional sequence trans-  duction. In ICASSP.  Duchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online learning and  stochastic optimization. The Journal of Machine Learning Research, 999999:2121–2159.  Graves, A. (2011). Practical variational inference for neural networks.  Information Processing Systems, pages 2348–2356.  In Advances in Neural  Graves, A. (2013). Generating sequences with recurrent neural networks.  arXiv:1308.0850.  arXiv preprint  Graves, A., Fern´andez, S., Liwicki, M., Bunke, H., and Schmidhuber, J. (2008). Unconstrained online handwriting recognition with recurrent neural networks. Advances in Neural Information Processing Systems, 20:1–8.  Graves, A., Mohamed, A.-r., and Hinton, G. (2013). Speech recognition with deep recurrent neural  networks. arXiv preprint arXiv:1303.5778.  Hammer, B. (2000). On the approximation capability of recurrent neural networks. Neurocomputing,  31(1):107–123.  Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. R. (2012). Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580.  Hochreiter, S. (1991). Untersuchungen zu dynamischen neuronalen netzen. Master’s thesis, Institut  f¨ur Informatik, Technische Universit¨at, M¨unchen.  Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural computation,  9(8):1735–1780.  Horn, R. A. and Johnson, C. R. (2012). Matrix analysis. Cambridge university press. J¨ager, H. et al. (2003). Adaptive nonlinear system identiﬁcation with echo state networks. networks,  8:9.  Julier, S. J. and Uhlmann, J. K. (1997). New extension of the kalman ﬁlter to nonlinear systems. In  AeroSense’97, pages 182–193. International Society for Optics and Photonics.  Lehmann, E. L. (1999). Elements of large-sample theory. Springer Verlag. Martens, J. and Sutskever, I. (2011). Learning recurrent neural networks with hessian-free optimiza-  tion. Proc. 28th Int. Conf. on Machine Learning.  Mikolov, T., Karaﬁ´at, M., Burget, L., Cernocky, J., and Khudanpur, S. (2010). Recurrent neural  network based language model. Proceedings of Interspeech.  Pachitariu, M. and Sahani, M. (2013). Regularization and nonlinearities for neural language models:  when are they needed? arXiv preprint arXiv:1301.5650.  Pascanu, R., Gulcehre, C., Cho, K., and Bengio, Y. (2013). How to construct deep recurrent neural  networks. arXiv preprint arXiv:1312.6026.  Pascanu, R., Mikolov, T., and Bengio, Y. (2012). On the difﬁculty of training recurrent neural  networks. Technical report, Technical Report.  Rifai, S., Dauphin, Y. N., Vincent, P., Bengio, Y., and Muller, X. (2011). The manifold tangent  classiﬁer. In Advances in Neural Information Processing Systems, pages 2294–2302.  Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning representations by back-  propagating errors. Nature, 323(6088):533–536.  10  Table 2: Hyper parameter ranges and parameter distributions for the musical data sets.  Hyper parameter #hidden layers #hidden units Transfer function p(“dropout input”) p(“dropout hidden to hidden”) p(“dropout hidden to output”) Use fast dropout for ﬁnal layer Step rate Momentum Decay W Win bh by ρ(Wrec) ν  Choices 1 200, 400, 600 tanh 0.0, 0.1, 0.2 0.0, 0.1, 0.2, 0.3, 0.4, 0.5 0.0, 0.2, 0.5 yes, no 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00001 0.0, 0.9, 0.95, 0.99, 0.995 0.8, 0.9 N (0, σ2), σ2 ∈ {0.1, 0.01, 0.001, 0.0001} N (0, σ2), σ2 ∈ {0.1, 0.01, 0.001, 0.0001} 0 −0.8 1.0, 1.05, 1.1, 1.2 15, 25, 35, 50 or no  Sutskever, I. (2013). Training Recurrent Neural Networks. PhD thesis, University of Toronto. Sutskever, I., Martens, J., Dahl, G., and Hinton, G. (2013). On the importance of initialization and  momentum in deep learning.  Sutskever, I., Martens, J., and Hinton, G. (2011). Generating text with recurrent neural networks.  Proceedings of the 2011 International Conference on Machine Learning (ICML-2011).  Tieleman, T. and Hinton, G. (2012). Lecture 6.5 - rmsprop: Divide the gradient by a running average  of its recent magnitude. COURSERA: Neural Networks for Machine Learning.  Wager, S., Wang, S., and Liang, P. (2013). Dropout training as adaptive regularization. arXiv  preprint arXiv:1307.1493.  Wang, S. and Manning, C. (2013). Fast dropout training. In Proceedings of the 30th International  Conference on Machine Learning (ICML-13), pages 118–126.  Zeiler, M., Ranzato, M., Monga, R., Mao, M., Yang, K., Le, Q., Nguyen, P., Senior, A., Vanhoucke,  V., Dean, J., et al. (2013). On rectiﬁed linear units for speech processing. ICASSP.  5 Appendix  5.1 Hyper Parameters for Musical Data Experiments  We show the hyper parameters ranges for the musical data in Table 2. The ones from which the numbers in Table 1 resulted are given in Table 3.  11  Table 3: Hyper parameters used for the musical data experiments.  Hyper parameter #hidden units p(“dropout input”) p(“dropout hidden to hidden”) p(“dropout hidden to output”) Use fast dropout for ﬁnal layer Step rate Momentum Decay σ2 for Wrec, Wout σ2 for Win ρ(Wrec) ν  Piano-midi.de Nottingham MuseData 600 0.2 0.3 0.0 yes 0.0005 0.995 0.9 0.1 0.0001 1.2 no  600 0.1 0.3 – no 0.005 0.995 0.8 0.1 0.0001 1.2 25  400 0.1 0.4 0.0 yes 0.001 0.99 0.9 0.001 0.1 1.2 no  JSBChorales 400 0.1 0.2 0.5 yes 0.001 0.99 0.8 0.0001 0.01 1.2 15  12  ","Recurrent Neural Networks (RNNs) are rich models for the processing ofsequential data. Recent work on advancing the state of the art has been focusedon the optimization or modelling of RNNs, mostly motivated by adressing theproblems of the vanishing and exploding gradients. The control of overfittinghas seen considerably less attention. This paper contributes to that byanalyzing fast dropout, a recent regularization method for generalized linearmodels and neural networks from a back-propagation inspired perspective. Weshow that fast dropout implements a quadratic form of an adaptive,per-parameter regularizer, which rewards large weights in the light ofunderfitting, penalizes them for overconfident predictions and vanishes atminima of an unregularized training loss. The derivatives of that regularizerare exclusively based on the training error signal. One consequence of this isthe absense of a global weight attractor, which is particularly appealing forRNNs, since the dynamics are not biased towards a certain regime. We positivelytest the hypothesis that this improves the performance of RNNs on four musicaldata sets."
1312.4695,2014,"Sparse, complex-valued representations of natural sounds learned with phase and amplitude continuity priors  ",['Wiktor Mlynarski'],https://arxiv.org/pdf/1312.4695.pdf,"4 1 0 2     b e F 8 1         ]  G L . s c [      3 v 5 9 6 4  .  2 1 3 1 : v i X r a  Sparse, complex-valued representations of natural sounds learned with phase and amplitude continuity  priors  Max-Planck Institute for Mathematics in the Sciences  Wiktor Młynarski  Leipzig, Germany  mlynar@mis.mpg.de  Abstract  Complex-valued sparse coding is a data representation which employs a dictionary of two-dimensional subspaces, while imposing sparse, factorial prior on complex amplitudes. When trained on a dataset of natural image patches, it learns phase invariant features which closely resemble receptive ﬁelds of complex cells in the visual cortex. Features trained on natural sounds however, rarely reveal phase in- variance and capture other aspects of the data. This observation is a starting point for the present work. As its ﬁrst contribution, it provides an analysis of natu- ral sound statistics by means of learning sparse, complex representations of short speech intervals. Secondly, it proposes priors over the basis function set, which bias them towards phase-invariant solutions. In this way, a dictionary of com- plex basis functions can be learned from the data statistics, while preserving the phase invariance property. Finally, representations trained on speech sounds with and without priors are compared. Prior-based basis functions reveal performance comparable to unconstrained sparse coding, while explicitely representing phase as a temporal shift. Such representations can ﬁnd applications in many perceptual and machine learning tasks.  1  Introduction  Natural sounds such as speech, environmental noises or animal calls possess a complex statistical structure. To achieve a good performance in real-world hearing tasks, neuronal and artiﬁcial sys- tems should utilize data representations which are adapted to regularities of the natural auditory environment, while making task relevant quantities explicit. One particular statistical property of natural sounds, is sparsness [14, 20, 3]. A typical sample of a sparse signal can be represented as a linear combination of only a few features belonging to a large dictionary. It has been suggested, that neural representations of sound, may take advantage of this regularity. Indeed - learning sparse codes of natural sounds has successfully predicted shapes of cochlear ﬁlters [20] and spectrotemporal receptive ﬁelds of the Inferior Colliculus neurons [5], suggesting adaptation of those systems to the natural environment. Sparse coding methods have also proven to be useful in machine hearing applications such as speech and music classiﬁcation [7]. However, in some cases linear sparse codes are not an appropriate representation of the data. Linear coding models are not robust to temporal jitter, since even a small temporal stimulus shift changes the encoding [19]. Additionally, phase (i.e. temporal displacement not larger than the waveform period) is not explicitly represented. Phase information is relevant for perceptual tasks such as spatial hearing [21] or recognition of conspeciﬁc songs in songbirds [9]. For performing such computations, a data representation different from linear sparse coding may be more useful.  1  Phase and amplitude information can be separated by representing the data using a combination of complex vectors. Complex-valued sparse coding [17] and Independent Component Analysis (ICA) [13] have been applied to learn representations of natural image patches. Real and imaginary parts of complex features emerging from natural images resemble Gabor ﬁlters of equal frequency, position scale and orientation in quadrature phase relationship. Such basis functions reveal phase invariance i.e. the value of the complex amplitude does not change with spatial stimulus shifts smaller than the oscillation period. Suggested by results obtained on natural images, one could think that complex sparse codes learned from natural sounds will also reveal phase invariance, since real-valued sparse codes yield features localized in time and frequency [14, 20]. This is, however, not true. Natural sounds possess highly non-local cross-frequency correlations [23]. Reﬂecting this structure, sparse, complex codes of natural acoustic stimuli capture frequency and bandwidth invariances. Only a small fraction is phase-shift (or time) invariant [26]. In order to learn from the statistics of the data a representation that preserves a desired property such as phase invariance, one could select a parametric form of basis functions and adapt the parameter set [24]. This method has been applied before to audio data by adapting a gammatone dictionary [28]. Despite many advantages of this solution, there exists a possibility, that parametric form of dictionary elements is not ﬂexible enough to efﬁciently span the data space. To alleviate this problem this paper proposes to learn a sparse and complex representation of natural sounds with the phase- invariance promoting priors. Proposed priors induce temporal continuity, i.e. slowness [6, 27] of both phase and amplitude, which turns out to be a correct assumption for learning phase-invariant features. The goal of the present work is, therefore, twofold:  1. To analyse high-order statistics of natural sounds. This is done by learning sparse, complex representations of a speech corpus and analysis of the obtained features. Learned dictionar- ies differ in cardinality (complete and two-times overcomplete) and are learned with and without prior knowledge.  2. To introduce priors useful in learning structured, phase-invariant dictionaries of any data, not only natural sounds. In addition to imposing a desired structure, priors improve con- vergence of learning algorithms and allow to use less data for learning.  The paper is structured as follows. In section 2 complex valued sparse coding model is introduced together with phase and amplitude continuity priors. Section 3 discusses statistical properties of complex features learned via complete and overcomplete sparse coding on natural, speech sounds. In section 4 coding efﬁciency of learned representations is assessed by comparing their performance in a denoising task and estimating entropies of learned coefﬁcients.  n/2(cid:88)  2 Sparse, complex-valued representations of natural sounds In a sparse coding model utilizing complex basis functions each data vector x ∈ RT is represented as:  R{s∗  i Ai(t)} + η  ˆx(t) =  (1) where si ∈ C are complex coefﬁcients, ∗ denotes a complex conjugation, Ai ∈ CT are complex basis functions and η ∼ N (0, σ) is an additive gaussian noise. Complex coefﬁcients in Euler’s √−1) therefore equation (1) can be rewritten to explicitely form become si = aiejφi (where j = (cid:16) represent phase φ and amplitude a as separate variables:  (cid:17)  i=1  n/2(cid:88)  ˆx(t) =  ai  cos φiAR  i (t) + sin φiAI  i (t)  + η  (2)  i=1  i and AI  Real and imaginary parts AR i=1 - span a subspace within which the position of a data sample is determined by amplitude ai and phase φi. Depending on a number of basis functions n, the representation can be complete (n = T ) or overcomplete (n > T ). If data vectors are chunks of a temporal signal, such as natural sounds, the basis functions A are functions of time (as opposed to basis functions of natural images which constitute a spatial representation).  i of basis functions {Ai}n/2  2  If one attempts to learn a representation of particular time-dependent properties, constraints should be placed on the basis function set.  Figure 1: Exemplary basis functions learned A) - with and B) - without phase and amplitude conti- nuity priors. First row - basis functions in the time domain, second row - polar coordinates of basis functions, third row - Fourier spectra.  In a probabilistic formulation, equations (1) (2) can be understood as a likelihood model of the data, given coefﬁcients s and basis functions A:  p(x|s, A) =  √ 1 2π  σ  − (x(t)−ˆx(t))2  2σ2  e  (3)  The prior over complex coefﬁcients assumes independence between subspaces and promotes sparse solutions i.e. solutions with most amplitudes close to 0:  p(s) =  e−S(ai)  1 Z  (4)  where Z is a normalizing constant. Function S(ai) penalizes large amplitude values. Since am- Z e−S(ai) is assumed to be exponential i.e. plitudes are always non-negative, distribution p(ai) = 1 S(ai) = λai. Due to this assumption, the model attempts to form a data representation keeping com- plex amplitudes maximally independent across subspaces, while still allowing dependence between coordinates sR, sI which determine position within each subspace. The posterior over coefﬁcients s becomes  p(s|x, A) ∝ p(x|A, s)p(s)  (5)  This model was introduced in [17] as a model of natural image patches. Assuming n = T and σ = 0, it is equivalent to 2-dimensional Independent Subspace Analysis(ISA) [10]. Figure 2 A) depicts four exemples of complex basis functions learned by from natural, speech sounds. In addition to temporal plots in Cartesian (ﬁrst row) and polar (second row) coordinates each basis function is also depicted in the frequency domain (third row). Real (AR - black lines) i  3  T(cid:89)  t=1  n(cid:89)  i=1  and imaginary (AI temporally localized, capturing the non-local strucutre of speech sounds.  i - gray lines) parts of basis functions do not resemble each other and are not  2.1 Phase and amplitude continuity priors  The sparse coding model described in the previous subsection, does not constrain the basis functions in any way. They are allowed to vary freely during the learning process. As visible on ﬁgure 2 A), an unconstrained adaptation to natural sound corpus yields complex basis functions invariant to numerous stimulus aspects such as frequency or time shifts, not necessarily phase. Learning a structured dictionary requires therefore placing priors over basis functions, which favour solutions of desired properties such as phase-invariance. Real and imaginary parts of a phase-shift invariant basis function, have equal, unimodal frequency spectra and both span the same temporal interval. Additionally, the imaginary part should be shifted in time a quarter of the cycle with respect to the real one. Before proposing a prior promoting such solutions, one should note that each basis function Ai(t) can be represented in polar coordinates as  (cid:16)  (cid:17)  Ai(t) = aA  i (t)  cos φA  i (t) + j sin φA  i (t)  (6)  i (t) and φA  where aA i (t) denote instantaneous phase and amplitude, respectively. Angular frequency can be deﬁned as a temporal derivative of instantaneous phase. If phase dynamics are highly variable and non-monotonic over time, real and imaginary components of this signal have non-identical spectra and/or their frequencies change in time (see ﬁgure 2 A), second and third rows). On the other hand, by enforcing phase φA i (t) to change smoothly and monotonically, one should obtain real and imaginary parts with matching frequency spectra. In the limiting case, when phase is a linear function of time, real and imaginary parts oscillate in the same frequency and are in a quadrature phase relationship. Furthermore, vectors which span a phase-shift invariant subspace should have the same temporal support, implying that the complex amplitude should also vary slowly in time. In order to learn a phase-shift invariant representation of natural sounds, the present work proposes a prior over basis functions of the following form:  p(Ai) = pφ(Ai)pa(Ai) =  e−(γSφ(Ai)+βSa(Ai))  1 Z  (7)  Function Sa(Ai) introduces the penalty proportional to the variance of amplitude’s temporal deriva- tive:  (cid:17)2  T(cid:88)  (cid:16)  t>1  Sa(Ai) =  ∆aA  i (t)  i (t) = aA  i (t − 1). It promotes basis functions with a slowly-varying envelope, where ∆aA highly correlated between consecutive time steps. Phase prior Sφ is deﬁned by function Sφ(Ai) of the following form:  i (t) − aA  Sφ(Ai) = − T(cid:88)  (cid:16)  (cid:17)(cid:16)  (cid:17)2  sgn  ∆φi(t)  ∆φi(t)  t>1  i (t) = φA  i (t) − φA  i (t − 1) and sgn denotes the sign function. Similarly to Sa(Ai) it where ∆φA promotes temporal slowness of phase. The additional factor −sgn(∆φi(t)) enforces φ(t) to be larger than φ(t − 1). In this way, it prevents phase from changing direction and causes it to be a non-increasing function of time. One could also enforce this by bounding the phase derivative from above: ∆φi(t) < Θ. This method would however require the hand tuning of the Θ parameter. The posterior over basis functions given a data sample x and its representation s becomes:  p(A|x, s) ∝ p(x|A, s)p(A)  (10) where the likelihood model p(x|A, s) is deﬁned by equation (3). Taken together, prior p(Ai) biases the learning process towards temporally localized basis functions with real and imaginary parts of the same instantaneous frequency.  4  (8)  (9)  Exemplary complex features learned with introduced priors are depicted on ﬁgure 2 B). Compared with unconstrained subspaces from ﬁgure 2 A), their amplitudes are smooth, and their phases change monotonically. Moreover, frequency spectra of AR i align well. Such bases form a phase invariant representation of the data.  i and AI  2.2 Learning and inference  Inference i.e. estimation of coefﬁcients s given a data sample x is performed by ﬁnding a maximum a posteriori estimate (MAP). This corresponds to ﬁnding a mode of the posterior distribution (5) and can be computed via a gradient descent on the negative log-posterior (for gradient derivations please refer to the supplementary material):  (cid:34) T(cid:88)  (cid:16)  (cid:17)2(cid:35)  n/2(cid:88)  Es =  1 2σ2  ˆx(t) − x(t)  t=1  i=1  + λ  S(ai)  (11)  Learning of basis functions A is performed in two steps as in[18, 17]. Firstly, coefﬁcients s are inferred given the data sample x. Secondly, using inferred s values, a gradient update is performed on basis functions shifting solutions towards the mode of the posterior distribution (10). This is equivalent to minimization of the following energy function:  (cid:34) T(cid:88)  (cid:16)  (cid:17)2(cid:35)  n/2(cid:88)  EA =  1 2σ2  ˆx(t) − x(t)  n/2(cid:88)  + γ  Sφ(Ai) + β  Sa(Ai)  (12)  t=1  i=1  i=1  where γ ∈ R and β ∈ R are free parameters which control the strength of each prior. They are set to be smaller than one to prevent from dominating over the error term. This prevents the model from learning trivial or non-data matching solutions (e.g. very low frequencies). The two steps are iterated until the algorithm converges. After every learning iteration, real and imaginary vectors spanning each subspace are orthogonalized by Gram-Schmidt orthogonalization. The error term in the equation (12) as well as amplitude and phase penalty terms Sa and Sφ usually have very different numerical values. For this reason, at every iteration of the learning process each term contributing to the gradient of function EA was normalized to the unit length. Prior-related terms are then multiplied by smaller than 1, strength-controlling constants γ ∈ [0, 1] and β ∈ [0, 1]. In this way, gradient information was used to ﬁnd an appropriate direction in the search space while preventing terms with larger numerical values from dominating the search.  3 Properties of representations learned using speech sounds  Natural sound statistics were analysed by learning complex dictionaries and analysing properties of obtained basis functions. Complete and twice overcomplete representations were learned with and without basis function priors. This resulted in the total number of four dictionaries. Due to space constraints, entire dictionaries are visualized in time and frequency domain in the supplementary material. All models were trained using a speech corpus from the International Phonetic Database Handbook [2]. This dataset contains sounds of human speakers telling a story in 27 different languages. Speech comprises a variety of acoustic structures, both harmonic and non-harmonic. One should note that it may not perfectly reﬂect environmental sound statistics, however it has been used before as a proxy for natural sounds [5, 23, 20]. All sound ﬁles were down sampled to 8000 Hz from their original sampling rates. For training, 50000 intervals were randomly sampled from all recording ﬁles. Each interval was 128 samples long which corresponds to 16 ms. Prior to training, 18 principal components of the data, explaining 0.001 of total variance, were rejected. This corresponds to low- pass ﬁltering the data with the 3500 Hz cutoff frequency. The sparsity control parameter λ was set to 0.1 value. After multiple experiments, the prior strengths γ and β were set to 0.2 and 0.1, respectively. Spectra of real and imaginary parts were compared by plotting their peaks against each other, and computing correlations. Spectral peaks of unconstrained basis functions do not match well (ﬁgure 3  5  Figure 2: A) Spectral peaks of real and imaginary vectors B) Concentration coefﬁcients of real and imaginary vectors C) Envelope variation plotted against peak frequency of the real part D) Distributions of spectra correlations  A), black circles). They are broadly scattered around the diagonal in both - complete and overcom- plete case. Each such complex basis function, consists of two vectors of different frequency. This is in contrast to basis functions learned with priors (gray triangles), which are concentrated around diagonals, with only a few exceptions. This means that spectra of their real and imaginary parts have the same frequency peak. To go beyond peak comparison, and to test how well basis functions aligned in the frequency do- main, correlations between their spectra were computed. Correlation equal to 1 means that real and imaginary parts covaried together, and strongly overlapped, while a low correlation value implies highly different spectra. Normalized correlation values were histogramed and are depicted on ﬁgure 3 D). A clear difference between prior based (gray lines) and unconstrained (black lines) dictionaries is visible. Correlation distributions of the latter ones are quite broad (although in the overcomplete case a stronger peak close to 1 is present) and include all possible values. Spectrum correlations of prior based basis functions, are in turn, strongly concentrated around the maximal value i.e. 1 implying similarity of real and imaginary parts. Spectral breadth of basis functions was assessed by computing the concentration index (CI) i.e. ratio between the peak value of the spectrum and the total spectral power. CI quantiﬁes how well are the basis functions localized in the frequency domain. This measure was used instead of computing bandwidth, since spectra of some basis functions consisted of two or more isolated peaks and band- width is not well deﬁned in such cases. CIs of real and imaginary vectors are plotted against each other on ﬁgure 3 B). Unconstrained basis functions (black circles) tend to cluster along the diagonal for low CI values and for higher ones they diverge. This means, that if either - real or imaginary vector has a pronounced, strongly localized peak in the frequency spectrum other one will be more broad. Prior-based basis functions lie along the diagonal, meaning that spectra of their components have similar degree of concentration. They are, however, much more broad than unconstrained basis functions, with CIs not exceeding the 0.2 value. It has been suggested that statistical models of natural sounds capture harmonic frequency relation- ships [23, 22]. The harmonic structure of natural sounds gives rise to non-local correlations in the frequency domain, which is different from local pixel correlations of natural image patches. To test whether cross-frequency couplings learned by the unconstrained model reﬂects harmonic strucutre of speech the following analysis was performed. Firstly each real and imaginary pair of vectors was  6  converted into frequency domain. Then, for each pair spectral peaks were extracted (a frequency was deﬁned as a peak if it contained more than 0.1 of the total spectral power). Ratios of the minimal peak value to remaining values were computed. Ratio histograms are depicted of ﬁgure 3  Figure 3: Peak ratio distributions. A) unconstrained dictionaries. B) prior based dictionaries. Mul- tiplications of 0.5 are marked black.  Prior based features contained mostly a single peak equal for real and imaginary vectors which is reﬂected by concentration of nearly all peak ratios at 1. Unconstrained features revealed different structure. Sharp histogram peaks are visible directly at or very close to multiplcations of 0.5 (one should compare this ﬁgure to ﬁgure 4 B in [22]). This is an indication that indeed, the unconstrained model has learned harmonic frequency relationships. This can be expected, since real and imaginary parts of complex valued features capture mutually dependent data aspects. To evaluate properties of basis functions in the temporal domain, variability of normalized ampli- tudes was computed, according to the equation (8). Results are plotted against peak frequency of the real part on ﬁgure 3 C). In both - complete and overcomplete case, the unconstrained basis func- tions (black circles) reveal a frequency dependence. For higher frequencies, amplitudes vary more strongly, although around 3 kHz variability decreases again. Amplitudes of features learned in the presence of priors is much lower, as expected. The slow amplitude prior quenches the variability almost to 0, with a slight raise observable in higher frequency regimes. In order to understand how learned basis functions tile the time-frequency plane, Wigner distribu- tions of each vector were computed. Wigner distributions describe spectrotemporal energy distribu- tion of temporal signal. In the next step, equiprobability contours corresponding to 0.7 probability value for real and 0.8 for imaginary parts were plotted (ﬁgure 3, black and blue contours respec- tively). Such representation of temporal basis functions on the time frequency plane was introduced by [1]. If both vectors spanning each subspace were indeed phase invariant, their equiprobability contours should lie within each other. Figure 3 shows, that this is rarely the case for unconstrained basis functions (ﬁrst column). While they tile the time-frequency plane uniformly, corresponding real and imaginary parts often lie far apart, modelling different regions. Prior-constrained basis func- tions can not vary their real and imaginary parts independently during the learning process. While unconstrained bases increase their temporal support with decreasing frequency, ones learned with priors are strongly localized in time, independent of their spectrum. In most cases, corresponding real and imaginary vectors occupy the same area, however a tendency is visible among imaginary components of low-frequency features to be elevated with respect to their real counterparts along the frequency axis. An interesting effect is visible in the region between 0.5 and 2 kHz. There, constrained basis functions become broadband and span large frequency intervals. In some cases the bandwidth reversal occurs. This is visible as ”banana-like” shapes. The empty regions above and below, are covered by equiprobability contours corresponding to probabilities lower than 0.7, which are not visible on the plot. These structures reﬂect temporal frequency variation of the basis functions (see ﬁgure 1 B, second row - phases are monotonic, but rather piecewise linear functions of time). A possible explanation of their emergence is that real and imaginary vectors tend to diverge from each other (as in the unconstrained model), but the prior forces them to stay close on the time frequency plane. Interestingly a qualitative change in Wigner distribution shapes was observed in this frequency regime by Abdallah and Plumbey [1], who studied independent components of natu- ral sounds. Such behaviour may imply, that data drives real and imaginary parts of basis functions  7  Figure 4: A) Contour plots of basis function Wigner distributions. Black contours are real and blue are imaginary B)Proportion of invariances learned by each dictionary. Black - spectrum invariance, dark gray - bandwidth invariance, light gray - phase invariance, white - time invariance.  to span distant frequencies, while priors ”keep them together”. Figure 3 A conﬁrms that constrained and unconstrained representations differ strongly in their spectro-temporal structure. As mentioned before each complex basis function forms an invariant representation of some data feature i.e. varying phase within subspace spanned by vectors AR and AI generates features of different quality, while keeping the complex amplitude constant. To summarize stimulus-invariances captured by every dictionary, basis functions were assigned to four classes of invariance, according to the following criteria:  1. Spectral invariance - frequency peaks of real and imaginary vectors differ by more than  100 Hz  2. Bandwidth invariance - real and imaginary parts have the same frequency peak, but differ-  ent concentration index CI  3. Time invariance - frequency peaks and concentrations match, real and imaginary vectors  are shifted in time more than a period of peak frequency  4. Phase invariance - same as above, with a shift smaller than the peak frequency period.  Figure 3 B) depicts how many basis functions from each dictionary fall within each invariance class. Unconstrained representations capture mostly spectral invariances (black color). Temporal invari- ances (white) are the second largest class, while phase invariant features (light gray) constitute a mi- nor fraction 11% in complete and 21% in the overcomplete case. In contrast, representations learned with phase and amplitude priors capture mostly phase invariances - 67% and 73% respectively. One should note, that spectral invariances learned by constrained models are much less diverse than ones captured by unconstrained ones. They result mostly from the slight misalignment of spectral peaks - see ﬁgure 3 A). In order to reassure that the structure of the unconstrained basis functions does not constitute a learning artifact, two control experiments were performed. In the ﬁrst one, a complete dictionary of prior based basis functions was used as an initialization for unconstrained learning. If features revealing complex invariances were a robust data property not merely a local minimum, the resulting dictionary should differ from the original one. This is indeed what happened. Figure 3 A depicts four randomly selected, prior based basis functions used as initial conditions. Figure 3 B shows  8  the same basis functions after 30000 learning iterations. Structure induced by priors has vanished. Spectra of single vectors are more localized, but not necessarily match each other (see the last basis function). Phases and amplitudes vary strongly in time. This observation conﬁrms that structure of the unconstrained dictionary forms a representation better spanning the data space. Secondly, unconstrained features were learned from natural image dataset (taken from [11]). Eight randomly selected subspaces are depicted on ﬁgure 3 C. As expected, they are localized in space and frequency and are in a quadrature phase relationship.  Figure 5: Control experiments. A) Initial conditions - basis functions learned with smoothness priors. B) The same basis functions after 30000 learning iterations. C) Complex basis functions learned without priors from the image data.  4 Performance of learned representations  In order to compare how well different dictionaries model the underlying data distribution, two dif- ferent criteria were used. Firstly, performance of a dictionary in a denoising task was measured and secondly relative coding efﬁciency was compared by estimating the entropy of linear coefﬁcients. Both tests were performed using a testing dataset of 20000 samples drawn from the IPA speech corpus. The test dataset was preprocessed in the same way as the training one.  4.1 Denoising  The ability of different dictionaries to match a typical structure in the data in the presensce of noise was quantiﬁed. This may be also understood as an indirect estimate of the likelihood, since it is known that models of high-likelihood perform well in denoising tasks [29]. Each vector x from the test dataset was blurred with an i.i.d. gaussian noise with sd σ = 0.1. Coefﬁcients s were inferred from the noisy sample. In the next step Peak Signal to Noise Ratio 1(cid:107)x−ˆx(cid:107) ) (where ˆx is the reconstruction of the data vector (PSNR) deﬁned as P SN R = 20 log10( given inferred s) was computed. Average PSNR for each dictionary is plotted on ﬁgure 4.1 A). For comparison, denoising was also performed using a basis consisting of orthogonalized white noise vectors. Surprisingly, all dictionar- ies adapted to the data showed very similar performance. Priors presence and overcompletness did not affect results much. Despite different basis function shapes dictionaries were able to reconstruct orginal sound chunks, while rejecting the noise. White noise bases gave, as expected, lower recon- struction quality, which has however risen in the overcomplete case. Such behavior is expected, since the model had more degrees of freedom to match the data structure.  9  L ≥ H(p) = −(cid:88)  Figure 6: A) Average performance in denoising task measured by PSNR in dB. Vertical bars indicate standard deviation. B) Average coefﬁcient entropy estimated from histograms.  4.2 Quantifying coding efﬁciency  One way to asess coding efﬁciency given a ﬁxed dictionary, is to estimate entropies of linear coefﬁ- cients sR and sI [15]. By Shannon’s source coding theorem, the entropy of the data distribution p constitutes a lower bound on code-word length:  p(x) log p(x)  (13)  x  i  ni N log2  L ≥ H(p) + DKL(p(cid:107)q)  estimated as ˆH(p) = −(cid:80)  If the true probability distribution p is unknown and is approximated by a proxy distribution q, the average code length becomes: (14) where DKL(p(cid:107)q) is the Kullblack-Leibler divergence from distribution p to q. Therefore, represen- tation which yields lower code length should be closer to the true underlying distribution of the data. For details see [16, 15]. Coefﬁcient entropies were estimated by creating normalized histograms with 256 bins. Entropy was N , where ni is a number of counts in i-th bin and N is the ni number of samples. One should note, that even though the estimation was strongly biased, the goal was to use entropy as a relative measure of coding efﬁciency using different representations, not as an absolute one. Average coefﬁcient entropy for all learned dictionaries is plotted on ﬁgure 4.1 B). Overall, over- complete representations yielded lower entropies. It means that models with more sparse causes than dimensions explain the data better and may be surprising when compared with natural image models, where overcomplete representations yield higher coefﬁcient entropies [15]. Representing the data using phase-invariant basis functions required slightly more bits than using unconstrained ones. This indicates that even though both representations gave good denoising performance, the un- constrained model may be closer to the true, data generating distribution. As expected, white noise bases required more bits per coefﬁcient to encode speech sounds and their overall performance was poor.  5 Summary  This work has addressed two lines of research. Firstly natural sound statistics were studied by learn- ing sparse complex valued representations and analysis of obtained features. It has been demon- strated that in contrary to natural images learned features are invariant to many different stimulus aspects, not only phase. One should note that phase in short sound waveforms is a very different physical quantity than spatial phase of natural image patches. It can be expected that temporal struc- ture of air pressure waveforms is going to have different statistical properties than spatial structure of reﬂected light. The present work shows that intuitions (phase invariance of sparse, complex dic- tionaries) gained from learning representations in one signal domain (images) may not transfer to others (e.g. sound). Present ﬁndings go along recent results by Terashima and colleagues [23, 22],  10  who suggested that differences in spatial organization of visual and auditory cortices may reﬂect different correlation structures of natural sounds and images. In parallel to analysis of sound statistics, priors promoting phase invariant sparse codes were proposed. The form of prior which penalizes temporal variability of the signal is long known [6, 27, 12, 25]. Recently, it was used to learn complex, temporally persistent representations from sequences of natural image patches [4]. Here slow priors were placed on amplitudes and phases of basis functions, not coefﬁcients. Phase prior includes also an additional term promoting monotonic change of phase in time. Complete and overcomplete speech representations were learned using unconstrained and prior- based models. Obtained basis functions highly differ in shape and spectro-temporal properties. Despite differences, they perform equally well in a denoising task, and yield similar coefﬁcient entropies. This implies that prior based dictionaries can be used without quality loss to represent natural sounds in tasks such spatial hearing, where phase information has to be made explicit. Proposed approach to learn complex dictionaries may ﬁnd applications outside natural scene statis- tics research. For instance, phase invariant dictionaries are useful in modelling time-epoched signal, where epochs can be misaligned [8]. Many other prior forms may be selected to learn structured signal representations. For instance penalizing variability of the second temporal phase derivative should yield basis functions which are well localized in frequency. Applicability and usefulness of such prior in learning efﬁcient representations of sensory data is a subject for future work.  Acknowledgments  This work was funded by the DFG Graduate College “Interneuro“  References [1] S. A. Abdallah and M. D. Plumbley. If the independent components of natural images are edges, what are the independent components of natural sounds. In Proceedings of International Conference on Indepen- dent Component Analysis and Signal Separation (ICA2001), pages 534–539, 2001.  [2] I. P. Association. Handbook of the international phonetic association: A guide to the use of the interna-  tional phonetic alphabet.  [3] A. J. Bell and T. J. Sejnowski. Learning the higher-order structure of a natural sound*. Network: Com-  putation in Neural Systems, 7(2):261–266, 1996.  [4] C. F. Cadieu and B. A. Olshausen. Learning intermediate-level representations of form and motion from  natural movies. Neural computation, 24(4):827–866, 2012.  [5] N. L. Carlson, V. L. Ming, and M. R. DeWeese. Sparse codes for speech predict spectrotemporal receptive  ﬁelds in the inferior colliculus. PLoS computational biology, 8(7):e1002594, 2012.  [6] P. F¨oldi´ak. Learning invariance from transformation sequences. Neural Computation, 3(2):194–200,  1991.  [7] R. Grosse, R. Raina, H. Kwong, and A. Y. Ng. Shift-invariance sparse coding for audio classiﬁcation.  arXiv preprint arXiv:1206.5241, 2012.  [8] S. Hitziger, M. Clerc, A. Gramfort, S. Saillet, C. B´enar, and T. Papadopoulo. Jitter-adaptive dictionary learning-application to multi-trial neuroelectric signals. International Conference on Learning Represen- tations, 2013.  [9] A. Hsu, S. M. Woolley, T. E. Fremouw, and F. E. Theunissen. Modulation power and phase spectrum of natural sounds enhance neural encoding performed by single auditory neurons. The Journal of neuro- science, 24(41):9201–9211, 2004.  [10] A. Hyv¨arinen and P. Hoyer. Emergence of phase-and shift-invariant features by decomposition of natural  images into independent feature subspaces. Neural Computation, 12(7):1705–1720, 2000.  [11] A. Hyv¨arinen, J. Hurri, and P. O. Hoyer. Natural Image Statistics, volume 39. Springer, 2009. [12] A. Hyv¨arinen, J. Hurri, and J. V¨ayrynen. Bubbles: a unifying framework for low-level statistical proper-  ties of natural image sequences. JOSA A, 20(7):1237–1252, 2003.  [13] V. Laparra, M. U. Gutmann, J. Malo, and A. Hyv¨arinen. Complex-valued independent component analysis of natural images. In Artiﬁcial Neural Networks and Machine Learning–ICANN 2011, pages 213–220. Springer, 2011.  11  [14] M. S. Lewicki. Efﬁcient coding of natural sounds. Nature neuroscience, 5(4):356–363, 2002. [15] M. S. Lewicki and B. A. Olshausen. Probabilistic framework for the adaptation and comparison of image  codes. JOSA A, 16(7):1587–1601, 1999.  [16] M. S. Lewicki and T. J. Sejnowski. Learning overcomplete representations. Neural computation,  12(2):337–365, 2000.  [17] B. A. Olshausen, C. F. Cadieu, and D. K. Warland. Learning real and complex overcomplete representa- tions from the statistics of natural images. In SPIE Optical Engineering+ Applications, pages 74460S– 74460S. International Society for Optics and Photonics, 2009.  [18] B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy employed by  v1? Vision research, 37(23):3311–3325, 1997.  [19] E. Smith and M. S. Lewicki. Efﬁcient coding of time-relative structure using spikes. Neural Computation,  17(1):19–45, 2005.  [20] E. C. Smith and M. S. Lewicki. Efﬁcient auditory coding. Nature, 439(7079):978–982, 2006. [21] Z. M. Smith, B. Delgutte, and A. J. Oxenham. Chimaeric sounds reveal dichotomies in auditory percep-  tion. Nature, 416(6876):87–90, 2002.  [22] H. Terashima and H. Hosoya. Sparse codes of harmonic natural sounds and their modulatory interactions.  Network: Computation in Neural Systems, 20(4):253–267, 2009.  [23] H. Terashima and M. Okada. The topographic unsupervised learning of natural sounds in the auditory  cortex. In Advances in Neural Information Processing Systems 25, pages 2321–2329, 2012.  [24] I. Tosic and P. Frossard. Dictionary learning. Signal Processing Magazine, IEEE, 28(2):27–38, 2011. [25] R. Turner and M. Sahani. A maximum-likelihood interpretation for slow feature analysis. Neural compu-  tation, 19(4):1022–1038, 2007.  [26] J. Wang, , B. Olshausen, and V. Ming. A sparse subspace model of higher-level sound structure. COSYNE  Proceedings, 2008.  [27] L. Wiskott and T. J. Sejnowski. Slow feature analysis: Unsupervised learning of invariances. Neural  computation, 14(4):715–770, 2002.  [28] M. Yaghoobi, L. Daudet, and M. E. Davies. Parametric dictionary design for sparse coding. Signal  Processing, IEEE Transactions on, 57(12):4800–4810, 2009.  [29] D. Zoran and Y. Weiss. From learning models of natural image patches to whole image restoration. In  Computer Vision (ICCV), 2011 IEEE International Conference on, pages 479–486. IEEE, 2011.  12  (1)  (2)  (3)  (4)  Supplementary material  5.1 Gradient derivations  In this section learning rules i.e. gradients over linear coefﬁcients and basis functions are derived.  5.1.1 Coefﬁcients gradient  Let ˆx be the reconstruction of the original data vector x using inferred coefﬁcients s and basis functions A:  n/2(cid:88)  ˆx(t) =  R{s∗  i Ai(t)}  Residue r(t) i.e. difference between the data vector and its reconstruction becomes:  r(t) = x(t) − ˆx(t)  i=1  Inference of coefﬁcients is equivalent to minimization of the following energy function:  (cid:32) T(cid:88)  (cid:33)  n/2(cid:88)  Es =  1 2σ2  r(t)2  + λ  S(ai)  t=1  i=1  (cid:113)  In the present work use of function S(ai) = ai is equivalent to placing an L1 norm penalty on amplitudes ai = (cid:107)si(cid:107) =  . The gradient over linear coefﬁcients sR, sI becomes:  sR2 i + sI2 i  T(cid:88)  i(cid:113)  sS  ∂Es ∂sS i  ∝ 1 σ2  AS  i (t)r(t) + λ  sR2 i + sI2 i Where S ∈ {R, I} indicates whether the coefﬁcient is real or imaginary.  t=1  5.1.2 Basis function gradient  Basis functions are learned by performing a gradient step given inferred s values. The negative log-posterior is given by:  (cid:32) T(cid:88)  (cid:33)  n/2(cid:88)  n/2(cid:88)  EA = ERes + γEφ + βESa =  1 2σ2  r(t)2  + γ  Sφ(Ai) + β  Sa(Ai)  (5)  t=1  i=1  i=1  Functions Sφ(Ai) and Sa(Ai) are of following forms:  Sa(Ai) =  ∆aA  i (t)  (cid:16)  T(cid:88) (cid:16)  t>1  (cid:17)2 (cid:17)(cid:16)  sgn  ∆φi(t)  ∆φi(t)  Sφ(Ai) = − T(cid:88)  t>1  (cid:17)2  (6)  (7)  i (t) − aA  i (t) = aA  where ∆aA Sφ temporal phase and amplitude correlations respectively. Gradient of equation 5 can be decomposed into three terms:  i (t) = φA  i (t − 1) and ∆φA  i (t) − φA  i (t − 1). Priors deﬁned by Sa and  ∂  ∂Ai(t)  EA =  ∂  ∂Ai(t)  ERes + β  ∂  ∂Ai(t)  ESa + γ  ∂  ∂Ai(t)  ESφ  (8)  13  representing the reconstruction error term and phase and amplitude priors consecutively. In polar coordinates, for 1 < t < T phase prior gradient is:  ∂ ∂φA i (t)  ∝ 2φA  i (t)  sgn  ∆φA  i (t + 1)  For boundary conditions i.e. t = 1 and t = T , this gradient becomes consecutively:  (cid:104)  (cid:16)  ∂Eφ i (1)  ∂φA  ∂Eφ i (T )  ∂φA  (cid:17)  (cid:105)  ∆φA  i (t)  φA  i (t)  (cid:17)  (cid:16) i (t + 1) − sgn φA (cid:17) (cid:16) (cid:17) (cid:16)  ∆φA  i (2)  φA  i (2)  Eφ ∝ 2φA  i (1)sgn  ∝ −2φA  i (T )sgn  ∆φA  i (T )  φA  i (T )  (cid:16)  (cid:17)  ∂Ea ∂aA i (t)  ∝ 2  ∆aA  i (t) − ∆aA  i (t + 1)  In the same way, the amplitude term gradient is deﬁned separately for 1 < t < T :  and separately for the boundary conditions (t = 1 and t = T ):  ∂Ea i (1)  ∂aA  ∝ −2∆aA  i (2)  ∂ ∂aA i (T )  Ea ∝ 2∆aA  i (T )  (9)  (10)  (11)  (12)  (13)  (14)  The residue term is most conveniently represented in Cartesian coordinates for real and imaginary i , where, as previously, S ∈ {R, I}, indicates whether coefﬁcient is real or imaginary: coefﬁcients sS  ∂ERes ∂AS i (t)  ∝ sS  i  σ2 r(t)  (15)  14  5.2 Dictionary plots  Figure 1: Complete set of complex, unconstrained basis functions. Black lines depict real parts, and gray - imaginary ones. A) Basis functions in temporal domain B) Basis function in frequency domain  15  Figure 2: Complete set of complex, basis functions learned with priors. Black lines depict real parts, and gray - imaginary ones. A) Basis functions in temporal domain B) Basis function in frequency domain  16  Figure 3: Two times overcomplete set of complex, unconstrained basis functions. Black lines depict real parts, and gray - imaginary ones. A) Basis functions in temporal domain B) Basis function in frequency domain  17  Figure 4: Two times overcomplete set of complex, basis functions learned with priors. Black lines depict real parts, and gray - imaginary ones. A) Basis functions in temporal domain B) Basis func- tion in frequency domain  18  ","Complex-valued sparse coding is a data representation which employs adictionary of two-dimensional subspaces, while imposing a sparse, factorialprior on complex amplitudes. When trained on a dataset of natural imagepatches, it learns phase invariant features which closely resemble receptivefields of complex cells in the visual cortex. Features trained on naturalsounds however, rarely reveal phase invariance and capture other aspects of thedata. This observation is a starting point of the present work. As its firstcontribution, it provides an analysis of natural sound statistics by means oflearning sparse, complex representations of short speech intervals. Secondly,it proposes priors over the basis function set, which bias them towardsphase-invariant solutions. In this way, a dictionary of complex basis functionscan be learned from the data statistics, while preserving the phase invarianceproperty. Finally, representations trained on speech sounds with and withoutpriors are compared. Prior-based basis functions reveal performance comparableto unconstrained sparse coding, while explicitely representing phase as atemporal shift. Such representations can find applications in many perceptualand machine learning tasks."
1306.3162,2014,Learning to encode motion using spatio-temporal synchrony  ,"['Kishore Reddy Konda', 'Roland Memisevic', 'Vincent Michalski']",https://arxiv.org/pdf/1306.3162.pdf,"Learning to encode motion using spatio-temporal synchrony  4 1 0 2     b e F 0 1         ]  V C . s c [      3 v 2 6 1 3  .  6 0 3 1 : v i X r a  Kishore Konda Goethe University Frankfurt, Frankfurt Roland Memisevic University of Montreal, Montreal Vincent Michalski Goethe University Frankfurt, Frankfurt  KONDA@INFORMATIK.UNI-FRANKFURT.DE  ROLAND.MEMISEVIC@UMONTREAL.CA  VMICHALS@RZ.UNI-FRANKFURT.DE  Abstract  We consider the task of learning to extract motion from videos. To this end, we show that the detec- tion of spatial transformations can be viewed as the detection of synchrony between the image se- quence and a sequence of features undergoing the motion we wish to detect. We show that learn- ing about synchrony is possible using very fast, local learning rules, by introducing multiplica- tive “gating” interactions between hidden units across frames. This makes it possible to achieve competitive performance in a wide variety of mo- tion estimation tasks, using a small fraction of the time required to learn features, and to outper- form hand-crafted spatio-temporal features by a large margin. We also show how learning about synchrony can be viewed as performing greedy parameter estimation in the well-known motion energy model.  1. Introduction The classic motion energy model turns the frames of a video into a representation of motion by summing over squares of Gabor ﬁlter responses (Adelson & Bergen, 1985; Wat- son & Albert J. Ahumada, 1985). One of the motivations for this computation is the fact that sums over squared ﬁlter responses allow us to detect “oriented” energies in spatio- temporal frequency bands. This, in turn, makes it possi- ble to encode motion independently of phase information, and thus to represent motion to some degree independent of what is moving. Related models have been proposed for binocular disparity estimation (e.g. Fleet et al., 1996), which also involves the estimation of the displacement of local features across multiple views. For many years, hand-crafted, Gabor-like ﬁlters have been used (see, e.g., Derpanis, 2012), but in recent years, un-  supervised deep learning techniques have become popu- lar which learn the features from videos (e.g. Taylor et al., 2010; Le et al., 2011; Ji et al., 2013; Memisevic & Hinton, 2007), The interest in learning-based models of motion is fueled in part by the observation that for activity recogni- tion, hand-crafted features tend to not perform uniformly well across tasks (Wang et al., 2009), which suggests learn- ing the features instead of designing them by hand. Unlike images, videos have been somewhat resistant to fea- ture learning, in that many standard models do not work well. On images, for example, models like the autoencoder or even K-means clustering, are known to yield highly struc- tured, Gabor-like, ﬁlters, which perform well in recognition tasks (Coates et al., 2011). The same does not seem to be true for videos, where neither autoencoders nor K-means were shown to work well (see, for example, Section 4). There are two notable exceptions: Feature learning models like ICA, where inference involves a search over ﬁlters that are sparse and at the same time minimize squared recon- struction error, were shown to learn at least visually good ﬁlters (see, for example, (Olshausen, 2003) and references in (Hyvarinen et al., 2009), Chapter 16). The other excep- tion are energy models, which compute sums over squared ﬁlter responses for inference, and which were shown to work well in activity recognition tasks (e.g. Le et al., 2011). In this work, we propose a possible explanation for why some models work well on videos, and other models do not. We show that a linear encoding permits the detection of transformations across time, because it supports the detec- tion of temporal “synchrony” between video and features. This makes it possible to interpret motion energy models as a way to combine two independent contributions to mo- tion encoding, namely the detection of synchrony, and the encoding of invariance. We show how disentangling these two contributions provides a different perspective onto the energy model and suggests new approaches to learning. In particular, we show that learning a linear encoding can be  Learning to encode motion using spatio-temporal synchrony  viewed as learning in the presence of multiplicative “gat- ing” interactions (e.g. Mel, 1994). This allows us to learn competitive motion features on conventional CPU-based hardware and in a small fraction of the time required by previous methods.  2. Motion from spatio-temporal synchrony Consider the task of computing a representation of motion, given two frames (cid:126)x1 and (cid:126)x2 of a video. The classic energy model (Adelson & Bergen, 1985) solves this task by detect- ing subspace energy. This amounts to computing the sum of squared quadrature Fourier or Gabor coefﬁcients across multiple frequencies and orientations (e.g. Hyvarinen et al., 2009). The motivation behind the energy model is that Fourier amplitudes are independent of stimulus phase, so they yield a representation of motion that is to some degree independent of image content. As we shall show below, this view confounds two independent contributions of the energy model, which may be disentangled in practice. An alternative to computing the sum over squares, which has originally been proposed for stereopsis, is the cross- correlation model (Arndt et al., 1995; Fleet et al., 1996), which computes the sum over products of ﬁlter-responses across the two frames. It can be shown that the sum over products of ﬁlter responses in quadrature encodes angles in the invariant subspaces associated with the transformation. The representation of angles thereby also yields a phase- invariant representation of motion (e.g. Fleet et al., 1996; Cadieu & Olshausen, 2011; Memisevic, 2012). Like the energy model, it also confounds invariance and represent- ing transformations as we shall show. It can be shown that cross-correlation models and energy models are closely related, and that there is a canonical op- eration that turns one into the other (e.g. Fleet et al., 1996; Memisevic, 2012). We shall revisit the close relationship between these models in Section 3.2.  2.1. Motion estimation by synchrony detection  We shall now discuss how synchrony detection allows us to compute motion, and how content-invariance can be achieved by pooling afterwards, if desired. To this end, consider two ﬁlters (cid:126)w1 and (cid:126)w2 which shall encode the transformation between two images (cid:126)x1 and (cid:126)x2. We restrict our attention to transformations that can be represented as an orthogo- nal transformation in “pixel space”, in other words, as an orthogonal image warp. As these include all permutations, they include, in particular, most common spatial transfor- mations such as local translations and their combinations (see, e.g. Memisevic, 2012, for a recent discussion). The assumption of orthogonality of transformations is made im- plicitly also by the motion energy model.  (cid:17)  (cid:16)  To detect an orthogonal transformation, P , between the two images, we propose to use ﬁlters for which  (cid:126)w2 = P (cid:126)w1  holds, and then to check whether the condition  (cid:126)wT  2 (cid:126)x2 = (cid:126)wT  1 (cid:126)x1  (1)  (2)  is true. We shall call this the ”synchrony condition”. It amounts to choosing a ﬁlter pair, such that it is an example of the transformation we want to detect (Eq. 1), and to de- termine whether the two ﬁlters yield equal responses when applied in sequence to the two frames (Eq. 2). We shall later relax the exact equality to an approximate equality. To see why the synchrony condition counts as evidence for the presence of the transformation, note ﬁrst that (cid:126)x2 = P (cid:126)x1 implies (cid:126)wT From this, we get:  2 (cid:126)x2 = (cid:126)wT  2 P (cid:126)x1.  ⇒ (cid:126)wT  (cid:126)x2 = P (cid:126)x1 (presence of P ) = (cid:126)wT 2 P (cid:126)x1 = (P T (cid:126)w2)T(cid:126)x1 =  (cid:126)wT  2 (cid:126)x2  1 (cid:126)x1 (3) The last equation follows from P T = P −1 (orthogonality of P ). This shows that the presence of the transformation P implies synchrony (Eq.2) for any two ﬁlters which them- selves are related through P , that is (cid:126)w2 = P (cid:126)w1. In order to detect the presence of P , we may thus look for the syn- chrony condition, using a set of ﬁlters transformed through P . This is an inductive (statistical) reasoning step, in that we can accumulate evidence for a transformation by look- ing for synchrony across multiple ﬁlters. The absence of the transformation implies that all ﬁlter pairs violate the synchrony condition. It is interesting to note that for Gabor ﬁlters, phase shifts and position shifts are locally the same (e.g. Fleet et al., 1996). For global Fourier features, phase shifts and posi- tion shifts are exactly identical. Thus, synchrony (Eq. 1) between the inputs and a sequence of phase-shifted Fourier (or Gabor) features, for example, allows us to detect trans- formations which are local translations. We shall discuss learning of ﬁlters from video data in Section 3. The synchrony condition can be extended to a sequence of more than two frames as follows: Let (cid:126)xi, (cid:126)wi (i = 1, . . . , T ) denote the input frames and corresponding ﬁlters. To detect a set of transformations Pi, each of which relates two ad- jacent frames ((cid:126)xi, (cid:126)xi+1), set (cid:126)wi+1 = Pi (cid:126)wi for all i. The condition for the presence of the sequence of transforma- tions now turns into  (cid:126)wT  i (cid:126)xi = (cid:126)wT  j (cid:126)xj ∀i, j = 1, . . . , T and i (cid:54)= j  (4)  Learning to encode motion using spatio-temporal synchrony  2.2. The insufﬁciency of weighted summation  1 (cid:126)x1 + (cid:126)wT  To check for the synchrony condition in practice, it is nec- essary to detect the equality of transformed ﬁlter responses across time (Eq. 2). Most current deep learning models are based on layers of weighted summation followed by a non- linearity. The detection of synchrony, unfortunately, cannot be performed in a layer of weighted summation plus non- linearity as we shall discuss now. The fact that the sum of ﬁlter responses, (cid:126)wT 2 (cid:126)x2, will attain its maximum for inputs that both match their ﬁl- ters seems to suggest that thresholding it would allow us to detect synchrony. This is not the case, however, because thresholding works well only for inputs which are very sim- ilar to the feature vectors themselves: Most inputs, in prac- tice, will be normalized superpositions of multiple feature vectors. Thus, to detect synchrony with a thresholded sum, we would need to use a threshold small enough to represent features, (cid:126)w1, (cid:126)w2, that explain only a fraction of the variabil- ity in (cid:126)x1, (cid:126)x2. If we assume, for example, that the two fea- tures (cid:126)w1, (cid:126)w2 account for 50% of the variance in the inputs (an overly optimistic assumption), then we would have to reduce the threshold to be one half of the maximum attain- able response to be able to detect synchrony. However, at this level, there is no way to distinguish between two stim- uli which do satisfy the synchrony condition (the motion in question is present), and two stimuli where one image is a perfect match to its ﬁlter and the other has zero overlap with its ﬁlter (the motion in question is not present). The situation can only become worse for feature vectors that account for less than 50% of the variability.  2.3. Synchrony detection using multiplicative  interactions  logical “OR” rather than an “AND” (e.g. Zetzsche & Nud- ing, 2005).  2.4. A locally learned gating module  It is important to note that multiplicative interactions will allow us to check for the synchrony condition using entirely local operations: Figure 1 illustrates how we may deﬁne a “neuron” that can detect synchrony by allowing for gating interactions within its “dendritic tree”. A model consist- ing of multiple of these synchrony detector units will be a single-layer model, as there is no cross-talk required be- tween the units. As we shall show, this fact allows us to use very fast local update rules for learning synchrony from data. This is in stark contrast to the learning of energy models and bi-linear models (e.g. Grimes & Rao, 2005; Hyv¨arinen & Hoyer, 2000; Memisevic & Hinton, 2007; Bethge et al., 2007; Taylor et al., 2010), which rely on non-local com- putations, such as back-prop, for learning (see also, Sec- tion 2.5). Although multiplicative interactions have been a common ingredient in most of these models their motiva- tion has been that they allow for the computation of sub- space energies or subspace angles rather than synchrony (eg. Memisevic, 2012). The usefulness of intra-dendritic gating has been discussed at lengths in the neuroscience literature, for example, in the work by Mel and colleagues (e.g. Archie & Mel, 2000; Mel, 1994). But besides multi-layer bilinear models dis- cussed above, it has not received much attention in ma- chine learning. Dendritic gating is reminiscent also of “Pi- Sigma” neurons (Shin & Ghosh, 1991), which have been applied to some supervised prediction tasks in the past.  If one is willing to abandon weighted sums as the only al- lowable type of module for constructing deep networks, then a simple way to detect synchrony is by allowing for multiplicative (“gating”) interactions between ﬁlter responses: The product  2 (cid:126)x2 or (cid:126)wT  2 (cid:126)x2 · (cid:126)wT p = (cid:126)wT 2 (cid:126)x2 and (cid:126)wT  (5) 1 (cid:126)x1 1 (cid:126)x1 both take on large (or will be large only if (cid:126)wT both very negative) values. Any sufﬁciently small response 1 (cid:126)x1 will shut off the response of p, of either (cid:126)wT regardless of the ﬁlter response on the other image. That way, even a low threshold on p will not sacriﬁce our ability to differentiate between the presence of some feature in one of the images vs. the partial presence of the transformed feature in both of the images (synchrony). A related, less formal, argument for product interactions is that synchrony detection amounts to an operation akin to a logical “AND”. This is at odds with the observation that weighted sums “accumulate” information and resemble a  Figure 1. Gating within a “dendritic tree.”  2.5. Pooling and energy models  Figure 2 shows an illustration of a product response using a 1-D example. The ﬁgure shows how the product of trans- formed ﬁlters and inputs yields a large response whenever (i) the input is well-represented by the ﬁlter and (ii) the  Learning to encode motion using spatio-temporal synchrony  input evolves over time in a similar way as the ﬁlter (sec- ond column in the ﬁgure). The ﬁgure also illustrates how failing to satisfy either (i) or (ii) will yield a small prod- uct response (two rightmost columns). The need to satisfy condition (i) makes the product response dependent on the input. This dependency can be alleviated by pooling over multiple products, involving multiple different ﬁlters, such that the top-level pooling unit ﬁres, if any subset of the synchrony detectors ﬁres. The classic energy model, for example, pools over ﬁlter pairs in quadrature to eliminate the dependence on phase (Adelson & Bergen, 1985; Fleet et al., 1996). In practice, however, it is not just phase but also frequency, position and orientation (or entirely differ- ent properties for non-Fourier features), which will deter- mine whether an image is aligned with a ﬁlter or not. We investigate pooling with a separately trained pooling layer in Section 3.  3. Learning synchrony from data We now discuss how to learn ﬁlters which allow us to de- tect the synchrony condition. There are in principle many ways to achieve this in practice, and we introduce a tempo- ral variant of the K-means algorithm to learn synchrony. In Appendix A we present another model based on the con- tractive autoencoder (Rifai et al., 2011), which we call syn- chrony autoencoder (SAE). In the following, we let (cid:126)x, (cid:126)y ∈ RN denote images, and we let Wx, Wy ∈ RQ×N denote matrices whose rows contain q ∈ Q feature vectors, which we will denote by (cid:126)W x RN .  q , (cid:126)W y  3.1. Synchrony K-means  Online K-means clustering has recently been shown to yield efﬁcient, and highly competitive image features for objec- tive recognition (Coates et al., 2011). We ﬁrst note that, given a set of Q cluster centers (cid:126)W x q , per- forming online gradient-descent on the standard (not syn- chrony) K-means clustering objective is equivalent to up- dating the cluster centers using the local competitive learn- ing rule (Rumelhart & Zipser, 1986)  s = η((cid:126)x − (cid:126)W x s )  ∆ (cid:126)W x  (6) where η is a step-size and s is the “winner-takes-all” as- signment  s = arg minq(cid:107)(cid:126)x − (cid:126)W x q (cid:107)2  (7) When cluster-centers (“features”) are contrast-normalized, the assignment function is equivalent to q )T(cid:126)x]  s = arg maxq[( (cid:126)W x  (8)  With the online K-means rule in mind, we now deﬁne a synchrony K-means (SK-means) model as follows. We  deﬁne the synchrony condition by ﬁrst introducing multi- plicative interactions in the assignment function:  s = arg maxq[(( (cid:126)W x  q )T(cid:126)x)(( (cid:126)W y  q )T(cid:126)y)]  (9)  Note that computing the multiplication is equivalent to re- placing the K-means winner-takes-all units by gating units (cf., Figure 1). This allows us to redeﬁne the K-means objective function to be the reconstruction error between one input and the assigned prototype vector, which is gated (multiplied elementwise) with the projection of the other input:  Lx = ((cid:126)x − (cid:126)W x  s )T(cid:126)y))2 The gradient of the reconstruction error is  s (( (cid:126)W y  (10)  = −4((cid:126)x( (cid:126)W y  s )T(cid:126)y − (cid:126)W x  s (( (cid:126)W y  s )T(cid:126)y)2)  (11)  ∂Lx ∂ (cid:126)W x s  This allows us to deﬁne the synchrony K-means learning rule:  ∆ (cid:126)W x  s = η((cid:126)x( (cid:126)W y  s )T(cid:126)y − (cid:126)W x  s (( (cid:126)W y  s )T(cid:126)y)2)  (12)  s (( (cid:126)W y  Similar to the online-kmeans rule (Rumelhart & Zipser, 1986), we obtain a Hebbian term (cid:126)x( (cid:126)W y s )T(cid:126)y, and an “active forgetting” term ( (cid:126)W x s )T(cid:126)y)2) which enforces compe- tition among the hiddens. The Hebbian term, in contrast to standard K-means, is “gated”, in that it involves both the “pre-synaptic” input (cid:126)x, and the projected pre-synaptic input ( (cid:126)W y s )T(cid:126)y coming from the other input. Similarly the update rule for (cid:126)W y  s is given by  ∆ (cid:126)W y  s = η((cid:126)y( (cid:126)W x  s )T(cid:126)x − (cid:126)W y  s (( (cid:126)W x  s )T(cid:126)x)2)  (13)  3.2. Synchrony detection using even-symmetric  non-linearities  i (cid:126)wT  function, applied to(cid:80)  sevic, 2012). The value of ((cid:80)  As deﬁned in Section 2.1, (cid:126)xi, (cid:126)wi (i = 1, . . . , T ) denote the input frames and corresponding ﬁlters. An even-symmetric nonlinearity with global minimum at zero, such as the square i (cid:126)xi, will be a detector of the syn- chrony condition, too. The reason is the binomial identity, which states that the square of the sum of terms contains the pair-wise products between all individual terms plus the squares of the individual terms. The latter do not change the preferred stimulus of the unit (Fleet et al., 1996; Memi- i (cid:126)xi)2 is high only when the individual terms are equal to each other and of high value, i.e, (cid:126)wT j (cid:126)xj which is the synchrony condition in case of sequences (Equation 4). Squaring non-linearities applied to the sum of phase-shifted Gabor ﬁlter responses have been the cornerstone of the energy model (Adelson & Bergen, 1985; Watson & Albert J. Ahumada, 1985; Hyvari- nen et al., 2009).  i (cid:126)xi = (cid:126)wT  i (cid:126)wT  Learning to encode motion using spatio-temporal synchrony  case-1  case-2  case-3  (cid:126)w1  (cid:126)w2  (cid:126)x1  (cid:126)x2  Figure 2. Demonstration of product responses p with two ﬁlters (cid:126)w1, (cid:126)w2 encoding a translation P . case-1: (cid:126)x2 = P (cid:126)x1; case-2: (cid:126)x2 (cid:54)= P (cid:126)x1; case-3: (cid:126)x2 = P (cid:126)x1 but (cid:126)x1 and (cid:126)w1 are out of phase by π/2.  p = 1231.02  p = 37.52  p = −0.002  Figure 3. Row 1: Filters learned on synthetic translations of natural image patches. Row 2: Filters learned on natural videos. Columns 1-4: Frames 1-4 of the learned ﬁlters. Column 5: Filter groupings learned by a separate layer of K-means (only ﬁrst frame ﬁlters shown). Each row in column 5 shows the six ﬁlters contributing the most to that cluster center.  Even-symmetric non-linearities implicitly compute pair-wise products and they may be implemented using multiplica- tive interactions, too: Consider the unit in Figure 1, using “tied” inputs (cid:126)x1 = (cid:126)x2, and assume that they contain a video sequence rather than a single image. If we also use tied weights (cid:126)w1 = (cid:126)w2, then the output, p, of this unit will be 1 (cid:126)x1. In practice, the model can equal to the square of (cid:126)wT learn to tie weights, if required. To enable the model from Section 3.1 to encode motion across multiple frames, we may thus proceed as follows: Let (cid:126)X ∈ RN be the concatenation of T frames (cid:126)xt ∈ RM , t = 1, . . . , T , and let W ∈ RQ×N denote the matrix containing the Q feature vectors (cid:126)Wq ∈ RN stacked row-wise. Each feature is composed of frame features (cid:126)wqt ∈ RM where each (cid:126)wqt spans one frame (cid:126)xt from the input video. The SK-means can be adapted to sequences by replacing  frames (cid:126)x, (cid:126)y with a sequence (cid:126)X and tying the weights Wx, Wy to W. The update rule for the SK-means model now be- comes  (14)  (15)  ∆ (cid:126)Ws = η( (cid:126)X( (cid:126)W T s  (cid:126)X) − (cid:126)Ws( (cid:126)W T  s  (cid:126)X)2)  where the assignment function s is  (cid:126)X)2]  Note that computing the square of (cid:80)  s( (cid:126)X) = arg maxq[( (cid:126)W T  q  qt(cid:126)xt above also  t (cid:126)wT accounts for synchrony as explained earlier. For inference in case of the SK-means model, we use a sigmoid activation function on the squared features in our experiments instead of winner-takes-all (cf., Eq. 15). As in the case of object classiﬁcation (Coates et al., 2011), relax- ing the harsh sparsity induced by K-means tends to yield codes better suited for recognition tasks.  Learning to encode motion using spatio-temporal synchrony  Example ﬁlters learned with the contractive SAE on se- quences are shown in Figure 3. In the ﬁrst row of the ﬁg- ure, columns 1 to 4 show ﬁlters learned on 50, 000 syn- thetic movies generated by translating image patches from the natural image dataset in (Martin et al., 2001). Columns 1 to 4 of the second row show ﬁlters learned on blocks sam- pled from videos of a broadcast TV database in (Hateren & Schaaf, 1998). We obtained similar ﬁlters using the SK- means model.  Table 1. Average accuracy on KTH.  ALGORITHM  PERFORMANCE(%)  SAE SK-means GRBM(TAYLOR ET AL., 2010) ISA MODEL(LE ET AL., 2011)  93.5 93.6 90.0 93.9  3.3. Learning a separate pooling layer  Table 2. Average accuracy on UCF sports.  To study the dependencies of features, we performed K- means clustering, using 500 centroids, on the hiddens ex- tracted from the training sequences. Column 5 of Figure 3 shows, for the most active clusters across the training data, the six features which contribute the most to each of the cluster centers. It shows that the “pooling units” (clus- ter centers) group together features with similar orientation and position, and with arbitrary frequency and phase. This is to be expected, as translation in any direction will af- fect all frequencies and phase angles, and only “nearby” orientations and positions. Note in particular, that pooling across phase angles alone, as done by the classic energy model, would not be sufﬁcient, and it is, in fact, not the solution found by pooling.  4. Application to activity recognition Activity recognition is a common task for evaluating mod- els of motion understanding. To allow for a fair compar- ison, we use the same pipeline as described in (Le et al., 2011; Wang et al., 2009), using the features learned by our models. We train our models on pca-whitened input patches of size 10 × 16 × 16. The number of training sam- ples is 200, 000. The number of product units are ﬁxed at 300. For inference sub blocks of the same size as the patch size are cropped from “super blocks” of size 14 × 20 × 20 (Le et al., 2011). The sub blocks are cropped with a stride of 4 on each axis giving 8 sub blocks per su- per block. The feature responses of sub blocks are con- catenated and dimensionally reduced using PCA to form the local feature. Using a separate layer of K-means, a vocabulary of 3000 spatio-temporal words is learned with 500, 000 samples for training. In all our experiments the super blocks are cropped densely from the video with a 50% overlap. Finally, a χ2-kernel SVM on the histogram of spatio-temporal words is used for classiﬁcation.  4.1. Datasets  We evaluated our models on several popular activity recog- nition benchmark datasets: KTH (Schuldt et al., 2004): Six actions performed by 25  ALGORITHM  PERFORMANCE(%)  SAE SK-means ISA MODEL(LE ET AL., 2011)  86.0 84.7 86.5  subjects. Samples divided into train and test data accord- ing to the authors original split. The multi-class SVM is directly used for classiﬁcation. UCF sports(Rodriguez et al., 2008): Ten action classes. The total number of videos in the dataset is 150. To in- crease the data we add horizontally ﬂipped version of each video to the dataset. Like in (Rodriguez et al., 2008) we train a multi-class SVM for classiﬁcation, and we use leave- one-out for evaluation. That is, each original video is tested with all other videos as training set except the ﬂipped ver- sion of the one being tested and itself. Hollywood2 (Marszałek et al., 2009): Twelve activity classes. It consists of 884 test samples and 823 train samples with some of the video samples belonging to multiple classes. Hence, a binary SVM is used to compute the average pre- cision (AP) of each class and the mean AP over all classes is reported (Marszałek et al., 2009). YUPENN dynamic scenes (Derpanis, 2012): Fourteen scene categories with 30 videos for each category. We only use the gray-scale version of the videos in our experiments. Leave-one-out cross-validation is used for performance eval- uation (Derpanis, 2012).  Table 3. Mean AP on Hollywood2.  ALGORITHM  PERFORMANCE(%)  SAE SK-means GRBM(TAYLOR ET AL., 2010) ISA MODEL(LE ET AL., 2011) COVAE (MEMISEVIC, 2011)  51.8 50.5 46.6 53.3 43.3  Learning to encode motion using spatio-temporal synchrony  Table 4. Average accuracy on YUPENN.  Table 6. Training time.  ALGORITHM  PERFORMANCE(%)  ALGORITHM  TIME  SAE (K-NN) SAE (χ2SVM) SK-means (χ2SVM) SOE (DERPANIS, 2012)  80.7 96.0 95.2 79.0  SK-means (GPU) SK-means (CPU) SAE (GPU) ISA (LE ET AL., 2011) GRBM (TAYLOR ET AL., 2010)  2 MINUTES 3 MINUTES 1 − 2 HOURS 1 − 2 HOURS 2 − 3 DAYS  Table 5. Performance on column dataset using SAE trained on row dataset.  4.4. Computational efﬁciency  DATASET  KTH UCF HOLLYWOOD2  KTH UCF HOLLYWOOD2  93.5 92.9 92.7  85.3 86.0 85.3  44.7 48.9 51.8  4.2. Results  The results are shown in Tables 1, 2, 3 and 4. They show that the SAE and SK-means are competitive with the state- of-the-art, although learning is simpler than for most exist- ing methods. To evaluate the importance of element-wise products of hidden units, we also evaluated K-means as well as a standard autoencoder with contraction as regular- ization on the Hollywood2 dataset. The models achieved an average precision of 42.1 and 42.7 respectively, which is much lower than the performance from SAE and SK- means. We also tested the covariance auto-encoder (Memi- sevic, 2011), which learns an additional mapping layer that pools over squared simple cell responses. Table 3 shows that the performance of this model is also considerably lower than our single-layer models, showing that learning the pool- ing layer along with features did not help.  4.3. Unsupervised learning and dataset bias  To show that our models learn features that can generalize across datasets (“self-taught learning” (Le et al., 2011)), we trained SAE on random samples from one of the datasets and used it for feature extraction to report performance on the others. The performances using the same metrics as before are shown in table 5. It can be seen that the perfor- mance gets reduced by only a fairly small fraction as com- pared to training on samples from the respective dataset. Only in the case where training on the KTH dataset, per- formance on Hollywood2 is considerably lower. This is probably due to the less diverse activities in KTH as com- pared to those in Hollywood2.  Training times for learning the motion features are shown in Table 6. They show that SK-means (trained on CPU) is orders of magnitude faster than all other models. For the GPU implementations, we used the theano library (Bergstra et al., 2010). We also calculated inference times using a similar metric as (Le et al., 2011) and computed the time required to extract descriptors for 30 videos from the Hol- lywood2 dataset with resolution 360 × 288 pixels (with “sigmoid-of-square” hiddens they are identical for SK-means or SAE). Average inference times (in seconds/frame) were 0.058 on CPU and 0.051 on GPU, making the models fea- sible in practical, and possibly real-time, applications. All experiments were performed on a system with a 3.20GHz CPU, 24GB RAM and a GTX 680 GPU.  5. Conclusions Our work shows that learning about motion from videos can be simpliﬁed and signiﬁcantly sped up by disentangling learning about the spatio-temporal evolution of the signal from learning about invariances in the inputs. This allows us to achieve competitive performance in activity recogni- tion tasks at a fraction of the computational cost for learn- ing motion features required by existing methods, such as the motion energy model (Le et al., 2011). We also showed how learning about motion is possible using entirely local learning rules. Computing products by using “dendritic gating” within in- dividual, but competing, units may be viewed as an efﬁcient compromise between bi-linear models, that are expensive because they encode interactions between all pairs of pix- els (Grimes & Rao, 2005; Memisevic & Hinton, 2007; Ol- shausen et al., 2007), and “factored” models (e.g. Cadieu & Olshausen, 2011; Taylor et al., 2010; Memisevic, 2012), which are multi-layer models that rely on more compli- cated training schemes such as back-prop and which do not work as well for recognition. Acknowledgments: This work was supported in part by the German Federal Ministry of Education and Research (BMBF) in project 01GQ0841 (BFNT Frankfurt), by an  Learning to encode motion using spatio-temporal synchrony  NSERC Discovery grant and by a Google faculty research award.  A. Synchrony autoencoder Here we present an additional approach to encoding motion across two frames, based on the contractive autoencoder (Rifai et al., 2011). Like the synchrony K-means algorithm, it can be extended to sequences with more than two frames, using an analogous construction (cf., Section 3.2). Given two images, we ﬁrst compute the linear ﬁlter responses (cid:126)f x = Wx(cid:126)x and (cid:126)f y = Wy(cid:126)y. Given the derivations in Sec- tion 2, an encoding of the motion, (cid:126)h = (cid:126)h((cid:126)x, (cid:126)y), inherent in the image sequence may then be deﬁned as  (cid:126)h = σ( (cid:126)f x (cid:12) (cid:126)f y)  (16) where (cid:12) is element-wise multiplication and σ is the sig- moid nonlinearity (1 + exp(−x))−1. This deﬁnition makes sense only, if features vectors are related by the transforma- tion we wish to detect. We shall now discuss how we can deﬁne a reconstruction criterion that enforces this criterion. The standard way to train an autoencoder on images is to add a decoder and to minimize reconstruction error. In our case, because of the presence of multiplicative interactions in the encoder, the encoding loses information about the sign of the input. However, note that we may interpret the multiplicative interactions as gating as discussed in the pre- vious section. This suggests deﬁning the reconstruction er- ror on one input, given the other. In the decoder we thus perform an element-wise multiplication of the hiddens and factors of one of the input to reconstruct the other. One may also view this as re-introducing the sign information at reconstruction time. Assuming an autoencoder with tied weights, the reconstructed inputs can then be deﬁned as  ˆx = (Wx)T((cid:126)h (cid:12) (cid:126)f y) ˆy = (Wy)T((cid:126)h (cid:12) (cid:126)f x)  (17) (18)  We deﬁne the reconstruction error as the average squared difference between the two inputs and their respective re- constructions:  L(((cid:126)x, (cid:126)y), (ˆ(cid:126)x, ˆ(cid:126)y)) = (cid:107)((cid:126)x − ˆ(cid:126)x)(cid:107)2 + (cid:107)((cid:126)y − ˆ(cid:126)y)(cid:107)2  (19)  Learning amounts to minimizing the reconstruction error wrt. the ﬁlters (Wx) and (Wy). In contrast to bi-linear models, which may be trained using similar criteria (e.g. Memisevic, 2011; Taylor et al., 2010), the representation of motion in Eq. 16 will be dependent on the image content, such as Fourier phase for translational motion. But this de- pendence can be removed using a separately trained pool- ing layer as we shall show. The absence of pooling during feature learning allows for much more efﬁcient learning as we show in Section 4. Note that, in practice, one may add bias terms to the deﬁnition of hiddens and reconstructions.  A.1. Contractive regularization It is well-known that regularization is important to extract useful features and to learn sparse representations. Here, we use contraction as regularization (Rifai et al., 2011). This amounts to adding the Frobenius norm of the Jaco- bian of the extracted features, i.e., the sum of squares of all partial derivatives of (cid:126)h with respect to (cid:126)x, (cid:126)y  (cid:18) ∂hj((cid:126)x, (cid:126)y)  (cid:19)2  ∂xi  +  ij  (cid:88) E =(cid:80) +(cid:80)  (cid:19)2  ij  ∂yi  (cid:18) ∂hj((cid:126)x, (cid:126)y) (cid:88) j )2(cid:80) j )2(cid:80)  i(W y i(W x  ij)2 ij)2  (20)  (21)  (cid:107)Je((cid:126)x, (cid:126)y)(cid:107)2  j(hj(1 − hj))2(f x j(hj(1 − hj))2(f y  which for the sigmoid-of-square non-linearity becomes  (cid:107)Je((cid:126)x, (cid:126)y)(cid:107)2  E =  For training, we add the regularization term to the recon- struction cost, using a hyperparameter λ. Contractive reg- ularization is not possible in (multi-layer) bi-linear mod- els, due to the computational complexity of computing the contraction gradient for multiple layers (e.g. Memisevic, 2011). Being a single layer model, the synchrony autoen- coder (SAE) makes the application of contractive regular- ization feasible. The contraction parameter λ are set by cross-validation. The SAE can be adapted to sequences by replacing frames (cid:126)x, (cid:126)y with a sequence (cid:126)X and tying the weights Wx, Wy to W. The representation of motion from Equation 16 can now be redeﬁned as,  (cid:16)  (( (cid:126)Wq)T (cid:126)X)2(cid:17)  (cid:16) Note that computing the square of (cid:80)  Hq = σ(F 2  q ) = σ  = σ  (cid:88)  (  t  qt(cid:126)xt above also accounts for synchrony as explained earlier. The recon- struction error and regularization term for this model can be derived by just replacing appropriate terms in Equations 19 and 21, respectively.  t (cid:126)wT  qt(cid:126)xt)2(cid:17)  (cid:126)wT  (22)  References Adelson, Edward H. and Bergen, James R. Spatiotemporal energy models for the perception of motion. J. OPT. SOC. AM. A, 2(2):284–299, 1985.  Archie, Kevin A. and Mel, Bartlett W. A model for in- tradendritic computation of binocular disparity. Nature Neuroscience, 3(1):54–63, January 2000.  Arndt, P.A., Mallot, H.A., and B¨ulthoff, H.H. Human stere- ovision without localized image features. Biological cy- bernetics, 72(4):279–293, 1995.  Bergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Desjardins, Guil- laume, Turian, Joseph, Warde-Farley, David, and Ben- gio, Yoshua. Theano: a CPU and GPU math expression compiler. In SciPy, 2010.  Learning to encode motion using spatio-temporal synchrony  Bethge, M, Gerwinn, S, and Macke, JH. Unsupervised learning of a steerable basis for invariant image repre- sentations. In Human Vision and Electronic Imaging XII. SPIE, 2007.  Cadieu, Charles F. and Olshausen, Bruno A. Learning Intermediate-Level Representations of Form and Motion from Natural Movies. Neural Computation, 24(4):827– 866, December 2011.  Coates, Adam, Lee, Honglak, and Ng, A. Y. An analysis of single-layer networks in unsupervised feature learning. In Artiﬁcial Intelligence and Statistics, 2011.  Derpanis, Konstantinos G. Dynamic scene understand- ing: The role of orientation features in space and time in scene classiﬁcation. In CVPR, 2012.  Fleet, D., Wagner, H., and Heeger, D. Neural encoding of binocular disparity: Energy models, position shifts and phase shifts. Vision Research, 36(12):1839–1857, June 1996.  Grimes, David and Rao, Rajesh. Bilinear sparse coding for invariant vision. Neural Computation, 17(1):47–73, 2005.  Hateren, J. H. van and Schaaf, A. van der.  Independent component ﬁlters of natural images compared with sim- ple cells in primary visual cortex. Proceedings: Biolog- ical Sciences, 265(1394):359–366, Mar 1998.  Hyv¨arinen, Aapo and Hoyer, Patrik. Emergence of phase- and shift-invariant features by decomposition of natural images into independent feature subspaces. Neural Com- put., 12:1705–1720, July 2000.  Hyvarinen, Aapo, Hurri, Jarmo, and Hoyer, Patrick O. Nat- ural Image Statistics: A Probabilistic Approach to Early Computational Vision. Springer Publishing Company, Incorporated, 2009.  Ji, Shuiwang, Xu, Wei, Yang, Ming, and Yu, Kai. 3D con- volutional neural networks for human action recognition. IEEE Transactions on Pattern Analysis and Machine In- telligence, 35(1):221–231, 2013.  Le, Q.V., Zou, W.Y., Yeung, S.Y., and Ng, A.Y. Learn- ing hierarchical invariant spatio-temporal features for ac- tion recognition with independent subspace analysis. In CVPR, 2011.  Marszałek, Marcin, Laptev, Ivan, and Schmid, Cordelia. In IEEE Conference on Computer  Actions in context. Vision & Pattern Recognition, 2009.  Martin, D., Fowlkes, C., Tal, D., and Malik, J. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In ICCV, 2001.  Mel, Bartlett W. Information processing in dendritic trees.  Neural Computation, 6(6):1031–1085, 1994.  Memisevic, Roland. Gradient-based learning of higher-  order image features. In ICCV, 2011.  Memisevic, Roland. On multi-view feature learning.  In  ICML, 2012.  Memisevic, Roland and Hinton, Geoffrey. Unsupervised  learning of image transformations. In CVPR, 2007.  Olshausen, B.A. Learning sparse, overcomplete represen- In Image Pro- tations of time-varying natural images. cessing, 2003. ICIP 2003. Proceedings. 2003 Interna- tional Conference on, volume 1, pp. I–41–4 vol.1, Sept 2003.  Olshausen, Bruno, Cadieu, Charles, Culpepper, Jack, and Warland, David. Bilinear models of natural images. In SPIE Proceedings: Human Vision Electronic Imaging XII, San Jose, 2007.  Rifai, Salah, Vincent, Pascal, Muller, Xavier, Glo- rot, Xavier, and Bengio, Yoshua. Contractive Auto- Encoders: Explicit Invariance During Feature Extrac- tion. In ICML, 2011.  Rodriguez, Mikel D., Ahmed, Javed, and Shah, Mubarak. Action mach: a spatio-temporal maximum average cor- In CVPR, relation height ﬁlter for action recognition. 2008.  Rumelhart, D. E. and Zipser, D. Parallel distributed pro- cessing: explorations in the microstructure of cognition, vol. 1. chapter Feature discovery by competitive learn- ing, pp. 151–193. MIT Press, 1986.  Schuldt, C., Laptev, I., and Caputo, B. Recognizing human actions: a local svm approach. In Pattern Recognition, 2004. ICPR 2004. Proceedings of the 17th International Conference on, 2004.  Shin, Yoan and Ghosh, Joydeep. The pi-sigma network: An efﬁcient higher-order neural network for pattern clas- siﬁcation and function approximation. In International Joint Conference on Neural Networks, 1991.  Taylor, Graham W., Fergus, Rob, LeCun, Yann, and Bregler, Christoph. Convolutional learning of spatio- temporal features. In Proceedings of the 11th European conference on Computer vision: Part VI, ECCV’10, 2010.  Wang, Heng, Ullah, Muhammad Muneeb, Kl¨aser, Alexan- der, Laptev, Ivan, and Schmid, Cordelia. Evaluation of local spatio-temporal features for action recognition. In University of Central Florida, U.S.A, 2009.  Watson, Andrew B. and Albert J. Ahumada, Jr. Model of human visual-motion sensing. J. Opt. Soc. Am. A, 2(2): 322–341, Feb 1985.  Zetzsche, Christoph and Nuding, Ulrich. Nonlinear and higher-order approaches to the encoding of natural scenes. Network (Bristol, England), 16(2-3):191–221, 2005.  ","We consider the task of learning to extract motion from videos. To this end,we show that the detection of spatial transformations can be viewed as thedetection of synchrony between the image sequence and a sequence of featuresundergoing the motion we wish to detect. We show that learning about synchronyis possible using very fast, local learning rules, by introducingmultiplicative ""gating"" interactions between hidden units across frames. Thismakes it possible to achieve competitive performance in a wide variety ofmotion estimation tasks, using a small fraction of the time required to learnfeatures, and to outperform hand-crafted spatio-temporal features by a largemargin. We also show how learning about synchrony can be viewed as performinggreedy parameter estimation in the well-known motion energy model."
1312.0786,2014,Image Representation Learning Using Graph Regularized Auto-Encoders  ,"['Yiyi Liao', 'Yue Wang', 'Yong Liu']",https://arxiv.org/pdf/1312.0786.pdf,"4 1 0 2     b e F 9 1         ]  G L . s c [      2 v 6 8 7 0  .  2 1 3 1 : v i X r a  Image Representation Learning Using Graph  Regularized Auto-Encoders  Yiyi Liao, Yue Wang, Yong Liu  State Key Laboratory of Industrial Control and Technology  Zhejiang University, China  yongliu@iipc.zju.edu.cn  Abstract  It is an important task to learn a representation for images which has low dimen- sion and preserve the valuable information in original space. At the perspective of manifold, this is conduct by using a series of local invariant mapping. Inspired by the recent successes of deep architectures, we propose a local invariant deep nonlinear mapping algorithm, called graph regularized auto-encoder (GAE). The local invariant is achieved using a graph regularizer, which preserves the local Eu- clidean property from original space to the representation space, while the deep nonlinear mapping is based on an unsupervised trained deep auto-encoder. This provides an alternative option to current deep representation learning techniques with its competitive performance compared to these methods, as well as existing local invariant methods.  1 Introduction  Although the dense original image can provide intuitive visual representation, it is well known that this representation may cover the hidden semantic patterns which need to be recognized by those image based learning tasks. On the other side, the performance of machine learning methods is also strongly dependent on the corresponding data representation on which they are applied. Thus image representation becomes a fundamental problem in visual analysis. Given an image data matrix X ∈ Rm×n, each column of X corresponding to an image, the image representation is to ﬁnd a representation function H = f (X)(H ∈ Rl×n), which can extract useful information from X. And each column vector of H is the representation of an image in this concept space. At the perspective of dimension reduction, the learned representation should have a lower dimension than the original one, i.e. l < m, and express the property of data better at the same time. The former is directive, while the later, usually be measured by the performance of clustering H. In this paper, we aim on this dimension reduction problem. One of the usual frameworks to model this problem is an optimization problem minimizing a cost shown as  C = Φ(X, H) + Ψ (H)  (1)  where the ﬁrst term measures the approximation of H to X, while the second term, constrains the representation space.  In this paper, we propose an implementation of (1) based on deep learning and manifold, called graph regularized auto-encoder (GAE). The choice of Φ is graph regularizer, which constrains the H to have the similar local geometry of original data space X. This is motivated by the property that a manifold resembles Euclidean space near each point (Wiki). Regard X as a manifold, then a neighborhood of each x has Euclidean property, which we want to be kept in H. However, whether this can be achieved depends on the choice of f , which maps X to H. It should have enough ex- pressive power to map the original space to the constrained representation space. So we choose deep  1  network to achieve better performance beyond the existing many interesting linear functions with its nonlinearity. It is also expected that many recent successes on deep learning based approaches in supervised tasks [9, 22] can be extended to the context of unsupervised ones.  The remainder of this paper is organized as follows: In section 2, we will give a brief review of auto-encoder based representation learning and the related works; Section 3 will introduce our graph regularized auto-encoder for image representation learning tasks including both unsupervised con- ditions and semi-supervised conditions. Extensive experimental results on clustering are presented in Section 4. Finally, we provide a conclusion and future works in Section 5.  2 Background  Auto-Encoder [7, 8, 3] is a special neural network, whose input is same as the output of the network. Given a data set X = {x1, ..., xn} ∈ Rm×n, each column of X is a sample vector. H ∈ Rl×n is a feature representation of the original data set X by an encoder function H = fθ(X). Normally, l < m, and H can be regarded as a low dimensional representation (or subspace) of the original data set X. And another feature mapping function, which is called decoder, maps from feature space back into input space, thus producing a reconstruction Q = qθ(H). A reconstruction error function L(X, Q), which is also called loss function, is deﬁned, and the set of parameters θ of the encoder and decoder are learned simultaneously on the task of reconstructing as well as possible the original input, i.e. attempting to incur the lowest possible reconstruction error of L(X, Q) 1. The most commonly used forms for the decoder and encoder are afﬁne mappings, optimally fol- lowed by a non-linearity as:  H = fθ(X) = sf (bH + WH X) Q = qθ(H) = sq(bQ + WQX)  (2) (3)  sf and sq are the encoder and decoder activation functions, e.g. non-linear functions of sig- moid and hyperbolic tangent or linear identify function etc. Then the set of parameters is θ = {WH , bH, WQ, bQ}, and the problem is formally presented as follows:  ˆθ = arg minθL(X, qθ(fθ(X)))  (4)  Formula (4) can be easily solved by the stochastic gradient descent based backpropagation ap- proaches. The auto-encoders are also able to support multiple layers, e.g in Hinton’s work [7], which train the encoder network (from X to Hi, i is the number of the layer) one-by-one using the Restricted Boltzamann Machines and the decoder layers of the network are formed by the inverse of the trained encoder layers, such as WH = (WQ)T in one layer auto-encoder. There are also some regularized auto-encoders such as sparse auto-encoders [17, 11, 5, 10], de- noising auto-encoders [24, 26, 25] and contractive auto-encoders [19, 18]. It is pointed out that the sparse penalty used in sparse auto-encoder will tend to make only few input conﬁgurations can have a low reconstruction error [16], which may hurt the numerical optimization of parameters. The other two kinds of regularized auto-encoders are regarded to make the representation as insensitive as pos- sible with respect to changes in input, which is commonly useful in supervised learning condition, however, it may not provide positive impacts in unsupervised and semi-supervised conditions.  Previous studies have also shown that the locally invariant idea [6] will play an important role in the image representation, especially for those tasks of unsupervised learning and semi-supervised learn- ing. There are many successful manifold learning algorithms, such as Locally Linear Embedding (LLE) [20], ISOMAP [23], and Laplacian Eigenmap [1], which all implement the locally invariant idea that nearby points are likely to have similar embeddings. However, these methods are all linear, which may not provide enough expressive power to ﬁnd a representation space that can preserve the local geometry.  There are some similar works on graph regularized neural network architecture. [14] proposed a graph regularizer that constrains the similarity between consecutive frames, which shows the human knowledge can be applied in this term. In [13], a graph constrains the data points belonging to the  1Normally, the loss function is deﬁned as the Euclidian distance of the two data set, that is k X − Q k2.  2  same label is proposed. In this work, the deep network is trained, and then minimizes only the graph regularizer using this network. Both these works use the correct graph regularizer since it is built using the correct supervised or human knowledge information.  Another work similar to us is [6], in which a convolutional neural network is applied to minimize a graph regularizer. In our work, we minimize a combined cost (1) using a fully connected network. The reason is that, the graph in the unsupervised tasks is not completely correct (pair of data point belonging to the different label actually), an introduction of ﬁrst term in (1) can act as a regularizer avoiding ﬁtting of wrong information. Besides, we do not introduce a mechanism that pulls apart discriminative pairs.  3 Graph Regularized Auto-Encoder  The problem is to design the second term Ψ (H) in (1) to constrain the representation space. In this section, we introduce our geometrical regularization that implement locally invariance in unsuper- vised and semi-supervised learning.  3.1 Single Layer Auto-Encoder Regularized with Graph  In our Graph regularized Auto-Encoder (GAE), both the decoder and the encoder use sigmoid as their activation functions. Denote sigmoid function as S(x) = 1/(1 + e−x). Then the encoder and decoder can be presented as follow:  H = fθ(X) = S(WH X + bH ) Q = qθ(H) = S(WQH + bQ)  (5) (6) As the representation should discover the latent structure of the original data, the geometrical struc- ture of the data will be the ideal latent structure in representation learning especially in unsupervised or semi-supervised learning. A reasonable assumption is that if two data points xi, xj are close in the intrinsic geometry of the data distribution, then their corresponding representations, hi, hj, should be also close to each other. This assumption is usually referred to as local invariance as- sumption [1, 20, 2], which plays an essential role in designing of veracious algorithms, such as dimensionality reduction and semi-supervised learning.  In manifold learning, the local property of the data space are preserved in the reduced representa- tion. But most algorithms considering this problem is linear. In our GAE, we introduce a locality preserved constraint to the nonlinear auto-encoder to better reﬂect its nature of manifold. Based on formula (4)-(6), we optimize the auto-encoder regularized with graph as follows  ˆθ = arg min(kX − Qk2 + λtr(HGH T ))  (7) where λ is a coefﬁcient of the training algorithm, tr(HGH T ) is the term of graph regularizer, tr(·) denotes the trace of a matrix, and G is a graph coding the local property of the original data X. Denote vij as the weight for the locality between data sample xi and xj, and their corresponding representations are hi and hj. Based on the local invariance assumption, the regularization that requires the points in subspace keeping the same geometrical structure as the original data can be presented as the following weighted formula.  vij khi − hjk2  Xi Xj i Xj =Xi  hT  vij hi +Xj  hT  j Xi  vij hj − 2Xi Xj  =tr(HD1H T ) + tr(HD2H T ) − 2tr(HV H T ) =tr(HGH T )  hivij hj  (8)  where vij are entries of V , and V is the weight matrix, D1 nd D2 are diagonal form with D1,ii =  Pj vij and D2,jj =Pi vij, and G = D1 + D2 − 2V . With this constraint, the local property of the  data in original space will be preserved after auto-encoder mapping. The matrix G has signiﬁcant expressive power of the structure in the original data space. It is calculated from the weight matrix V , whose design will be introduced in section 3.3.  3  Formula (7) can be solved by the Broyden-Fletcher-Goldfarb-Shanno (BFGS) training algorithm. In our GAE, the weight-tying constraint is not used, which means our GAE does not require WH = (WQ)T .  3.2 Multiple-layer Auto-Encoders Regularized with Graph  The auto-encoder was proposed in the context of the neural network, which is later applied to train the deep structure of networks to obtain better expressive power. In data representation, the idea of representing with multiple layers still works. Thus the representation of the original data can be presented with one layer of mapping, as well as multiple-layer mapping. We also implement the locally invariant constraint into the multiple layer auto-encoders by adding the graph regularized terms.  As training all the layers simultaneous in multiple-layer auto-encoders may be stacked, in our multiple-layer GAE, we train the multiple-layer GAE layer-by-layer. We use Hi to denote the data representation of the ith layer, and its corresponding decoder is denoted as Qi. The input data of the ith layer is the data representation of i − 1 th layer 2. That is:  Hi = fθi(Hi−1) = S(WHi Hi−1 + bHi ) Qi = qθi(Hi) = S(WQi Hi + bQi )  (9) (10)  Here, θi = {WHi , WQi , bHi, bQi }, and then the objective function of the ith layer of the GAE is,  ˆθi = arg min(kHi−1 − Qik2 + λtr(HiGi−1H T  i ))  (11)  Where Gi−1 is the graph regularizer generated from data Hi−1. Then the Multiple-Layer GAE (ML-GAE) algorithm can be given as follow:  Algorithm 1: ML-GAE Input: X, total layer number j Output: WH1 , bH1, ..., WHj , bHj  1 for i = 1 to j do 2 3 end  Solve θi by formula (11), and obtain the ith layer of data representation Hi;  3.3 Graph Regularizer Design  As mentioned above, the performance of date representation regularized with graph mainly lies on the design of the weight matrix V since it encodes the local invariance information of the data space. In this section, we will focus on the weight matrix design of supervised learning and unsupervised learning in the context of data representation with auto-encoders.  3.3.1 Unsupervised Learning  In unsupervised learning, the label for the data is unavailable. We can only obtain the structure of the data from the local property of the data samples. There are three kinds of weights employed in our GAE, which are introduced as follows,  • KNN-graph: It ﬁrst constructs the k-nearest neighbor sets for each data sample. If xi lies in xj’s k-nearest neighbor set, the weights vij is set as the distance between these two data samples, that is exp(−kxi − xjk), otherwise vij is set to zero.  • ǫ-graph: It ﬁrst constructs the ǫ-neighbor sets for each data sample, the data sample xi’s ǫ-neighbor set contains all the data samples whose distances to xi are less than ǫ. If xi lies in xj’s ǫ-nearest neighbor set, the weights vij is set as the distance between these two data samples, that is exp(−kxi − xjk), otherwise vij is set to zero.  2If i = 1, then the input data is the original data set X, and the training process is back to single layer GAE.  4  • l1-graph: The weight setting is considered to resolve the following optimization problem,  vij = arg min kxi − Xj=1...n,j6=i  vij xjk + λ Xj=1...n,j6=i  |vij |  3.3.2 Semi-supervised Learning  In semi-supervised learning, the data labels are partially available, which brings some ground truth information to the estimation of the data representation. In our GAE, the graph regularizer design for semi-supervised learning task is similar to the unsupervised learning task. We ﬁrst construct the k-nearest neighbor sets or ǫ-neighbor sets for the whole data set, and set the weight vij to zero if xi and xj are not neighbors. For the condition that xi and xj are neighbors, the weights are calculated as follow:  vij =( exp(−kxi − xjk)  1 0  either xi or xj is unlabeled xi, xj have the same label xi, xj have different labels  (12)  As mentioned before, the weights in the graph are computed by exp(−kxi − xjk), which is a value less than 1 but larger than 0. Since the labeled data will provide ground truth information, the weights of two samples with the same labels are directly set to 1. And the weights between two samples with different labels are directly set to 0. The graph constraint constructed with formula (12) is called semi-graph regularizer. Apparently, the marginal value 0 and 1 give the most conﬁdent level of the similarity since their corresponding are labeled. With this semi-graph regularizer, both labeled and unlabeled data samples are regarded fairly.  4 Experimental Results  In this section, comparison experiments are carried out to demonstrate the performance of our pro- posed method in tasks of both unsupervised learning and semi-supervised learning. To evaluate image representations learned by different methods quantitatively, the k-means clustering is applied to the representations learned by different methods. Two metrics, the normalized mutual informa- tion(MI) and accuracy(AC) are used to measure the clustering performance. For fair comparison, the dimension of learned representation through all algorithms are set to be the same.  The normalized mutual information(MI) is given in [4], which is a normalized measure to evaluate how similar two sets of clusters are. The accuracy (AC) is used to measure the percentage of correct labels compared to the ground truth label provided by the data set. Speciﬁcally, given a data sample xi with clustered label and ground truth label ci and gi, the accuracy is deﬁned as  AC = Pi δ(gi, map(ci))  n  where n is total number of samples, δ(a, b) is delta function, which outputs 1 when a = b and outputs 0 otherwise, map(c) is the permutation mapping function that maps each clustered label ci to the best label provided by the data set. This function is implemented using the code published by [4]. For the normalized mutual information, denote C and C ′ as the set of clusters obtained from the ground truth and our algorithm. We ﬁrst compute the mutual information as follows,  M I(C, C ′) = Xci∈C,c′  j ∈C ′  p(ci, c′  j) log  p(ci, c′ j) p(ci)p(c′ j )  where p(ci) and p(c′ cluster ci and c′ both ci and c′  j) are the probabilities that a sample selected from the data set that belong to j) is the probability that a sample selected from the data set that belong to  j, p(ci, c′  j. Then, the normalized mutual information can be computed as  M I(C, C ′) =  M I(C, C ′)  max(H(C), H(C ′))  5  where H(C) and H(C ′) are the entropy of C and C ′. When M I = 1, the two clusters are identical. when M I = 0, the two clusters are independent. The experimental results are all average value of multiple times of random experiment. We set that the reduced representation of different dimension reduction techniques share the same dimensions. The data sets employed for the experiments including: ORL[21], Yale and COIL20, whose statistics are shown in table 1.  Table 1: Statistics of data sets employed in the experiments.  Datasets  #Samples  #Classes  #Samples per class  ORL Yale  COIL20  400 165 1440  40 15 20  10 11 72  4.1 Variants Comparison  Fine-tuning[7] of a pre-trained deep network can sometimes improve the performance in many su- pervised learning tasks. However, in unsupervised learning, the weight matrix built on Euclidean distance may include some wrong information, i.e. samples with different labels may be connected. We can compute the error rate as the ratio of wrong connections and the total connections.  We construct a 2 layer auto-encoder, and implement layer-wise pre-training based on the method in section3.2. Then we ﬁne-tune the deep auto-encoder with the single graph regularization, i.e. only the second term of (11). The input data is chosen from COIL20. It has 20 classes. In this experiment, we select 8 classes in random for comparison. So the unsupervised weight matrix is also kind of random based on the data set. Experiment result in table 2 shows that when the weight matrix is constructed with no error, the performance will be promoted with ﬁne-tuning. However, when the weight matrix contains wrong connection, then the result turns out to be worse.  We also conduct an experiment that pre-train the GAE without reconstruction error but only with graph regularization which show in the last two rows in table 2. The result is interesting that the deep architecture even cannot learn a meaningful representation. It may give some insights on the reconstruction error term.  As a result, in the next two sections, we train our deep auto-encoders using layer-wise pre-training with both reconstruction error and graph regularization.  Table 2: Variant experiments.  Model  GAE  GAE GAE  No Reconstruction Error  Train Method  Pre-train Fine-tune Pre-train Fine-tune Pre-train Fine-tune  MI  0.8856 0.7666 0.8020 0.9026 0.0156 0.2129  AC  0.9167 0.6689 0.7465 0.8732 0.2060 0.0134  Error Rate  3.1%  0%  0%  4.2 Experiments in Unsupervised Learning  For unsupervised learning task, all samples have no labels. So they are directly fed into the algorithm for measure evaluation. The dimension of the reduced representation is set to the number of classes in the input data set. The methods used for comparison including:  • k-means:  this is the baseline method which simply performs clustering in the original  feature space.  • PCA: Principal Components Analysis. This is the classical linear technique for dimensional  reduction.  6  • GAE: Graph Auto-Encoder (2 layers). It is a contribution of this paper that introduces the graph constraint to the auto-encoder. In the experiments, our GAE employs the KNN graph. There are two coefﬁcients, k for the number of neighbors in the KNN graph and λ for the intensity of the graph regularizer. They are all selected by the grid based search.  • SAE: Sparse+Auto-encoder(2 layers)[15]. The sparse constraint is equipped to the auto- encoder, which is a very common constraint choice in the ﬁeld of auto-encoder. The for- mula is given as follows  ˆθ = arg min kX − ˆXk2 + ηXj  KL(ρ|ρj)  (13)  where ρ is the user deﬁned sparsity coefﬁcient, ρj is the average response of the jth hidden unit for the whole dataset. The penalty term can make the hidden response more sparse. The coefﬁcient η is also selected by grid based search.  • GNMF: Graph regularized Nonnegative Matrix Factorization. It is proposed in [4]. It is a combination of nonnegative constraint and locally invariant constraint. The graph param- eter settings are similar to the GAE, which employ the KNN graph and the coefﬁcients k and λ are selected by the optimal grid search.  The results for whole datasets are shown in table 3, 4, 5. To randomize the experiments, we carry out evaluation with different cluster classes. For each given number of the cluster classes, we random choose the cluster classes from the whole datasets for 5 times. One can see that graph regularized auto-encoder achieves the best performance. Although the GNMF and GAE are employed the same encodes on the locally invariant information, the auto-encoder which implement the nonlinear sig- moid scaling on the deep structure will performance better than the nonnegative matrix factorization based approach.  Table 3: Results of the unsupervised learning tasks on ORL.  Class  5  GAE MI SAE MI GNMF MI PCA MI  0.8955 0.8632 0.8464 0.8436 0.6921 0.9333 0.9067 0.8840 0.8800 Kmeans AC 0.7400  Kmeans MI GAE AC SAE AC GNMF AC PCA AC  10  0.8967 0.8670 0.7958 0.7969 0.7392 0.8167 0.8067 0.7480 0.7380 0.6720  15  0.8694 0.8438 0.7634 0.7740 0.6907 0.7733 0.7800 0.6773 0.6800 0.5933  20  0.8531 0.8338 0.7548 0.7492 0.6939 0.7483 0.7533 0.6310 0.6190 0.5690  25  0.8477 0.8423 0.7759 0.7411 0.6972 0.7400 0.7373 0.6272 0.5976 0.5552  30  0.8315 0.8054 0.7724 0.7354 0.7088 0.6878 0.6867 0.6173 0.5567 0.5300  35  0.8282 0.8062 0.7627 0.7505 0.7186 0.6876 0.6543 0.5714 0.5777 0.5354  40  0.8344 0.7933 0.7509 0.7500 0.7078 0.6839 0.6217 0.5748 0.5625 0.5050  Average 0.8571 0.8319 0.7778 0.7676 0.7060 0.7589 0.7433 0.6664 0.6514 0.5875  Table 4: Results of the unsupervised learning tasks on Yale.  Class  3  GAE MI SAE MI GNMF MI PCA MI  0.7283 0.6140 0.5175 0.3650 0.4198 0.8889 0.8485 0.7758 0.6424 Kmeans AC 0.6303  Kmeans MI GAE AC SAE AC GNMF AC PCA AC  12  0.5395 0.5171 0.5034 0.4859 0.4251 0.5177 0.4975 0.4818 0.4652 0.4182  15  0.5689 0.5350 0.4518 0.5103 0.4532 0.5313 0.4828 0.4162 0.4412 0.4048  Average 0.5916 0.5440 0.4994 0.4436 0.4020 0.6303 0.6119 0.5594 0.5073 0.4612  6  0.5748 0.5154 0.5111 0.3883 0.3082 0.6515 0.6313 0.5818 0.4909 0.4303  9  0.5467 0.5386 0.5130 0.4686 0.4035 0.5623 0.5993 0.5414 0.4970 0.4222  7  Table 5: Results of the unsupervised learning tasks on COIL20. 20  10  12  14  16  6  8  Class  GAE MI SAE MI GNMF MI PCA MI  0.9739 0.9264 0.8690 0.7350 0.6550 0.9846 0.9483 0.8690 0.7435 Kmeans AC 0.7106  Kmeans MI GAE AC SAE AC GNMF AC PCA AC  0.8888 0.8953 0.8469 0.7544 0.7490 0.8779 0.8709 0.8097 0.7035 0.7372  0.8762 0.8527 0.8696 0.7790 0.7714 0.8495 0.8352 0.8358 0.7553 0.7472  0.8684 0.8280 0.8302 0.7988 0.7720 0.8279 0.7731 0.7692 0.7461 0.7338  0.8657 0.8344 0.8379 0.7686 0.7382 0.8056 0.7665 0.7579 0.6917 0.6579  0.8552 0.8078 0.8460 0.7894 0.7392 0.7888 0.7237 0.7566 0.7148 0.6646  0.8502 0.8013 0.8449 0.7817 0.7354 0.7981 0.7255 0.7172 0.6871 0.6096  Average 0.8826 0.8494 0.8492 0.7724 0.7372 0.8475 0.8062 0.7879 0.7203 0.6944  Table 6: Results of the semi-supervised learning tasks for ORL.  Class  5  0.9242 GAE MI 0.8257 CNMF MI 0.9533 GAE AC CNMF AC 0.8680  10  0.9335 0.7998 0.9167 0.7580  15  0.8996 0.8275 0.8400 0.7520  20  0.8872 0.8161 0.7783 0.7290  25  0.8772 0.8007 0.7800 0.6824  30  0.8787 0.7943 0.7756 0.6620  35  0.8681 0.7885 0.7543 0.6354  40  0.8694 0.7873 0.7325 0.6250  Average 0.8923 0.8050 0.8163 0.7140  4.3 Experiments in Semi-supervised Learning  For semi-supervised learning task, a small part of the samples are labeled. In this experiment, these labeled samples are selected in random. For COIL20, 10% samples are labeled in each class, so there are 7 labeled samples in each class. For ORL and Yale, 20% are labeled, then 2 samples are labeled in each class 3. Still referring to the unsupervised learning experiments, the dimension of the learned representation is equal to the number of classes in the data set. The comparison methods used in this experiments including:  • CNMF: constrained NMF. It is proposed in [12]. In their framework, the samples with the same label are required to have the same representation in the reduced space. There is no user deﬁned parameters either.  • SGAE: Semi-Graph regularized Auto-Encoder (2 layers). It is a representation learning algorithm proposed in this paper consisting of the auto-encoder regularized by the semi- graph regularizer presented in section 3.3.2. The parameters include the intensity of the graph constraint, λ and the number of neighbors in the KNN graph, k. Similarly to the GAE in unsupervised learning experiment, these two parameters are selected by optimal grid search.  The clustering results on all classes of the datasets are shown in table 6, 7, 8. Similarly to the randomize experiment of the unsupervised learning, we also conduct the randomize experiment  3Here one labeled sample is meaningless to both CNMF and SGAE, so we label 20% of the samples in each  class.  Table 7: Results of the semi-supervised learning tasks for Yale.  Class  3  0.6400 GAE MI 0.5915 CNMF MI 0.8384 GAE AC CNMF AC 0.7636  6  0.7322 0.5733 0.7172 0.6636  9  0.6336 0.5620 0.6604 0.5939  12  0.6126 0.5710 0.6162 0.5439  15  0.6550 0.5543 0.5818 0.4949  Average 0.6547 0.5704 0.6828 0.6120  8  Table 8: Results of the semi-supervised learning tasks for COIL20. 20  16  10  12  14  6  8  Class  0.9182 GAE MI 0.7861 CNMF MI 0.9529 GAE AC CNMF AC 0.7755  0.9406 0.8129 0.9630 0.7951  0.9281 0.7580 0.9000 0.7396  0.9131 0.7606 0.9028 0.7043  0.9215 0.7576 0.8991 0.7131  0.8908 0.7904 0.8313 0.7109  0.8642 0.7440 0.8236 0.6505  Average 0.9109 0.7728 0.8961 0.7270  for semi-supervised learning on different number of classes. The classes for the experiment are also randomly sampled from the whole datasets with 5 times, and the average clustering results are shown in rightmost column of the table. It can be found that the semi-graph regularized auto-encoder gives a signiﬁcant improvement of performance compared to the constrained NMF. The reason may be that the CNMF only utilizes the labeled data while ignoring the geometric structure hidden in the unlabeled data. When it comes to the semi-graph regularized auto-encoder, all the information from labeled and unlabeled data are all considered. As we expected, semi-graph regularized auto-encoder achieves better performance compared to the unsupervised clustering results in table 3, 4, 5.  5 Conclusion  In this paper, we proposed a novel graph regularized auto-encoder, which can learn a locally invari- ant representation of the images for both unsupervised and semi-supervised learning tasks. In un- supervised learning, our approach trains the image representation by an multiple-layer auto-encoder regularized with the graph, which encodes the locally neighborhood relationships of the original data. And in semi-supervised learning, the graph regularizer used in our auto-encoders is extended to semi-graph regularizer, which adds the penalty and reward obtained from the labeled data points to the locally neighborhood weight matrix. Experimental results on image clustering show our method provides better performance comparing with the stat-of-the-art approaches. The further work may focus on investigating the affections of the parameter settings in GAE and the impacts of the deep structure is also a possible future work.  References  [1] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In  NIPS, pages 585–591, 2001.  [2] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A geometric framework for learning  from labeled and unlabeled examples. Journal of Machine Learning Research, 7:2399–2434, 2006.  [3] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. IEEE  Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798–1828, 2013.  [4] D. Cai, X. He, J. Han, and T. S. Huang. Graph regularized nonnegative matrix factorization for data representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(8):1548–1560, 2011. In  [5] I. Goodfellow, Q. Le, A. Saxe, H. Lee, and A. Y. Ng. Measuring invariances in deep networks.  Advances in Neural Information Processing Systems 22, pages 646–654. 2009.  [6] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduction by learning an invariant mapping. In Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2, CVPR ’06, pages 1735–1742, Washington, DC, USA, 2006. IEEE Computer Society.  [7] G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science,  313(5786):504 – 507, 2006.  [8] G. E. Hinton and R. S. Zemel. Autoencoders, minimum description length and helmholtz free energy. In  NIPS, pages 3–10, 1993.  [9] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deep convolutional neural net- works. In P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1106–1114. 2012.  [10] H. Larochelle and Y. Bengio. Classiﬁcation using discriminative restricted boltzmann machines. In ICML,  pages 536–543, 2008.  [11] H. Lee, C. Ekanadham, and A. Y. Ng. Sparse deep belief net model for visual area v2. In NIPS, 2007. [12] H. Liu, Z. Wu, D. Cai, and T. S. Huang. Constrained nonnegative matrix factorization for image repre-  sentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(7):1299–1311, 2012.  9  [13] M. R. Min, L. Maaten, Z. Yuan, A. J. Bonner, and Z. Zhang. Deep supervised t-distributed embedding. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 791–798, 2010.  [14] H. Mobahi, R. Collobert, and J. Weston. Deep learning from temporal coherence in video. In Proceedings  of the 26th Annual International Conference on Machine Learning, pages 737–744. ACM, 2009.  [15] C. Poultney, S. Chopra, Y. L. Cun, et al. Efﬁcient learning of sparse representations with an energy-based  model. In Advances in neural information processing systems, pages 1137–1144, 2006.  [16] M. Ranzato, Y.-L. Boureau, and Y. LeCun. Sparse feature learning for deep belief networks. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 1185–1192. MIT Press, Cambridge, MA, 2008.  [17] M. Ranzato, C. S. Poultney, S. Chopra, and Y. LeCun. Efﬁcient learning of sparse representations with  an energy-based model. In NIPS, pages 1137–1144, 2006.  [18] S. Rifai, G. Mesnil, P. Vincent, X. Muller, Y. Bengio, Y. Dauphin, and X. Glorot. Higher order contractive auto-encoder. In Proceedings of the 2011 European conference on Machine learning and knowledge discovery in databases - Volume Part II, ECML PKDD’11, pages 645–660, Berlin, Heidelberg, 2011. Springer-Verlag.  [19] S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y. Bengio. Contractive Auto-Encoders: Explicit invariance  during feature extraction. In ICML, 2011.  [20] S. T. Roweis and L. K. Saul. Nonlinear Dimensionality Reduction by Locally Linear Embedding. Science,  290(5500):2323–2326, 2000.  [21] F. S. Samaria and A. C. Harter. Parameterisation of a stochastic model for human face identiﬁcation. In Applications of Computer Vision, 1994., Proceedings of the Second IEEE Workshop on, pages 138–142. IEEE, 1994.  [22] R. Socher, B. Huval, B. P. Bath, C. D. Manning, and A. Y. Ng. Convolutional-recursive deep learning for  3d object classiﬁcation. In NIPS, pages 665–673, 2012.  [23] J. B. Tenenbaum, V. de Silva, and J. C. Langford. A global geometric framework for nonlinear dimen-  sionality reduction. Science, 290(5500):2319–23, Dec 2000.  [24] P. Vincent. A connection between score matching and denoising autoencoders. Neural Comput.,  23(7):1661–1674, July 2011.  [25] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, ICML ’08, pages 1096–1103, New York, NY, USA, 2008. ACM.  [26] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. J. Mach. Learn. Res., 11:3371–3408, Dec. 2010.  10  ","We consider the problem of image representation for the tasks of unsupervisedlearning and semi-supervised learning. In those learning tasks, the raw imagevectors may not provide enough representation for their intrinsic structuresdue to their highly dense feature space. To overcome this problem, the rawimage vectors should be mapped to a proper representation space which cancapture the latent structure of the original data and represent the dataexplicitly for further learning tasks such as clustering.Inspired by the recent research works on deep neural network andrepresentation learning, in this paper, we introduce the multiple-layerauto-encoder into image representation, we also apply the locally invariantideal to our image representation with auto-encoders and propose a novelmethod, called Graph regularized Auto-Encoder (GAE). GAE can provide a compactrepresentation which uncovers the hidden semantics and simultaneously respectsthe intrinsic geometric structure.Extensive experiments on image clustering show encouraging results of theproposed algorithm in comparison to the state-of-the-art algorithms onreal-word cases."
1312.4894,2014,Deep Convolutional Ranking for Multilabel Image Annotation  ,"['Sergey Ioffe', 'Alexander Toshev', 'Yangqing Jia', 'Thomas Leung', 'Yunchao Gong']",https://arxiv.org/pdf/1312.4894.pdf,"4 1 0 2    r p A 4 1         ]  V C . s c [      2 v 4 9 8 4  .  2 1 3 1 : v i X r a  Deep Convolutional Ranking for  Multilabel Image Annotation  Yunchao Gong UNC Chapel Hill  Yangqing Jia Google Research  Thomas K. Leung Google Research  yunchao@cs.unc.edu  jiayq@google.com  leungt@google.com  Alexander Toshev Google Research  toshev@google.com  Sergey Ioffe  Google Research  sioffe@google.com  Abstract  Multilabel image annotation is one of the most important challenges in computer vision with many real-world applications. While existing work usually use con- ventional visual features for multilabel annotation, features based on Deep Neural Networks have shown potential to signiﬁcantly boost performance. In this work, we propose to leverage the advantage of such features and analyze key compo- nents that lead to better performances. Speciﬁcally, we show that a signiﬁcant per- formance gain could be obtained by combining convolutional architectures with approximate top-k ranking objectives, as thye naturally ﬁt the multilabel tagging problem. Our experiments on the NUS-WIDE dataset outperforms the conven- tional visual features by about 10%, obtaining the best reported performance in the literature.  1  Introduction  Multilabel image annotation [25, 14] is an important and challenging problem in computer vision. Most existing work focus on single-label classiﬁcation problems [6, 21], where each image is as- sumed to have only one class label. However, this is not necessarily true for real world applications, as an image may be associated with multiple semantic tags (Figure 1). As a practical example, images from Flickr are usually accompanied by several tags to describe the content of the image, such as objects, activities, and scene descriptions. Images on the Internet, in general, are usually associated with sentences or descriptions, instead of a single class label, which may be deemed as a type of multitagging. Therefore, it is a practical and important problem to accurately assign multiple labels to one image. Single-label image classiﬁcation has been extensively studied in the vision community, the most recent advances reported on the large-scale ImageNet database [6]. Most existing work focus on designing visual features for improving recognition accuracy. For example, sparse coding [33, 36], Fisher vectors [28], and VLAD [18] have been proposed to reduce the quantization error of “bag of words”-type features. Spatial pyramid matching [21] has been developed to encode spatial informa- tion for recognition. Very recently, deep convolutional neural networks (CNN) have demonstrated promising results for single-label image classiﬁcation [20]. Such algorithms have all focused on learning a better feature representation for one-vs-rest classiﬁcation problems, and it is not yet clear how to best train an architecture for multilabel annotation problems. In this work, we are interested in leveraging the highly expressive convolutional network for the problem of multilabel image annotation. We employed a similar network structure to [20], which contains several convolutional and dense connected layers as the basic architecture. We studied  1  Figure 1: Sample images from the NUS-WIDE dataset, where each image is annotated with several tags.  and compared several other popular multilabel losses, such as the ranking loss [19] that optimizes the area under ROC curve (AUC), and the cross-entropy loss used in Tagprop [14]. Speciﬁcally, we propose to use the top-k ranking loss, inspired by [34], for embedding to train the network. Using the largest publicly available multilabel dataset NUS-WIDE [4], we observe a signiﬁcant performance boost over conventional features, reporting the best retrieval performance on the benchmark dataset in the literature.  1.1 Previous Work  In this section we ﬁrst review related works on multilabel image annotation and then brieﬂy discuss works on deep convolutional networks. Modeling Internet images and their corresponding textural information (e.g., sentences, tags) have been of great interest in the vision community [2, 10, 12, 15, 30, 29, 34]. In this work, we focus on the image annotation problem and summarize several important lines of related research. Early work in this area was mostly devoted to annotation models inspired by machine translation techniques [1, 8]. The work by Barnard et al. [1, 8] applied machine translation methods to parse natural images and tried to establish a relationship between image regions and words. More recently, image annotation has been formulated as a classiﬁcation problem. Early works fo- cused on generative model based tagging [1, 3, 26], centred upon the idea of learning a parametric model to perform predictions. However, because image annotation is a highly nonlinear problem, a parametric model might not be sufﬁcient to capture the complex distribution of the data. Several recent works on image tagging have mostly focused on nonparametric nearest-neighbor methods, which offer higher expressive power. The work by Makadia et al. [25], which proposed a sim- ple nearest-neighbor-based tag transfer approach, achieved signiﬁcant improvement over previous model-based methods. Recent improvements on the nonparametric approach include TagProp [14], which learns a discriminative metric for nearest neighbors to improve tagging. Convolutional neural networks (CNNs) [20, 22, 23, 17, 5] are a special type of neural network that utilizes speciﬁc network structures, such as convolutions and spatial pooling, and have exhibited good generalization power in image-related applications. Combined with recent techniques such as Dropout and fast parallel training, CNN models have outperformed existing hancrafted features. Krizhevsky et al. [20] reported record-breaking results on ILSVRC 2012 that contains 1000 visual- object categories. However, this study was mostly concerned with single-label image classiﬁcation, and the images in the dataset only contain one prominent object class. At a ﬁner scale, several meth- ods focus on improving speciﬁc network designs. Notably, Zeiler et al. [37] investigated different pooling methods for training CNNs, and several different regularization methods, such as Dropout [16], DropConnect [32], and Maxout [13] have been proposed to improve the robustness and repre- sentation power of the networks. In adition, Earlier studies [7] have shown that CNN features are suitable as a general feature for various tasks under the conventional classiﬁcation schemes, and our work focuses on how to directly train a deep network from raw pixels, using multilabel ranking loss, to address the multilabel annotation problem.  2 Multilabel Deep Convolutional Ranking Net  In our approach for multilabel image annotation, we adopted the architecture proposed in [20] as our basic framework and mainly focused on training the network with loss functions tailored for multi-label prediction tasks.  2  Tags: green, flower sun, flowers, zoo, day, sunny, sunshine Tags: london, traffic, raw Tags: art, girl, woman, wow, dance, jump, dancing 2.1 Network Architecture  The basic architecture of the network we use is similar to the one used in [20]. We use ﬁve convo- lutional layers and three densely connected layers. Before feeding the images to the convolutional layers, each image is resized to 256×256. Next, 220×220 patches are extracted from the whole image, at the center and the four corners to provide an augmentation of the dataset. Convolution ﬁlter sizes are set to squares of size 11, 9, and 5 respectively for the different convolutional layers; and max pooling layers are used in some of the convolutional layers to introduce invariance. Each densely connected layer has output sizes of 4096. Dropout layers follow each of the densely con- nected layers with a dropout ratio of 0.6. For all the layers, we used rectiﬁed linear units (RELU) as our nonlinear activation function. The optimization of the whole network is achieved by asynchronized stochastic gradient descent with a momentum term with weight 0.9, with mini-batch size of 32. The global learning rate for the whole network is set to 0.002 at the beginning, and a staircase weight decay is applied after a few epochs. The same optimization parameters and procedure are applied to all the different methods. For our dataset with 150,000 training images, it usually takes one day to obtain a good model by training on a cluster. Unlike previous work that usually used ImageNet to pre-train the network, we train the whole network directly from the training images from the NUS-WIDE dataset for a fair comparison with conventional vision baselines.  2.2 Multilabel Ranking Losses  We mainly focused on loss layer, which speciﬁes how the network training penalizes the deviation between the predicted and true labels, and investigated several different multilabel loss functions for training the network. The ﬁrst loss function was inspired by Tagprop [14], for which we minimized the multilabel softmax regression loss. The second loss was a simple modiﬁcation of a pairwise- ranking loss [19], which takes multiple labels into account. The third loss function was a multilabel variant of the WARP loss [34], which uses a sampling trick to optimize top-k annotation accuracy. For notations, assume that we have a set of images x and that we denote the convolutional network by f (·) where the convolutional layers and dense connected layers ﬁlter the images. The output of f (·) is a scoring function of the data point x, that produces a vector of activations. We assume there are n image training data and c tags.  2.2.1 Softmax  The softmax loss has been used for multilabel annotation in Tagprop [14], and is also used in single- label image classiﬁcation [20]; therefore, we adopted it in our context. The posterior probability of an image xi and class j can be expressed as  (cid:80)c  pij =  exp(fj(xi)) k=1 exp(fk(xi))  ,  (1)  where fj(xi) means the activation value for image xi and class j. We then minimized the KL- Divergence between the predictions and the ground-truth probabilities. Assuming that each image has multiple labels, and that we can form a label vector y ∈ R1×c where yj = 1 means the presence of a label and yj = 0 means absence of a label for an image, we can obtain ground-truth probability by normalizing y as y/(cid:107)y(cid:107)1. If the ground truth probability for image i and class j is deﬁned as ¯pij, the cost function to be minimized is  n(cid:88)  c(cid:88)  i=1  j=1  n(cid:88)  c+(cid:88)  i=1  j=1  J = − 1 m  ¯pij log(pij) = − 1 m  1 c+  log(pij)  where c+ denotes the number of positive labels for each image. For the ease of exposition and without loss of generality, we set c+ to be the same for all images.  2.2.2 Pairwise Ranking  The second loss function we considered was the pairwise-ranking loss [19], which directly models the annotation problem. In particular, we wanted to rank the positive labels to always have higher  3  n(cid:88)  c+(cid:88)  c−(cid:88)  i=1  j=1  k=1  J =  r(cid:88)  j=1  n(cid:88)  c+(cid:88)  c−(cid:88)  J =  scores than negative labels, which led to the following minimization problem:  max(0, 1 − fj(xi) + fk(xi)),  (2)  i=1  j=1  k=1  where c+ is the positive labels and c− is the negative labels. During the back-propagation, we computed the sub-gradient of this loss function. One limitation of this loss is that it optimizes the area under the ROC curve (AUC) but does not directly optimize the top-k annotation accuracy. Because for image annotation problems we were mostly interested in top-k annotations, this pairwise ranking loss did not best ﬁt our purpose.  2.2.3 Weighted Approximate Ranking (WARP)  The third loss we considered was the weighted approximate ranking (WARP), which was ﬁrst de- scribed in [34]. It speciﬁcally optimizes the top-k accuracy for annotation by using a stochastic sampling approach. Such an approach ﬁts the stochastic optimization framework of the deep archi- tecture very well. It minimizes  L(rj) max(0, 1 − fj(xi) + fk(xi)).  (3)  where L(·) is a weighting function for different ranks, and rj is the rank for the jth class for image i. The weighting function L(·) used in our work is deﬁned as:  L(r) =  αj, with α1 ≥ α2 ≥ . . . ≥ 0.  (4)  We deﬁned the αi as equal to 1/j, which is the same as [34]. The weights deﬁned by L(·) control the top-k of the optimization. In particular, if a positive label is ranked top in the label list, then L(·) will assign a small weight to the loss and will not cost the loss too much. However, if a positive label is not ranked top, L(·) will assign a much larger weight to the loss, which pushes the positive label to the top. The last question was how to estimate the rank rj . We followed the sampling method in [34]: for a positive label, we continued to sample negative labels until we found a violation; then we recorded the number of trials s we sampled for negative labels. The rank was estimated by the following formulation  rj = (cid:98) c − 1  s  (cid:99),  (5)  for c classes and s sampling trials. We computed the sub-gradient for this layer during optimization. As a minor noite, the approximate objective we optimize is a looser upper bound compared to the original WARP loss proposed in [34]. To see this, notice that in the original paper, it is assumed #Y −1 with a positive example (x, y) with rank that the probability of sampling a violator is p = r r, where #Y is the number of labels. Thus, there are r labels with higher scores than y. This is true only if all these r labels are negative. However, in our case, since there may be other positive labels having higher scores than y due to the multi-label nature of the problem, we effectively have p ≤ r  #Y −1.  3 Visual Feature based Image Annotation Baslines  We used a set of 9 different visual features and combined them to serve as our baseline features. Although such a set of features might not have been the best possible ones we could obtain, they already serve as a very strong visual representation, and the computation of such features is non- trivial. On top of these features, we ran two simple but powerful classiﬁers (kNN and SVM) for image annotation. We also experimented with Tagprop [14], but found it cannot easily scale to a large training set because of the O(n2) time complexity. After using a small training set to train the Tagprop model, we found the performance to be unsatisfactory and therefore do not compare it here.  4  3.1 Visual Features GIST [27]: We resized each image to 200×200 and used three different scales [8, 8, 4] to ﬁlter each RGB channel, resulting in 960-dimensional (320×3) GIST feature vectors. SIFT: We used two different sampling methods and three different local descriptors to extract texture features, which gave us a total of 6 different features. We used dense sampling and a Harris corner detector as our patch-sampling methods. For local descriptors, we extracted SIFT [24], CSIFT [31], and RGBSIFT [31], and formed a codebook of size 1000 using kmeans clustering; then built a two- level spatial pyramid [21] that resulted in a 5000-dimensional vector for each image. We will refer to these six features as D-SIFT, D-CSIFT, D-RGBSIFT, H-SIFT, H-CSIFT, and H-RGBSIFT. HOG: To represent texture information at a larger scale, we used 2×2 overlapping HOG as described in [35]. We quantized the HOG features to a codebook of size 1000 and used the same spatial pyramid scheme as above, which resulted in 5000-dimensional feature vectors. Color: We used a joint RGB color histogram of 8 bins per dimension, for a 512-dimensional feature.  The same set of features were used in [11], and achieved state-of-the-art performance for image re- trieval and annotation. The combination of this set of features has a total dimensionality of 36,472, which makes learning very expensive. We followed [11] to perform simple dimensionality reduc- tions to reduce computation. In particular, we performed a kernel PCA (KPCA) separately on each feature to reduce the dimensionality to 500. Then we concatenated all of the feature vectors to form a 4500-dimensional global image feature vector and performed different learning algorithms on it.  3.2 Visual feature + kNN  The simplest baseline that remains very powerful involves directly applying a weighted kNN on the visual feature vectors. kNN is a very strong baseline for image annotation, as suggested by Makadia et al. [25], mainly because multilabel image annotation is a highly nonlinear problem and handling the heavily tailed label distribution is usually very difﬁcult. By contrast, kNN is a highly nonlinear and adaptive algorithm that better handles rare tags. For each test image, we found its k nearest neighbors in the training set and computed the posterior probability p(c|i) as  p(c|i) =  exp(−||xi − xj||2  2  σ  1 k  )yjk,  (6)  k(cid:88)  j=1  where yik indexes the labels of training data, yik = 1 when there is one label for this image, and yik = 0 when there is no label for this image. σ is the bandwidth that needs to be tuned. After obtaining the prediction probabilities for each image, we sorted the scores and annotated each testing image with the top-k tags.  3.3 Visual feature + SVM  Another way to perform image annotation is to treat each tag separately and to train c different one- vs-all classiﬁers. We trained a linear SVM [9] for each tag and used the output of the c different SVMs to rank the tags. Because we had already performed nonlinear mapping to the data during the KPCA stage, we found a linear SVM to be sufﬁcient. Thus we assigned top-k tags to one image, based on the ranking of the output scores of the SVMs.  4 Experiments  4.1 Dataset  We performed experiments on the largest publicly available multilabel dataset, NUS-WIDE [4]. This dataset contains 269,648 images downloaded from Flickr that have been manually annotated, with several tags (2-5 on average) per image. After ignoring the small subset of the images that are not annotated by any tag, we had a total of 209,347 images for training and testing. We used a subset of 150,000 images for training and used the rest of the images for testing. The tag dictionary for the images contains 81 different tags. Some sample images and annotations are shown in Figure 1.  5  method / metric Upper bound  Visual Feature + kNN Visual Feature + SVM  CNN + Softmax CNN + Ranking CNN + WARP  per-class recall per-class precision overall recall overall precision N + 100.00 91.36 82.72 98.76 95.06 96.29  82.76 53.44 35.87 59.52 58.00 60.49  66.49 42.93 28.82 47.82 46.59 48.59  97.00 19.33 18.79 31.22 26.83 35.60  44.87 32.59 21.51 31.68 31.93 31.65  Table 1: Image annotation results on NUS-WIDE with k = 3 annotated tags per image. See text in section 5.4 for the deﬁnition of “Upper bound”.  method / metric Upper bound  Visual Feature + kNN Visual Feature + SVM  CNN + Softmax CNN + Ranking CNN + WARP  per-class recall per-class precision overall recall overall precision N + 100.00 95.06 96.30 98.76 97.53 100.00  96.40 66.98 47.15 74.04 72.78 75.00  46.22 32.29 22.73 35.69 35.08 36.16  99.57 32.14 34.19 48.24 42.48 52.03  28.83 22.56 18.79 21.98 22.74 22.31  Table 2: Image annotation results on NUS-WIDE with k = 5 annotated tags per image. See text in section 5.4 for the deﬁnition of “Upper bound”.  4.2 Evaluation Protocols  We followed previous research [25] in our use of the following protocols to evaluate different meth- ods. For each image, we assigned k (e.g., k = 3, 5) highest-ranked tags to the image and compared the assigned tags to the ground-truth tags. We computed the recall and precision for each tag sepa- rately, and then report the mean-per-class recall and mean-per-class precision:  per-class recall =  1 c  ,  per-class precision =  (7)  i=1 is the number of correctly annotated image for tag i, N g i  where c is the number of tags, N c is i the number of ground-truth tags for tag i, and N p is the number of predictions for tag i. The i above evaluations are biased toward infrequent tags, because making them correct would have a very signiﬁcant impact on the ﬁnal accuracy. Therefore we also report the overall recall and overall precision:  i=1  overall recall =  ,  overall precision =  .  (8)  c(cid:88)  1 c  N c i N p i  (cid:80)c (cid:80)c  i=1 N c i i=1 N p i  c(cid:88)  N c i N g i  (cid:80)c (cid:80)c  i=1 N c i i=1 N g i  For the above two metrics, the frequent classes will be dominant and have a larger impact on ﬁnal performance. Finally, we also report the percentage of recalled tag words out of all tag words as N+. We believe that evaluating all of these metrics makes the evaluation unbiased.  4.3 Baseline Parameters  In our preliminary evaluation, we optimized the parameters for the visual-feature-based baseline systems. For visual-feature dimensionality reduction, we followed the suggestions in Gong et al. [11] to reduce the dimensionality of each feature to 500 and then concatenated the PCA-reduced vectors into a 4500-dimensional global image descriptor, which worked as well as the original fea- ture. For kNN, we set the bandwidth σ to 1 and k to 50, having found that these settings work best. For SVM, we set the regularization parameter to C = 2, which works best for this dataset.  4.4 Results  We ﬁrst report results with respect to the metrics introduced above. In particular, we vary the number k of predicted keywords for each image and mainly consider k = 3 and k = 5. Before doing so, however, we must deﬁne an upper bound for our evaluation. In the dataset, each image had different numbers of ground-truth tags, which made it hard for us to precisely compute an upper  6  Figure 2: Analysis of per-class recall of the 81 tags in NUS-WIDE dataset with k = 3.  Figure 3: Analysis of per-class precision of the 81 tags in NUS-WIDE dataset with k = 3.  bound for performance with different k. For each image, when the number of ground-truth tags was larger than k, we randomly chose k ground-truth tags and assigned them to that image; when the number of ground-truth tags was smaller than k, we assigned all ground-truth tags to that image and randomly chose other tags for that image. We believe this baseline represents the best possible performance when the ground truth is known. The results for assigning 3 keywords per image are reported in Table 1. The results indicate that the deep network achieves a substantial improvement over existing visual-feature-based annotation methods. The CNN+Softmax method outperforms the VisualFeature+SVM baseline by about 10%. Comparing the same CNN network with different loss functions, results show that softmax already gives a very powerful baseline. Although using the pairwise ranking loss does not improve softmax, by using the weighted approximated-ranking loss (WARP) we were able to achieve a substantial improvement over softmax. This is probably because pairwise-ranking is not directly optimizing the top-k accuracy, and because WARP pushes classes that are not ranked top heavier, which boosts the performance of rare tags. From these results, we can see that all loss functions achieved comparable overall-recall and overall-precision, but that WARP loss achieved signiﬁcantly better per-class recall and per-class precision. Results for k = 5, which are given in Table 2, show similar trends to k = 3. We also provide a more detailed analysis of per-class recall and per-class precision. The recall for each tags appears in Figure 2, and the precision for each tag in Figure 3. The results for different tags are sorted by the frequency of each tag, in descending order. From these results, we see that the accuracy for frequent tags greater than for infrequent tags. Different losses performed comparably to each other for frequent classes, and WARP worked better than other loss functions for infrequent classes. Finally, we show some image annotation examples in Figure 4. Even though some of the predicted tags for these do not match the ground truth, they are still very meaningful.  5 Discussion and Future Work  In this work, we proposed to use ranking to train deep convolutional neural networks for multil- abel image annotation problems. We investigated several different ranking-based loss functions for training the CNN, and found that the weighted approximated-ranking loss works particularly well for multilabel annotation problems. We performed experiments on the largest publicly available multilabel image dataset NUS-WIDE, and demonstrated the effectiveness of using top-k ranking to train the network. In the future, we would like to use very large amount of noisy-labeled multilabel images from the Internet (e.g., from Flickr or image searches) to train the network.  7  0102030405060708000.20.40.60.81Tags (decreasing frequency)Recall  SoftmaxRankingWARP0102030405060708000.20.40.60.8Tags (decreasing frequency)Precision  SoftmaxRankingWARPFigure 4: Qualitative image annotation results obtained with WARP.  References [1] Kobus Barnard and David Forsyth. Learning the semantics of words and pictures. In ICCV,  2001.  [2] Tamara Berg and David Forsyth. Animals on the web. CVPR, 2007. [3] Gustavo Carneiro, Antoni B Chan, Pedro J Moreno, and Nuno Vasconcelos. Supervised learn- ing of semantic classes for image annotation and retrieval. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 29(3):394–410, 2007.  [4] Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhiping Luo, and Yan-Tao. Zheng. Nus-wide: A real-world web image database from national university of singapore. In Proc. of ACM Conf. on Image and Video Retrieval (CIVR’09), Santorini, Greece., July 8-10, 2009.  [5] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc Le, Mark Mao, Marc’Aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, and Andrew Ng. Large scale distributed deep networks. In P. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1232– 1240, 2012.  [6] Jia Deng, W. Dong, R. Socher, Lijia Li, Kai Li, and Li Fei-Fei.  hierarchical image database. CVPR, 2009.  Imagenet: A large-scale  [7] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. arXiv preprint arXiv:1310.1531, 2013.  [8] Pinar Duygulu, Kobus Barnard, Nando de Freitas, and David Forsyth. Object recognition as  machine translation: Learning a lexicon for a ﬁxed image vocabulary. In ECCV, 2002.  [9] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. Liblinear: A library for large  linear classiﬁcation. JMLR, 2008.  [10] Rob Fergus, Antonio Torralba, and Yair Weiss. Semi-supervised learning in gigantic image  collections. NIPS, 2009.  [11] Yunchao Gong, Qifa Ke, Michael Isard, and Svetlana Lazebnik. A multi-view embedding  space for internet images, tags, and their semantics. IJCV, 2013.  [12] Yunchao Gong and Svetlana Lazebnik.  learning binary codes. CVPR, 2011.  Iterative quantization: An procrustean approach to  8  Image	  Ground	  truth	  Predic2ons	  Image	  Ground	  truth	  Predic2ons	  Boat Cloud Ocean Vehicle Water Lake Ocean Cloud Sky Water Grass	  Cloud	  House	  Animal	  Grass	  Sky	  	  Cloud Flower plant Sky Valley Grass Plant Cloud Flower Sky Road Grass Animal Bird Food Toy Beach Rock Sky Sunset Water Rock Water Ocean Cloud Sky Cloud Sky Snow Sunset Tree Tree Snow Cloud Sky Water Animal Cloud Cow Grass Horse Animal Cloud Grass Sky Cloud Mountain Rock Sky Valley Mountain Building Sky Cloud [13] Ian Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio.  Maxout networks. ICML, 2013.  [14] M. Guillaumin, T. Mensink, J. Verbeek, and C. Schmid. Tagprop: Discriminative metric learn-  ing in nearest neighbor models for image auto-annotation. ICCV, 2009.  [15] Matthieu Guillaumin, Jakob Verbeek, and Cordelia Schmid. Multimodal semi-supervised  learning for image classiﬁcation. CVPR, 2010.  [16] Geoffrey Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov. Improving neural networks by preventing co-adaptation of feature detectors. Arxiv, 2012. [17] What is the best multi-stage architecture for object recognition? K. jarrett and k. kavukcuoglu  and m. a. ranzato and y. lecun. CVPR, 2009.  [18] Herv´J´egou, M. Douze, Cordelia Schmid, and Patrick Perez. Aggregating local descriptors into  a compact image representation. CVPR, 2010.  [19] Thorsten Joachims. Optimizing search engines using clickthrough data. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’02, pages 133–142, New York, NY, USA, 2002. ACM.  [20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep  convolutional neural networks. NIPS, 2012.  [21] Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. Beyond bags of features: Spatial pyra-  mid matching for recognizing natural scene categories. CVPR, 2006.  [22] Y. LeCun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, and L.D. Jackel.  Handwritten digit recognition with a back-propagation network. NIPS, 1990.  [23] H. Lee, R. Grosse, R. Ranganath, and A.Y. Ng. Convolutional deep belief networks for scalable  unsupervised learning of hierarchical representations. ICML, 2009.  [24] David G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 2004. [25] Ameesh Makadia, Vladimir Pavlovic, and Sanjiv Kumar. A new baseline for image annotation.  In ECCV, 2008.  [26] F. Monay and D. Gatica-Perez. Plsa-based image autoannotation: Constraining the latent  space. In ACM Multimedia, 2004.  [27] Aude Oliva and Antonio Torralba. Modeling the shape of the scene: a holistic representation  of the spatial envelope. IJCV, 2001.  [28] Florent Perronnin and Christopher R. Dance. Fisher kernels on visual vocabularies for image  categorization. CVPR, 2007.  [29] A. Quattoni, M. Collins, and T. Darrell. Learning visual representations using images with  captions. CVPR, 2007.  [30] N. Rasiwasia, PJ Moreno, and N. Vasconcelos. Bridging the gap: Query by semantic example.  IEEE Transactions on Multimedia, 2007.  [31] Koen E. A. van de Sande, Theo Gevers, and Cees G. M. Snoek. Evaluating color descriptors  for object and scene recognition. PAMI, 2010.  [32] Li Wan, Matt Zeiler, Sixin Zhang, Yann Lecun, and Rob Fergus. Regularization of neural  networks using dropconnect. ICML, 2013.  [33] Jinjun Wang, Jianchao Yang, Kai Yu, Fengjun Lv, Thomas Huang, and Yihong Gong. Locality-  constrained linear coding for image classiﬁcation. CVPR, 2010.  [34] Jason Weston, Samy Bengio, and Nicolas Usunier. Wsabie: Scaling up to large vocabulary  image annotation. In IJCAI, 2011.  [35] Jianxiong Xiao, James Hays, Krista Ehinger, Aude Oliva, and Antonio Torralba. Sun database:  Large-scale scene recognition from abbey to zoo. CVPR, 2010.  [36] Jianchao Yang, Kai Yu, Yihong Gong, and Thomas Huang. Linear spatial pyramid matching  uisng sparse coding for image classiﬁcation. CVPR, 2009.  [37] Matt Zeiler and Rob Fergus. Stochastic pooling for regularization of deep convolutional neural  networks. ICLR, 2013.  9  ","Multilabel image annotation is one of the most important challenges incomputer vision with many real-world applications. While existing work usuallyuse conventional visual features for multilabel annotation, features based onDeep Neural Networks have shown potential to significantly boost performance.In this work, we propose to leverage the advantage of such features and analyzekey components that lead to better performances. Specifically, we show that asignificant performance gain could be obtained by combining convolutionalarchitectures with approximate top-$k$ ranking objectives, as thye naturallyfit the multilabel tagging problem. Our experiments on the NUS-WIDE datasetoutperforms the conventional visual features by about 10%, obtaining the bestreported performance in the literature."
1312.1847,2014,Understanding Deep Architectures using a Recursive Convolutional Network  ,"['David Eigen', 'Jason Rolfe', 'Rob Fergus', 'Yann LeCun']",https://arxiv.org/pdf/1312.1847.pdf,"4 1 0 2     b e F 9 1         ]  G L . s c [      2 v 7 4 8 1  .  2 1 3 1 : v i X r a  Understanding Deep Architectures using a  Recursive Convolutional Network  David Eigen  Jason Rolfe Rob Fergus Yann LeCun  Dept. of Computer Science, Courant Institute, New York University  {deigen,rolfe,fergus,yann}@cs.nyu.edu  Abstract  A key challenge in designing convolutional network models is sizing them appro- priately. Many factors are involved in these decisions, including number of layers, feature maps, kernel sizes, etc. Complicating this further is the fact that each of these inﬂuence not only the numbers and dimensions of the activation units, but also the total number of parameters. In this paper we focus on assessing the in- dependent contributions of three of these linked variables: The numbers of layers, feature maps, and parameters. To accomplish this, we employ a recursive con- volutional network whose weights are tied between layers; this allows us to vary each of the three factors in a controlled setting. We ﬁnd that while increasing the numbers of layers and parameters each have clear beneﬁt, the number of feature maps (and hence dimensionality of the representation) appears ancillary, and ﬁnds most of its beneﬁt through the introduction of more weights. Our results (i) empir- ically conﬁrm the notion that adding layers alone increases computational power, within the context of convolutional layers, and (ii) suggest that precise sizing of convolutional feature map dimensions is itself of little concern; more attention should be paid to the number of parameters in these layers instead.  Introduction  1 Convolutional networks have recently made signiﬁcant progress in a variety of image classiﬁcation and detection tasks [13, 2, 23], with further gains and applications continuing to be realized. At the same time, performance of these models is determined by many interrelated architectural fac- tors; among these are the number of layers, feature map dimension, spatial kernel extents, number of parameters, and pooling sizes and placement. Notably, multiple layers of unpooled convolution [13, 30] have been utilized lately with considerable success. These architectures must be carefully designed and sized using good intuition along with extensive trial-and-error experiments on a vali- dation set. But are there any characteristics to convolutional layers’ performance that might be used to help inform such choices? In this paper, we focus on disentangling and assessing the independent effects of three important variables: the numbers of layers, feature maps per layer, and parameters. We accomplish this via a series of three experiments using a novel type of recursive network model. This model has a convolutional architecture and is equivalent to a deep convolutional network where all layers have the same number of feature maps and the ﬁlters (weights) are tied across layers. By aligning the architecture of this model to existing convolutional approaches, we are able to tease apart these three factors that determine performance. For example, adding another layer increases the number of parameters, but it also puts an additional non-linearity into the system. But would the extra parameters be better used expanding the size of the existing layers? To provide a general answer to this type of issue is difﬁcult since multiple factors are conﬂated: the capacity of the model (and of each layer) and its degree of non-linearity. However, we can design a recursive model to have the same number of layers and parameters as the standard convolutional model, and thereby see if the number of feature maps (which differs) is important or not. Or we can match the number of feature maps and parameters to see if the number of layers (and number of non-linearities) matters.  1  We consider convolutional models exclusively in this paper for several reasons. First, these models are widely used for image recognition tasks. Second, they are often large, making architecture search tedious. Third, and most signiﬁcantly, recent gains have been found by using stacks of multiple un- pooled convolution layers. For example, the convolutional model proposed by Krizhevsky et al. [13] for ImageNet classiﬁcation has ﬁve convolutional layers which turn out to be key to its performance. In [30], Zeiler and Fergus reimplemented this model and adjusted different parts in turn. One of the largests effects came from changing two convolutional layers in the middle of the model: remov- ing them resulted in a 4.9% drop in performance, while expanding them improved performance by 3.0%. By comparison, removing the top two densely connected layers yielded a 4.3% drop, and expanding them a 1.7% gain, even though they have far more parameters. Hence the use of multiple convolutional layers is vital and the development of superior models relies on understanding their properties. Our experiments have particular bearing in characterizing these layers.  1.1 Related Work  The model we employ has relations to recurrent neural networks. These are are well-studied models [11, 21, 27], naturally suited to temporal and sequential data. For example, they have recently been shown to deliver excellent performance for phoneme recognition [8] and cursive handwriting recognition [7]. However, they have seen limited use on image data. Socher et al. [26] showed how image segments could be recursively merged to perform scene parsing. More recently [25], they used a convolutional network in a separate stage to ﬁrst learn features on RGB-Depth data, prior to hierarchical merging. In these models the input dimension is twice that of the output. This contrasts with our model which has the same input and output dimension. Our network also has links to several auto-encoder models. Sparse coding [18] uses iterative algo- rithms, such as ISTA [1], to perform inference. Rozell et al. [20] showed how the ISTA scheme can be unwrapped into a repeated series of network layers, which can be viewed as a recursive net. Gregor & LeCun [9] showed how to backpropagate through such a network to give fast approxima- tions to sparse coding known as LISTA. Rolfe & LeCun [19] then showed in their DrSAE model how a discriminative term can be added. Our network can be considered a purely discriminative, convolutional version of LISTA or DrSAE. There also are interesting relationships with convolutional Deep Belief Networks [15], as well as Multi-Prediction Deep Boltzmann Machines [6]. As pointed out by [6], mean ﬁeld inference in such models can be unrolled and viewed as a type of recurrent network. In contrast to the model we use, however, [15] trains unsupervised using contrastive divergence, while [6] is nonconvolutional and focuses on conditioning on random combinations of inputs and targets.  2 Approach  Our investigation is based on a multilayer Convolutional Network [14], for which all layers beyond the ﬁrst have the same size and connection topology. All layers use rectiﬁed linear units (ReLU) [3, 4, 16]. We perform max-pooling with non-overlapping windows after the ﬁrst layer convolutions and rectiﬁcation; however, layers after the ﬁrst use no explicit pooling. We refer to the number of feature maps per layer as M, and the number of layers after the ﬁrst as L. To emphasize the difference between the pooled ﬁrst layer and the unpooled higher layers, we denote the ﬁrst convolution kernel by V and the kernels of the higher layers by Wl. A per-map bias bl is applied in conjunction with the convolutions. A ﬁnal classiﬁcation matrix C maps the last hidden layer to softmax inputs. Since all hidden layers have the same size, the transformations at all layers beyond the ﬁrst have the same number of parameters (and the same connection topology). In addition to the case where all layers are independently parameterized, we consider networks for which the parameters of the higher layers are tied between layers, so that Wi = Wj and bi = bj for all i, j. As shown in Fig. 1, tying the parameters across layers renders the deep network dynamics equivalent to recurrence: rather than projecting through a stack of distinct layers, the hidden representation is repeatedly processed by a consistent nonlinear transformation. The convolutional nature of the transformation performed at each layer implies another set of ties, enforcing translation-invariance among the parameters. This novel recursive, convolutional architecture is reminiscent of LISTA [9], but without a direct projection from the input to each hidden layer.  2  Figure 1: Our model architecture prior to the classiﬁcation layer, as applied to CIFAR and SVHN datasets. (b): Version with tied weights. Kernels connected by dotted lines are constrained to be identical. (c): The network with tied weights from (b) can be represented as a recursive network.  (a): Version with un-tied weights in the upper layers.  2.1  Instantiation on CIFAR-10 and SVHN  We describe our models for the CIFAR-10 [12] and SVHN [17] datasets used in our experiments. In both cases, each image Xn is of size 32 × 32 × 3. In the equations below, we drop the superscript n indicating the index in the dataset for notational simplicity. The ﬁrst layer applies a set of M kernels Vm of size 8 × 8 × 3 via spatial convolution with stride one (denoted as ∗), and per-map bias b0 m, followed by the element-wise rectiﬁcation nonlinearity. We use a “same” convolution (i.e. zero- padding the edges), yielding a same-size representation P of 32 × 32 × M. This representation is then max-pooled within each feature map with non-overlapping 4× 4 windows, producing a hidden layer Z1 of size 8 × 8 × M.  All L succeeding hidden layers maintain this size, applying a set of M kernels Wl 3 × M, also via “same” spatial convolution with stride one, and per-map bias bl rectiﬁcation nonlinearity:  m of size 3 × m, followed by the  i,j,m =  max  i(cid:48),j(cid:48)∈{0,··· ,3} (P4·i+i(cid:48),4·j+j(cid:48),m)  m + Vm ∗ X(cid:1) , Z1 m = max(cid:0)0, bl−1  Zl  Pm = max(cid:0)0, b0  In the case of the tied model (see Fig. 1(b)), the kernels W l (and biases bl) are constrained to be the same. The ﬁnal hidden layer is subject to pixel-wise L2 normalization and passed into a logistic classiﬁer to produce a prediction Y:  m ∗ Zl−1(cid:1) m + Wl−1 (cid:88)  Y(cid:48)  k =  i,j,m  where  i,j,m · ZL+1 Ck  i,j,m/||ZL+1  i,j  ||  (cid:80)  exp(Y (cid:48) k) k exp(Y (cid:48) k)  Yk =  The network is trained to minimize the logistic loss function L =(cid:80)  The ﬁrst-layer kernels Vm are initialized from a zero-mean Gaussian distribution with standard de- m are initialized viation 0.1 for CIFAR-10 and 0.001 for SVHN. The kernels of the higher layers Wl to the identity transformation Wi(cid:48),j(cid:48),m(cid:48),m = δi(cid:48),0 · δj(cid:48),0 · δm(cid:48),m, where δ is the Kronecker delta. k(n)) and k(n) is the true class of the nth element of the dataset. The parameters are not subject to explicit regularization. Training is performed via stochastic gradient descent with minibatches of size 128, learning rate 10−3, and momentum 0.9:  n log(Yn  g = 0.9 · g +  (cid:88)  n∈minibatch  ∂Ln  ∂ {V, W, b}  {V, W, b} = {V, W, b} − 10−3 · g  ;  3  ...convolutionWZ3M8M...Conv & ReLU4x4 max poolingFilters VFeature map Z1Input image X323288MMIntermediate representation P}...Conv & ReLUM8MFeature map Z2Filters W1...Conv & ReLUM8MFeature map Z3Filters W2...(a)...Conv & ReLU4x4 max poolingFilters VFeature map Z1Input image X323288MMIntermediate representation P}...Conv & ReLUM8MFeature map Z2Filters W...Conv & ReLUM8MFeature map Z3Filters W...(b)...Conv & ReLU4x4 max poolingFilters VFeature map Z1Input image X323288MMIntermediate representation P}...Conv & ReLUM8MFeature mapZ2, Z3, ...Filters W(c)3 Experiments  3.1 Performance Evaluation  We ﬁrst provide an overview of the model’s performance at different sizes, with both untied and tied weights, in order to examine basic trends and compare with other current systems. For CIFAR-10, we tested the models using M = 32, 64, 128, or 256 feature maps per layer, and L = 1, 2, 4, 8, or 16 layers beyond the ﬁrst. For SVHN, we used M = 32, 64, 128, or 256 fea- ture maps and L = 1, 2, 4, or 8 layers beyond the ﬁrst. That we were able to train networks at these large depths is due to the initialization of all W l m to the identity: this initially copies activations at the ﬁrst layer up to the last layer, and gradients from the last layer to the ﬁrst. Both untied and tied models had trouble learning with zero-centered Gaussian initializations at some of the larger depths. Results are shown in Figs. 2 and 3. Here, we plot each condition on a grid according to numbers of feature maps and layers. To the right of each point, we show the test error (top) and training error (bottom). Contours show curves with a constant number of parameters: in the untied case, the number of parameters is determined by the number of feature maps and layers, while in the tied case it is determined solely by the number of feature maps; Section 3.2.1 examines the behavior along these lines in more detail. First, we note that despite the simple architecture of our model, it still achieves competitive perfor- mance on both datasets, relative to other models that, like ours, do not use any image transformations or other regularizations such as dropout [10, 28], stochastic pooling [29] or maxout [5] (see Table 1). Thus our simpliﬁcations do not entail a departure from current methods in terms of performance. We also see roughly how the numbers of layers, feature maps and parameters affect performance of these models at this range. In particular, increasing any of them tends to improve performance, particularly on the training set (a notable exception to CIFAR-10 at 16 layers in the tied case, which goes up slightly). We now examine the independent effects of each of these variables in detail.  Figure 2: Classiﬁcation performance on CIFAR-10 as a function of network size, for untied (left) and tied (right) models. Contours indicate lines along which the total number of parameters remains constant. The top number by each point is test error, the bottom training error.  CIFAR-10 Ours Snoek et al. [24] Ciresan et al. [2] Hinton et al. [10] Coates & Ng [3]  Test error (%)  16.0 15.0 15.9 16.6 18.5  SVHN Ours Zeiler & Fergus (max pool) [29] Sermanet et al. [22]  Test error (%)  3.1 3.8 4.9  Table 1: Comparison of our largest model architecture (measured by number of parameters) against other approaches that do not use data transformations or stochastic regularization methods.  4  3264128256512# Feature Maps Per Layer124816# Layers2^162^172^182^192^202^212^222^232^24Test / Train Error: Upper Layers Untied0.2560.1890.2380.1550.2180.1050.2170.0400.2230.1400.2010.0800.1880.0280.1980.0000.2050.0980.1770.0350.1750.0010.1800.0000.1970.0650.1640.0080.1610.0000.1770.0340.1600.0010.2400.0033264128256512# Feature Maps Per Layer124816# Layers2^162^172^182^192^202^21Test / Train Error: Upper Layers Tied0.2530.1920.2470.1770.2500.1650.2610.1550.2290.1390.2140.1090.2080.0730.2310.0300.2020.0990.1900.0580.1820.0090.2000.0000.1970.0660.1720.0160.1720.0000.1780.0340.1600.0020.2790.159Figure 3: Classiﬁcation performance on Street View House Numbers as a function of network size, for untied (left) and tied (right) models.  3.2 Effects of the Numbers of Feature maps, Parameters and Layers In a traditional untied convolutional network, the number of feature maps M, layers L and parame- ters P are interrelated: Increasing the number of feature maps or layers increases the total number of parameters in addition to the representational power gained by higher dimensionality (more feature maps) or greater nonlinearity (more layers). But by using the tied version of our model, we can investigate the effects of each of these three variables independently. To accomplish this, we consider the following three cases, each of which we investigate with the described setup:  1. Control for M and P , vary L: Using the tied model (constant M and P ), we evaluate  performance for different numbers of layers L.  2. Control for M and L, vary P : Compare pairs of tied and untied models with the same numbers of feature maps M and layers L. The number of parameters P increases when going from tied to untied model for each pair.  3. Control for P and L, vary M: Compare pairs of untied and tied models with the same number of parameters P and layers L. The number of feature maps M increases when going from the untied to tied model for each pair.  Note the number of parameters P is equal to the total number of independent weights and biases over all layers, including initial feature extraction and classiﬁcation. This is given by the formula below for the untied model (for the tied case, substitute L = 1 regardless of the number of layers):  P = 8 · 8 · 3 · M + 3 · 3 · M 2 · L + M · (L + 1) + 64 · M · 10 + 10  (ﬁrst layer)  (higher layers)  (biases)  (classiﬁer)  3.2.1 Case 1: Number of Layers  We examine the ﬁrst of these cases in Fig. 4. Here, we plot classiﬁcation performance at differ- ent numbers of layers using the tied model only, which controls for the number of parameters. A different curve is shown for different numbers of feature maps. For both CIFAR-10 and SVHN, performance gets better as the number of layers increases, although there is an upward tick at 8 layers for CIFAR-10 test error. The predominant cause of this appears to be overﬁtting, since the training error still goes down. At these depths, therefore, adding more layers alone tends to increase performance, even though no additional parameters are introduced. This is because additional layers allow the network to learn more complex functions by using more nonlinearities. This conclusion is further supported by Fig. 5, which shows performance of the untied model accord- ing to numbers of parameters and layers. Note that vertical cross-sections of this ﬁgure correspond to the constant-parameter contours of Fig. 2. Here, we can also see that for any given number of parameters, the best performance is obtained with a deeper model. The exception to this is again the 8-layer models on CIFAR-10 test error, which suffer from overﬁtting.  5  3264128256# Feature Maps Per Layer1248# Layers2^162^172^182^192^202^21Test / Train Error: Upper Layers Untied0.0730.0350.0570.0260.0450.0180.0400.0150.0600.0270.0460.0190.0370.0120.0340.0080.0530.0220.0390.0140.0350.0080.0330.0040.0460.0180.0360.0110.0310.0063264128256# Feature Maps Per Layer1248# Layers2^162^172^182^19Test / Train Error: Upper Layers Tied0.0640.0300.0570.0260.0510.0240.0500.0200.0390.0160.0380.0130.0430.0150.0350.0100.0320.0060.0340.0100.0330.0060.0730.0350.0600.0270.0530.0220.0460.018Experiment 1a: Error by Layers and Features (tied model)  Test Error  Training  Error  (a) CIFAR-10  (b) SVHN  Figure 4: Comparison of classiﬁcation error for different numbers of layers in the tied model. This controls for the number of parameters and features. We show results for both (a) CIFAR-10 and (b) SVHN datasets.  Experiment 1b: Error by Parameters and Layers (untied model)  Test Error  Training  Error  (a) CIFAR-10  (b) SVHN  Figure 5: Comparison of classiﬁcation error for different numbers of parameters in the untied model, for (a) CIFAR-10 and (b) SVHN datasets. Larger numbers of both parameters and layers help performance. In addition, for a ﬁxed budget of parameters, allocating them in more layers is generally better (CIFAR-10 test error increases above 4 layers due to overﬁtting).  6  12345678# Layers0.180.200.220.240.26Test Error32 features64 features128 features256 features12345678# Layers0.0300.0350.0400.0450.0500.0550.0600.0650.0700.075Test Error32 features64 features128 features256 features12345678# Layers0.000.050.100.150.20Training Error32 features64 features128 features256 features12345678# Layers0.0050.0100.0150.0200.0250.0300.035Training Error32 features64 features128 features256 features104105106107# Parameters0.160.180.200.220.240.26Test Error1 layers2 layers4 layers8 layers104105106107# Parameters0.0300.0350.0400.0450.0500.0550.0600.0650.0700.075Test Error1 layers2 layers4 layers8 layers104105106107# Parameters0.000.050.100.150.20Training Error1 layers2 layers4 layers8 layers104105106107# Parameters0.0000.0050.0100.0150.0200.0250.0300.035Training Error1 layers2 layers4 layers8 layers3.2.2 Case 2: Number of Parameters  To vary the number of parameters P while holding ﬁxed the number of feature maps M and layers L, we consider pairs of tied and untied models where M and L remain the same within each pair. The number of parameters P is then greater for the untied model. The result of this comparison is shown in Fig. 6. Each point corresponds to a model pair; we show classiﬁcation performance of the tied model on the x axis, and performance of the untied model on the y axis. Since the points fall below the y = x line, classiﬁcation performance is better for the untied model than it is for the tied. This is not surprising, since the untied model has more total parameters and thus more ﬂexibility. Note also that the two models converge to the same test performance as classiﬁcation gets better — this is because for the largest numbers of L and M, both models have enough ﬂexibility to achieve maximum test performance and begin to overﬁt.  Experiment 2: Same Feature Maps and Layers, Varied Parameters  Test Error  Training  Error  (a) CIFAR-10  (b) SVHN  Figure 6: Comparison of classiﬁcation error between tied and untied models, controlling for the number of feature maps and layers. Linear regression coefﬁcients are in the bottom-right corners.  3.2.3 Case 3: Number of Feature Maps  We now consider the third condition from above, the effect of varying the number of feature maps M while holding ﬁxed the numbers of layers L and parameters P . For a given L, we ﬁnd model pairs whose numbers of parameters P are very close by varying the number of feature maps. For example, an untied model with L = 3 layers and M = 71 feature maps has P = 195473 parameters, while a tied model with L = 3 layers and M = 108 feature maps has P = 195058 parameters — a difference of only 0.2%. In this experiment, we randomly sampled model pairs having the same number of layers, and where the numbers of parameters were within 1.0% of each other. We considered models where the number of layers beyond the ﬁrst was between 2 and 8, and the number of feature maps was between 16 and 256 (for CIFAR-10) or between 16 and 150 (for SVHN). Fig. 7 shows the results. As before, we plot a point for each model pair, showing classiﬁcation performance of the tied model on the x axis, and of the untied model on the y axis. This time, however, each pair has ﬁxed P and L, and tied and untied models differ in their number of feature maps M. We ﬁnd that despite the different numbers of feature maps, the tied and untied models  7  0.140.160.180.200.220.240.260.280.30test error: tied params0.140.160.180.200.220.240.260.280.30test error: untied paramstrend: y=0.70x+0.04 r=0.960.030.040.050.060.07test error: tied params0.030.040.050.060.07test error: untied paramstrend: y=0.67x+0.01 r=0.930.050.000.050.100.150.20train error: tied params0.050.000.050.100.150.20train error: untied paramstrend: y=0.52x+-0.00 r=0.740.0000.0050.0100.0150.0200.0250.0300.035train error: tied params0.0000.0050.0100.0150.0200.0250.0300.035train error: untied paramstrend: y=0.75x+0.00 r=0.94Experiment 3: Same Parameters and Layers, Varied Feature Maps  Test Error  Training  Error  (a) CIFAR-10  (b) SVHN  Figure 7: Comparison of classiﬁcation error between tied and untied models, controlling for the number of parameters and layers. Linear regression coefﬁcients in the bottom-right corners.  perform about the same in each case. Thus, performance is determined by the number of parameters and layers, and is insensitive to the number of feature maps.  4 Discussion  Above we have demonstrated that while the numbers of layers and parameters each have clear effects on performance, the number of feature maps has little effect, once the number of parameters is taken into account. This is perhaps somewhat counterintuitive, as we might have expected the use of higher-dimensional representations to increase performance; instead we ﬁnd that convolutional layers are insensitive to this size. This observation is also consistent with Fig. 5: Allocating a ﬁxed number of parameters across multiple layers tends to increase performance compared to putting them in few layers, even though this comes at the cost of decreasing the feature map dimension. This is precicesly what one might expect if the number of feature maps had little effect compared to the number of layers. Our analysis employed a special tied architecture and comes with some important caveats, however. First, while the tied architecture serves as a useful point of comparison leading to several interesting conclusions, it is new and thus its behaviors are still relatively unknown compared to the common untied models. This may particularly apply to models with a large number of layers (L > 8), or very small numbers of feature maps (M < 16), which have been left mostly unexamined in this paper. Second, our experiments all used a simpliﬁed architecture, with just one layer of pooling. While we believe the principles found in our experiments are likely to apply in more complex cases as well, this is unclear and requires further investigation to conﬁrm. The results we have presented provide empirical conﬁrmation within the context of convolutional layers that increasing layers alone can yield performance beneﬁts (Experiment 1a). They also indi- cate that ﬁlter parameters may be best allocated in multilayer stacks (Experiments 1b and 3), even at the expense of having fewer feature maps. In conjunction with this, we ﬁnd the feature map di- mension itself has little effect on convolutional layers’ performance, with most sizing effects coming from the numbers of layers and parameters (Experiments 2 and 3). Thus, focus would be best placed on these variables when determining model architectures.  8  0.160.170.180.190.200.21test error: tied params0.160.170.180.190.200.21test error: untied paramstrend: y=0.96x+0.01 r=0.920.0320.0340.0360.0380.0400.0420.0440.0460.048test error: tied params0.0320.0340.0360.0380.0400.0420.0440.0460.048test error: untied paramstrend: y=0.81x+0.01 r=0.850.010.000.010.020.030.040.050.06train error: tied params0.010.000.010.020.030.040.050.06train error: untied paramstrend: y=1.00x+0.00 r=0.970.0000.0050.0100.0150.0200.025train error: tied params0.0000.0050.0100.0150.0200.025train error: untied paramstrend: y=0.79x+0.00 r=0.97References [1] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.  SIAM Journal on Imaging Sciences, 2(1):183–202, 2009. 2  [2] D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, and J. Schmidhuber. Flexible, high performance  convolutional neural networks for image classiﬁcation. In IJCAI, 2011. 1, 4  [3] A. Coates and A. Y. Ng. The importance of encoding versus training with sparse coding and vector  quantization. In ICML, volume 8, pages 921–928, 2011. 2, 4  [4] X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectiﬁer networks. In AISTATS, volume 15, pages  315–323, 2011. 2  [5] I. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio. Maxout networks. In ICML,  2013. 4  [6] I. J. Goodfellow, M. Mirza, A. Courville, and Y. Bengio. Multi-prediction deep boltzmann machines. In  NIPS, 2013. 2  [7] A. Graves, M. Liwicki, S. Fernandez, R. Bertolami, H. Bunke, and J. Schmidhuber. A novel connectionist  system for improved unconstrained handwriting recognition. PAMI, 31(5), 2009. 2  [8] A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In  ICASSP, 2013. 2  [9] K. Gregor and Y. LeCun. Learning fast approximations of sparse coding. In ICML, 2010. 2 [10] G.E. Hinton, N. Srivastave, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. networks by preventing co-adaptation of feature detectors. arXiv:1207.0580, 2012. 4  Improving neural  [11] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735–1780,  1997. 2  [12] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical Report TR-2009, Uni-  versity of Toronto, 2009. 3  [13] A. Krizhevsky, I. Sutskever, and G.E. Hinton.  networks. In NIPS, 2012. 1, 2  Imagenet classiﬁcation with deep convolutional neural  [14] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.  Proceedings of the IEEE, 86(11):2278–2324, 1998. 2  [15] H. Lee, R. Grosse, R. Ranganath, and A. Ng. Convolutional deep belief networks for scalable unsuper-  vised learning of hierarchical representations. In ICML, volume 26, 2009. 2  [16] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In ICML, pages  807–814, 2010. 2  [17] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with  unsupervised feature learning. In NIPS Workshop, 2011. 3  [18] B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy employed by  V1? Vision Research, 37(23):3311–3325, 1997. 2  [19] J. Rolfe and Y. LeCun. Discriminative recurrent sparse auto-encoders. In ICLR, 2013. 2 [20] C. J. Rozell, D. H. Johnson, R. G. Baraniuk, and B. A. Olshausen. Sparse coding via thresholding and  local competition in neural circuits. Neural Computation, 20(10):2526–2563, October 2008. 2  [21] J. Schmidhuber, D. Wierstra, M. Gagliolo, and F. Gomez. Training recurrent networks by evolino. Neural  Computation, 19(3):757–779, 2007. 2  [22] P. Sermanet, S. Chintala, and Y. LeCun. Convolutional neural networks applied to house numbers digit  classiﬁcation. In ICPR, 2012. 4  [23] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition,  localization and detection using convolutional networks. http://arxiv.org/abs/1312.6229, 2014. 1  [24] J. Snoek, H. Larochelle, and R. Adams. Practical bayesian optimzation of machine learning algorithms.  In NIPS, 2012. 4  [25] R. Socher, B. Huval, B. Bhat, C. D. Manning, and A. Y. Ng. Convolutional-Recursive Deep Learning for  3D Object Classiﬁcation. In NIPS, 2012. 2  [26] R. Socher, C. C. Lin, A. Y. Ng, and C. D. Manning. Parsing Natural Scenes and Natural Language with  Recursive Neural Networks. In ICML, 2011. 2  [27] I. Sutskever and G. Hinton. Temporal kernel recurrent neural networks. Neural Networks, 23:239–243,  2010. 2  [28] L. Wan, M. Zeiler, Z. Sixin, Y. LeCun, and R. Fergus. Regularization of neural networks using dropcon-  nect. In ICML, 2013. 4  [29] M. Zeiler and R. Fergus. Stochastic pooling. In ICLR, 2013. 4 [30] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks.  1131.2901v3, 2013. 1, 2  In Arxiv.org  9  ","A key challenge in designing convolutional network models is sizing themappropriately. Many factors are involved in these decisions, including numberof layers, feature maps, kernel sizes, etc. Complicating this further is thefact that each of these influence not only the numbers and dimensions of theactivation units, but also the total number of parameters. In this paper wefocus on assessing the independent contributions of three of these linkedvariables: The numbers of layers, feature maps, and parameters. To accomplishthis, we employ a recursive convolutional network whose weights are tiedbetween layers; this allows us to vary each of the three factors in acontrolled setting. We find that while increasing the numbers of layers andparameters each have clear benefit, the number of feature maps (and hencedimensionality of the representation) appears ancillary, and finds most of itsbenefit through the introduction of more weights. Our results (i) empiricallyconfirm the notion that adding layers alone increases computational power,within the context of convolutional layers, and (ii) suggest that precisesizing of convolutional feature map dimensions is itself of little concern;more attention should be paid to the number of parameters in these layersinstead."
1312.4209,2014,Feature Graph Architectures  ,"['Richard Davis', 'Sanjay Chawla', 'Philip Leong']",https://arxiv.org/pdf/1312.4209.pdf,"3 1 0 2    c e D 5 1         ]  G L . s c [      1 v 9 0 2 4  .  2 1 3 1 : v i X r a  000 001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053  Feature Graph Architectures  Anonymous Author(s)  Afﬁliation Address email  Abstract  In this article we propose feature graph architectures (FGA), which are deep learn- ing systems employing a structured initialisation and training method based on a feature graph which facilitates improved generalisation performance compared with a standard shallow architecture. The goal is to explore alternative perspec- tives on the problem of deep network training. We evaluate FGA performance for deep SVMs on some experimental datasets, and show how generalisation and stability results may be derived for these models. We describe the effect of per- mutations on the model accuracy, and give a criterion for the optimal permutation in terms of feature correlations. The experimental results show that the algorithm produces robust and signiﬁcant test set improvements over a standard shallow SVM training method for a range of datasets. These gains are achieved with a moderate increase in time complexity.  1 Introduction  In a recent review, Bengio Bengio (2013) emphasised that deep learning algorithms which can do a much better job of disentangling the underlying factors of variation in machine learning prob- lems would have tremendous impact. Towards this goal we propose feature graph architectures (FGAs), which are hierarchical networks of learning modules (such as support vector machines) with some advantageous properties. FGA is a deep learning model, similar to deep belief networks Hinton & Osindero (2006) and deep support vector machines M.A. Wiering & Schomaker (2013). The training method involves three new components. It partitions the features into a subspace tree architecture, initialises the structure to give the same output as an optimised shallow SVM, and then identiﬁes the subspaces at each part of the structure in which improved generalisation can be found. This method ensures that any changes that are found can only improve the training set error, and within certain generalisation bounds, the test error will improve with high probability also.  The key motivation behind this work is that the method is able to generate signiﬁcant improvements in test set accuracy compared with a standard SVM using improved ﬁts in the parameter subspaces, exploiting the SVM initialisation and selective node training to improve performance over a standard shallow SVM. The method consistently achieves signiﬁcantly improved test error performance over standard SVM models in practise. We provide experimental evidence verifying the improved train- ing and test performance of FGA on a range of standard UCI datasets Bache & Lichman (2013) as well as synthetic problems. A derivation of a generalisation bound is provided to illustrate how this type of analysis may be performed for the FGA. We analyse the stability of the FGA under changes in the training data and demonstrate that there is a clear tradeoff between generalisation accuracy and stability in these models. We also investigate the dependence of the FGA on permutations of the input nodes. Permutation of the inputs to ensure decorrelation in the ﬁnal layers enables im- proved ﬁtting and generalisation. Theoretical stability and correlation analysis results are supported by numerical experiments, and an analysis of the time complexity of the FGA algorithm is given.  1  054 055 056 057 058 059 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 100 101 102 103 104 105 106 107  2 Existing work on SVM architectures  Support Vector Machines have been applied to a wide range of applications, and there are many variants. These can be differentiated into linear and nonlinear, regression and classiﬁcation, and a range of alternatives for the kernel function. See Cristianini & Shawe-Taylor for an overview, and Ivanciuc (2005) for an extensive list of widely used packages covering the major types of SVM implementations. SVM complexity and generalisation are discussed in Burges (1998); Mohri et al. (2012). For sparse, high-dimensional data, linear support vector machines have proven effective Joachims (2006). Bottou (2007) gives an overview of scaling SVMs for large datasets.  There have been a number of attempts at organising SVMs into hierarchical structures D´ıez et al. (2010). SVMs have been combined with Self-Organising Maps Cao (2003) which ﬁrst divide the dataset into subsets, to which SVMs are applied separately. Support Vector based clustering was investigated by Ben-Hur et al. (2001).  SVMs have been used to implement decision trees by placing an SVM at each node of the tree Bennett & Blue (1997); Madzarov & Gjorgjevikj (2009). Ensemble methods such as bagging and random forests have been applied to binary SVM decision trees Madzarov et al.. Hierarchical SVM networks have been constructed by several authors by dividing the dataset into subsets using SVMs for clustering Yu et al. (2005). Chen and Crawford (Chen et al., 2004) organised SVMs into hi- erarchical classiﬁcation architectures, where the output classes were subdivided in a hierarchical manner.  Deep learning using support vector machines (deep-SVM, or DSVM) has been investigated by several researchers. In Abdullah et al. (2009) an ensemble of DSVMs was applied to im- age categorisation. Recently, backpropagation training methods for DSVM were given in M.A. Wiering & Schomaker (2013) which were shown to often, but not always, provide some gen- eralisation performance improvements compared with a simple SVM.  In this paper we propose and study the generalisation performance of feature graph architectures (FGA). We also investigate the stability of the FGA predictor, and the effect of permuting the inputs on generalisation error. The method is novel due to its organisation of the features into a speciﬁc subspace tree architecture, the initialization using the coefﬁcients of a shallow model (in our case, SVM), and the selective training of each node to the target, scaled appropriately at each node. The key reason for this architecture is to exploit the structure of subspaces of the full feature space in order to achieve improved generalization performance. The relevant calculations are given in a series of derivations in the appendices, and described in the following sections.  3 Feature Graph Architectures  We consider a distribution D over X × Y where X is the feature space of dimension d and Y ∈ R is the target space. We deﬁne the expected loss or generalisation error R(h) with respect to the function h as the expectation  R(h) = E  x∼D  [L(h(x), f (x))]  (1)  and the empirical loss or error of h ∈ H for a sample of points S = {(xi, yi) : i = 1 . . . m} of size m from D as the mean  m  ˆR(h) =  1 m  L(h(xi), yi)  Xi=1  (2)  A feature graph is a lattice of machine learning modules, organized into a tree structure. In this article we focus on support vector machines, and also brieﬂy touch on a neural network implemen- tation. A potentially large set of input features is partitioned into small subsets and fed into the ﬁrst layer of the tree. Each node is trained to the same target using an algorithm such as SVM. The model is then reapplied to the training set to generate outputs which are used in subsequent nodes in the lattice. The feature grouping can be selected either by iterating over multiple groupings, which can be time consuming, or alternatively a heuristic such as grouping to maximise feature correlation  2  108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161  Input features  Intermediate   layers  Output  prediction  y y s;11  Intermediate targets are scaled  y y s;21  y  Final target  is unscaled  y y s;22  Intermediate targets are only used  if they improve the final training error  y y s;12  y y s;13  y y s;14  y y s;15  y y s;16  x1 x 2 x 3 . . .  x16 x 17 x 18  Figure 1: Loss-optimised feature graph architecture. After initialising to a shallow SVM, each node is trained to a common target, y, with additional scaling ys;m for node m and the predictions of each node on the training set are used as features for training subsequent nodes.  may be used. In a later section we provide justiﬁcation for this. The optimal heuristic for differ- ent problems is an open (and non-trivial) question, but from our experiments this heuristic is very effective in practice.  The feature graph algorithm may be implemented in several forms, depending upon the required architecture. The simplest training method is the layer-based feature graph, which we include as a reference when studying the performance of the main algorithm, the loss-optimised FGA. The layer-based FGA algorithm is as follows:  Layer-based feature graph  Given the empirical error function ˆR, form L successive layers of SVM models (see ﬁgure 1). There are Ml nodes in layer l. For all layers:  1. Form feature groups of size M in each layer and ﬁt an SVM to those features using the  common target y  2. Apply the SVM to those features and use the predictions as features in the next layer 3. Repeat the process for each successive layer until the ﬁnal layer containing only one SVM  is reached.  The method is detailed in algorithm 1.  Our main focus will be the loss-optimised FGA, which decorrelates input variables in the early layers of the FGA, initialises the FGA to the coefﬁcients of a simple SVM, and then selectively trains nodes in the graph. This algorithm is used in the generalisation and stability analysis later in the manuscript. The algorithm is as follows.  Loss-optimised Feature Graph Architectures  3  162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215  Algorithm 1 Layer-based FGA training algorithm Require: xi ∈ RD, yi ∈ R, M ≥ 2, ǫ > 0, L,{Ml}  while δ ˆR > ǫ do  for l = 1 to L do  for m = 1 to Ml do  FGA[l; m] ← svm.train(xl;m, y) δ ˆR ← ||FGA.evaluate(x) − y||  end for  end for end while  In the loss-optimised FGA, we ﬁrst initialise the FGA with the parameters of the tuned SVM in the ﬁrst layer, and set the weights and intercepts to w = 1, b = 0 respectively in subsequent layers. This will guarantee that its output exactly matches that of the standard SVM with linear kernels. We then iterate through the architecture, training each local node to a scaled copy of the target y. The scaling preserves the mean output of the node being trained, so that retraining a node produces an output of a scale suitable for the next downstream node in its current state. Only local modiﬁcations to a node which improve the overall l2-error accuracy on the training set are retained. The steps are detailed in algorithm 2. Any parameter adjustments which generate improvements in a local cost function are veriﬁed against the global l2 metric before being retained. As in the layered case, a range of input vector permutations should be used to determine the best architecture. This guarantees an improved training error.  Algorithm 2 Loss-optimised FGA training algorithm Require: xi ∈ X , yi ∈ Y, ǫ > 0, M ≥ 2, L,{Ml}  FGAl=1 ← SVMtuned FGAl>1 ← w = 1, b = 0 δ ˆRbest ← ∞ while δ ˆR > ǫ do  for l = 1 to L do  for m = 1 to Ml do  ¯y  previous.node ← FGA[l; m] ys ← y ¯yl;m FGA[l; m] ← svm.train(xl;m, ys) δ ˆR ← ||FGA.evaluate(x) − y|| if δ ˆR > δ ˆRbest then FGA[l; m] ← previous.node δ ˆRbest ← δE  else  end if end for  end for end while  Permutation optimised FGA The loss-optimised FGA may be further reﬁned by permuting the input set and selecting the per- mutation with the lowest training error. From the derivation in the next section, permutations which optimise the training error will improve the test error with high probability for sufﬁciently large m. In this algorithm we search over permutations to maximise the training error. In addition, in section 6 we show that the best permutation is one where FGA correlations in the early layers are higher, and correlations in later layers are lower (this was also conﬁrmed numerically in our test).  4  216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269  4 Generalisation bounds  A range of generalisation bounds may be derived for a support vector regression problem. See Mohri et al. (2012), chapter 10 for details. Here we illustrate how such bounds may be used to derive associated bounds for a speciﬁc FGA, using the ﬁrst bound of theorem 10.8, page 254 of Mohri et al. (2012) as a starting point, since this is speciﬁcally designed for a single SVM. See appendix A for details of the derivation. Theorem 1 Consider any node of the FGA with dimension d ≤ dmax, where dmax is the maximum dimensionality of each node over the FGA. Assume we have selected a ﬁxed feature permutation using a heuristic. Let K : X × X → R be a PDS kernel, let Φ : X → H be a feature mapping associated to K and let H = {x → w · Φ(x) : ||w||H ≤ Λ}. Assume that there exists r > 0 such that K(x, x) ≤ r2 and |f (x)| ≤ Λr for all x ∈ X. Fix ǫ > 0. | · |ǫ denotes the ǫ-insensitive loss function (see Mohri et al. (2012), p252). Then, for any δ > 0, for all h ∈ H we have 2  √m 1 +s log 1   RF GA − RSV M ≤ ˆRF GA − ˆRSV M + |V |ǫ + |V |  with probability at least 1−|V |δ, where there are |V | nodes in the FGA, and Λ = dmax. This implies that the FGA has a lower generalization error than the corresponding SVM with high probability, for sufﬁciently large m, since the FGA training error is guaranteed to be less than (or if no node improvements are found, equal to) the SVM training error by construction.  2rΛ  (3)  δ  nodes are modiﬁed (in our tests it was usually around one in three nodes, resulting in a relatively  Although the bound increases with |V |, to achieve the same probability we can use a smaller value of δ, but δ appears inside the logarithmic term, and there is an additional square root. This gives an additionalplog |V | dependence. This may seem onerous, but in practise we ﬁnd that relatively few small value of the |V |plog |V | dependence in the above bound, for constant values of 1 − |V |δ. This result applies to layer-based FGA, selective loss-optimised FGA, and permutation-optimised FGA, since the training error is improved and the FGA loss function is the same in each case. The proof is given in appendix A. Note that the bound improves if only |V ′| < |V | nodes yield training error improvements, since only those nodes are modiﬁed.  We can add equation 3 to the standard SVM generalisation bound given in appendix A, equation 9, to obtain  RF GA ≤ ˆRF GA + (|V | + 1)ǫ + (|V | + 1) with probability at least 1 − (|V | + 1)δ, by the union bound. 5 Stability  2rΛ  2  √m 1 +s log 1   δ  (4)  Whilst the generalisation bound derived in the previous section does not require a stability analysis, we include it here to give the reader a sense of the tradeoff between stability and accuracy in the FGA. An algorithm is said to be uniformly stable Bousquet & Elisseeff (2002) if the following holds:  Deﬁnition 1 Uniform stability. A learning algorithm A : ∪∞ w.r.t. the loss function L : R × R → R if the following holds for all i ∈ 1, . . . , m.  m=1Z m → F is said to be βm-stable (5)  ∃z ∈ Z m : ∃x ∈ Z : |L(fz(x)) − L(fz\i(x))| ≤ βm  Theorem 2 An FGA is β-stable with stability given by  vL  βFGA = βSVM + Xv∈V  βv  Yiv =v  wiv  (6)  where βSVM is the stability of a simple SVM, βv is the stability of an individual node v, and wiv are the weights along paths from nodes v ∈ V to the root vL of the FGA tree.  5  270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323  The proof is given in appendix B, together with some numerical examples. From this result it is apparent that the FGA has a lower stability than the corresponding simple SVM, although it has improved generalisation performance.  6 Optimising Permutations  Consider an arbitrary permutation of nodes. Intuitively, the dominant contributions to the test error are correlations in the ﬁnal layer. It is natural to minimise these in preference to early layer cor- relations, since their effect reaches over more input nodes. This means the optimal permutation is one where the correlation in the ﬁnal layers is minimized and the correlation in the early layers is maximized. Any improvements in training and test errors in the FGA due to node permutations are governed by the generalisation bound of equation 3. For the following simpliﬁed situation we give an illustrative result to justify this heuristic. Theorem 3. Consider an FGA with four input features 1, 2, 3, 4. The second layer outputs are given by X12, X34. A permutation of the input features gives second layer outputs X13, X24. If the input vectors are equally weighted, in the sense that K = hY, X12i = hY, X34i = hY, X13i = hY, X24i and hX12, X34i ≤ hX13, X24i then the empirical errors for the two permutations satisfy  R12,34 < R13,24  (7)  Proof. The proof is shown in appendix C. From equation 3 we see that training improvements imply test error improvements with high probability for sufﬁciently large training sets, so selecting permutations on the basis of training error and pairwise feature correlation is a reliable method for sufﬁciently large training sets.  However, in practise the situation is not always so simple, refer to the appendix for examples.  7 Computational complexity  For a space of D dimensions, a standard SVM will scale as O(D). Since the FGA depth is O(log(D)) and each node of dimension m has complexity O(m), a layer of size d ≤ D will have complexity O(d). Thus the complexity of the corresponding FGA the complexity will be O(D log D), summing over all layers. This does not include searching over multiple input permu- tations, of which there are a very large number, but as mentioned the best results are achieved using a permutation which maximises the early-layer correlations and minimises later layer correlations, so in practise it is not necessary to do extensive permutation searches.  8 Numerical results  In this section we show the result of applying the FGA architecture by applying and compar- ing some widely used R packages (R Development Core Team, 2012): linear regression, Neu- ralNet (G¨unther & Fritsch), RandomForest (Liaw & Wiener, 2002) and support vector machines (Dimitriadou et al., 2010). Default parameters were used for the neural networks, with 5 nodes in the hidden layer. Default settings were used for the random forests run. The SVM used a linear kernel and tuned on the range 10(−10:10). The FGA architecture for a neural network is similar to the SVM. The output ranges of all nodes are scaled so that they lie in the non-asymptotic region of the following layer, ensuring that the network can be trained easily. The network is initialised using a single-layer network, with the second layer initialised so that it acts like an identity map, just as was done for the FGA-SVM.  It is not possible to cover all possible datasets, so we give results for one synthetic and four UCI datasets to illustrate the method.  6  324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377  8.1 Regression, neural network and PCA comparison  We compared standard and FGA approaches on a synthetic datasets corresponding to the equation  y =  D Xi=1  xi!p  (8)  for a power p = 2. The dataset dimension was D = 25, with 5 features per node in the SVM. Figure 3 shows the greatly enhanced accuracy of the FGA SVM on the test set. The results for simple and FGA architectures on linear regression, PCA-SVM (principle components analysis on the features), neural networks, FGA using neural network nodes (FGA-NN), random forests (RF), support vector machines and FGA using support vector machines (FGA-SVM) are shown in table 1.  PCA-SVM NN (2 layer)  FGA-NN (2-layer)  Model  LR  RF SVM  FGA-SVM L-opt., one perm. only  FGA-SVM L-opt., optimised perm.  L = l2 error  1.508 1.177 0.959 0.754 0.753 0.537 0.333 0.323  Table 1: L = l2 error results using simple linear regresion (LR), neural networks (NN), FGA-neural networks (FGA-NN), random forests (RF), support vector machines (SVM), PCA support vector machines (PCA-SVM), and FGA support vector machines (FGA-SVM).  8.2 FGA-SVM comparison on some standard regression datasets  To illustrate the effectiveness of the method on some standard datasets of realistic size, ﬁgure 2 gives a summary of the results comparing SVM, loss-optimised FGA-SVM, and loss-optimised FGA-SVM with optimised permutations. We see that the test error is never worse than for the SVM, and signiﬁcant improvements of up to 43% were observed, depending on the dataset. Initialising to a permutation in which early layer nodes are highly correlated and later layer nodes are less corre- lated, followed by optimising over additional pairwise permutations, was able to provide signiﬁcant additional improvements for the majority of datasets we tried. For the four sets presented here, this optimisation was very effective for two cases, and had negligible effect in the other two cases. In one of those two cases the FGA was not able to ﬁnd signiﬁcant improvements over the standard SVM and so the permutation had no effect. In the other case the permutation resulted in signiﬁcant training improvements with only a very small test error increase, so we ﬁnd no signiﬁcant gener- alisation beneﬁt from this permutation change. However the strong performance of the method for other datasets would justify evaluating permutations.  Substituting the values in table 2 into equation 3 we see in all cases the bound is satisﬁed, providing experimental conﬁrmation of the bound.  9 Conclusions  We have introduced the FGA architecture and described an algorithm incorporating several novel elements which enables effective and robust training of a deep SVM. We have also demonstrated effective performance when the deep SVM is replaced by a deep neural network. We have shown that the FGA is able to provide signiﬁcant improvements in test error over a shallow SVM, and in particular these improvements are robust, in that for all the datasets we tested it was never sig- niﬁcantly worse, and was usually better. We have given examples of how to derive generalisation bounds for the FGA, and illustrated the tradeoff between generalisation and stability in the FGA. We have described the effect of input permutations on the FGA accuracy, and given a criterion for a good permutation heuristic. The FGA training architecture can be used to provide a smooth tran- sition from a shallow network to a deep network, with improved test errors at each stage of the  7  378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431  Dataset  Method  Kernel Ntrain Ntest D  ˆR  Triazines  Tuned Simple SVM Loss-optimised Tuned FGA-SVM Loss-opt. max 50 rand. perm.  Space GA Tuned Simple SVM  Loss-optimised Tuned FGA-SVM Loss-opt. max 50 rand. perm.  Pyrim  Housing  Tuned Simple SVM Loss-optimised Tuned FGA-SVM Loss-opt. max 50 rand. perm.  Tuned Simple SVM Loss-optimised Tuned FGA-SVM Loss-opt. max 50 rand. perm.  Linear  100  60  60  Linear  500  3200  6  Linear  40  34  27  Linear  300  206  13  342.3 318.3 279.18  567,594 567,594 566,429  31.93 24.04 17.79  321.2 231.4 198.3  R:  977.1 745.1 749.2  537,717 537,717 536,642  179.3 177.7 151.1  6883 5204 3872  Figure 2: Train and test error results using simple SVM and the two loss-optimised FGA strategies for a linear kernel. In all cases the cross-validation tuning ranges used were C: 2−2, 2−1, 2−0, 21, 22, 23, 24, 25. The C parameter for the tuned SVM was used throughout the FGA architecture. We found signiﬁcant improvements for Pyrim and Housing. The dimensionality of Space GA was only 6 which was why the FGA was not able to ﬁnd signiﬁcant improvements on the training or test sets. For Triazines, signiﬁcant training set improvements were found but the test set error was only marginally worse, which shows that the FGA is not overﬁtting signiﬁcantly relative to the SVM.  transition. It provides a practical and effective way to initialise and train a deep network. Whilst subject to generalisation bounds, we have demonstrated through a range of examples that the im- provements can be very signiﬁcant. As the neural network results appear promising on synthetic data, they and other deep learning implementations of the FGA appear promising. The FGA can easily be extended to non-linear kernels, and combined with other types of learning systems. These topics will be investigated in future work.  References Abdullah, Azizi, Veltkamp, Remco C., and Wiering, Marco A. An ensemble of deep support vec- tor machines for image categorization. In Proceedings of the 2009 International Conference of Soft Computing and Pattern Recognition, SOCPAR ’09, pp. 301–306, Washington, DC, USA, 2009. IEEE Computer Society. ISBN 978-0-7695-3879-2. doi: 10.1109/SoCPaR.2009.67. URL http://dx.doi.org/10.1109/SoCPaR.2009.67.  Bache, K.  and Lichman, M.  UCI machine  learning repository,  2013.  URL  http://archive.ics.uci.edu/ml.  Ben-Hur, Asa, Horn, David, Siegelmann, Hava T., and Vapnik, Vladimir.  tor clustering. http://jmlr.csail.mit.edu/papers/volume2/horn01a/rev1/horn01ar1.pdf.  Journal of Machine Learning Research, 2:125–137, 2001.  Support vec- URL  Bengio, Yoshua. Deep learning of representations: Looking forward. CoRR, abs/1305.0445, 2013. Bennett, K. P. and Blue, J.A. A support vector machine approach to decision trees. In Department of Mathematical Sciences Math Report No. 97-100, Rensselaer Polytechnic Institute, pp. 2396– 2401, 1997. L.  Kernel Machines.  Information  Bottou,  Neural  Series. Mit  cessing http://books.google.com.au/books?id=MDup2gE3BwgC. Bousquet, Olivier and Elisseeff, Andr´e. Stability and generalization.  ISBN  2007.  9780262026253.  2:499–526, March 2002. http://dx.doi.org/10.1162/153244302760200704.  ISSN 1532-4435.  J. Mach. Learn. Res., doi: 10.1162/153244302760200704. URL  Large-Scale Press,  Pro- URL  Burges, Christopher J.C. A tutorial on support vector machines for pattern recognition. Data Mining  and Knowledge Discovery, 2:121–167, 1998.  8  432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485  Cao, Lijuan.  Support vector machines experts for time series forecasting. Neurocomput- ISSN 0925-2312. doi: 10.1016/S0925-2312(02)00577-5. URL  ing, 51(0):321 – 339, 2003. http://www.sciencedirect.com/science/article/pii/S0925231202005775.  Chen, Yangchi, Crawford, Melba M., and Ghosh, Joydeep. Integrating support vector machines in a hierarchical output decomposition framework. In In 2004 International Geosci. and Remote Sens. Symposium, pp. 949–953, 2004.  Cristianini, Nello and Shawe-Taylor, John. An Introduction to Support Vector Machines and Other Kernel-based Learning Methods. Cambridge University Press, 1 edition, March . ISBN 0521780195.  D´ıez,  J.,  del Coz, learn  to  approach November 2010. http://dx.doi.org/10.1016/j.patcog.2010.06.001.  classiﬁers.  doi:  J.,  J. hierarchical ISSN 0031-3203.  and Bahamonde, A.  A semi-dependent decomposition 43(11):3795–3804, URL  Pattern Recogn., 10.1016/j.patcog.2010.06.001.  Dimitriadou, Evgenia, Hornik, Kurt, Leisch, Friedrich, Meyer, David,  , and Weingessel, An- dreas. e1071: Misc Functions of the Department of Statistics (e1071), TU Wien, 2010. URL http://CRAN.R-project.org/package=e1071.  G¨unther, Frauke and Fritsch, Stefan. neuralnet: Training of neural networks. The R Journal, (1):  30–38, June .  Hinton, Geoffrey E. and Osindero, Simon. A fast learning algorithm for deep belief nets. Neural  Computation, 18:2006, 2006.  Ivanciuc,  Ovidiu.  Svm  software  collection,  2005.  URL  http://support-vector-machines.org/SVM_soft.html.  Joachims, Thorsten. Training linear svms in linear time. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’06, pp. 217–226, New York, NY, USA, 2006. ACM. ISBN 1-59593-339-5. doi: 10.1145/1150402.1150429. URL http://doi.acm.org/10.1145/1150402.1150429.  Liaw, sion http://cran.r-project.org/web/packages/randomForest/randomForest.pdf.  Matthew. R  and Wiener,  randomforest.  regres- URL  Classiﬁcation  Andy by  2(3):18–22,  News,  2002.  and  M.A. Wiering, M. Schutten, A. Millea A. Meijster and Schomaker, L.R.B. Deep support vector machines for regression problems. Workshop on Advances in Regularization, Opti- mization, Kernel Methods, and Support Vector Machines: theory and application, 2013. URL www.esat.kuleuven.be/sista/ROKS2013/files/abstracts/MAWiering.pdf. Madzarov, G. and Gjorgjevikj, D. Multi-class classiﬁcation using support vector machines in de- cision tree architecture. In EUROCON 2009, EUROCON ’09. IEEE, pp. 288–295, 2009. doi: 10.1109/EURCON.2009.5167645.  Madzarov, Gjorgji, Gjorgjevikj, Dejan, and Chorbev, Ivan. A multi-class svm classiﬁer utilizing  binary decision tree. Informatica (Slovenia), (2):225–233.  Mohri, Mehryar, Rostamizadeh, Afshin, and Talwalkar, Ameet. Foundations of Machine Learning.  The MIT Press, 2012. ISBN 026201825X, 9780262018258.  R Development Core Team.  R: A Language and Environment  puting. http://www.R-project.org/. ISBN 3-900051-07-0.  R Foundation for Statistical Computing, Vienna, Austria, 2012.  for Statistical Com- URL  Yu, Hwanjo, Yang, Jiong, Han, Jiawei, and Li, Xiaolei. Making SVMs Scalable to Large Data Sets using Hierarchical Cluster Indexing. Data Mining and Knowledge Discovery, 11 (3):295–321, November 2005. ISSN 1384-5810. doi: 10.1007/s10618-005-0005-7. URL http://dx.doi.org/10.1007/s10618-005-0005-7.  9  486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539  A Generalisation bounds on the FGA  Theorem 1. We refer the reader to the ﬁrst bound of theorem 10.8, page 254 in Mohri et al. (2012). Assume we have selected a ﬁxed feature permutation for the FGA using a heuristic. Consider any node of the FGA with dimension d ≤ dmax, where dmax is the maximum dimen- sionality of each node over the FGA. Assume we have selected a ﬁxed feature permutation using a heuristic. Let K : X × X → R be a PDS kernel, let Φ : X → H be a feature mapping associated to K and let H = {x → w · Φ(x) : ||w||H ≤ Λ}. Assume that there exists r > 0 such that K(x, x) ≤ r2 and |f (x)| ≤ Λr for all x ∈ X. Fix ǫ > 0. Then, for any δ > 0, for all h ∈ H we have  RF GA − RSV M ≤ ˆRF GA − ˆRSV M + |V |ǫ + |V |  2rΛ  2  √m 1 +s log 1   δ  with probability at least 1 − |V |δ by the union bound, where there are |V | nodes in the FGA, and Λ = dmax is the maximum dimensionality of each node. This implies that the FGA has a lower generalization error than the corresponding SVM with high probability, for sufﬁciently large m, since the FGA training error is guaranteed to be less than (or if no node improvements are found, equal to) the SVM training error by construction. Proof. With probability 1 − δ the SVM satisﬁes Mohri et al. (2012)  so  and  |RSV M − ˆRSV M|ǫ ≤  RSV M − ˆRSV M − ǫ ≤  ˆRSV M − RSV M − ǫ ≤  δ  2rΛ  2rΛ  2  √m 1 +s log 1  2  √m 1 +s log 1  √m 2  1 +s log 1   2rΛ  δ  δ  Any FGA with linear kernels may be expanded into a simple linear function y = w.x + b. Since the FGA acts on the same space with the same loss function, it satisﬁes the same bound, namely  and  RF GA − ˆRF GA − ǫ ≤  ˆRF GA − RF GA − ǫ ≤  We can add equations 11 and 12 to give  δ  δ  2rΛ  2rΛ  2  √m 1 +s log 1  2  √m 1 +s log 1  2  √m 1 +s log 1   2rΛ  δ  10  RF GA − ˆRF GA − RSV M + ˆRSV M ≤ 2ǫ + 2  with probability at least 1 − 2δ, for a single node retraining, by the union bound. Next, note that an identical equation can be written for a pair of FGAs which differ by a single node retraining. We  (9)  (10)  (11)  (12)  (13)  (14)  540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593  can therefore sum over all node trainings (of which there are at most |V |, where every node has at most dmax features (as |H| = dmax for an SVM with dmax features per node), to obtain 2  √m 1 +s log 1   RF GA − RSV M ≤ ˆRF GA − ˆRSV M + |V |ǫ + |V |  with probability at least 1−|V|δ, by the union bound. Thus if we improve the training error, the test error will also improve with probability at least 1 − |V|δ for sufﬁciently large m. Since the FGA only modiﬁes nodes which improve the global training error, the test error must improve with high probability within the above bound.(cid:3).  (15)  2rΛ  δ  SVM SVM FGA FGA−SVM  0  .  1  8  .  0  6 0  .  4 . 0  2 . 0  l l  a a u u  t t c c a a  0.2  0.4  0.6  0.8  1.0  predicted predicted  Figure 3: Comparison showing predicted vs actual values on the test set for SVM and FGA-SVM.  B Stability analysis using incremental learning  Theorem 2. The FGA has stability given by  vL  βFGA = βSVM + Xv∈V  βv  Yiv =v  wiv  (16)  where βSVM is the stability of a simple SVM, βv is the stability of an individual node v, and wiv are the weights along paths from nodes v ∈ V to the root vL of the FGA tree. Proof. We can derive a relationship between the FGA and SVM stabilities as follows. Consider four cases a) SV M - no points removed b) SV M\1 - one point removed c) F GA - no points removed d) F GA\1 - one point removed The FGA starts off initialised to the simple SVM:  Next, train the ﬁrst node in the FGA (call this node N1)  F GA = SV M  F GA\1 = SV M\1  F GAN 1 = SV M + N1  F GAN1\ 1 = SV M\1 + N1\1  11  (17) (18)  (19) (20)  594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647  The stability of the updated FGA is then  βF GAN1  = e − e′ = F GAN1 − F GAN1\1 = SV M − SV M\1 + N1 − N1\1  L  = βSVM + βN1 .  wi  Yi=1  Similarly for all other node retrainings, so  βFGA = βSVM +Xv∈V  βv  wiv  L  Yiv =1  (21) (22) (23)  (24)  (25)  If we train only the ﬁrst layer we are just adding beta terms for each node. We know that roughly,  βSVM = Xv1∈V1  βv1  (26)  summed over ﬁrst layer terms.  Thus typically the ﬁrst layer variation will effectively give an FGA with double the SVM’s beta. This result agrees well with observations, with FGA β values between two and ﬁve times that of the simple SVM, with the ratio increasing with the number of layers. (cid:3)  B.1 Stability results  Figure 4 shows the changes in model weights as a result of removing one training point in a set of 50. We see the weights change by less for the SVM, but the FGA error is still relatively unchanged. This is a small price to pay for increased accuracy, shown in ﬁgure 3. In this example wl=2 = 1, m = 5, L = 2 giving a beta ratio of 2, which is close to the observed ratio in ﬁgure 5.  Histogram of |W−W_\1| for SVM  Histogram of |W−W_\1| for FGA  y c n e u q e r F  2 1  0 1  8  6  4  2  0  y c n e u q e r F  0 1  8  6  4  2  0  0.0000  0.0005  0.0010  0.0015  0.0020  0.0035  0.0040  0.0045  0.0050  0.0055  0.0060  0.0065  |W−W_\1|  |W−W_\1|  Figure 4: Stability of weights for the SVM and FGA.  Figure 5 shows the error distribution as a result of removing one training point in a set of 50. It is interesting to note that the range of the error differences is very similar, although the distribution is different.  12  648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701  Histogram of |e−e_\1| for SVM  Histogram of |e−e_\1| for FGA  y c n e u q e r F  0 3  5 2  0 2  5 1  0 1  5  0  y c n e u q e r F  7  6  5  4  3  2  1  0  0.000  0.002  0.004  0.006  0.008  0.000  0.002  |e−e_\1|  0.004  |e−e_\1|  0.006  0.008  Figure 5: Stability of test errors for the SVM and FGA.  L  6 4 3 2 2 1  m  2 4 8 16 32  simple SVM  ||e − e\1|| for 64 features  0.02948 0.02000 0.01714 0.01441 0.01740 0.009765  Table 2: Stability of the FGA as a function of the number of layers for 64 features.  B.2 Stability as a function of the number of layers  Tables 2 and 3 show the stability as a function of the number of layers and the number of features per node for 64 and 256 features respectively. The e1071 package Dimitriadou et al. (2010) was used for the SVM implementation. We see as the number of nodes and layers increases the stability of the FGA approaches roughly three times that of the SVM, in agreement with equation 6.  L  8 4 3 2 2 1  m  2 4 8 16 32  simple SVM  ||e − e\1|| for 256 features  0.009216 0.009644 0.008501 0.003544 0.007390 0.001873  Table 3: Stability of the FGA as a function of the number of layers for 256 features.  C Optimal Permutations  Theorem 3. Consider an FGA with four input features 1, 2, 3, 4. The second layer outputs are given by X12, X34. A permutation of the input features gives second layer outputs X13, X24. If the input vectors are equally weighted, in the sense that K = hY, X12i = hY, X34i = hY, X13i = hY, X24i  13  702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755  and hX12, X34i ≤ hX13, X24i then the empirical errors for the two permutations satisfy  R12,34 < R13,24  Proof. Here we are only considering the case where all vectors are normalised and have the same correlation with the target Y . Our assumption is that hX12, X34i ≥ hY13, Y24i. All weights are uniform so the error is  hY − X12 − X34, Y − X12 − X34i  = hY, Y i + hX12, X12i + hX34, X34i  − 2hY, X12i − 2hY, X34i + 2hX12, X34i  hY − X13 − X24, Y − X13 − X24i  = hY, Y i + hY13, X13i + hX24, X24i  − 2hY, X13i − 2hY, X24i + 2hX13, X24i  thus  R12,34 = 1 + 1 + 1 − 4K + 2hX12, X34i  ≤ R13,24 = 1 + 1 + 1 − 4K + 2hX13, X24i  (27) (28) (29) (30) (31) (32)  (33) (34)  showing that minimising later-layer correlations leads to lower prediction errors for this simpliﬁed example. (cid:3)  As mentioned in the main article, the situation can become more complex. The table 4 shows a search over multiple permutations, recording the best error found so far in the ﬁrst column. The second column shows the number of statistically correlated adjacent variables in the ﬁrst layer blocks of the FGA. The third column shows the sum of the p-values of these statistically correlated pairs. The fourth column is the average correlation of such pairs. We see there is no particular trend in columns three and four, but column two shows a clear trend: a better permutation is one with fewer correlations in the ﬁrst layer. The reader may ask, isn’t this in conﬂict with theorem 3? Theorem 3 assumes strong correlations in the higher order layers. The difference is that for this example, most of the second and higher order layers are not trained, so they are effectively bypassed. Only a few nodes in the whole FGA are trained, so it is a mostly shallow network. Thus it is preferable for those nodes to have fewer correlations in the early layers, and place the correlations in the few nodes that are retrained, which gives the FGA algorithm the best chance to model the difﬁcult subspaces well.  Permutation Improvement Best Err 903.6 815.6 815.6 769.1 769.0 769.0 769.0 567.1  1 2 3 4 5 6 7 8  Sig p-value count  22 19 17 15 14 14 12 11  Sum 0.0404 0.0362 0.0166 0.0151 0.0151 0.0151 0.0043 0.0333  Average 0.00183 0.00191 0.00098 0.00101 0.00108 0.00108 0.00036 0.00303  Table 4: Successive permutation improvements. The p-values are summed over adjacent input fea- tures in the ﬁrst layer of the FGA. as this FGA was quite ﬂat, with only a few retrainings in the second and higher layers, it is better to have less correlation in the early layers.  14  ","In this article we propose feature graph architectures (FGA), which are deeplearning systems employing a structured initialisation and training methodbased on a feature graph which facilitates improved generalisation performancecompared with a standard shallow architecture. The goal is to explorealternative perspectives on the problem of deep network training. We evaluateFGA performance for deep SVMs on some experimental datasets, and show howgeneralisation and stability results may be derived for these models. Wedescribe the effect of permutations on the model accuracy, and give a criterionfor the optimal permutation in terms of feature correlations. The experimentalresults show that the algorithm produces robust and significant test setimprovements over a standard shallow SVM training method for a range ofdatasets. These gains are achieved with a moderate increase in time complexity."
1310.0354,2014,Deep and Wide Multiscale Recursive Networks for Robust Image Labeling  ,"['Gary B. Huang', 'Viren Jain']",https://arxiv.org/pdf/1310.0354.pdf,"3 1 0 2   c e D 6         ]  V C . s c [      3 v 4 5 3 0  .  0 1 3 1 : v i X r a  Deep and Wide Multiscale Recursive Networks  for Robust Image Labeling  Gary B. Huang and Viren Jain Janelia Farm Research Campus Howard Hughes Medical Institute  19700 Helix Drive, Ashburn, VA, USA  {huangg, jainv}@janelia.hhmi.org  Abstract  Feedforward multilayer networks trained by supervised learning have recently demonstrated state of the art performance on image labeling problems such as boundary prediction and scene parsing. As even very low error rates can limit practical usage of such systems, methods that perform closer to human accuracy remain desirable. In this work, we propose a new type of network with the following properties that address what we hypothesize to be limiting aspects of existing methods: (1) a ‘wide’ structure with thousands of features, (2) a large ﬁeld of view, (3) recursive iterations that exploit statistical dependencies in label space, and (4) a parallelizable architecture that can be trained in a fraction of the time compared to benchmark multilayer convolutional networks. For the speciﬁc image labeling problem of boundary prediction, we also introduce a novel example weighting algorithm that improves segmentation accuracy. Experiments in the challenging domain of connectomic reconstruction of neural circuity from 3d electron microscopy data show that these “Deep And Wide Multiscale Recursive” (DAWMR) networks lead to new levels of image labeling performance. The highest performing architecture has twelve layers, interwoven supervised and unsupervised stages, and uses an input ﬁeld of view of 157,464 voxels (543) to make a prediction at each image location. We present an associated open source software package that enables the simple and ﬂexible creation of DAWMR networks.  1  Introduction  Image labeling tasks generate a pixel-wise ﬁeld of predictions across an image space. In boundary prediction, for example, the goal is to predict whether each pixel in an image belongs to the interior or boundary of an object [24]; in scene parsing, the goal is to associate with each pixel a multidimensional vector that denotes the category of object to which that pixel belongs [9]. These types of tasks are distinguished from traditional object recognition, for which pixel-wise assigments are usually irrelevant and the goal is to produce a single global prediction about object identity. Densely-labeled pixel-wise ground truth data sets have recently been generated for image labeling tasks that were traditionally solved by entirely hand-designed methods [24]. This has enabled the use of learning methods that require extensive supervised parameter learning. As a result, a common class of methods, supervised multilayer neural networks, have recently been found to excel at image labeling and object recognition tasks. This approach has led to state-of-the-art and in some cases breakthrough performance on a diverse array of problems and data sets [9, 19, 12, 3, 15]. Despite these improvements, for most practical applications even higher accuracy is required to achieve reliable automated image analysis. For example, in the main application studied in this paper, reconstruction of neurons from nanometer-resolution electron microscopy images of brain tissue, even small pixel-wise error rates can catastrophically deteriorate the utility of automated analysis [14].  1  In this paper, we identify limitations in existing multilayer network architectures, propose a novel architecture that addresses these limitations, and then conduct detailed experiments in the domain of connectomic reconstruction of electron microscopy data. The primary contributions of our work are: 1. A ‘wide’ and multiscale core architecture whose labeling accuracy exceeds a standard benchmark of a feedforward multilayer convolutional network. By exploiting parallel computing on both CPU clusters and GPUs, the core architecture can be trained in a day, compared with two weeks for a GPU implementation of the convolutional network.  2. A recursive pipeline consisting of repeated iterations of the core architecture. Through this recursion, the network is able to increase the ﬁeld of view used to make a prediction at a given image location and exploit statistical structure in label space, resulting in substantial gains in accuracy.  3. A computationally efﬁcient scheme for weighting training set examples in the speciﬁc image labeling problem of boundary prediction. This approach, which we refer to as ‘local error density’ (LED) weighting, is used to focus supervised learning on difﬁcult and topologically relevant image locations, and leads to more useful boundary predictions results.  2 Networks for Image Labeling: Prior Work and Desiderata  Multilayer networks for visual processing combine ﬁltering, normalization, pooling, and subsampling operations to extract features from image data. Feature extraction is followed by additional processing layers that perform linear or nonlinear classiﬁcation to generate the desired prediction variables [17]. Farabet et al. recently adapted convolutional networks to natural image scene labeling by training 2d networks that process the image at multiple scales [9]. Boundary prediction in large- scale biological datasets has been investigated using 3d architectures that have ﬁve to six layers of processing [13, 33, 12, 32] and, in the work of Ciresan et al., 2d architectures with pooling operations and ensembles of multiple networks [3]. These studies have shown that multilayer networks often outperform alternative machine learning methods, such as MRFs and random forest classiﬁers. We hypothesize that image labeling accuracy could be further improved by a network architecture that simultaneously addresses all of the following issues: Narrow vs wide feature representations: The number of features in each layer of a network plays a major role in determining the overall capacity available to represent different aspects of the input space. Most multilayer models for image labeling have thus far been relatively ‘narrow’, i.e., containing a small number of features in each layer. For example, networks described in Jain et al. and Farabet et al. used respectively 12 and 16 features in the ﬁrst layer, while those in Ciresan et al. used 48. We would like to transition to much wider architectures that utilize thousands of features. Large ﬁeld of view: Local ambiguity in an image interpretation task can be caused by noise, clutter, or intrinsic uncertainty regarding the interpretation of some local structures. Global image information can be used to resolve local ambiguity, and thus effective integration of image data over large ﬁelds of view is critical to solving an image labeling task. In multilayer visual processing architectures, there are a variety of factors that determine the effective size of the ﬁeld of view used to compute a prediction for a speciﬁc pixel location: ﬁlter size, network depth, pooling structure, and multiscale processing pathways. Experiments in this work and others suggest that appropriate usage of all of these architectural components is likely to be necessary to achieve highly accurate image labeling. While the 2d architecture proposed for scene labeling in Farabet et al. is already multiscale, converting the architecture to utilize 3d ﬁlters lengthens training time into weeks or more. The 3d boundary prediction networks in Jain et al. and Turaga et al. have also been augmented with multiscale capabilities, but these modiﬁcations lengthen training times from weeks into months. Modeling and exploiting statistical structure in labels: In the multilayer networks introduced thus far, predictions about neighboring image locations are nearly independent and become potentially correlated only due to a dependence on overlapping parts of the input image. However, in image labeling tasks there is usually a substantial amount of statistical structure among labels at neighboring image locations. This observation suggests that image labeling is a structured prediction problem in which statistics among output variables should be explicitly modeled [31]. Markov random ﬁeld (MRF) image models are an example of a generative approach to structured prediction. These methods consist of an observation model p(X|Y), encoding the conditional distribution of the image X given  2  the labels Y, and a prior model p(Y), specifying the distribution over different label conﬁgurations. Given a noisy input image X, inference for p(Y|X) can thus involve both image-dependent aspects (to invert the observation model) as well as interactions among the random variables Y ∈ Y that reﬂect prior statistics on valid label conﬁgurations [22]. Multilayer network models for image analysis typically outperform MRFs, as the computational expense associated with probabilistic learning and inference substantially restricts the modeling capability of MRF models that are practical to work with [13, 15]. An alternative approach is pursued by Farabet et al., in which a multilayer network is augmented by a simple three-parameter CRF post-processing step designed to ‘clean up’ classiﬁer predictions. In this work, we propose and investigate a recursive approach in which outputs from one network become input for a subsequent network, thereby allowing for explicit and powerful modeling of statistics among output predictions. Reasonable training time: We regard it as critical that a network can be learned in a reasonable amount of wall-clock time (within a few days at most, but more ideally within hours). Many existing approaches could conceptually be scaled up to address the limitations that we discuss, but would then require weeks or more in order to train. Such long training times can prohibit certain usage scenarios (for example, interactively adding new labeled data based on rapid classiﬁer retraining [30]). More generally, experimenting in the space of different cost functions, architectures, labeling strategies, etc., is only feasible if a single experiment can be performed in a reasonable duration of time. To achieve a reasonable training time, in this paper we assume access to both graphics processing units (GPUs) and multi-core CPUs or cluster computing environments. Many recent and notable results in machine learning would not have been possible without parallel computing on GPUs or CPUs [19, 4, 28].  3 Deep and Wide Multiscale Recursive Networks  We formalize the image labeling problem as follows: given an input image I, we want to predict at each location l ∈ I a vector of labels Yl. For the main data set considered, I is a three dimensional volume of size on the order of 10003, and with each location is associated a vector of 3 labels (|Yl| = 3), indicating 3d neighbor connectivity (see Section 4.1.2 for details of the data and labels). For ease of notation we will treat the location l as a single dimension with regard to operations discussed later, such as pooling, but in practice such operations are applied in 3d. In this section, we describe our proposed method for image labeling, Deep and Wide Multiscale Recursive (DAWMR) networks. DAWMR networks process images by recursive iteration of a core network architecture. Overall, a DAWMR network may have dozens of individual processing layers between the raw input image and ﬁnal labeling output. A schematic overview of a typical DAWMR network is given in Figure 1.  3.1 Single Iteration Core Architecture  The core network architecture in each iteration consists of two sequential processing modules: feature extraction and classiﬁcation. These stages are conceptually distinct, learned using differing levels of supervision, and implemented using different parallel computing strategies. Feature Extraction: Given a location l, the feature extraction module produces hl, a representation of the input image centered at l. These representations are subsequently passed to the classiﬁer to learn the speciﬁc image labeling that is encoded in the training data. (Generally hl is normalized such that each feature has zero mean and unit standard deviation prior to being passed to the classiﬁer.) At a high level, each feature extraction module consists of multiple processing layers of feature encoding using vector quantization (VQ), with intermediate layers that apply operations such as pooling, subsampling, and whitening. An entire set of processing layers can be replicated within a single module to process the image at multiple different downsampled scales (where downsampling is achieved by simple averaging). We take advantage of the recent observation that unsupervised clustering and dictionary learning techniques can be used to efﬁciently learn thousands of features for image recognition tasks [7, 5]. Following Coates and Ng [5], the core vector quantization component in the feature extraction module consists of a dictionary learned using a greedy variant of orthogonal matching pursuit (OMP-1) and encoding using soft-thresholding with reverse polarity.  3  Figure 1: Illustration of a DAWMR network with three recursive iterations.  2  2  2  2  i=l− m  , . . . , fl+ m  Given a learned dictionary for performing vector quantization, we can produce an encoding fi centered at a location i. We consider two contrasting methods for forming a ﬁnal hidden representation hl from these encoding fi. The ﬁrst method uses the encoding itself as the representation, at various pixel locations centered at i. In what we call an m receptive ﬁeld (RF) architecture, the hidden representation hl is formed by concatenating m features, hl = {fl− m }. This is similar to the convolutional network architectures used in [13, 16, 33]; in those networks, classiﬁcation is based on input from all feature maps in the ﬁnal hidden layer from feature map values within a 53 pixel window centered at the location being classiﬁed. The second method is a foveated representation that incorporates pooling operations. Given some neighborhood size m, we ﬁrst perform max pooling over the neighborhood, (gl)j = maxl+ m (hi)j. The foveated representation is then the concatenation of the feature encoding centered at l and the pooled feature, hl = {fl, gl}. We also experimented with average pooling but found max pooling to give better results in general. We note that an m RF architecture and a foveated representation with a pooling neighborhood of size m have the same ﬁeld of view of the data. However, if the dimensionality of the encoding is |fi| = k, then the dimensionality of the hidden representation using an m RF architecture is |hl| = mk, whereas with a foveated representation |hl| = 2k. Therefore, these two methods lie at opposite ends of the spectrum of ‘narrow’ versus ‘wide’ network architectures. Given a ﬁxed hidden representation dimensionality |hl| = d, the m3 RF architecture will have a narrow VQ dictionary ( d m3 ) whereas the foveated representation will be able to support a wider VQ dictionary ( d 2 ). Classiﬁcation: Following the feature extraction module, we have a standard supervised learning problem consisting of features hl and labels Yl. For the classiﬁcation module, we use a multilayer perceptron (MLP) with a single hidden layer trained by mini-batch stochastic gradient descent [11]. In preliminary experiments, we found that an MLP outperformed a linear SVM. For image labeling problems that involved predicting multiple labels at each location l, we also found that using a single MLP with multiple output units outperformed an architecture with multiple single output MLPs. Recursive Application of Core Network: In recursive approaches to prediction, a classiﬁer is repeatedly applied to an input (and previous classiﬁer results) to build a structured interpretation of some data. Pioneering work established graph transformer networks for solving segmentation and recognition tasks arising in automated document processing [2]. More recently, recursive approaches have been revived for superpixel agglomeration [29, 16] and text parsing [8]. In image labeling tasks, each pixel in an input image generates a scalar or vector output that encodes predictions about the variables of interest. A straightforward way to directly model statistics similar to the labels is to use the output of an initial iteration of the architecture described in Section 3.1 and provide that “network iteration 1” (N1) output as input to another instance of such an architecture  4  DAWMRIteration 1DAWMRIteration 3Original ScaleDownsampledFurtherDownsampledInput Channels(3d image +4d afﬁnity graph).  .  .121000.  .  .. . .121000.  .  .. . .121000.  .  .. . .Unsupervised Feature Extraction (ﬁltering, encoding, pooling, subsampling).  .  ..  .  .12200.      .      .xyzSupervisedClassiﬁcation(MLP or SVM)Input ImageAfﬁnity GraphAfﬁnity GraphAfﬁnity GraphArchitecture of Single IterationDAWMRIteration 2(N2). The N1 output is accompanied by the raw image, and thus feature extraction and subsequent predictions from N2 are based upon structure in both the original image as well as the output representation from N1. Recursive construction of such classiﬁers is repeated for N3, ..., Nk, where k is as large as computation time permits or cross-validation performance justiﬁes. Each additional recursive iteration also increases the overall ﬁeld of view used to predict the output at a particular pixel location. Thus, recursive processing enables DAWMR networks to simultaneously model statistical regularities in label-space and use increasing amounts of image context, with the overall goal of reﬁning image labeling predictions from one iteration to the next.  4 Boundary Prediction Experiments  We performed detailed experiments in the domain of boundary prediction in electron microscopy images of neural tissue. This application has signiﬁcant implications for the feasability of ‘connec- tomics’, an emerging endeavour in neurobiology to measure large-scale maps of neural circuitry at the resolution of single-synapse connectivity [23]. Reconstruction is currently the bottleneck in large-scale mapping projects due to the slow rate of purely manual reconstruction techniques [14]. Fully-automated methods for reconstruction would therefore be ideal. Current pipelines typically begin with a boundary prediction step, followed by oversegmentation of the resulting boundary map, and ﬁnally application of an agglomeration algorithm to piece together object fragments [1, 16]. Improvements in boundary prediction are desirable, as certain types of errors (such as subtle under- segmentation) can sometimes be difﬁcult to correct during later steps of the reconstruction pipeline.  4.1 Experimental Setup  Here we describe the details of the the image data and training/test sets. Experiments were run using our parallel computing software package, available online1; for more details see Section C.1.  4.1.1 Image Acquisition  Neuropil from drosophila melanogaster was imaged using focused ion-beam scanning electron microscopy (FIB-SEM [18]) at a resolution of 8x8x8 nm. The tissue was prepared using high- pressure freeze substitution and stained with heavy metals for contrast during electron microscopy. As compared to traditional electron microscopy methods such as serial-section transmission electron microscopy (ssTEM), FIB-SEM provides the ability to image tissue at very high resolution in all three spatial dimensions. Isotropic resolution at the sub-10nm scale is particularly advantageous in drosophila due to the small neurite size that is typical throughout the neuropil.  4.1.2 Training, Test Sets  Two image volumes were obtained using the above acquisition process. The ﬁrst volume was used for training and the second for both validation and testing. Initially, human annotators densely labeled subvolumes from both images. These labels form a preliminary training set, referred to in the sequel as the small training set (5.2 million labels), and the validation set (16 million labels), respectively. Afterward, an interactive procedure was used wherein human annotators ‘proofread’ machine- generated segmentations by visually examining dense reconstructions within small subvolumes and correcting any mistakes. The proofreading interface enabled annotators to make pixel-level modiﬁcations. The proofread annotations were then added to the small training set and validation set to form the ‘full’ training set (120 million labels) and test set (46 million labels), respectively.2 The labels are binary and indicate connectivity of adjacent voxels, where positive labels indicate that two voxels belong to the same foreground object and negative labels indicate that two voxels belong to differing objects or both belong to the background. As the image volume is three dimensional, each location is associated with a vector of 3 labels in each direction. The set of such labels (or their inferred probabilistic values) over an image volume is referred to as the afﬁnity graph. By specifying  1http://sites.google.com/site/dawmrlib/ 2The validation set is a subset of the test set. Measuring segmentation accuracy requires large densely-labeled subvolumes, and due to the expense in obtaining such data, we believe that measuring ﬁnal test results on a larger set of data is more valuable for evaluation than splitting off a subset to only be used as validation.  5  image data  image data  z-slice  ground-truth afﬁnity graph  z-slice  ground-truth segmentation  z-slice  ground-truth segmentation  Figure 2: A 793 subcube of the test data and z-slice taken from the center of the cube. Shown are the original image data, ground-truth afﬁnity graph, and ground-truth segmentation.  connectivity through an afﬁnity graph it is possible to represent situations (such as directly adjacent, distinct objects) that would be impossible to represent with a more typical pixel-wise exterior/interior labeling [32]. Figure 2 shows a 2d slice of the test image data, afﬁnity graph, and segmentation.  4.1.3 Evaluation Measures  Given a densely-labeled ground truth segmentation, one can measure performance in two ways: classiﬁcation metrics on binary representations of the segmentation (such as a boundary map or afﬁnity graph), or segmentation-based measures that interpret the volume as a clustering of pixels. In this work, we report both types of measures. Boundary prediction performance is reported by treating afﬁnity graph edge labeling as a stan- dard binary classiﬁcation task, where we compute results for each edge direction separately and then average the results over the three edge directions. As the ground truth has a class im- balance skewed toward positive (connected) edges, we report balanced class accuracy (bal-acc: 0.5 · accuracy on positive edges + 0.5 · accuracy on negative edges). We also compute area under the receiver operating characteristic curve, when varying the decision threshold for classifying positive/negative edges (AUC-edge). One can also segment a ground truth afﬁnity graph into clusters of connected voxels, forming a set of foreground objects and background. By segmenting an inferred afﬁnity graph (whose labels may be real-valued) at a particular threshold, one can follow the same procedure to form an inferred clustering. We supplement the connected components segmentation by ‘growing out’ segmented objects until they touch each other, using a marker-based watershed algorithm adapted to afﬁnity graphs. Segmentation performance is then measured by computing the Rand Index [34]. We report an area under the curve measure that integrates performance over different binarization thresholds (AUC-RI), as well as a maximum score (max RI).  4.2 Model Selection Experiments on a Validation Set  Like other deep, multilayer architectures, DAWMR networks have a number of model/architecture parameters that can be varied. In this section, we perform model selection experiments with the validation set. Unless explicitly stated otherwise, our experiments use the following set-up: the feature extraction modules produce a feature representation of dimension hu l = 8000, individual ﬁlters use 3d 53 patches, and classiﬁcation is performed using an MLP with a single hidden layer with 200 hidden units and trained with a balanced sampling of positive and negative training examples.  4.2.1 Single-Iteration Classiﬁers and Comparison With Convolutional Networks  We begin by evaluating performance of single-iteration DAWMR classiﬁers and a supervised convo- lutional network. We consider ﬁve DAWMR architectures: 53 RF, single-scale vector quantization without pooling (SS), single-scale VQ with foveated representation (SS-FV), and multiscale VQ with foveated representation (MS-FV). We also test a version of the SS-FV architecture with 2d ﬁlters (other architectures use 3d ﬁlters). For both architectures using a foveated representation, we pool over a 53 neighborhood, and thus the 53 RF and SS-FV architectures have the same ﬁeld of view. Table 1 provides an overview of the architectures.  6  arch. VQ dict. size ﬁeld of view feature dims  53 RF  32 93 8000  SS 4000 53 8000  SS-FV-2d  2000 92 8000  SS-FV MS-FV 1000 2000 183 93 8000 8000  Table 1: Vector quantization dictionary size (i.e. number of feature maps) and ﬁeld of view for feature extraction modules in model selection experiments. The multiscale MS-FV architecture uses two dictionaries of size 1000, one for each scale. Table 5 provides details regarding number of free parameters in the network architectures.  Validation performance of single iteration DAWMR networks using the above feature extraction architectures is shown in Table 2. The performance of a supervised convolutional network (CN) is also provided, and represents a strong baseline for comparison; identical types of networks have been extensively used in recent studies involving boundary prediction in 3d electron microscopy data [12, 33, 10]. The CN used in our experiment has 5 hidden layers, 16 feature maps per layer, all-to-all connectivity between hidden layers, and a ﬁlter size of 53. The CN was trained on a GPU with an implementation based on the CNS framework [26].  training set (small: 5M)  bal-acc AUC-edge network 0.9555 CN 0.8976 53 RF 0.8565 SS 0.8844 SS-FV-2d 0.9223 SS-FV MS-FV 0.9623 MS-FV-DO 0.9497  0.9872 0.9628 0.9386 0.9583 0.9799 0.9935 0.9899  tr. time 2 weeks 1.5 days 1.5 days 1.5 days 1.5 days 1.5 days 1.5 days  validation set: 16M examples  bal-acc AUC-edge AUC-RI max RI 0.8549 0.8322 0.9194 0.8200 0.7946 0.8757 0.8496 0.8024 0.9049 0.8129 0.9086 0.8305 0.8372 0.9327  0.6692 0.6764 0.6569 0.6537 0.6799 0.6796 0.7119  0.8873 0.8933 0.8859 0.8888 0.8981 0.9011 0.9196  Table 2: Validation performance of single iteration (non-recursive) DAWMR classiﬁers for various feature extraction architectures, and comparison with a multilayer convolutional network (CN). SS: single-scale, MS: multiscale, FV: foveated, DO: drop-out. All architectures use 3d ﬁlters except SS-FV-2d.  The multiscale foveated architecture (MS-FV) achieves slightly better results than the convolutional network on most metrics, for both the training and test set. The DAWMR classiﬁer is also learned in an order of magnitude less time than the convolutional network. Adding drop-out regularization (MS-FV-DO) improves performance of the single iteration DAWMR classiﬁer even further [11].  4.2.2 Recursive Multiscale Foveated Dropout (MS-FV-DO) Architecture  A speciﬁc core architecture can be recursively applied over multiple iterations, as discussed at the end of Section 3.1. In this section we experiment with this approach using the multiscale foveated dropout (MS-FV-DO) architecture. For the second and third iteration classiﬁers, which accept as input both an afﬁnity graph as well as the original image, there is a model selection choice related to whether ﬁlters in the feature extraction stage receive input from only the image, only the afﬁnity graph, or both. We found that dividing the set of features into an equal number which look exclusively at each type of input channel worked better than having all ﬁlters receive input from all input channels.  model  MS-FV-DO MS-FV-DO MS-FV-DO  iter 1 2 3  training set (large) bal-acc AUC-edge 0.9292 0.9447 0.9473  0.9809 0.9858 0.9883  validation set: 16M examples  bal-acc AUC-edge AUC-RI max RI 0.8833 0.9565 0.9691 0.8975 0.9024 0.9691  0.9497 0.9551 0.9628  0.7171 0.7276 0.7356  Table 3: Performance of multiscale foveated architecture over multiple recursive iterations.  Table 3 shows the results from recursive application of the MS-FV-DO architecture. Note that each iteration learns its own unsupervised and supervised parameters, thereby tripling the model parameters used to generate the ﬁnal output, and that each iteration adds 183 to the total ﬁeld of view used to generate an output prediction by the third iteration classiﬁer (543). Recursive experiments were  7  image data  ground-truth afﬁnity graph  ground-truth segmentation  MS-FV-DO  iter 1  MS-FV-DO  iter 2  MS-FV-DO  iter 3  MS-FV-DO w  iter 3  Figure 3: Example image data, ground truth, and DAWMR network output for two different locations from the test set. The second row depicts a difﬁcult situation in which mitochondria from distinct cells are directly adjacent; a recursive DAWMR network trained with LED weighting (MS-FV-DO w iter 3) is able to interpret the data correctly.  limited to three iterations.3 We observe consistent improvements in classiﬁcation and segmentation metrics as we recursively iterate the core MS-FV-DO architecture.  4.2.3 Recursive MS-FV-DO Architecture with Local Error Density (LED) Weighting  Visual inspection of recursive output conﬁrmed that boundary prediction generally improves over multiple iterations, but also revealed that predictions at certain rare image locations did not improve. These locations were characterized by a speciﬁc property: a high local density of boundary prediction errors present even in the ﬁrst iteration output. Locally correlated boundary prediction errors are prone to causing mistakes in segmentation and are thus important to avoid. Yet because these locations are rare, they have a negligible impact on boundary prediction accuracy (the metric actually being optimized during training). Previous work has addressed this issue by proposing learning algorithms that directly optimize segmentation performance [32, 12]. These algorithms are computationally expensive, and can make convergence of online gradient descent sensitive to various parameter choices in the loss function and optimization procedure. Therefore we sought a simpler alternative.  model  MS-FV-DO w MS-FV-DO w MS-FV-DO w  iter 1 2 3  training set (large) bal-acc AUC-edge 0.9336 0.9453 0.9536  0.9818 0.9867 0.9904  validation set: 16M examples  bal-acc AUC-edge AUC-RI max RI 0.9579 0.8909 0.9834 0.8941 0.9018 0.9860  0.9502 0.9524 0.9606  0.7178 0.7330 0.7487  Table 4: Performance of multiscale foveated architecture over recursive iterations with LED weighting (w).  Prior to each recursive iteration we train a DAWMR classiﬁer for 20% of the normal number of updates and compute afﬁnity graph output on the training set. We then create a binary weighting mask with non-zero entries for each pixel location in which more than 50% of the afﬁnity edge classiﬁcations in a 53 neighborhood are incorrect. This simple criteria proves effective in selectively identifying those rare locations where the failure mode occurred. The weighting mask is used during training of the full classiﬁer by sampling weighted locations at a 10x higher rate than normal, and the mask is combined across iterations by ‘or’ing. Table 3 shows results from training a recursive MS-FV-DO architecture with this LED weighting. Weighting increased segmentation accuracy, particularly in the second and third recursive iterations. Boundary prediction classiﬁcation accuracy was unaffected or even slightly diminished compared to non-weighted results. This is consistent with the idea that weighting alters the cost function to put greater emphasis on speciﬁc locations that inﬂuence segmentation accuracy, at the expense of overall boundary prediction performance.  3Additional iterations would require a ﬁeld of view so large that signiﬁcant amounts of labeled data in the  training and validation set would no longer be usable due to insufﬁcient image support.  8  4.3 Test Set Evaluation and Comparison  Based on the experiments performed on the validation set, we selected a few architectures for evaluation on the full test set. Details of the architectures are reviewed in Table 5, summary results are shown in Table 6, full plots of boundary prediction and segmentation performance are shown in Figure 4, and example 2d slices of the predicted afﬁnity graphs are shown in Figure 3.  model CN MS-FV-DO MS-FV-DO w iter 3  |unsup|  0  250000 750000  |sup| 136000 1600803 4802409  fov 253 183 543  training time  2 weeks 1.5 days 5 days  Table 5: Model comparison of architectures chosen from validation set experiments to be evaluated on the test set. |unsup| = number of unsupervised parameters, |sup| = number of supervised parameters, and fov = ﬁeld of view used to generate boundary predictions for a single image location.  The test set results are consistent with the validation set experiments. Using a 5-million example training set (‘sm’), the MS-FV-DO architecture outperforms the CN with far less training time. Switching to a larger training set (‘lg’) improves MS-FV-DO boundary prediction performance.4 Recursive iterations and LED weighting further improves segmentation performance of the DAWMR architecture quite substantially.  model CN MS-FV-DO MS-FV-DO MS-FV-DO w iter 3  tr set sm sm lg lg  training  bal-acc AUC-edge 0.9555 0.9497 0.9292 0.9536  0.9872 0.9899 0.9809 0.9904  test set: 46M examples  bal-acc AUC-edge AUC-RI max RI 0.7048 0.8388 0.8894 0.8445 0.9012 0.9128 0.9522 0.9226  0.5927 0.6798 0.6939 0.7383  0.8943 0.9293 0.9627 0.9735  Table 6: Performance on the full test set for CN and DAWMR architectures.  The results also conﬁrm previous observations that small differences in boundary prediction ac- curacy may be associated with large differences in segmentation accuracy [12]. For example, the non-recursive MS-FV-DO architecture outperforms the convolutional network only slightly when measured by AUC-edge, but much more substantially under Rand Index metrics. Visual inspection re- vealed that the convolutional network afﬁnity graphs are more prone to generating undersegmentation errors due to false positive afﬁnity edges between distinct objects.  5 Discussion  Diverse strategies for exploiting image context: The DAWMR networks explored in this work use several different strategies for manipulating the size of the ﬁeld of view: multiscale processing, pooling, and recursive iteration. It is likely that each strategy offers different modeling capabilities and beneﬁts. For example, multiscale processing is an efﬁcient way to model image features that appear at fundamentally different scales, while recursive processing of the image and afﬁnity graph may be more effective for careful integration of high-frequency features (e.g., contour completion). In the supplementary, Table 8 shows that a non-recursive architecture that achieves very large ﬁeld of view in a single iteration performs worse compared to output of a third iteration recursive architecture with a smaller total ﬁeld of view. As we lack an overall theory for the design of such networks, ﬁnding the architecture that makes optimal use of image context requires empirical model selection. A spectrum of weak vs ﬁne tuning in feature learning schemes: We employ simple unsupervised learning algorithms to learn features in DAWMR networks. These features are likely to be only ‘weakly’ tuned for a speciﬁc prediction task, as compared to the ‘ﬁnely’ tuned features learned in a convolutional network trained by supervised backpropogation. The trade-off, which our empirical  4Technical limitations in our GPU implementation of convolutional networks prevented us from being able  to train the CN with the large (lg) training set.  9  Figure 4: Results on the test set. For Rand Index, x-axis is scaled by the number of clusters in segmentations generated at various binarization thresholds (the speciﬁc thresholds are chosen custom to each methods output, in order to yield one-thousand evenly spaced quantiles from the analog afﬁnity graph values).  results suggest are well worth it for the problem of boundary prediction, is in the size of the representation – DAWMR networks can quickly learn thousands of features, whereas for convolutional networks it is currently only practical to use a few dozen at most. Improvements in computing hardware, or fundamentally more parallel versions of stochastic gradient descent may enable larger convolutional network architectures in the future [27, 20]. Recursive iterations and end-to-end learning: In recursive DAWMR networks, each iteration is learned without regard to future iterations. This is in contrast to true ‘end-to-end’ learning, in which each step is optimized based on updates back-propagated from the ﬁnal output [21, 25]. While end-to- end learning may lead to superior discriminative performance, the cost is twofold: a requirement for using processing stages that are (at least approximately) differentiable, as well as the computational expense of performing a ‘forward’ and ‘backward’ pass through all steps for each parameter update. We avoid end-to-end learning primarily to minimize training time, but the freedom to use non- differentiable processing steps in conjunction with intermediate afﬁnity graphs presents interesting opportunities. For example, afﬁnity graphs from intermediate iterations could be converted into segmentations from which object-level geometric and morphological features are computed. These features, which may be difﬁcult to represent via differentiable operations such as ﬁltering (e.g., geodesic and histogram-based measures), could be used as additional input to further recursive iterations that reﬁne the afﬁnity graph. This strategy for exploiting object-level representations is an alternative to superpixel-based approaches, and may more easily enable correction of labeling errors that lead to undersegmentation, which is difﬁcult to address in superpixel approaches. Acknowledgements: We thank Zhiyuan Lu for sample preparation, Shan Xu and Harald Hess for FIB-SEM imaging, and Corey Fisher and Chris Ordish for data annotation.  References  [1] B. Andres, U. Koethe, T. Kroeger, M. Helmstaedter, K. L. Briggman, W. Denk, and F. A. Hamprecht. 3d segmentation of sbfsem images of neuropil by a graphical model over supervoxel boundaries. Medical image analysis, 16(4):796–805, 2012. 4  [2] L. Bottou, Y. Bengio, and Y. Le Cun. Global training of document processing systems using graph transformer networks. In Computer Vision and Pattern Recognition, 1997. Proceedings., 1997 IEEE Computer Society Conference on, pages 489–494. IEEE, 1997. 3.1  [3] D. Ciresan, A. Giusti, J. Schmidhuber, et al. Deep neural networks segment neuronal membranes in electron microscopy images. In Advances in Neural Information Processing Systems 25, pages 2852–2860, 2012. 1, 2  [4] A. Coates, A. Karpathy, and A. Ng. Emergence of object-selective features in unsupervised feature learning.  In Advances in Neural Information Processing Systems 25, pages 2690–2698, 2012. 2  10  00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91fprtprBoundary Prediction ROC  CN (sm)MS−FV−DO (sm)MS−FV−DO (lg)MS−FV−DO  w iter 3 (lg)00.10.20.30.40.50.60.70.80.910.40.50.60.70.80.91number of clusters (normalized)Rand IndexSegmentation Rand Index  CN (sm)MS−FV−DO (sm)MS−FV−DO (lg)MS−FV−DO  w iter 3 (lg)[5] A. Coates and A. Ng. The importance of encoding versus training with sparse coding and vector quan- tization. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 921–928, 2011. 3.1, A.6, A.7  [6] A. Coates and A. Ng. Selecting receptive ﬁelds in deep networks. In Advances in Neural Information  Processing Systems, pages 2528–2536, 2011. A.2  [7] A. Coates, A. Y. Ng, and H. Lee. An analysis of single-layer networks in unsupervised feature learning. In  International Conference on Artiﬁcial Intelligence and Statistics, pages 215–223, 2011. 3.1  [8] R. Collobert. Deep learning for efﬁcient discriminative parsing. In International Conference on Artiﬁcial  Intelligence and Statistics, pages 224–232, 2011. 3.1  [9] C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Scene parsing with multiscale feature learning, purity  trees, and optimal covers. arXiv preprint arXiv:1202.2160, 2012. 1, 2  [10] M. Helmstaedter, K. L. Briggman, S. C. Turaga, V. Jain, H. S. Seung, and W. Denk. Connectomic reconstruction of the inner plexiform layer in the mouse retina. Nature, 500(7461):168–174, 2013. 4.2.1 [11] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012. 3.1, 4.2.1  [12] V. Jain, B. Bollmann, M. Richardson, D. Berger, M. Helmstaedter, K. Briggman, W. Denk, J. Bowden, J. Mendenhall, W. Abraham, K. Harris, N. Kasthuri, K. Hayworth, R. Schalek, J. Tapia, J. Lichtman, and H. Seung. Boundary Learning by Optimization with Topological Constraints. In Computer Vision and Pattern Recognition, IEEE Computer Society Conference on, 2010. 1, 2, 4.2.1, 4.2.3, 4.3, B.1  [13] V. Jain, J. F. Murray, F. Roth, S. C. Turaga, V. Zhigulin, K. L. Briggman, M. N. Helmstaedter, W. Denk, and H. S. Seung. Supervised learning of image restoration with convolutional networks. Computer Vision, IEEE International Conference on, 0:1–8, 2007. 2, 3.1, B.1  [14] V. Jain, H. Seung, and S. Turaga. Machines that learn to segment images: a crucial technology for  connectomics. Current opinion in neurobiology, 2010. 1, 4  [15] V. Jain and S. Seung. Natural image denoising with convolutional networks. In Advances in Neural  Information Processing Systems, pages 769–776, 2008. 1, 2  [16] V. Jain, S. C. Turaga, K. Briggman, M. N. Helmstaedter, W. Denk, and H. S. Seung. Learning to agglomerate superpixel hierarchies. In Advances in Neural Information Processing Systems, pages 648–656, 2011. 3.1, 4  [17] K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun. What is the best multi-stage architecture for object recognition? In Computer Vision, 2009 IEEE 12th International Conference on, pages 2146–2153. IEEE, 2009. 2  [18] G. Knott, H. Marchman, D. Wall, and B. Lich. Serial section scanning electron microscopy of adult brain  tissue using focused ion beam milling. Journal of Neuroscience, 28(12):2959, 2008. 4.1.1  [19] A. Krizhevsky, I. Sutskever, and G. Hinton.  Imagenet classiﬁcation with deep convolutional neural  networks. In Advances in Neural Information Processing Systems 25, pages 1106–1114, 2012. 1, 2  [20] Q. V. Le, M. Ranzato, R. Monga, M. Devin, K. Chen, G. S. Corrado, J. Dean, and A. Y. Ng. Building  high-level features using large scale unsupervised learning. arXiv preprint arXiv:1112.6209, 2011. 5  [21] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.  Proceedings of the IEEE, 86(11):2278–2324, 1998. 5  [22] S. Li. Markov random ﬁeld models in computer vision. Springer, 1994. 2 [23] J. W. Lichtman and W. Denk. The big and the small: challenges of imaging the brain’s circuits. Science,  334(6056):618–623, 2011. 4  [24] D. R. Martin, C. C. Fowlkes, and J. Malik. Learning to detect natural image boundaries using local  brightness, color, and texture cues. IEEE Trans. Patt. Anal. Mach. Intell., pages 530–549, 2004. 1  [25] U. Muller, J. Ben, E. Cosatto, B. Flepp, and Y. L. Cun. Off-road obstacle avoidance through end-to-end  learning. In Advances in neural information processing systems, pages 739–746, 2005. 5  [26] J. Mutch, U. Knoblich, and T. Poggio. CNS: a GPU-based framework for simulating cortically-organized  networks. Technical report, Massachussetts Institute of Technology, 2010. 4.2.1  [27] F. Niu, B. Recht, C. R´e, and S. J. Wright. Hogwild!: A lock-free approach to parallelizing stochastic  gradient descent. arXiv preprint arXiv:1106.5730, 2011. 5  [28] N. Pinto, D. Doukhan, J. J. DiCarlo, and D. D. Cox. A high-throughput screening approach to discovering good forms of biologically inspired visual representation. PLoS computational biology, 5(11):e1000579, 2009. 2  [29] R. Socher, C. C. Lin, A. Ng, and C. Manning. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 129–136, 2011. 3.1  [30] C. Sommer, C. Straehle, U. Kothe, and F. A. Hamprecht. Ilastik: Interactive learning and segmentation toolkit. In Biomedical Imaging: From Nano to Macro, 2011 IEEE International Symposium on, pages 230–233. IEEE, 2011. 2  [31] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured and  interdependent output variables. In Journal of Machine Learning Research, pages 1453–1484, 2005. 2  11  [32] S. C. Turaga, K. L. Briggman, M. Helmstaedter, W. Denk, and H. S. Seung. Maximin afﬁnity learning of  image segmentation. In NIPS, 2009. 2, 4.1.2, 4.2.3  [33] S. C. Turaga, J. F. Murray, V. Jain, F. Roth, M. Helmstaedter, K. Briggman, W. Denk, and H. S. Seung. Convolutional networks can learn to generate afﬁnity graphs for image segmentation. Neural Computation, 22(2):511–538, 2010. 2, 3.1, 4.2.1, B.1  [34] R. Unnikrishnan, C. Pantofaru, and M. Hebert. Toward objective evaluation of image segmentation  algorithms. IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(6):929, 2007. 4.1.3  12  A Supplementary: Additional Model Selection  We report the results of additional model selection experiments for architectures and learning parame- ters that were less central to achieving the highest performance in the main presentation.  A.1 Effect of Training Set Size  As noted in Section 4.1.2, two training sets were produced, a small training set (5.2M examples) and a full training set (120M examples), in order to examine how training set size affects DAWMR classiﬁcation performance. We augment the full training set by transforming the original data to create synthetic training examples. Speciﬁcally, we apply rotations and reﬂections to the original image data and labels, using a total of seven additional transformations to augment the full training set by a factor of eight. Given the large size of the augmented full training set (120M ∗ 8), we also subsample examples within each densely labeled subvolume in order to reduce computational load. Training examples that are nearby spatially are likely to have similar statistics, and thus we found that we can achieve comparable performance while using only a subset (10%) of the full training data.5 The augmented, subsampled version of the full training data constitutes the ‘large’ (lg) training set referenced in further experiments. Performance while varying the training set is shown in Table 7.  model SS-FV-DO SS-FV-DO MS-FV-DO MS-FV-DO  tr set sm lg sm lg  training  bal-acc AUC-edge 0.9139 0.9092 0.9497 0.9292  0.9737 0.9721 0.9899 0.9809  validation set: 16M examples  bal-acc AUC-edge AUC-RI max RI 0.9411 0.8383 0.9311 0.8667 0.9327 0.8372 0.8833 0.9565  0.9175 0.9386 0.9196 0.9497  0.6989 0.6907 0.7119 0.7171  Table 7: Performance of single iteration DAWMR classiﬁers when varying the size of the training set.  Expanding the training set results in a signiﬁcant increase in boundary prediction classiﬁcation accuracy, but has a somewhat ambiguous impact on segmentation performance. These results suggest that, as training sets become large, improvements in segmentation accuracy may require additional model capacity or learning algorithms more explicitly focused on segmentation performance. We investigate both of these issues in subsequent experiments.  A.2 Deeper Feature Extraction Stage  In the DAWMR architectures discussed in the main text, the unsupervised stage had a layer of ﬁltering, followed by encoding and pooling. We experimented with adding a second set of ﬁltering, encoding, and pooling steps. This modiﬁcation adds the ability to learn higher-order image features from the data, and also dramatically increases the ﬁeld of view of a single iteration architecture. We used a pairwise-similarity scheme to group ﬁrst layer ﬁlters [6].  model MS-FV-DO MS-FV-DO iter 3 MS-FV-DO-DFE  fov 183 543 623  training set (large) bal-acc AUC-edge 0.9292 0.9473 0.9373  0.9809 0.9883 0.9851  40% of validation set  bal-acc AUC-edge AUC-RI max RI 0.9464 0.8831 0.9633 0.9021 0.8876 0.9627  0.9496 0.9628 0.9539  0.7186 0.7371 0.7263  Table 8: Performance of DAWMR architectures from the main text compared to an architecture with a deeper feature extraction stage (MS-FV-DO-DFE). Validation numbers are on a subset of the validation set and thus not comparable to numbers in the main text (a subset was used due to clipping effects caused by the large ﬁeld of view of the MS-FV-DO-DFE architecture).  5The training data consists of many densely labeled subvolumes; using only 10% of the subvolumes would lead to a much different and less informative training set as compared to the subsampling scheme we propose – using all labeled subvolumes and randomly sampling 10% of the examples within each.  13  We ﬁnd that this deeper single-iteration architecture, MS-FV-DO-DFE, improves performance over the standard architecture (MS-FV-DO). However, performance of the recursive architecture is superior, even while using less image context, suggesting that immediately jumping to a large ﬁeld of view based on deeper unsupervised feature extraction is not necessarily ideal. We also note that inference in the MS-FV-DO-DFE architecture is signiﬁcantly more computational expensive than the MS-FV-DO architecture, due to the much larger number of ﬁltering computations in the feature extraction stage.  A.3 Varying Feature Dimensionality  We experimented with varying the dimensionality of the feature representation produced by the unsupervised feature extraction stage of the DAWMR networks, by varying the size of the dictionary used for vector quantization. The results, shown in Table 9, conﬁrm our general hypothesis that wider networks produced by using a large dictionary yield increased performance.  dim. model 400 MS-FV-DO 800 MS-FV-DO 2000 MS-FV-DO 4000 MS-FV-DO MS-FV-DO 8000 MS-FV-DO 12000  training set (large) bal-acc AUC-edge 0.9103 0.9176 0.9247 0.9270 0.9292 0.9301  0.9727 0.9749 0.9775 0.9793 0.9809 0.9803  validation set: 16M examples  bal-acc AUC-edge AUC-RI max RI 0.9236 0.8710 0.8760 0.9444 0.9533 0.8810 0.9452 0.8828 0.9565 0.8833 0.8878 0.9375  0.6995 0.7078 0.7027 0.7136 0.7171 0.7144  0.9405 0.9466 0.9467 0.9468 0.9497 0.9495  Table 9: Performance of DAWMR architectures when varying the dimensionality of the feature representation from the unsupervised stage.  A.4 Varying the Number of MLP Hidden Units  We also experimented with varying the number of hidden units used in the supervised MLP classiﬁer, with results shown in Table 10. All classiﬁers were trained using the same ﬁxed number of updates. In general we found the results to not be especially sensitive to this parameter, and used 200 as a balance between sufﬁcient capacity and faster training and convergence.  model MS-FV-DO MS-FV-DO MS-FV-DO MS-FV-DO  # h.u.  50 100 200 400  training set (large) bal-acc AUC-edge 0.9225 0.9279 0.9292 0.9278  0.9770 0.9795 0.9809 0.9762  validation set: 16M examples  bal-acc AUC-edge AUC-RI max RI 0.8793 0.9444 0.9532 0.8849 0.9565 0.8833 0.8833 0.9272  0.9467 0.9491 0.9497 0.9438  0.7197 0.7063 0.7171 0.7027  Table 10: Performance of DAWMR architectures when varying the number of hidden units (h.u.) used in supervised MLP.  A.5 Varying the Number of MLP Hidden Layers  We experimented with adding additional layers of hidden units in the supervised MLP classiﬁer, with results shown in Table 11. The network was kept at a ﬁxed with of 200 hidden units at each hidden layer, and a drop-out rate of 0.5 was used at each hidden layer. All classiﬁers were trained using the same ﬁxed number of updates.  A.6 Whitening  Previous work with vector quantization and deep learning architectures has noted the importance of whitening the data prior to dictionary learning [5]. We experimented with adding contrast normalization and ZCA whitening to the DAWMR networks. As shown in Table 12, we generally found that both contrast normalization and whitening generally decreased performance slightly. These results seem to indicate the importance of keeping information about intensity values relative to the  14  model MS-FV-DO MS-FV-DO MS-FV-DO MS-FV-DO  # h.l.  1 2 3 4  training set (large) bal-acc AUC-edge 0.9330 0.9363 0.9396 0.9323  0.9820 0.9848 0.9851 0.9829  validation set: 16M examples  bal-acc AUC-edge AUC-RI max RI 0.9519 0.8890 0.9547 0.8910 0.8945 0.9586 0.9476 0.8888  0.9521 0.9560 0.9594 0.9563  0.7206 0.7308 0.7393 0.7311  Table 11: Performance of DAWMR architectures when varying the number of hidden layers (h.l.) used in supervised MLP.  global data rather than just a local patch, for this particular data set and in distinction to other data such as natural images. For DAWMR networks with multiple feature encoding steps, as presented in Section A.2, we have had success with combining a small number of features produced by a single VQ step and no whitening with a larger number of features produced by multiple VQ steps and whitening.  model MS-FV-DO + CN,WH 0.9191 MS-FV-DO 0.9292  0.9746 0.9809  training set (large) bal-acc AUC-edge  validation set: 16M examples  bal-acc AUC-edge AUC-RI max RI 0.9142 0.8722 0.8833 0.9565  0.7039 0.7171  0.9411 0.9497  Table 12: Performance of DAWMR architecture when adding contrast normalization (CN) and ZCA whitening (WH).  A.7 Orthogonal Matching Pursuit vs K-means  In initial experiments, we used K-means for dictionary learning and “triangle K-means” for feature encoding [5]. This is compared with the Orthogonal Matching Pursuit that we used in the main presentation in Table 13. In general, we found both to give comparable results, with OMP allowing for faster feature encoding and seeming to be more amenable to multiple layers of feature encoding (Section A.2).  model learning MS-FV-DO K-means MS-FV-DO  OMP  training set (large) bal-acc AUC-edge 0.9317 0.9292  0.9819 0.9809  validation set: 16M examples  bal-acc AUC-edge AUC-RI max RI 0.9321 0.8852 0.8833 0.9565  0.7195 0.7171  0.9499 0.9497  Table 13: Performance of DAWMR architectures when varying the unsupervised stage dictionary learning method and feature encoding.  B Supplementary: Network Details  Here we present details and values of parameters used in our models.  B.1 Convolutional Network  The convolutional network was trained in accordance with procedures outlined in previous work [13, 33]. We used sigmoid units and performed greedy layer-wise training of the architecture: 5e5 updates after adding each layer, 2e6 updates for the ﬁnal architecture. Networks trained with signiﬁcantly fewer iterations exhibited much worse training set performance. During training, we used a balanced sampling strategy that alternated between negative and positive edge locations and selected a 53 cube around each edge as a minibatch. Learning rates were set to 0.1, except for the last layer (set to 0.01). A square-square loss [33, 12] was optimized with a margin of 0.3.  15  Figure 5: Computation architecture used in experiments. A CPU cluster is used to pre-compute feature vectors prior to GPU-based training of a supervised classiﬁer.  B.2 Multilayer Perceptron  The multilayer perceptrons in DAWMR architectures were trained using minibatch sizes of 40 with a balanced sampling of positive and negative edges. Learning rates were set to 0.02. We used sigmoid output units and rectiﬁed linear units in the hidden layer. For networks trained with drop- out regularization, the drop-out rate was set to 0.5 for the hidden layer and 0 for the input layer. We performed 5e5 updates. Optimization was performing using a cross-entropy loss function. To regularize and prevent overﬁtting, we used an “inverse margin” of 0.1, meaning that target labels were set to 0.1/0.9 rather than 0/1, penalizing over-conﬁdent predictions.  C Supplementary: DAWMR Implementation and Training Time  In this section we describe the code implementation details and training time analysis for DAWMR networks.  C.1  Implementation  The design of DAWMR networks permits the use of parallel computing strategies that result in fast training time. A schematic illustration of our pipeline is given in Figure 5. Unsupervised feature learning is performed on a traditional multicore CPU. Next, features are extracted for potentially millions of locations distributed across a large 3d image volume with billions to trillions of voxels. In our experiments, this computation is spread across a CPU cluster comprising thousands of cores. Each worker loads image data for the locations it has been assigned, extracts features, and writes the ﬁnal feature vector to a ﬁle or distributed database. Lastly, supervised learning is performed on a single GPU-equipped machine by repeatedly loading a random selection of feature vectors and performing online minibatch gradient updates with a GPU implementation of a multilayer perceptron. Gradient-based supervised training of deep convolutional networks requires performing forward and backward pass computations through many layers of processing, and the intricate nature of these computations limits the extent of parallelism that can typically be achieved. By learning the feature representation via efﬁcient unsupervised algorithms, DAWMR networks are able to ‘pre-compute’ feature vectors for each example in parallel across a large CPU cluster (in our experiments, this phase of computation can be accomplished in tens of minutes for even one-hundred million examples). An open source Matlab/C software package that implements DAWMR networks is available online: http://sites.google.com/site/dawmrlib/.  C.2 Training Time  Training DAWMR networks with parallel computation hardware (a CPU cluster and GPUs) results in training times on the order of a single day for a single iteration classiﬁer, and multiple days for multiple recursive iterations. This compares favorably with purely supervised multilayer convolutional  16  Unsupervised Feature LearningFeature ExtractionSupervised LearningTrainingPipelineImplementationArchitecture.  .  ..  .  .. ..Storage( HDD or ﬂash memory).  .  .Multicore CPU SystemThousands of CPU coresGPUnetworks (typically on the order of weeks for GPU implementations with 3d ﬁlters, even without multiscale processing). An analysis of our pipeline (Figure 5) reveals that the vast majority of time is spent training the multilayer perceptron (MLP). Moreover, during the GPU-based MLP training, most of the time is spent on I/O to retrieve feature vectors from the ﬁlesystem for each randomly constructed minibatch. In our experiments, the ﬁlesystem was a large-scale EMC Isilon installation accessed via 10-gigabit networking. Substantial improvements in training time could thus be achieved by additional engineering that simply reduced the overhead associated with accessing feature vectors. In-memory databases, ﬂash storage, and more efﬁcient distributed ﬁlesystems are likely to enable such improvements.  17  ","Feedforward multilayer networks trained by supervised learning have recentlydemonstrated state of the art performance on image labeling problems such asboundary prediction and scene parsing. As even very low error rates can limitpractical usage of such systems, methods that perform closer to human accuracyremain desirable. In this work, we propose a new type of network with thefollowing properties that address what we hypothesize to be limiting aspects ofexisting methods: (1) a `wide' structure with thousands of features, (2) alarge field of view, (3) recursive iterations that exploit statisticaldependencies in label space, and (4) a parallelizable architecture that can betrained in a fraction of the time compared to benchmark multilayerconvolutional networks. For the specific image labeling problem of boundaryprediction, we also introduce a novel example weighting algorithm that improvessegmentation accuracy. Experiments in the challenging domain of connectomicreconstruction of neural circuity from 3d electron microscopy data show thatthese ""Deep And Wide Multiscale Recursive"" (DAWMR) networks lead to new levelsof image labeling performance. The highest performing architecture has twelvelayers, interwoven supervised and unsupervised stages, and uses an input fieldof view of 157,464 voxels ($54^3$) to make a prediction at each image location.We present an associated open source software package that enables the simpleand flexible creation of DAWMR networks."
