Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  A Declarative Approach to Data-Driven Fact Checking  Julien Leblay  Artiﬁcial Intelligence Research Center, AIST, Japan  ﬁrstname.lastname@aist.go.jp  Abstract  Fact checking is an essential part of any investigative work. For linguistic, psychological and social reasons, it is an in- herently human task. Yet, modern media make it increasingly difﬁcult for experts to keep up with the pace at which infor- mation
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  The Simultaneous Maze Solving Problem  Stefan Funke and Andr´e Nusser  Universit¨at Stuttgart  Institut f¨ur Formale Methoden der Informatik  70569 Stuttgart, Germany  {funke,nusser}@fmi.uni-stuttgart.de  Sabine Storandt  Julius-Maximilians-Universit¨at W¨urzburg  Institut f¨ur Informatik  97072 W¨urzburg, Germany  storandt@informatik.uni-wuerzburg.de  Abstract  A grid maze is a binary matrix where ﬁelds contai
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  An Exact Algorithm for the Maximum Weight Clique Problem in Large Graphs  Hua Jiang  Huazhong University  of Science and Technology, China  email: jh hgt@163.com  Chu-Min Li∗ Jules Verne, France  MIS, Universit´e de Picardie  email: chu-min.li@u-picardie.fr  Felip Many`a  Artiﬁcial Intelligence Research Institute (IIIA), CSIC, Spain  email: felip@iiia.csic.es  Abstract  We describe an exact branch-and-bound alg
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Systematic Exploration of Larger Local Search  Neighborhoods for the Minimum Vertex Cover Problem  Maximilian Katzmann, Christian Komusiewicz  Institut f¨ur Informatik  Friedrich-Schiller-Universit¨at Jena, Jena, Germany  {maximilian.katzmann, christian.komusiewicz}@uni-jena.de  Abstract  We investigate the potential of exhaustively exploring larger neighborhoods in local search algorithms for MINIMUM VER- TEX 
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  New Lower Bound for the Minimum Sum Coloring Problem  Cl´ement Lecat, Corinne Lucet, Chu-Min Li  Laboratoire de Mod´elisation, Information et Syst`eme EA 4290,  Universit´e de Picardie Jules Verne, Amiens, France  {clement.lecat, corinne.lucet, chu-min.li}@u-picardie.fr  Abstract  The Minimum Sum Coloring Problem (MSCP) is an NP- Hard problem derived from the graph coloring problem (GCP) and has practical appli
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Efﬁcient Evaluation of Answer Set Programs with External Sources Based on External Source Inlining∗  Christoph Redl  Institut f¨ur Informationssysteme, Technische Universit¨at Wien  Favoritenstraße 9-11, A-1040 Vienna, Austria  redl@kr.tuwien.ac.at  Abstract  HEX-programs are an extension of answer set programming (ASP) towards external sources. To this end, external atoms provide a bidirectional interface betw
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  On Equivalence and Inconsistency of  Answer Set Programs with External Sources∗  Christoph Redl  Institut f¨ur Informationssysteme, Technische Universit¨at Wien  Favoritenstraße 9-11, A-1040 Vienna, Austria  redl@kr.tuwien.ac.at  Abstract  HEX-programs extend of answer-set programs (ASP) with ex- ternal sources. In previous work, notions of equivalence of ASP programs under extensions have been developed. Most 
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  ICU Mortality Prediction: A Classiﬁcation  Algorithm for Imbalanced Datasets  Sakyajit Bhattacharya, Vaibhav Rajan, Harsh Shrivastava  Xerox Research Centre India, Bangalore, India  Abstract  Determining mortality risk is important for critical decisions in Intensive Care Units (ICU). The need for machine learn- ing models that provide accurate patient-speciﬁc prediction of mortality is well recognized. We pres
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Learning Residual Alternating Automata  Sebastian Berndt, Maciej Li´skiewicz,∗ Matthias Lutter, R¨udiger Reischuk  Institute for Theoretical Computer Science, University of L¨ubeck  Ratzeburger Allee 160, 23562 L¨ubeck, Germany {berndt,liskiewi,lutter,reischuk}@tcs.uni-luebeck.de  Abstract  Residuality plays an essential role for learning ﬁnite au- tomata. While residual deterministic and non-deterministic auto
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Classiﬁcation with Minimax Distance Measures  Morteza Haghir Chehreghani Xerox Research Centre Europe - XRCE  6 chemin de Maupertuis 38240 Meylan, France  morteza.chehreghani@xrce.xerox.com  Abstract  Minimax distance measures provide an effective way to cap- ture the unknown underlying patterns and classes of the data in a non-parametric way. We develop a general-purpose framework to employ Minimax distances w
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Modeling Skewed Class Distributions  by Reshaping the Concept Space  Kyle D. Feuz  School of Computing Weber State University  Abstract  We introduce an approach to learning from imbalanced class distributions that does not change the underlying data distri- bution. The ICC algorithm decomposes majority classes into smaller sub-classes that create a more balanced class distribu- tion. In this paper, we explain 
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Convex Co-Embedding for Matrix  Completion with Predictive Side Information  Yuhong Guo  School of Computer Science  Carleton University, Ottawa, Canada  yuhong.guo@carleton.ca  Abstract  Matrix completion as a common problem in many applica- tion domains has received increasing attention in the machine learning community. Previous matrix completion methods have mostly focused on exploiting the matrix low-rank 
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Alternating Back-Propagation  for Generator Network  Tian Han,† Yang Lu,† Song-Chun Zhu, Ying Nian Wu  Department of Statistics University of California  Los Angeles, USA  Abstract  This paper proposes an alternating back-propagation algorithm for learning the generator network model. The model is a non- linear generalization of factor analysis. In this model, the map- ping from the continuous latent factors to
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Cost-Sensitive Feature Selection via F-Measure Optimization Reduction  Meng Liu,† Chang Xu,‡ Yong Luo,§‡ Chao Xu,† Yonggang Wen,§ Dacheng Tao‡  †Key Laboratory of Machine Perception (MOE), Cooperative Medianet Innovation Center, School of Electronics Engineering and Computer Science, PKU, Beijing 100871, China  ‡Centre for Artiﬁcial Intelligence, UTS, Sydney, NSW 2007, Australia §School of Computer Science and 
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Lifted Inference for Convex Quadratic Programs  Martin Mladenov  TU Dortmund University, Germany martin.mladenov@cs.tu-dortmund.de  Leonard Kleinhans  TU Dortmund University, Germany leonard.kleinhans@cs.tu-dortmund.de  Kristian Kersting  TU Dortmund University, Germany kristian.kersting@cs.tu-dortmund.de  Abstract  Symmetry is the essential element of lifted inference that has recently demonstrated the possibi
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Matching Node Embeddings for Graph Similarity  Giannis Nikolentzos  ´Ecole Polytechnique and AUEB  nikolentzos@aueb.gr  Polykarpos Meladianos ´Ecole Polytechnique and AUEB  pmeladianos@aueb.gr  Michalis Vazirgiannis  ´Ecole Polytechnique and AUEB mvazirg@lix.polytechnique.fr  Abstract  Graph kernels have emerged as a powerful tool for graph comparison. Most existing graph kernels focus on local prop- erties of 
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Top-k Hierarchical Classiﬁcation  Sechan Oh  Moloco  165 University Avenue  Palo Alto, California 94301  Abstract  This paper studies a top-k hierarchical classiﬁcation problem. In top-k classiﬁcation, one is allowed to make k predictions and no penalty is incurred if at least one of k predictions is correct. In hierarchical classiﬁcation, classes form a struc- tured hierarchy, and misclassiﬁcation costs depend
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Self-Correcting Models for  Model-Based Reinforcement Learning  Erik Talvitie  Department of Mathematics and Computer Science  Franklin & Marshall College Lancaster, PA 17604-3003 erik.talvitie@fandm.edu  Abstract  When an agent cannot represent a perfectly accurate model of its environment’s dynamics, model-based reinforcement learning (MBRL) can fail catastrophically. Planning involves composing the predictio
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Variable Kernel Density Estimation in High-Dimensional Feature Spaces  Christiaan M. van der Walt a,b Etienne Barnard b a Modelling and Digital Science, CSIR, Pretoria, South Africa  b Multilingual Speech Technologies Group, North-West University, Vanderbijlpark, South Africa  cvdwalt@csir.co.za  etienne.barnard@nwu.ac.za  Abstract  Estimating the joint probability density function of a dataset is a central tas
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Cleaning the Null Space:  A Privacy Mechanism for Predictors  Ke Xu,∗ Tongyi Cao,† Swair Shah,∗ Crystal Maung,∗ Haim Schweitzer∗ {ke.xu5,swair,hschweitzer}@utdallas.edu,  tcao@umass.edu,  Crystal.Maung@gmail.com  Abstract  In standard machine learning and regression setting feature values are used to predict some desired information. The pri- vacy challenge considered here is to prevent an adversary from using 
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  An Exact Penalty Method for Binary  Optimization Based on MPEC Formulation  Ganzhao Yuan, Bernard Ghanem  King Abdullah University of Science and Technology (KAUST), Saudi Arabia  yuanganzhao@gmail.com, bernard.ghanem@kaust.edu.sa  Abstract  Binary optimization is a central problem in mathematical op- timization and its applications are abundant. To solve this problem, we propose a new class of continuous optim
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Improving Surveillance Using Cooperative Target Observation  Rashi Aswani, Sai Krishna Munnangi and Praveen Paruchuri  Machine Learning Lab, Kohli Center on Intelligent Systems  International Institute of Information Technology - Hyderabad, India  {rashi.aswani, krishna.munnangi}@research.iiit.ac.in, praveen.p@iiit.ac.in  Abstract  The Cooperative Target Observation (CTO) problem has been of great interest in t
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Solving Seven Open Problems of Ofﬂine and Online Control in Borda Elections  Marc Neveling Institut für Informatik  Heinrich-Heine-Universität Düsseldorf  40225 Düsseldorf, Germany  Jörg Rothe  Institut für Informatik  Heinrich-Heine-Universität Düsseldorf  40225 Düsseldorf, Germany  Abstract  Standard (ofﬂine) control scenarios in elections (such as adding, deleting, or partitioning either voters or candidates
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Neural Bag-of-Ngrams  Bofang Li, Tao Liu, Zhe Zhao, Puwei Wang,∗ Xiaoyong Du  School of Information, Renmin University of China, Beijing, China  Key laboratory of Data Engineering and Knowledge Engineering, MOE, Beijing, China  {libofang, tliu, helloworld, wangpuwei, duyong}@ruc.edu.cn  Abstract  Bag-of-ngrams (BoN) models are commonly used for repre- senting text. One of the main drawbacks of traditional BoN i
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Improving Event Causality Recognition with Multiple Background Knowledge  Sources Using Multi-Column Convolutional Neural Networks  Canasai Kruengkrai, Kentaro Torisawa, Chikara Hashimoto,  Julien Kloetzer, Jong-Hoon Oh, Masahiro Tanaka  National Institute of Information and Communications Technology, Kyoto, 619-0289, Japan  {canasai, torisawa, ch, julien, rovellia, mtnk}@nict.go.jp  Abstract  We propose a meth
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Structural Correspondence Learning for Cross-Lingual Sentiment Classiﬁcation with One-to-Many Mappings  Nana Li  School of Computer Science  and Engineering,  Hebei University of Technology  Tianjin 300401, China lin@binghamton.edu  Shuangfei Zhai, Zhongfei Zhang  Computer Science Department  Binghamton University  Binghamton, NY 13902,USA  szhai2@binghamton.edu,  zhongfei@cs.binghamton.edu  Boying Liu  School 
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Validating Domains and Plans for Temporal Planning via Encoding into Inﬁnite-State Linear Temporal Logic  Alessandro Cimatti Fondazione Bruno Kessler  Trento, 38123, Italy  cimatti@fbk.eu  Andrea Micheli  Fondazione Bruno Kessler  Trento, 38123, Italy amicheli@fbk.eu  Marco Roveri  Fondazione Bruno Kessler  Trento, 38123, Italy  roveri@fbk.eu  Abstract  Temporal planning is an active research area of Artiﬁcial 
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  On the Disruptive Effectiveness of Automated Planning  for LTLf-Based Trace Alignment  Giuseppe De Giacomo  Sapienza - Universit´a di Roma, Italy  degiacomo@dis.uniroma1.it  Fabrizio Maria Maggi University of Tartu, Estonia  f.m.maggi@ut.ee  Andrea Marrella  Sapienza - Universit´a di Roma, Italy  marrella@dis.uniroma1.it  Fabio Patrizi  Sapienza - Universit´a di Roma, Italy  patrizi@dis.uniroma1.it  Abstract  O
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Human-Aware Plan Recognition  Hankz Hankui Zhuo  Department of Computer Science,  Sun Yat-Sen University, Guangzhou, China. 510006  zhuohank@mail.sysu.edu.cn  Abstract  Plan recognition aims to recognize target plans given ob- served actions with history plan libraries or domain models in hand. Despite of the success of previous plan recognition approaches, they all neglect the impact of human preferences on pl
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Latent Dirichlet Allocation for Unsupervised  Activity Analysis on an Autonomous Mobile Robot  Paul Duckworth, Muhannad Alomari,  James Charles, David C. Hogg, Anthony G. Cohn School of Computing, University of Leeds, Leeds, LS2 9JT, UK. {p.duckworth, scmara, j.charles, d.c.hogg, a.g.cohn}@leeds.ac.uk  Abstract  For autonomous robots to collaborate on joint tasks with hu- mans they require a shared understandin
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Phase Transitions for  Scale-Free SAT Formulas Tobias Friedrich, Anton Krohmer,  Ralf Rothenberger, Andrew M. Sutton  Algorithm Engineering Group  Hasso Plattner Institute  Potsdam, Germany  Abstract  Recently, a number of non-uniform random satisﬁability mod- els have been proposed that are closer to practical satisﬁability problems in some characteristics. In contrast to uniform ran- dom Boolean formulas, sca
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  RQUERY: Rewriting Natural Language Queries on Knowledge Graphs to  Alleviate the Vocabulary Mismatch Problem  Saeedeh Shekarpour  Kno.e.sis Center  Dayton, United States saeedeh@knoesis.org  Edgard Marx  AKSW Research Group  Leipzig, Germany  marx@informatik.uni-leipzig.de  Sören Auer  EIS Research Group  Bonn, Germany  auer@cs.uni-bonn.de  Amit Sheth Kno.e.sis Center  Dayton, United States  amit@knoesis.org  A
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Multi-Path Feedback Recurrent Neural Networks for Scene Parsing  Xiaojie Jin,1 Yunpeng Chen,2 Zequn Jie,2Jiashi Feng,2 Shuicheng Yan3,2  1NUS Graduate School for Integrative Science and Engineering, NUS  {xiaojie.jin, chenyunpeng, jiezequn, elefjia}@u.nus.edu, yanshuicheng@360.cn  2Department of ECE, NUS  3360 AI Institute  Abstract  In this paper, we consider the scene parsing problem and pro- pose a novel Mul
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Natural Language Acquisition and  Grounding for Embodied Robotic Systems  Muhannad Alomari, Paul Duckworth, David C. Hogg and Anthony G. Cohn  School of Computing, University of Leeds, Leeds, UK (scmara, p.duckworth, d.c.hogg, a.g.cohn)@leeds.ac.uk  Abstract  We present a cognitively plausible novel framework capable of learning the grounding in visual semantics and the gram- mar of natural language commands gi
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  (cid:3) (cid:3)  (cid:36)(cid:81)(cid:68)(cid:79)(cid:82)(cid:74)(cid:76)(cid:70)(cid:68)(cid:79)(cid:3)(cid:38)(cid:75)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:49)(cid:68)(cid:87)(cid:88)(cid:85)(cid:68)(cid:79)(cid:3)(cid:47)(cid:68)(cid:81)(cid:74)(cid:88)(cid:68)(cid:74)(cid:72)(cid:3)(cid:44)(cid:81)(cid:86)(cid:87)(cid:85)(cid:88)(cid:70)(cid:87)(c
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Inductive Reasoning about Ontologies Using Conceptual Spaces  Zied Bouraoui  Cardiff University, UK  BouraouiZ@Cardiff.ac.uk  Shoaib Jameel  Cardiff University, UK JameelS1@Cardiff.ac.uk  Steven Schockaert Cardiff University, UK  SchockaertS1@Cardiff.ac.uk  Abstract  Structured knowledge about concepts plays an increasingly important role in areas such as information retrieval. The available ontologies and know
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Flexible Model Induction through Heuristic Process Discovery  Pat Langley  Institute for the Study of Learning and Expertise  2164 Staunton Court, Palo Alto, CA 94306  Adam Arvay  Department of Computer Science  University of Auckland, Auckland 1142 NZ  Abstract  Inductive process modeling involves the construction of ex- planatory accounts for multivariate time series. As typically speciﬁed, background knowled
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  ConceptNet 5.5: An Open Multilingual Graph of General Knowledge  Robyn Speer  Luminoso Technologies, Inc. 675 Massachusetts Avenue  Cambridge, MA 02139  Joshua Chin Union College 807 Union St.  Schenectady, NY 12308  Catherine Havasi  Luminoso Technologies, Inc. 675 Massachusetts Avenue  Cambridge, MA 02139  Abstract  Machine learning about language can be improved by sup- plying it with speciﬁc knowledge and s
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Conﬁguration Planning with Temporal Constraints  Uwe K¨ockemann and Lars Karlsson  Center for Applied Autonomous Sensor Systems, ¨Orebro University, Sweden  Abstract  Conﬁguration planning is a form of task planning that takes into consideration both causal and information dependencies in goal achievement. This type of planning is interesting, for instance, in smart home environments which contain various senso
Proceedings of the Twenty-Ninth AAAI Conference on Innovative Applications (IAAI-17))  Large-Scale Occupational Skills  Normalization for Online Recruitment  Faizan Javed, Phuong Hoang, Thomas Mahoney, Matt McNair  {Faizan.Javed, Phuong.Hoang, Thomas.Mahoney, Matt.McNair}@CareerBuilder.com  CareerBuilder  5550-A Peachtree Parkway Peachtree Corners, GA 30092  Abstract  Job openings often go unfulﬁlled despite a surfeit of unem- ployed or underemployed workers. One of the main reasons for this is 
Proceedings of the Twenty-Ninth AAAI Conference on Innovative Applications (IAAI-17))  ParkUs: A Novel Vehicle Parking Detection System  Pietro Carnelli,* Joy Yeh, Mahesh Sooriyabandara, Aftab Khan  Toshiba Research Europe Ltd., 32 Queen Square, Bristol, UK  pietro.carnelli@toshiba-trel.com  Abstract  Finding on-street parking in congested urban areas is a chal- lenging chore that most drivers worldwide dislike. Previous vehicle trafﬁc studies have estimated that around thirty per- cent of vehic
Proceedings of the Twenty-Ninth AAAI Conference on Innovative Applications (IAAI-17))  Constraint-Based Veriﬁcation of a Mobile App Game Designed  for Nudging People to Attend Cancer Screening∗  Arnaud Gotlieb, Marine Louarn  SIMULA, Norway  Mari Nygard, Tomas Ruiz-Lopez  Cancer Registry of Norway  Sagar Sen  SIMULA, Norway  Cancer Registry of Norway  Abstract  In Norway, cervical cancer prevention involves the participa- tion of as many eligible women aged 25-69 years as possible. However, reac
Proceedings of the Seventh Symposium on Educational Advances in Artificial Intelligence (EAAI-17)  Creating Serious Robots That Improve Society   Susan P. Imberman1, Jean McManus2, Gina Otts3   1susan.imberman@csi.cuny.edu, College of Staten Island, 2800 Victory Blvd., Staten Island, NY 10314   2jean.m.mcmanus@verizon.com, Verizon Labs, One Verizon Way, Basking Ridge, NJ 07920   3gina.otts@verizon.com, Verizon Labs, 700 Hidden Ridge. Irving, TX      Abstract   The  Grace  Hopper  conference  has
Proceedings of the Seventh Symposium on Educational Advances in Artificial Intelligence (EAAI-17)  Open-Ended Robotics Exploration Projects for Budding Researchers  David R. Musicant, Abha Laddha, Tom Choi  Carleton College  1 North College Street Northﬁeld, MN 55057  Abstract  There are many beneﬁts to introducing students to the idea of doing projects where the outcome is unknown or unsure. Some have proposed that engaging students in research can help with retention of underrepresented groups
Proceedings of the Seventh Symposium on Educational Advances in Artificial Intelligence (EAAI-17)  Dude, Where’s My Robot?  A Localization Challenge for Undergraduate Robotics  Paul Ruvolo  Paul.Ruvolo@olin.edu  Olin College of Engineering  Abstract  I present a robotics localization challenge based on the in- expensive Neato XV robotic vacuum cleaner platform. The challenge teaches skills such as computational modeling, probabilistic inference, efﬁciency vs. accuracy tradeoffs, de- bugging, par
Proceedings of the Seventh Symposium on Educational Advances in Artificial Intelligence (EAAI-17)  A Monte Carlo Localization Assignment  Using a Neato Vacuum with ROS  Zuozhi Yang and Todd W. Neller  Gettysburg College  Gettysburg, Pennsylvania, 17325 USA  tneller@gettsyburg.edu  Abstract  Monte Carlo Localization (MCL) is a sampling-based algorithm for mobile robot localization. In this paper we describe an MCL assignment and its required hardware and software. The Neato vacuum robot and a Ras
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Why Teaching Ethics to AI Practitioners Is Important  Judy Goldsmith, Emanuelle Burton  Abstract  We argue that it is crucial to the future of AI that our students be trained in multiple complementary modes of ethical rea- soning, so that they may make ethical design and implemen- tation choices, ethical career decisions, and that their software will be programmed to take into account the complexities of acting
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Progress and Challenges in  Research on Cognitive Architectures  Pat Langley  Institute for the Study of Learning and Expertise  2164 Staunton Court, Palo Alto, CA 94306  Abstract  Research on cognitive architectures attempts to develop uni- ﬁed theories of the mind. This paradigm incorporates many ideas from other parts of AI, but it differs enough in its aims and methods that it merits separate treatment. In 
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Latent Tree Analysis  Nevin L. Zhang  Leonard K. M. Poon  Department of Computer Science & Engineering  The Hong Kong University of Science & Technology  Department of Mathematics & Information Technology  The Education University of Hong Kong  lzhang@cse.ust.hk  kmpoon@eduhk.hk  Abstract  Latent tree analysis seeks to model the correlations among a set of random variables using a tree of latent variables. It w
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Improving Performance of Analogue Readout Layers for  Photonic Reservoir Computers with Online Learning  Piotr Antonik,1† Marc Haelterman,2 Serge Massar1 1 Laboratoire d’Information Quantique, Universit´e libre de Bruxelles,  Avenue F. D. Roosevelt 50, CP 224, Brussels, Belgium  2 Service OPERA-Photonique, Universit´e libre de Bruxelles, Avenue F. D. Roosevelt 50, CP 194/5, Brussels, Belgium † Email: pantonik@u
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Chaotic Time Series Prediction Using a Photonic  Reservoir Computer with Output Feedback  Piotr Antonik,1† Michiel Hermans,1 Marc Haelterman,2 Serge Massar1  1 Laboratoire d’Information Quantique, Universit´e libre de Bruxelles,  Avenue F. D. Roosevelt 50, CP 224, Brussels, Belgium  2 Service OPERA-Photonique, Universit´e libre de Bruxelles, Avenue F. D. Roosevelt 50, CP 194/5, Brussels, Belgium † Email: panton
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Learning Options in Multiobjective Reinforcement Learning  Rodrigo Cesar Bonini, Felipe Leno da Silva, Anna Helena Reali Costa∗  Escola Polit´ecnica da Universidade de S˜ao Paulo, S˜ao Paulo, Brazil  Av. Prof. Luciano Gualberto, n.158, Engenharia El´etrica, Cidade Universit´aria, S˜ao Paulo, CEP: 05508-970.  {rodrigo cesarb,f.leno,anna.reali}@usp.br  Abstract  Reinforcement Learning (RL) is a successful techniq
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  An Advising Framework for Multiagent Reinforcement Learning Systems  Felipe Leno da Silva, Ruben Glatt, Anna Helena Reali Costa∗  Escola Polit´ecnica da Universidade de S˜ao Paulo, S˜ao Paulo, Brazil  {f.leno,ruben.glatt,anna.reali}@usp.br  Abstract  (MAS) composed of simultaneously learning agents.  Reinforcement Learning has long been employed to solve se- quential decision-making problems with minimal input 
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  The Complexity of Succinct Elections  Zack Fitzsimmons  College of Computing and Inf. Sciences  Rochester Inst. of Technology  Rochester, NY 14623  zmf6921@rit.edu  Edith Hemaspaandra Dept. of Computer Science Rochester Inst. of Technology  Rochester, NY 14623  eh@cs.rit.edu  Abstract  The computational study of elections generally assumes that the preferences of the electorate come in as a list of votes. Depen
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Policy Reuse in Deep Reinforcement Learning∗  Av. Prof. Luciano Gualberto, n.158, Engenharia El´etrica, S˜ao Paulo, CEP: 05508-970.  Ruben Glatt, Anna Helena Reali Costa  Escola Polit´ecnica da Universidade de S˜ao Paulo, Brazil  {ruben.glatt,anna.reali}@usp.br  http://www.cowhi.org, Phone: +55 11 97019-3738  Abstract  Objective  Driven by recent developments in Artiﬁcial Intelligence re- search, a promising ne
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Wikitop: Using Wikipedia Category Network to Generate Topic Trees  Saravana Kumar, Prasath Rengarajan, and Arockia Xavier Annie Department of Computer Science and Engineering, College of Engineering, Guindy  Anna University, Sardar Patel Road, Guindy, Chennai, Tamil Nadu 600025  {savkumar90,prasathindiarajan,aucs.annie}@gmail.com  Abstract  Automated topic identiﬁcation is an essential component in various info
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  A Systematic Practice of Judging the Success of a  Robotic Grasp Using Convolutional Neural Network  Hengshuang Liu, Pengcheng Ai, Junling Chen  College of Physical Science and Technology, Central China Normal University  NO.152 Luoyu Road, Wuhan, Hubei, P.R.China 430079 {hengshuangliu, pengcheng.ai, cjl1213}@mails.ccnu.edu.cn  Abstract  In this abstract, we present a novel method using the deep convolutional n
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Predicting User Roles from Computer Logs  Using Recurrent Neural Networks  Aaron Tuor,∗ Samuel Kaplan, Brian Hutchinson  Western Washington University  Bellingham, WA  Nicole Nichols and Sean Robinson  Paciﬁc Northwest National Laboratory  Seattle, WA  Introduction  Model  Network and other computer administrators typically have access to a rich set of logs tracking actions by users. How- ever, they often lack 
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Semantic Connection Based Topic Evolution  (∼)School of Computer Science and Information Engineering, Hefei University of Technology, 230009, Anhui, China  (+)School of Computing and Informatics, University of Louisiana at Lafayette, 70503, Louisiana, USA  Jiamiao Wang,(∼) Xindong Wu,(+) Lei Li(∼)  wjmzjx@163.com, xwu@louisiana.edu, lilei@hfut.edu.cn  Abstract  Contrary to previous studies on topic evolution th
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Enhancing the Privacy of Predictors  Ke Xu,∗ Swair Shah,∗ Tongyi Cao,† Crystal Maung,∗ Haim Schweitzer∗ {ke.xu5,swair,hschweitzer}@utdallas.edu, tcao@umass.edu, Crystal.Maung@gmail.com  Abstract  The privacy challenge considered here is to prevent an ad- versary from using available feature values to predict conﬁ- dential information. We propose an algorithm providing such privacy for predictors that have a lin
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Authorship Attribution with Topic Drift Model  Min Yang,1,2 Dingju Zhu,1∗ Yong Tang,1 Jingxuan Wang2  1South China Normal University, China 2University of Hong Kong, Hong Kong  myang@cs.hku.hk {zhudj,ytang}@scnu.edu.cn jingxuan@hku.hk  Abstract  Detecting authorship attribution is an active research direc- tion due to its legal and ﬁnancial importance. The goal is to identify the authorship of anonymous texts. 
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)       An Evolutionary Algorithm Based Framework    for Task Allocation in Multi-Robot Teams   Muhammad Usman Arif   Faculty of Computer Science,    Institute of Business Administration, Karachi, Pakistan.   musman@iba.edu.pk          Abstract   Multi-Robot  Task  Allocation  (MRTA)  has  no  formal  framework which could provide solutions covering different  domains within the MRTA taxonomy without changing the 
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Accelerating Multiagent Reinforcement Learning through Transfer Learning  Felipe Leno da Silva and Anna Helena Reali Costa∗ Escola Polit´ecnica da Universidade de S˜ao Paulo, S˜ao Paulo, Brazil  {f.leno,anna.reali}@usp.br  Abstract  Reinforcement Learning (RL) is a widely used solution for sequential decision-making problems and has been used in many complex domains. However, RL algorithms suffer from scalabili
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  Improving Deep Reinforcement Learning with Knowledge Transfer∗  Ruben Glatt, Anna Helena Reali Costa  Escola Polit´ecnica da Universidade de S˜ao Paulo, Brazil  {ruben.glatt,anna.reali}@usp.br  Abstract  Recent successes in applying Deep Learning techniques on Reinforcement Learning algorithms have led to a wave of breakthrough developments in agent theory and established the ﬁeld of Deep Reinforcement Learning
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)  V for Veriﬁcation: Intelligent Algorithm of  Checking Reliability of Smart Systems  Anna Lukina  Cyber-Physical Systems Group Technische Universit¨at Wien  Treitlstrasse 3/3, Vienna, Austria  anna.lukina@tuwien.ac.at  Introduction  Cyber-physical systems (CPS) are intended to receive infor- mation from the environment through sensors and perform appropriate actions using actuators of the controller.  In the las
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Dual Deep Neural Networks Cross-Modal Hashing  Zhen-Duo Chen, Wan-Jin Yu, Chuan-Xiang Li, Liqiang Nie, Xin-Shun Xu∗  School of Computer Science and Technology, Shandong University  {chenzd.sdu, rcwanjinyu, chuanxiang.lee, nieliqiang}@gmail.com, xuxinshun@sdu.edu.cn  School of Software, Shandong University  Abstract  Recently, deep hashing methods have attracted much atten- tion in multimedia retrieval task. Some of them can 
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  (cid:3)    A Network-Specific Markov Random    Field Approach to Community Detection    Dongxiao He,1 Xinxin You,2 Zhiyong Feng,2 Di Jin,1 Xue Yang1, Weixiong Zhang3,4  (cid:3)  (cid:20)(cid:54)(cid:70)(cid:75)(cid:82)(cid:82)(cid:79)(cid:3)(cid:82)(cid:73)(cid:3)(cid:38)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:72)(cid:85)(cid:3)(cid:54)(cid:70)(cid:76)(cid:72)(cid:81)(cid:70)(cid:72)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3
(cid:3)  The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  (cid:3) (cid:3)     Robust Detection of Link Communities in Large   Social Networks by Exploiting Link Semantics    Di Jin,1 Xiaobao Wang,1 Ruifang He,1 Dongxiao He,1 Jianwu Dang,1,2 Weixiong Zhang3,4   2  (cid:20)(cid:3)(cid:54)(cid:70)(cid:75)(cid:82)(cid:82)(cid:79)(cid:3)(cid:82)(cid:73)(cid:3)(cid:38)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:72)(cid:85)(cid:3)(cid:54)(cid:70)(cid:76)(cid:72)(cid:81)(cid:70)(
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Asymmetric Action Abstractions for Multi-Unit  Control in Adversarial Real-Time Games  Rubens O. Moraes, Levi H. S. Lelis  Departamento de Inform´atica, Universidade Federal de Vic¸osa, Brazil  {rubens.moraes, levi.lelis}@ufv.br  Abstract  Action abstractions restrict the number of legal actions avail- able during search in multi-unit real-time adversarial games, thus allowing algorithms to focus their search on a set of pro
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  A Bayesian Clearing Mechanism  for Combinatorial Auctions  Gianluca Brero University of Zurich  brero@iﬁ.uzh.ch  Abstract  We cast the problem of combinatorial auction design in a Bayesian framework in order to incorporate prior information into the auction process and minimize the number of rounds to convergence. We ﬁrst develop a generative model of agent valuations and market prices such that clearing prices become maximu
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Human-in-the-Loop SLAM  Samer B. Nashed, Joydeep Biswas College of Information and Computer Sciences  140 Governors Drive  Amherst MA, USA 01003  Abstract  Building large-scale, globally consistent maps is a challeng- ing problem, made more difﬁcult in environments with lim- ited access, sparse features, or when using data collected by novice users. For such scenarios, where state-of-the-art map- ping algorithms produce glob
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Complexity of Veriﬁcation in  Incomplete Argumentation Frameworks  Dorothea Baumeister, Daniel Neugebauer, J¨org Rothe, Hilmar Schadrack  Institut f¨ur Informatik  Heinrich-Heine-Universit¨at D¨usseldorf  40225 D¨usseldorf, Germany  Abstract  Abstract argumentation frameworks are a well-established formalism to model nonmonotonic reasoning processes. However, the standard model cannot express incomplete or conﬂicting knowled
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Weighted Abstract Dialectical Frameworks  Gerhard Brewka, Hannes Strass  Leipzig University, Computer Science Institute  Leipzig, Germany  Abstract  Abstract Dialectical Frameworks (ADFs) generalize Dung’s argumentation frameworks allowing various relationships among arguments to be expressed in a systematic way. We further generalize ADFs so as to accommodate arbitrary ac- ceptance degrees for the arguments. This makes ADFs
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  SELF: Structural Equational Likelihood  Framework for Causal Discovery  Ruichu Cai,1 Jie Qiao,1 Zhenjie Zhang,2 Zhifeng Hao1, 3  1 School of Computer Science, Guangdong University of Technology, China 2 Advanced Digital Sciences Center, Illinois at Singapore Pte. Ltd., Singapore  3 School of Mathmatics and Big Data, Foshan University, China  cairuichu@gdut.edu.cn, qiaojie.chn@qq.com, zhenjie@adsc.com.sg, zfhao@gdut.edu.cn  A
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Measuring Strong Inconsistency  Department of Computer Science  Institute for Web Science and Technologies  Department of Computer Science  Markus Ulbricht  Leipzig University  Germany  Matthias Thimm  University of Koblenz-Landau  Germany  Gerhard Brewka  Leipzig University  Germany  Abstract  We address the issue of quantitatively assessing the severity of inconsistencies in nonmonotonic frameworks. While mea- suring incon
The Thirty-Second AAAI Conference The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18) on Artificial Intelligence (AAAI-18)  Predicting Vehicular Travel Times by Modeling Heterogeneous Inﬂuences between Arterial Roads  Avinash Achar, Venkatesh Sarangan  Rohith Regikumar  TCS Research, IIT – Madras Research Park  Chennai 600113, INDIA  Abstract  Predicting travel times of vehicles in urban settings is a useful and tangible quantity of interest in the context of intelligent trans
The Thirty-Second AAAI Conference The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18) on Artificial Intelligence (AAAI-18)  DeepHeart: Semi-Supervised Sequence  Learning for Cardiovascular Risk Prediction  Brandon Ballinger, Johnson Hsieh,  Avesh Singh,Nimit Sohoni, Jack Wang  Cardiogram  San Francisco, CA  Geoffrey H. Tison, Gregory M. Marcus,  Jose M. Sanchez, Carol Maguire, Jeffrey E. Olgin, Mark J. Pletcher  Department of Medicine, University of California  San Francisco, 
The Thirty-Second AAAI Conference The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18) on Artificial Intelligence (AAAI-18)  CSWA: Aggregation-Free Spatial-Temporal Community Sensing  Jiang Bian,∗ Haoyi Xiong,∗ Yanjie Fu, Sajal K. Das Missouri University of Science and Technology, United States  *Equal Contribution  Abstract  In this paper, we present a novel community sensing paradigm CSWA –Community Sensing Without Sensor/Location Data Aggregation. CSWA is designed to obtain 
The Thirty-Second AAAI Conference The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18) on Artificial Intelligence (AAAI-18)  Tap and Shoot Segmentation  Ding-Jie Chen, Jui-Ting Chien, Hwann-Tzong Chen, Long-Wen Chang  {djchen.tw, ydnaandy123}@gmail.com , {htchen, lchang}@cs.nthu.edu.tw  National Tsing Hua University, Taiwan  Abstract  We present a new segmentation method that leverages latent photographic information available at the moment of taking pictures. Photography on a 
The Thirty-Second AAAI Conference The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18) on Artificial Intelligence (AAAI-18)  (cid:3)  (cid:3)  (cid:54)(cid:72)(cid:68)(cid:85)(cid:70)(cid:75)(cid:3)(cid:36)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:54)(cid:72)(cid:84)(cid:88)(cid:72)(cid:81)(cid:70)(cid:72)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:76)(cid:81)(cid:74)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:47)(cid:82)(cid:81)(cid:74)(cid:3)(cid:54)(
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Discriminant Projection Representation-Based  Classiﬁcation for Vision Recognition  Qingxiang Feng, Yicong Zhou*  Computer and Information Science, University of Macau  fengqx1988@gmail.com, yicongzhou@umac.mo, *Corresponding author.  Abstract  Representation-based classiﬁcation methods such as sparse representation-based classiﬁcation (SRC) and linear regres- sion classiﬁcation (LRC) have attracted a lot of attentions. In o
The Thirty-Second AAAI Conference The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18) on Artificial Intelligence (AAAI-18)  r-BTN: Cross-Domain Face Composite  and Synthesis from Limited Facial Patches  Yang Song, Zhifei Zhang, Hairong Qi  Department of Electrical Engineering and Computer Science  University of Tennessee, Knoxville, TN 37996, USA  {ysong18, zzhang61, hqi@utk.edu}  Abstract  Recent face composite and synthesis related works have shown promising results in gener
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  ARC: Adversarial Robust Cuts for  Semi-Supervised and Multi-Label Classiﬁcation  Sima Behpour, Wei Xing, Brian D. Ziebart  Department of Computer Science University Of Illinois at Chicago {sbehpo2,wxing3,bziebart}@uic.edu  Abstract  Many structured prediction tasks arising in computer vision and natural language processing tractably reduce to making minimum cost cuts in graphs with edge weights learned us- ing maximum margin
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Diverse Exploration for Fast and Safe Policy Improvement  Andrew Cohen  Binghamton University  acohen13@binghamton.edu  Lei Yu  Binghamton University  Yantai University  lyu@binghamton.edu  Robert Wright  Assured Information Security  wrightr@ainfosec.com  Abstract  We study an important yet under-addressed problem of quickly and safely improving policies in online reinforce- ment learning domains. As its solution, we propos
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Learning to Rank Based on Analogical Reasoning  Mohsen Ahmadi Fahandar, Eyke H¨ullermeier  Department of Computer Science  Paderborn University  Pohlweg 49-51, 33098 Paderborn, Germany  ahmadim@mail.upb.de, eyke@upb.de  Abstract  Object ranking or “learning to rank” is an important problem in the realm of preference learning. On the basis of training data in the form of a set of rankings of objects represented as feature vec
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Learning Combinatory Categorial Grammars for Plan Recognition  Christopher W. Geib  SIFT LLC.  319 1st Ave. South, Suite 400  Minneapolis, MN 55401  cgeib@sift.net  Department of Computer Science, Drexel University  Pavan Kantharaju  3141 Chestnut St  Philadelphia, PA 19104  pk398@drexel.edu  Abstract  This paper deﬁnes a learning algorithm for plan grammars used for plan recognition. The algorithm learns Combinatory Categor
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Non-Discriminatory Machine Learning  through Convex Fairness Criteria  Naman Goel, Mohammad Yaghini, Boi Faltings  Artiﬁcial Intelligence Laboratory,  ´Ecole Polytechnique F´ed´erale de Lausanne,  {naman.goel, mohammad.yaghini, boi.faltings}@epﬂ.ch  Lausanne, Switzerland, 1015  Abstract  Biased decision making by machine learning systems is in- creasingly recognized as an important issue. Recently, tech- niques have been pro
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Belief Reward Shaping in Reinforcement Learning  Oﬁr Marom  University of the Witwatersrand  Johannesburg, South Africa  Benjamin Rosman  University of the Witwatersrand Johannesburg, South Africa, and  Council for Scientiﬁc and Industrial Research  Pretoria, South Africa  Abstract  A key challenge in many reinforcement learning problems is delayed rewards, which can signiﬁcantly slow down learning. Although reward shaping h
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  SAGA: A Submodular Greedy Algorithm for Group Recommendation  Shameem A Puthiya Parambath  QCRI, HBKU, Doha, Qatar spparambath@hbku.edu.qa  Nishant Vijayakumar Apptopia Inc., Boston, USA  nishant.vijayakumar@gmail.com  Sanjay Chawla  QCRI, HBKU, Doha, Qatar  schawla@hbku.edu.qa  Abstract  In this paper, we propose a uniﬁed framework and an algo- rithm for the problem of group recommendation where a ﬁxed number of items or al
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Labeled Memory Networks for Online Model Adaptation  Shiv Shankar  shiv shankar@iitb.ac.in  IIT Bombay  Sunita Sarawagi  sunita@iitb.ac.in  IIT Bombay  Abstract  Augmenting a neural network with memory that can grow without growing the number of trained parameters is a recent powerful concept with many exciting applications. In this pa- per, we establish their potential in online adapting a batch trained neural network to do
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  No Modes Left Behind: Capturing the  Data Distribution Effectively Using GANs  Shashank Sharma, Vinay P. Namboodiri  Dept. of Computer Science and Engineering  Indian Institute of Technology, Kanpur  Abstract  Generative adversarial networks (GANs) while being very versatile in realistic image synthesis, still are sensitive to the input distribution. Given a set of data that has an imbalance in the distribution, the networks
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Dynamic Optimization of Neural Network Structures Using Probabilistic Modeling  Shinichi Shirakawa  Yokohama National University shirakawa-shinichi-bg@ynu.ac.jp  Yasushi Iwata  Yokohama National University  iwata-yasushi-ct@ynu.jp  Youhei Akimoto Shinshu University  y akimoto@shinshu-u.ac.jp  Abstract  Deep neural networks (DNNs) are powerful machine learn- ing models and have succeeded in various artiﬁcial intelli- gence ta
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Learning Graph-Structured Sum-Product Networks for Probabilistic Semantic Maps  Kaiyu Zheng, Andrzej Pronobis, Rajesh P. N. Rao∗  Abstract  We introduce Graph-Structured Sum-Product Networks (GraphSPNs), a probabilistic approach to structured predic- tion for problems where dependencies between latent vari- ables are expressed in terms of arbitrary, dynamic graphs. While many approaches to structured prediction place strict 
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Manipulative Elicitation — A New Attack on Elections with Incomplete Preferences  Palash Dey palash@tifr.res.in  Tata Institute of Fundamental Research, Mumbai  Abstract  Lu and Boutilier (2011) proposed a novel approach based on “minimax regret” to use classical score based voting rules in the setting where preferences can be any partial (instead of complete) orders over the set of alternatives. We show here that such an ap
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Control Argumentation Frameworks  Yannis Dimopoulos  Department of Computer Science  University of Cyprus yannis@cs.ucy.ac.cy  Jean-Guy Mailly  LIPADE  Paris Descartes University, France jean-guy.mailly@parisdescartes.fr  Pavlos Moraitis  LIPADE  Paris Descartes University, France  pavlos@mi.parisdescartes.fr  Abstract  Dynamics of argumentation is the family of techniques con- cerned with the evolution of an argumentation f
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Strategic Coalitions with Perfect Recall  Pavel Naumov  Department of Computer Science  Vassar College  Poughkeepsie, New York 12604  pnaumov@vassar.edu  Jia Tao  Department of Computer Science  Lafayette College  Easton, Pennsylvania 18042  taoj@lafayette.edu  Abstract  The paper proposes a bimodal logic that describes an inter- play between distributed knowledge modality and coalition know-how modality. Unlike other simila
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)       Effective Broad-Coverage Deep Parsing   James F. Allen, Omid Bahkshandeh, William de Beaumont, Lucian Galescu, Choh Man Teng   IHMC, 40 S. Alcaniz St, Pensacola, FL 32501    {jallen, omidb, wbeaumont, lgalescu, cmteng}@ihmc.us,         Abstract   Current semantic parsers either compute shallow representa- tions over a wide range of input, or deeper representations  in very limited domains. We describe a system that prov
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  280 Birds with One Stone: Inducing Multilingual Taxonomies  from Wikipedia Using Character-Level Classiﬁcation  Amit Gupta, R´emi Lebret, Hamza Harkous, Karl Aberer  ´Ecole Polytechnique F´ed´erale de Lausanne  Route Cantonale, 1015 Lausanne  Abstract  We propose a novel fully-automated approach towards induc- ing multilingual taxonomies from Wikipedia. Given an English taxonomy, our approach ﬁrst leverages the interlanguage
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Leveraging Lexical Substitutes  for Unsupervised Word Sense Induction  Domagoj Alagi´c, Jan ˇSnajder  TakeLab, Faculty of Electrical Engineering  and Computing, University of Zagreb {domagoj.alagic,jan.snajder}@fer.hr  Unska 3, 10000 Zagreb, Croatia  Abstract  Word sense induction is the most prominent unsupervised ap- proach to lexical disambiguation. It clusters word instances, typically represented by their bag-of-words c
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Generalizing and Improving Bilingual Word Embedding  Mappings with a Multi-Step Framework of Linear Transformations  Mikel Artetxe, Gorka Labaka, Eneko Agirre  IXA NLP Group  University of the Basque Country (UPV/EHU) {mikel.artetxe,gorka.labaka,e.agirre}@ehu.eus  Abstract  Using a dictionary to map independently trained word em- beddings to a shared space has shown to be an effective ap- proach to learn bilingual word embed
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Improving Language Modelling with Noise Contrastive Estimation  Farhana Ferdousi Liza, Marek Grzes  School of Computing, University of Kent  Canterbury, CT2 7NF, UK {ﬂ207, m.grzes}@kent.ac.uk  Abstract  Neural language models do not scale well when the vo- cabulary is large. Noise contrastive estimation (NCE) is a sampling-based method that allows for fast learning with large vocabularies. Although NCE has shown promising pe
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Few Shot Transfer Learning Between Word Relatedness  and Similarity Tasks Using a Gated Recurrent Siamese Network  James O’ Neill, Paul Buitelaar  Insight Centre for Data Analytics  National University of Ireland, Galway  {james.oneill, paul.buitelaar}@insight-centre.org  Abstract  Word similarity and word relatedness are fundamental to nat- ural language processing and more generally, understanding how humans relate concept
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Two Knowledge-Based Methods for  High-Performance Sense Distribution Learning  Tommaso Pasini, Roberto Navigli  {pasini,navigli}@di.uniroma1.it  Abstract  Knowing the correct distribution of senses within a corpus can potentially boost the performance of Word Sense Disam- biguation (WSD) systems by many points. We present two fully automatic and language-independent methods for com- puting the distribution of senses given a 
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Learning Better Name Translation  for Cross-Lingual Wikiﬁcation  Chen-Tse Tsai Bloomberg LP∗ New York, NY  ctsai54@bloomberg.net  Abstract  A notable challenge in cross-lingual wikiﬁcation is the prob- lem of retrieving English Wikipedia title candidates given a non-English mention, a step that requires translating names1 written in a foreign language into English. Creating training data for name translation requires signiﬁc
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Semi-Distantly Supervised Neural Model for Generating  Compact Answers to Open-Domain Why Questions  Ryo Ishida, Kentaro Torisawa, Jong-Hoon Oh, Ryu Iida, Canasai Kruengkrai, Julien Kloetzer  National Institute of Information and Communications Technology, Kyoto, 619-0289, Japan  {ishida.ryo, torisawa, rovellia, ryu.iida, canasai, julien}@nict.go.jp  Abstract  This paper proposes a neural network-based method for generating 
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  totSAT — Totally-Ordered Hierarchical Planning through SAT  Institute of Artiﬁcial Intelligence, Ulm University, D-89069 Ulm, Germany  Gregor Behnke, Daniel H¨oller, Susanne Biundo {gregor.behnke, daniel.hoeller, susanne.biundo}@uni-ulm.de  Abstract  In this paper, we propose a novel SAT-based planning ap- proach for hierarchical planning by introducing the SAT- based planner totSAT for the class of totally-ordered HTN plann
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Sublinear Search Spaces for Shortest  Path Planning in Grid and Road Networks  Johannes Blum  Julius-Maximilians-Universit¨at W¨urzburg  97072 W¨urzburg, Germany  blum@informatik.uni-wuerzburg.de  Stefan Funke Universit¨at Stuttgart  70569 Stuttgart, Germany funke@fmi.uni-stuttgart.de  Sabine Storandt  Julius-Maximilians-Universit¨at W¨urzburg  97072 W¨urzburg, Germany  storandt@informatik.uni-wuerzburg.de  Abstract  Shortes
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Finite Sample Analyses for TD(0) with Function Approximation  Gal Dalal∗  gald@campus.technion.ac.il  Bal´azs Sz¨or´enyi∗  szorenyi.balazs@gmail.com  Gugan Thoppe∗  gugan.thoppe@gmail.com  Shie Mannor  shie@ee.technion.ac.il  Abstract  TD(0) is one of the most commonly used algorithms in re- inforcement learning. Despite this, there is no existing ﬁnite sample analysis for TD(0) with function approximation, even for the line
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Armstrong’s Axioms and Navigation Strategies  Kaya Deuser, Pavel Naumov  Vassar College  124 Raymond Avenue  Poughkeepsie, NY 12604  {kdeuser, pnaumov}@vassar.edu  Abstract  The paper investigates navigability with imperfect informa- tion. It shows that the properties of navigability with perfect recall are exactly those captured by Armstrong’s axioms from database theory. If the assumption of perfect recall is omitted, then
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Learning Integrated Holism-Landmark Representations  for Long-Term Loop Closure Detection  Fei Han, Hua Wang, Hao Zhang  Department of Computer Science, Colorado School of Mines, Golden, CO 80401  fhan@mines.edu, huawangcs@gmail.com, hzhang@mines.edu  Abstract  Semantic landmarks X(cid:1488)Rd×n by HOG  W=g(X) (cid:1488)Rd×r  Loop closure detection is a critical component of large-scale simultaneous localization and mapping 
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  On Cryptographic Attacks Using Backdoors for SAT ∗  Alexander Semenov,1 Oleg Zaikin,1 Ilya Otpuschennikov,1  Stepan Kochemazov,1 Alexey Ignatiev2,1  1 Matrosov Institute for System Dynamics and Control Theory SB RAS, Irkutsk, Russia  2 LASIGE, Faculdade de Ciˆencias, Universidade de Lisboa, Portugal  Abstract  Propositional satisﬁability (SAT) is at the nucleus of state-of- the-art approaches to a variety of computationally 
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Adversarial Discriminative Heterogeneous Face Recognition  Lingxiao Song, Man Zhang, Xiang Wu, Ran He∗  National Laboratory of Pattern Recognition, CASIA  Center for Research on Intelligent Perception and Computing, CASIA  Center for Excellence in Brain Science and Intelligence Technology, CAS  Abstract  Overall architecture  The gap between sensing patterns of different face modalities remains a challenging problem in heter
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Audio Visual Attribute Discovery for  Fine-Grained Object Recognition  State Key Laboratory of Information Security (SKLOIS), Institute of Information Engineering, CAS, Beijing, China  zhanghua@iie.ac.cn, caoxiaochun@iie.ac.cn(cid:2), wangrui@iie.ac.cn  Hua Zhang, Xiaochun Cao,∗ Rui Wang  Abstract  Visual  Audio-Visual attributes  Audio  Current progresses on ﬁne-grained recognition are mainly fo- cus on learning the discrim
The Thirtieth AAAI Conference on Innovative Applications of Artificial Intelligence (IAAI-18)  Horizontal Scaling with a Framework for Providing  AI Solutions within a Game Company  John F. Kolen, Mohsen Sardari, Marwan Mattar, Nick Peterson, Meng Wu  EADP Intelligent Systems  Electronic Arts  209 Redwood Shores Parkway Redwood City, California 94069  {jkolen,msardari,mmattar,npeterson,mewu}@ea.com  Abstract  Games have been a major focus of AI since the ﬁeld formed seventy years ago. Recently, 
The Thirtieth AAAI Conference on Innovative Applications of Artificial Intelligence (IAAI-18)  Classiﬁcation of Malware by Using  Structural Entropy on Convolutional Neural Networks  Daniel Gibert  Blueliv, Leap in Value  Barcelona, Spain  Carles Mateu University of Lleida  Lleida, Spain  Jordi Planes  University of Lleida  Lleida, Spain  Ramon Vicens Blueliv, Leap in Value  Barcelona, Spain  Abstract  The number of malicious programs has grown both in num- ber and in sophistication. Analyzing t
The Thirtieth AAAI Conference on Innovative Applications of Artificial Intelligence (IAAI-18)  Discovering Program Topoi through Clustering  Carlo Ieva,1,2 Arnaud Gotlieb,1 Souhila Kaci,2 Nadjib Lazaar2  1SIMULA Research Laboratory, Oslo, Norway 2LIRMM, University of Montpellier, France  {carlo, arnaud}@simula.no, {souhila.kaci, nadjib.lazaar}@lirmm.fr  Abstract  Understanding source code of large open-source software projects is very challenging when there is only little docu- mentation. New de
The Thirtieth AAAI Conference on Innovative Applications of Artificial Intelligence (IAAI-18)  Multi-Task Deep Learning for Predicting Poverty from Satellite Images  Shailesh M. Pandey, Tushar Agarwal, Narayanan C Krishnan  Department of Computer Science and Engineering {shailesh.pandey, tushar.agarwal, ckn}@iitrpr.ac.in  Indian Institute of Technology Ropar, India  Abstract  Estimating economic and developmental parameters such as poverty levels of a region from satellite imagery is a chal- len
The Thirtieth AAAI Conference on Innovative Applications of Artificial Intelligence (IAAI-18)  Investigating the Role of Ensemble Learning in High-Value Wine Identiﬁcation  Luigi Portinale  Computer Science Institute, DiSIT Universit`a del Piemonte Orientale  Alessandria, Italy  Monica Locatelli  Department of Pharmaceutical Sciences,  Universit`a del Piemonte Orientale  Novara, Italy  Abstract  We tackle the problem of authenticating high value Italian wines through machine learning classiﬁcati
The Thirtieth AAAI Conference on Innovative Applications of Artificial Intelligence (IAAI-18)       Batting Order Setup in One Day International Cricket    Masoumeh Izadi, Simranjeet Narula   Television Content Analytics, 1 Changi North Street 1, Singapore 498789   masoumeh.izadi@tvconal.com, simran.narula@tvconal.com         Abstract   In the professional sport of cricket, batting order assignment  is of significant interest and importance to coaches, players,  and fans as an influencing parame
The Eighth AAAI Symposium on Educational Advances in Artificial Intelligence 2018 (EAAI-18)  An E-Learning Recommender that  Helps Learners Find the Right Materials  Blessing Mbipo, Stewart Massie, Susan Craw  School of Computing Science & Digital Media  Robert Gordon University  Aberdeen, UK  Abstract  Learning materials are increasingly available on the Web making them an excellent source of information for building e-Learning recommendation systems. However, learners of- ten have difﬁculty ﬁn
The Eighth AAAI Symposium on Educational Advances in Artificial Intelligence 2018 (EAAI-18)  (cid:3) (cid:3)  (cid:36)(cid:71)(cid:71)(cid:85)(cid:72)(cid:86)(cid:86)(cid:76)(cid:81)(cid:74)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:55)(cid:72)(cid:70)(cid:75)(cid:81)(cid:76)(cid:70)(cid:68)(cid:79)(cid:15)(cid:3)(cid:51)(cid:75)(cid:76)(cid:79)(cid:82)(cid:86)(cid:82)(cid:83)(cid:75)(cid:76)(cid:70)(cid:68)(cid:79)(cid:15)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:40)(cid:87)(cid:75)(cid:76)(ci
The Eighth AAAI Symposium on Educational Advances in Artificial Intelligence 2018 (EAAI-18)  Introducing AI to Undergraduate Students  via Computer Vision Projects  Kaiman Zeng, Yancheng Li, Yida Xu, Di Wu, Nansong Wu  Arkansas Tech University, Russellville, Arkansas 72801  {kzeng, yli, yxu1, dwu2, nwu}@atu.edu  Abstract  Computer vision, as a subﬁeld in the general artiﬁcial in- telligence (AI), is a technology can be visualized and eas- ily found in a large number of state-of-art applications.
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Imagination Machines:  A New Challenge for Artiﬁcial Intelligence  Sridhar Mahadevan  sridhar.i.mahadevan@gmail.com  College of Information and Computer Sciences  140 Governor’s Drive Amherst, MA 01003  Abstract  The aim of this paper is to propose a new overarching chal- lenge for AI: the design of imagination machines. Imagina- tion has been deﬁned as the capacity to mentally transcend time, place, and/or circumstance. Muc
The Twenty-Third AAAI/SIGAI Doctoral Consortium  Cross-Lingual Learning with Distributed Representations    Matúš Pikuliak   Faculty of Informatics and Information Technologies   Slovak University of Technology, Bratislava, Slovak Republic   matus.pikuliak@stuba.sk   Even  though  researchers  already  tried  to  perform  such  transfer with various deep NLP architectures, there is still a  lot of open questions to be answered. First, the correspond- ence  can  be  established  on  character-lev
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Deep Modeling of Social  Relations for Recommendation  Wenqi Fan, Qing Li, Min Cheng  Department of Computer Science, City Univeristy of Hong Kong  83 Tat Chee Avenue, Kowloon, Hong Kong  wenqifan1990@hotmail.com, itqli@cityu.edu.hk, mc.cheng@my.cityu.edu.hk  Abstract  Social-based recommender systems have been recently pro- posed by incorporating social relations of users to alleviate sparsity issue of user-to-item rating d
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Enhancing RNN Based OCR by Transductive    Transfer Learning from Text to Images    Yang  He, Jingling  Yuan,  Lin Li   Wuhan University  of Technology     yanghe@whut.edu.cn, yjl@whut.edu.cn, cathylilin@whut.edu.cn   Abstract   This  paper  presents  a  novel  approach  for  optical  character  recognition (OCR) on acceleration and to avoid underfitting  by  text.  Previously  proposed  OCR  models  typically  take  much ti
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Decision Making Over Combinatorially-Structured Domains  Andrea Martin, K. Brent Venable  Department of Computer Science, Tulane University, LA, USA  amartin@tulane.edu and kvenabl@tulane.edu  Introduction and Motivation  Preferences are a very important notion in decision making. As such they have been studied in multiple disciplines such as psychology, philosophy and business and especially in marketing. Recently, it has g
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Goal Recognition in Incomplete Domain Models  Ramon Fraga Pereira, Felipe Meneguzzi  Pontiﬁcal Catholic University of Rio Grande do Sul (PUCRS), Brazil  Postgraduate Programme in Computer Science, School of Informatics (FACIN)  Porto Alegre, RS, Av. Ipiranga 668, CEP: 90619-900  ramon.pereira@acad.pucrs.br felipe.meneguzzi@pucrs.br  +55 51 3553 8622  Abstract  Recent approaches to goal recognition have progressively re- laxe
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)       Playing SNES Games with NeuroEvolution of Augmenting Topologies    Son Pham, Keyi Zhang, Tung Phan, Jasper Ding, Christopher L. Dancy   Department of Computer Science, Bucknell University, Lewisburg, PA 17837  {son.pham, keyi.zhang, tung.phan, cd032, christopher.dancy}@bucknell.edu            Abstract   Teaching a computer to play video games has generally been  seen as a reasonable benchmark for developing new AI tech-
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Automated Question Answering System  for Community-Based Questions  Chanin Pithyaachariyakul, Anagha Kulkarni  cpithyaa@mail.sfsu.edu, ak@sfsu.edu  San Francisco State University  1600 Holloway Ave, San Francisco, California, 94132  Abstract  The emergence of community question answering sites, such as, Yahoo! Answer (Y!A), and Quora, indicate that for certain information needs, users prefer receiving focused answers to thei
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Adversary Is the Best Teacher: Towards Extremely Compact Neural Networks  Ameya Prabhu,* Harish Krishna,* Soham Saha  Center for Visual Information Technology  Kolhi Center for Intelligent Systems  {ameya.prabhu,harishkrishna.v,soham.saha} @ research.iiit.ac.in  IIIT-Hyderabad  asterisk indicates equal contribution  Abstract  With neural networks rapidly becoming deeper, there emerges a need for compact models. One popular a
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Solving Generalized Column  Subset Selection with Heuristic Search  Swair Shah, Baokun He, Ke Xu, Crystal Maung, Haim Schweitzer  {swair,baokun.he,ke.xu5,ktm016100,hschweitzer}@utdallas.edu  Abstract  We address the problem of approximating a matrix by the linear combination of a column sparse matrix and a low rank matrix. Two variants of a heuristic search algorithm are de- scribed. The ﬁrst produces an optimal solution but
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Different Cycle, Different Assignment:  Diversity in Assignment Problems with Multiple Cycles  Helge Spieker,∗ Arnaud Gotlieb  Simula Research Laboratory  P.O. Box 134  1325 Lysaker, Norway  {helge,arnaud}@simula.no  Morten Mossige University of Stavanger  Stavanger, Norway  ABB Robotics Bryne, Norway  morten.mossige@uis.no  Abstract  We present approaches to handle diverse assignments in multi-cycle assignment problems. The
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Label Space Driven Heterogeneous Transfer  Learning with Web Induced Alignment  Sanatan Sukhija  Department of Computer Science and Engineering Indian Institute of Technology Ropar, Punjab, India  sanatan@iitrpr.ac.in  Abstract  Heterogeneous Transfer Learning (HTL) algorithms lever- age knowledge from a heterogeneous source domain to per- form a task in a target domain. We present a novel HTL al- gorithm that works even whe
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Joint Learning of Evolving Links for Dynamic Network Embedding  Aakas Zhiyuli,1,2 Xun Liang,1 YanFang Chen,1 Peng Shu,2 Xiaoping Zhou1  1. Renmin University of China, No.59 Zhongguancun Road, Beijing, China 100872 2. Sogou, Inc., AD-Tech, No.1 Zhongguancun East Road, Beijing, China 100084  Email: {zhiyulee, xliang, xpzhou, cyf}@ruc.edu.cn; {ps-adwr,shupeng203672}@sogou-inc.com;  Introduction and Key idea  This paper studies 
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Interactive Machine Learning at Scale with CHISSL  Dustin Arendt,1 Emily Grace,2 Svitlana Volkova2  1Visual Analytics, 2Data Science and Analytics, Paciﬁc Northwest National Laboratory  902 Battelle Blvd, Richland, WA 99354  Abstract  We demonstrate CHISSL a scalable client-server system for real-time interactive machine learning. Our system is capa- ble of incorporating user feedback incrementally and imme- diately without 
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)  Learning an Image-Based Obstacle Detector with Automatic Acquisition of Training Data  Stefano Toniolo, J´erˆome Guzzi, Luca Maria Gambardella, Alessandro Giusti  Dalle Molle Institute for Artiﬁcial Intelligence, USI-SUPSI, Lugano (Switzerland)  Email: stefano.toniolo@idsia.ch  Abstract  We detect and localize obstacles in front of a mobile robot by means of a deep neural network that maps images acquired from a forward-look
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Beyond Speech: Generalizing D-Vectors for Biometric Veriﬁcation∗  Jacob Baldwin, Ryan Burnham, Andrew Meyer, Robert Dora, Robert Wright  Assured Information Security, Inc.  153 Brooks Rd. Rome, NY 13441  {baldwinj, burnhamr, meyera, dorar, wrightr}@ainfosec.com  Abstract  Deep learning based automatic feature extraction methods have radically transformed speaker identiﬁcation and facial recognition. Current approaches are typi
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Deep Latent Generative Models for Energy Disaggregation  Gissella Bejarano SUNY Binghamton  gbejara1@binghamton.edu  David DeFazio SUNY Binghamton  ddefazi1@binghamton.edu  Arti Ramesh SUNY Binghamton artir@binghamton.edu  Abstract  Thoroughly understanding how energy consumption is dis- aggregated into individual appliances can help reduce house- hold expenses, integrate renewable sources of energy, and lead to efﬁcient use o
The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19)  A Memetic Approach for Sequential Security Games on a Plane with Moving  Targets  Jan Karwowski,1 Jacek Ma´ndziuk,1 Adam ˙Zychowski,1 Filip Grajek,1 Bo An2  1 Warsaw University of Technology, Faculty of Mathematics and Information Science, Koszykowa 75, 00-662 Warsaw, Poland  2 Nanyang Technlogical University School of Computer Science and Engineering, Singapore 639798  {j.karwowski, j.mandziuk, a.zychowski}@mini.pw.edu.pl, b
   The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)        On Strength Adjustment for MCTS-Based Programs    I-Chen Wu,* Ti-Rong Wu,* An-Jen Liu,* Hung Guei, Tinghan Wei   Department of Computer Science, National Chiao Tung University, 1001 University Road, Hsinchu, Taiwan, ROC   {icwu,kds285,andyliu,ghung,ting}@aigames.nctu.edu.tw   Abstract   This paper proposes an approach to strength adjustment for  MCTS-based  game-playing  programs.  In  this  approach,  we  use  a  sof
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  MetaStyle: Three-Way Trade-off among  Speed, Flexibility, and Quality in Neural Style Transfer  Chi Zhang, Yixin Zhu, Song-Chun Zhu  International Center for AI and Robot Autonomy (CARA)  {chizhang,yzhu,sczhu}@cara.ai  Abstract  An unprecedented booming has been witnessed in the re- search area of artistic style transfer ever since Gatys et al. in- troduced the neural method. One of the remaining challenges is to balance a tra
The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19)  SAFE: A Neural Survival Analysis Model for Fraud Early Detection  Panpan Zheng,∗ Shuhan Yuan,∗ Xintao Wu  University of Arkansas, Fayetteville, AR, USA  {pzheng, sy005, xintaowu}@uark.edu  Abstract  Many online platforms have deployed anti-fraud systems to detect and prevent fraudulent activities. However, there is usually a gap between the time that a user commits a fraud- ulent action and the time that the user is suspended
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Model-Based Diagnosis of Hybrid Systems Using Satisﬁability Modulo Theory  Alexander Diedrich, Alexander Maier  Fraunhofer IOSB-INA  Fraunhofer Center for Machine Learning  Lemgo, Germany  alexander.diedrich@iosb-ina.fraunhofer.de alexander.maier@iosb-ina.fraunhofer.de  Oliver Niggemann Institute Industrial IT  Lemgo, Germany  oliver.niggemann@hs-owl.de  Abstract  Currently, detecting and isolating faults in hybrid systems is 
The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19)  A Nonconvex Projection Method for Robust PCA  Aritra Dutta,∗ Filip Hanzely,† Peter Richt´arik‡  Abstract  Robust principal component analysis (RPCA) is a well-studied problem whose goal is to decompose a matrix into the sum of low-rank and sparse components. In this paper, we propose a nonconvex feasibility reformulation of RPCA problem and apply an alternating projection method to solve it. To the best of our knowledge, this
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Regular Boardgames  Jakub Kowalski, Maksymilian Mika, Jakub Sutowicz, Marek Szykuła  Institute of Computer Science, University of Wrocław, Wrocław, Poland  jko@cs.uni.wroc.pl, mika.maksymilian@gmail.com, jakubsutowicz@gmail.com, msz@cs.uni.wroc.pl  Abstract  We propose a new General Game Playing (GGP) language called Regular Boardgames (RBG), which is based on the theory of regular languages. The objective of RBG is to join ke
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Generalized Distance Bribery  Dorothea Baumeister, Tobias Hogrebe, Lisa Rey  Institut f¨ur Informatik  Heinrich-Heine-Universit¨at D¨usseldorf  40225 D¨usseldorf, Germany  Abstract  The bribery problem in elections asks whether an external agent can make some distinguished candidate win or prevent her from winning, by bribing some of the voters. This prob- lem was studied with respect to the weighted swap distance between two 
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Deep Bayesian Trust : A Dominant and Fair Incentive Mechanism for Crowd  Naman Goel, Boi Faltings Artiﬁcial Intelligence Laboratory  ´Ecole Polytechnique F´ed´erale de Lausanne  Lausanne, Switzerland, 1015  {naman.goel, boi.faltings}@epﬂ.ch  Abstract  An important class of game-theoretic incentive mechanisms for eliciting effort from a crowd are the peer based mecha- nisms, in which workers are paid by matching their answers w
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  A Framework for Approval-Based Budgeting Methods  Nimrod Talmon Ben-Gurion University  Be’er Sheva, Israel talmonn@bgu.ac.il  Abstract  We deﬁne and study a general framework for approval-based budgeting methods and compare certain methods within this framework by their axiomatic and computational properties. Furthermore, we visualize their behavior on certain Euclidean distributions and analyze them experimentally.  Introduct
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Evolving Action Abstractions  for Real-Time Planning in Extensive-Form Games  Julian R. H. Mari˜no,1 Rubens O. Moraes,2 Claudio Toledo,1 Levi H. S. Lelis2  1Departamento de Sistemas de Computac¸˜ao, ICMC, Universidade de S˜ao Paulo, Brazil  2Departamento de Inform´atica, Universidade Federal de Vic¸osa, Brazil  julianmarino@usp.br, rubens.moraes@ufv.br, claudio@icmc.usp.br, levi.lelis@ufv.br  Abstract  A key challenge for plan
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Stepping Stones to Inductive Synthesis of Low-Level Looping Programs  Christopher D. Rosin  Parity Computing, Inc.  San Diego, California, USA christopher.rosin@gmail.com  Abstract  Inductive program synthesis, from input/output examples, can provide an opportunity to automatically create pro- grams from scratch without presupposing the algorithmic form of the solution. For induction of general programs with loops (as opposed 
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Satisﬁability in Strategy Logic Can Be Easier than Model Checking  Erman Acar,1 Massimo Benerecetti,2 Fabio Mogavero2  1Vrije Universiteit Amsterdam, The Netherlands  1erman.acar@vu.nl, 2{massimo.benerecetti, fabio.mogavero}@unina.it  2Universit`a di Napoli “Federico II”, Italy  Abstract  In the design of complex systems, model-checking and sat- isﬁability arise as two prominent decision problems. While model-checking requires
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Certifying the True Error: Machine Learning in Coq with  Veriﬁed Generalization Guarantees  Alexander Bagnall, Gordon Stewart  Ohio University, Athens, OH 45701 {ab667712, gstewart}@ohio.edu  Abstract  We present MLCERT, a novel system for doing practical mechanized proof of the generalization of learning procedures, bounding expected error in terms of training or test error. ML- CERT is mechanized in that we prove generalizat
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Extension Removal in Abstract Argumentation – An Axiomatic Approach  Computer Science Institute, Leipzig University, Germany  Ringo Baumann, Gerhard Brewka {baumann,brewka}@informatik.uni-leipzig.de  Abstract  This paper continues the rather recent line of research on the dynamics of non-monotonic formalisms. In particular, we consider semantic changes in Dung’s abstract argumentation formalism. One of the most studied problem
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Model-Based Diagnosis for Cyber-Physical Production Systems  Based on Machine Learning and Residual-Based Diagnosis Models Oliver Niggemann Andreas Bunte  Benno Stein  OWL University of Applied Sciences  Institute Industrial IT  Langenbruch 6, 32657 Lemgo, Germany  andreas.bunte@hs-owl.de  Bauhaus-Universit¨at Weimar Faculty of Media, Webis Group  99421 Weimar, Germany benno.stein@uni-weimar.de  OWL University of Applied Scien
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  A Data-Independent Transformation That Preserves Assertion Entailment  From Horn-SRIQ to Datalog:  David Carral,1 Larry Gonz´alez,1 Patrick Koopmann2  1Center for Advancing Electronics Dresden (cfaed), Technische Universit¨at Dresden, Germany  2Institute for Theoretical Computer Science, Technische Universit¨at Dresden, Germany  ﬁrstname.lastname@tu-dresden.de  Abstract  Ontology-based access to large data-sets has recently ga
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Bi-Kronecker Functional Decision Diagrams:  A Novel Canonical Representation of Boolean Functions  Xuanxiang Huang, Kehang Fang, Liangda Fang,* Qingliang Chen, Zhao-Rong Lai, Linfeng Wei  Department of Computer Science, Jinan University  cshxx@stu2016.jnu.edu.cn; fkh@stu2014.jnu.edu.cn; {fangld,tpchen,laizhr,twei}@jnu.edu.cn  Guangzhou 510632, China  Abstract  In this paper, we present a novel data structure for compact repres
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Ontology-Based Query Answering for Probabilistic Temporal Data  Patrick Koopmann  Institute for Theoretical Computer Science Technische Universit¨at Dresden, Germany  ﬁrstname.lastname@tu-dresden.de  Abstract  We investigate ontology-based query answering for data that are both temporal and probabilistic, which might occur in contexts such as stream reasoning or situation recognition with uncertain data. We present a framework
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Belief Change and Non-Monotonic Reasoning Sans Compactness  Jandson S. Ribeiro  Macquarie University, Australia University of S˜ao Paulo, Brazil  jandson.santos-ribeiro-sant@hdr.mq.edu.au  jandson@ime.usp.br  Abhaya Nayak  Macquarie University, Australia  abhaya.nayak@mq.edu.au  Renata Wassermann  University of S˜ao Paulo, Brazil  renata@ime.usp.br  Abstract  Belief change and non-monotonic reasoning are arguably dif- ferent p
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  An Open-World Extension to Knowledge Graph Completion Models  Haseeb Shah,1 Johannes Villmow,2 Adrian Ulges,2 Ulrich Schwanecke,2 Faisal Shafait1  1National University of Science and Technology, Pakistan  2RheinMain University of Applied Sciences, Germany  {hshah.bese15seecs, faisal.shafait}@seecs.edu.pk  {johannes.villmow, adrian.ulges, ulrich.schwanecke}@hs-rm.de  Abstract  We present a novel extension to embedding-based kno
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Iterated Belief Base Revision: A Dynamic Epistemic Logic Approach  Marlo Souza,1 ´Alvaro Moreira,2 Renata Vieira3 1Department of Computer Science, UFBA, Salvador, Brazil  2Institute of Informatics, UFRGS, Porto Alegre, Brazil  3Polytechnic School, PUCRS, Porto Alegre, Brazil  msouza1@ufba.br, alvaro.moreira@ufrgs.br, renata.vieira@pucrs.br  Abstract  AGM’s belief revision is one of the main paradigms in the study of belief cha
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Enhanced Random Forest Algorithms  for Partially Monotone Ordinal Classiﬁcation  Christopher Bartley, Wei Liu, Mark Reynolds  Computer Science and Software Engineering  christopher.bartley@research.uwa.edu.au,{wei.liu,mark.reynolds}@uwa.edu.au  University of Western Australia, Perth, Australia  Abstract  One of the factors hindering the use of classiﬁcation models in decision making is that their predictions may contradict exp
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Online Learning from Data Streams with Varying Feature Spaces  Ege Beyazit  Jeevithan Alagurajah  University of Louisiana at Lafayette  University of Louisiana at Lafayette  University of Louisiana at Lafayette  Lafayette, LA, USA  exb6143@louisiana.edu  Lafayette, LA, USA  jxa4540@louisiana.edu  Xindong Wu  Lafayette, LA, USA xwu@louisiana.edu  Abstract  We study the problem of online learning with varying fea- ture spaces. T
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Deep Convolutional Sum-Product Networks  Cory J. Butz butz@cs.uregina.ca University of Regina  Canada  Jhonatan S. Oliveira oliveira@cs.uregina.ca University of Regina  Canada  Abstract  We give conditions under which convolutional neural net- works (CNNs) deﬁne valid sum-product networks (SPNs). One subclass, called convolutional SPNs (CSPNs), can be implemented using tensors, but also can suffer from being too shallow. Fortu
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  FRAME Revisited: An Interpretation View Based on Particle Evolution  Xu Cai,1† Yang Wu,1† Guanbin Li,1 Ziliang Chen,1 Liang Lin1,2∗  1School of Data and Computer Science, Sun Yat-Sen University, China  caitree@foxmail.com, wuyang36@mail2.sysu.edu.cn,  liguanbin@mail.sysu.edu.cn, c.ziliang@yahoo.com, linliang@ieee.org  2Dark Matter AI Inc.  Abstract  FRAME (Filters, Random ﬁelds, And Maximum Entropy) is an energy-based descript
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  EA-CG: An Approximate Second-Order  Method for Training Fully-Connected Neural Networks  Sheng-Wei Chen  HTC Research & Healthcare  sw chen@htc.com  Chun-Nan Chou  HTC Research & Healthcare  jason.cn chou@htc.com  Edward Y. Chang  HTC Research & Healthcare  edward chang@htc.com  Abstract  For training fully-connected neural networks (FCNNs), we propose a practical approximate second-order method includ- ing: 1) an approximatio
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  A Layer Decomposition-Recomposition Framework for Neuron Pruning towards Accurate Lightweight Networks  Weijie Chen, Yuan Zhang, Di Xie, Shiliang Pu {chenweijie5, zhangyuan, xiedi, pushiliang}@hikvision.com  Hikvision Research Institute  Abstract  Neuron pruning is an efﬁcient method to compress the net- work into a slimmer one for reducing the computational cost and storage overhead. Most of state-of-the-art results are ob- t
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Diverse Exploration via Conjugate Policies for Policy Gradient Methods  Andrew Cohen  Binghamton University  acohen13@binghamton.edu  Xingye Qiao  Binghamton University  qiao@math.binghamton.edu  Lei Yu  Binghamton University  Yantai University  lyu@cs.binghamton.edu  Elliot Way  Binghamton University eway1@binghamton.edu  Abstract  We address the challenge of effective exploration while main- taining good performance in polic
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Using Benson’s Algorithm for Regularization Parameter Tracking  Joachim Giesen, S¨oren Laue, Andreas L¨ohne  Friedrich-Schiller-Universit¨at Jena  Faculty of Mathematics and Computer Science  Ernst-Abbe-Platz 2 07743 Jena, Germany  Abstract  Regularized loss minimization, where a statistical model is obtained from minimizing the sum of a loss function and weighted regularization terms, is still in widespread use in machine lea
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)            AFS: An Attention-Based Mechanism for Supervised Feature Selection    2 School of Informatics, Zhejiang Sci-Tech University, China, dongfangdecheng@gmail.com, fayssica@gmail.com   1School of Software, Central South University, China, ninggui@gmail.com   Ning Gui,1 Danni Ge,2 Ziyin Hu2   Abstract   As  an  effective  data  preprocessing  step,  feature  selection  has shown its effectiveness to prepare high-dimensiona
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  The SpectACl of Nonconvex Clustering:  A Spectral Approach to Density-Based Clustering  Sibylle Hess  TU Dortmund University Computer Science Faculty  Artiﬁcial Intelligence LS VIII D-44221 Dortmund, Germany sibylle.hess@tu-dortmund.de  Wouter Duivesteijn  Philipp Honysz, Katharina Morik  Technische Universiteit Eindhoven Faculteit Wiskunde & Informatica  Data Mining Group  Eindhoven, the Netherlands  w.duivesteijn@tue.nl  TU 
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Fast Incremental SVDD Learning Algorithm with the Gaussian Kernel  Hansi Jiang, Haoyu Wang, Wenhao Hu, Deovrat Kakde, Arin Chaudhuri  SAS Institute Inc.  100 SAS Campus Drive  Cary, North Carolina 27513  {Hansi.Jiang; Haoyu.Wang; Wenhao.Hu; Dev.Kakde; Arin.Chaudhuri}@sas.com  Abstract  Support vector data description (SVDD) is a machine learn- ing technique that is used for single-class classiﬁcation and outlier detection. The
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  DoPAMINE: Double-Sided Masked CNN for  Pixel Adaptive Multiplicative Noise Despeckling  Sunghwan Joo,1 Sungmin Cha,1 Taesup Moon1  1 Department of Electrical and Computer Engineering, Sungkyunkwan University  {shjoo840, csm9493, tsmoon}@skku.edu  Abstract  We propose DoPAMINE, a new neural network based mul- tiplicative noise despeckling algorithm. Our algorithm is in- spired by Neural AIDE (N-AIDE), which is a recently pro- p
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Guided Dropout  Rohit Keshari, Richa Singh, Mayank Vatsa  IIIT-Delhi, India  {rohitk, rsingh, mayank}@iiitd.ac.in  Abstract  Dropout is often used in deep neural networks to prevent over-ﬁtting. Conventionally, dropout training invokes random drop of nodes from the hidden layers of a Neural Network. It is our hypothesis that a guided selection of nodes for intelli- gent dropout can lead to better generalization as compared to 
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Evolutionary Manytasking Optimization Based on Symbiosis in Biocoenosis  Rung-Tzuo Liaw, Chuan-Kang Ting Department of Power Mechanical Engineering  National Tsing Hua University  Hsinchu 30013, Taiwan  rtliaw@mx.nthu.edu.tw, ckting@pme.nthu.edu.tw  Abstract  Evolutionary multitasking is a signiﬁcant emerging search paradigm that utilizes evolutionary algorithms to concur- rently optimize multiple tasks. The multi-factorial ev
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  State-Augmentation Transformations for Risk-Sensitive Reinforcement Learning  Shuai Ma, Jia Yuan Yu  Concordia Institute of Information System Engineering, Concordia University  1455 De Maisonneuve Blvd. W., Montreal, Quebec, Canada H3G 1M8  m shua@encs.concordia.ca, jiayuan.yu@concordia.ca  Abstract  In the framework of MDP, although the general reward function takes three arguments—current state, action, and successor state;
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  DyS: A Framework for Mixture Models in Quantiﬁcation  Andr´e Maletzke, Denis dos Reis, Everton Cherman, Gustavo Batista  Instituto de Ciˆencias Matem´aticas e de Computac¸˜ao, Universidade de S˜ao Paulo  {andregustavo,denismr,evertoncherman}@usp.br, gbatista@icmc.usp.br  Abstract  Quantiﬁcation is an expanding research topic in Machine Learning literature. While in classiﬁcation we are interested in obtaining the class of indi
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Multigrid Backprojection  Super–Resolution and Deep Filter Visualization  Pablo Navarrete Michelini, Hanwen Liu, Dan Zhu  BOE Technology Group Inc., Ltd.  Beijing, China  Abstract  We introduce a novel deep–learning architecture for image upscaling by large factors (e.g. 4×, 8×) based on examples of pristine high–resolution images. Our target is to reconstruct high–resolution images from their downscale versions. The proposed 
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Sparse Reject Option Classiﬁer Using Successive Linear Programming  Kulin Shah, Naresh Manwani  Machine Learning Lab, KCIS  IIIT, Hyderabad-500032  Abstract  In this paper, we propose an approach for learning sparse re- ject option classiﬁers using double ramp loss Ldr. We use DC programming to ﬁnd the risk minimizer. The algorithm solves a sequence of linear programs to learn the reject option clas- siﬁer. We show that the lo
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Deep Metric Learning by Online Soft Mining and Class-Aware Attention  Xinshao Wang,1,2 Yang Hua,1 Elyor Kodirov,2 Guosheng Hu,1,2 Neil M. Robertson1,2  1School of Electronics, Electrical Engineering and Computer Science, Queen’s University Belfast, UK  {xwang39, y.hua, n.robertson}@qub.ac.uk, {elyor, guosheng.hu}@anyvision.co  2Anyvision Research Team, UK  Abstract  Deep metric learning aims to learn a deep embedding that can 
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Self-Ensembling Attention Networks:  Addressing Domain Shift for Semantic Segmentation  Yonghao Xu,1, 2 Bo Du,1∗ Lefei Zhang,1 Qian Zhang,3 Guoli Wang,3 Liangpei Zhang2  1School of Computer Science, Wuhan University, Wuhan 430072, P. R. China.  2State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing,  Wuhan University, Wuhan 430079, P. R. China.  3Horizon Robotics, Inc., Beijing 100190, P. R.
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  AutoSense Model for Word Sense Induction Reinald Kim Amplayo, Seung-won Hwang, Min Song  {rktamplayo, seungwonh, min.song}@yonsei.ac.kr  Yonsei University  Abstract  Word sense induction (WSI), or the task of automatically dis- covering multiple senses or meanings of a word, has three main challenges: domain adaptability, novel sense detection, and sense granularity ﬂexibility. While current latent variable models are known to
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Recurrent Stacking of Layers for Compact Neural Machine Translation Models  Raj Dabre, Atsushi Fujita  National Institute of Information and Communications Technology  3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan  ﬁrstname.lastname@nict.go.jp  Abstract  In encoder-decoder based sequence-to-sequence modeling, the most common practice is to stack a number of recurrent, convolutional, or feed-forward layers in the
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  MNCN: A Multilingual Ngram-Based Convolutional  Network for Aspect Category Detection in Online Reviews  Erfan Ghadery, Sajad Movahedi, Heshaam Faili, Azadeh Shakery  University of Tehran  Tehran, Iran  {erfan.ghadery, s.movahedi, hfaili, shakery}@ut.ac.ir  Abstract  The advent of the Internet has caused a signiﬁcant growth in the number of opinions expressed about products or services on e-commerce websites. Aspect category d
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  A Hierarchical Multi-Task Approach for  Learning Embeddings from Semantic Tasks  Victor Sanh,1 Thomas Wolf,1 Sebastian Ruder2,3 1Hugging Face, 20 Jay Street, Brooklyn, New York, United States  2Insight Research Centre, National University of Ireland, Galway, Ireland  3Aylien Ltd., 2 Harmony Court, Harmony Row, Dublin, Ireland  {victor, thomas}@huggingface.co, sebastian@ruder.io  Abstract  Much effort has been devoted to evalua
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Learning Semantic Representations for  Novel Words: Leveraging Both Form and Context  Timo Schick Sulzer GmbH  Munich, Germany  timo.schick@sulzer.de  Center for Information and Language Processing  Hinrich Sch¨utze  LMU Munich, Germany inquiries@cislmu.org  Abstract  Word embeddings are a key component of high-performing natural language processing (NLP) systems, but it remains a challenge to learn good representations for no
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Bringing Order to Chaos – A Compact  Representation of Partial Order in SAT-Based HTN Planning  Institute of Artiﬁcial Intelligence, Ulm University, D-89069 Ulm, Germany  Gregor Behnke, Daniel H¨oller, Susanne Biundo {gregor.behnke, daniel.hoeller, susanne.biundo}@uni-ulm.de  Abstract  HTN planning provides an expressive formalism to model complex application domains. It has been widely used in real- world applications. Howeve
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Temporal Planning with Temporal Metric Trajectory Constraints  Andrea Micheli  Fondazione Bruno Kessler  Trento, 38123, Italy amicheli@fbk.eu  Enrico Scala  Fondazione Bruno Kessler  Trento, 38123, Italy  escala@fbk.eu  Abstract  In several industrial applications of planning, complex tem- poral metric trajectory constraints are needed to adequately model the problem at hand. For example, in production plants, items must be pr
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  An Innovative Genetic Algorithm for the Quantum Circuit Compilation Problem  Riccardo Rasconi, Angelo Oddi  Institute of Cognitive Sciences and Technologies, National Research Council of Italy (ISTC-CNR)  {riccardo.rasconi, angelo.oddi}@istc.cnr.it  Rome, Italy  Abstract  Quantum Computing represents the next big step towards speed boost in computation, which promises major break- throughs in several disciplines including Arti
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Rotational Diversity in Multi-Cycle Assignment Problems  Helge Spieker, Arnaud Gotlieb  Simula Research Laboratory  P.O. Box 134  1325 Lysaker, Norway  {helge,arnaud}@simula.no  Morten Mossige University of Stavanger  Stavanger, Norway  ABB Robotics Bryne, Norway  morten.mossige@uis.no  Abstract  In multi-cycle assignment problems with rotational diversity, a set of tasks has to be repeatedly assigned to a set of agents. Over 
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Memory Bounded Open-Loop Planning  in Large POMDPs Using Thompson Sampling  Thomy Phan LMU Munich  thomy.phan@iﬁ.lmu.de  Lenz Belzner MaibornWolff  lenz.belzner@maibornwolff.de  Marie Kiermeier  LMU Munich  marie.kiermeier@iﬁ.lmu.de  Markus Friedrich  LMU Munich  markus.friedrich@iﬁ.lmu.de  Kyrill Schmid  LMU Munich  kyrill.schmid@iﬁ.lmu.de  Claudia Linnhoff-Popien  LMU Munich  linnhoff@iﬁ.lmu.de  Abstract  State-of-the-art ap
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Visual Place Recognition via Robust (cid:96)2-Norm  Distance Based Holism and Landmark Integration  Kai Liu, Hua Wang, Fei Han, Hao Zhang  Department of Computer Science, Colorado School of Mines, Golden, CO 80401  liukaizhijia@gmail.com, huawangcs@gmail.com, fhan@alumni.mines.edu, hzhang@mines.edu  Abstract  Visual place recognition is essential for large-scale simulta- neous localization and mapping (SLAM). Long-term robot o
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Object Detection Based on Region Decomposition and Assembly  Computer Vision Lab., Department of Computer Science and Engineering  Incheon National University, 119 Academy-ro, Yeonsu-gu, Incheon, 22012, Korea  Seung-Hwan Bae  shbae@inu.ac.kr  Abstract  Region-based object detection infers object regions for one or more categories in an image. Due to the recent advances in deep learning and region proposal methods, object detec
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Data Fine-Tuning  Saheb Chhabra, Puspita Majumdar, Mayank Vatsa, Richa Singh  {sahebc, pushpitam, mayank, rsingh}@iiitd.ac.in  IIIT-Delhi, India  Abstract  In real-world applications, commercial off-the-shelf systems are utilized for performing automated facial analysis includ- ing face recognition, emotion recognition, and attribute pre- diction. However, a majority of these commercial systems act as black boxes due to the in
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Learning Incremental Triplet Margin for Person Re-Identiﬁcation  Yingying Zhang, Qiaoyong Zhong, Liang Ma, Di Xie, Shiliang Pu  {zhangyingying7,zhongqiaoyong,maliang6,xiedi,pushiliang}@hikvision.com  Hikvision Research Institute  Abstract  Person re-identiﬁcation (ReID) aims to match people across multiple non-overlapping video cameras deployed at different locations. To address this challenging problem, many met- ric learning
The Thirty-First AAAI Conference on Innovative Applications of Artiﬁcial Intelligence (IAAI-19)  Early Detection of Vacant Parking Spaces Using Dashcam Videos  Ming-Che Wu, Mei-Chen Yeh  Department of Computer Science and Information Engineering  National Taiwan Normal University, Taipei, Taiwan  Email: myeh@csie.ntnu.edu.tw  Abstract  A major problem in metropolitan areas is ﬁnding parking spaces. Existing parking guidance systems often adopt ﬁxed sensors or cameras that cannot provide informat
The Ninth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence (EAAI-19)  Machine Learning Based Heuristic Search  Algorithms to Solve Birds of a Feather Card Game  Bryon Kucharski, Azad Deihim, Mehmet Ergezer  Wentworth Institute of Technology  550 Huntington Ave, Boston, MA 02115 {kucharskib, deihima, ergezerm}@wit.edu  Abstract  This research was conducted by an interdisciplinary team of two undergraduate students and a faculty to explore solutions to the Birds of a Feather (BoF) 
The Ninth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence (EAAI-19)  An Integrative Framework for Artiﬁcial Intelligence Education  Pat Langley  Institute for the Study of Learning and Expertise, 2164 Staunton Court, Palo Alto, CA 94306 USA  Department of Computer Science, University of Auckland,  Private Bag 92019, Auckland 1142 NZ  Abstract  Modern introductory courses on AI do not train students to create intelligent systems or provide broad coverage of this complex ﬁeld. In 
The Ninth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence (EAAI-19)  Efﬁcient Solving of Birds of a Feather Puzzles  Todd W. Neller, Connor Berson, Jivan Kharel, Ryan Smolik  {tneller,bersco01,kharji01,smolry01}@gettysburg.edu  Gettysburg College  Abstract  In this article, we describe the lessons learned in creating an efﬁcient solver for the solitaire game Birds of a Feather. We introduce a new variant of depth-ﬁrst search that we call best-n depth-ﬁrst search that achieved a 
The Ninth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence (EAAI-19)  Computer Generation of Birds of a Feather Puzzles  Todd W. Neller, Daniel Ziegler {tneller,ziegda03}@gettysburg.edu  Gettysburg College  Abstract  In this article, we describe a computer-aided design process for generating high-quality Birds of a Feather solitaire card puzzles. In each iteration, we generate puzzles via combina- torial optimization of an objective function. After solving and subjectively rating
The Ninth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence (EAAI-19)        A Monte Carlo Tree Search Player for Birds of a Feather Solitaire    Christian Roberson, Katarina Sperduto   croberson@flsouthern.edu, ksperduto@mocs.flsouthern.edu   Florida Southern College          Abstract   Artificial  intelligence  in  games  serves  as  an  excellent  plat- form for facilitating collaborative research with undergradu- ates. This paper explores several aspects of a research chal- le
The Ninth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence (EAAI-19)  A Neural Network Approach for  Birds of a Feather Solvability Prediction  Benjamin Sang, Sejong Yoon  The College of New Jersey  Ewing Township, New Jersey 08618  Abstract  Birds of a Feather is a single player, perfect information card game. The game can have multiple board sizes with larger boards introducing larger search spaces that grow exponen- tially. In this paper, we investigate the solvability of the 
The Ninth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence (EAAI-19)  Artiﬁcial Intelligence Competencies  for Data Science Undergraduate Curricula Scott Buck Intel Corporation  Andrea Danyluk  Williams College  47 Lab Campus Drive  Williamstown, MA 01267 andrea@cs.williams.edu  5000 W. Chandler Blvd., m/s CH7-230  Chandler, AZ 85226 scott.buck@intel.com  Abstract  In August 2017, the ACM Education Council initiated a task force to add to the broad, interdisciplinary conversation
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Explainable, Normative, and Justiﬁed Agency  Pat Langley  Institute for the Study of Learning and Expertise, 2164 Staunton Court, Palo Alto, CA 94306 USA  Department of Computer Science, University of Auckland,  Private Bag 92019, Auckland 1142 NZ  Abstract  In this paper, we pose a new challenge for AI researchers – to develop intelligent systems that support justiﬁed agency. We illustrate this ability with examples and relat
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Borda Count in Collective Decision  Making: A Summary of Recent Results  J¨org Rothe  Institut f¨ur Informatik  Heinrich-Heine-Universit¨at D¨usseldorf  40225 D¨usseldorf, Germany rothe@cs.uni-duesseldorf.de  Abstract  Borda Count is one of the earliest and most important vot- ing rules. Going far beyond voting, we summarize recent advances related to Borda in computational social choice and, more generally, in collective deci
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Global Remote Operation of Intelligent Space Robot Assistants  Daniel Leidner,1 Peter Schmaus,1 Florian Schmidt,1 Benedikt Pleintinger,1  Ralph Bayer,1 Adrian S. Bauer,1 Thomas Krueger,2 Neal Y. Lii1  1Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Wessling, Germany, {ﬁrst.last}@dlr.de 2Human Robot Interaction Lab, European Space Agency (ESA), Noordwijk, The Netherlands, {ﬁrst.last}@esa.int  Abstract  I
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Realtime Generation of Audible Textures Inspired by a Video Stream  Simone Mellace, J´erˆome Guzzi, Alessandro Giusti, Luca M. Gambardella  Dalle Molle Institute for Artiﬁcial Intelligence, USI-SUPSI, Lugano (Switzerland)  Abstract  We showcase a model to generate a soundscape from a cam- era stream in real time. The approach relies on a training video with an associated meaningful audio track; a granu- lar synthesizer generat
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  DBA: Dynamic Multi-Armed Bandit Algorithm  Sadegh Nobari  Rakuten, Inc.  sadegh@sqnco.com  Abstract  We introduce Dynamic Bandit Algorithm (DBA), a practical solution to improve the shortcoming of the pervasively em- ployed reinforcement learning algorithm called Multi-Arm Bandit, aka Bandit. Bandit makes real-time decisions based on the prior observations. However, Bandit is heavily biased to the priors that it cannot quickly
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Learning Flexible Latent  Representations via Encapsulated Variational Encoders  Wenjun Bai, Changqin Quan, Zhi-Wei Luo  Department of Computational Science  Kobe University, Japan  bwj@cs11.cs.kobe-u.ac.jp, quanchqin@gold.kobe-u.ac.jp, luo@gold.kobe-u.ac.jp,  Introduction  Representation learning – aims to capture certain aspects of the observed data – has fuelled majority of downstream AI applications. As an emerging techniq
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  An Imperfect Algorithm for Coalition Structure Generation  Narayan Changder, Samir Aknine, Animesh Dutta  Department of Computer Science and Engineering, NIT Durgapur, India, narayan.changder@gmail.com  LIRIS laboratory Claude Bernard University, Lyon 1, France, samir.aknine@univ-lyon1.fr  Department of Computer Science and Engineering, NIT Durgapur, India,animeshnit@gmail.com  Abstract  Optimal Coalition Structure Generation 
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Reinforcement Learning under Threats  Victor Gallego  ICMAT-CSIC  victor.gallego@icmat.es  Roi Naveiro ICMAT-CSIC  roi.naveiro@icmat.es  David Rios Insua  ICMAT-CSIC  david.rios@icmat.es  Abstract  In several reinforcement learning (RL) scenarios, mainly in se- curity settings, there may be adversaries trying to interfere with the reward generating process. However, when non-stationary environments as such are considered, Q-le
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Mind Your Language: Abuse and Offense Detection for Code-Switched Languages  Raghav Kapoor  MIDAS Lab, NSIT-Delhi raghavk.co@nsit.net.in  Rajiv Ratn Shah MIDAS Lab, IIIT-Delhi  rajivratn@iiitd.ac.in  Abstract  Yaman Kumar  MIDAS Lab, IIIT-Delhi yaman@nsitonline.in  Kshitij Rajput  MIDAS Lab, NSIT-Delhi rajput.kshitij97@gmail.com  Ponnurangam Kumaraguru  IIIT, Delhi  pk@iiitd.ac.in  Roger Zimmermann  NUS, Singapore  rogerz@comp
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  A Dynamic Bayesian Network Based Merge Mechanism for Autonomous Vehicles  Kherroubi Zine el abidine  Groupe Renault  1 Avenue du Golf, 78180 Guyancourt  Aknine Samir  Claude Bernard Lyon 1 university  69100 Villeurbanne  Bacha Rebiha Groupe Renault  1 Avenue du Golf, 78180 Guyancourt  Abstract  This work explores the design of a central collaborative driv- ing strategy between connected cars with the objective of im- proving r
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Cross-Domain Recommendation via Coupled Factorization Machines  Lile Li, Quan Do, Wei Liu  Advanced Analytics Institute, School of Software, Faculty of Engineering and Information Technology  Univeristy of Technology Sydney, Sydney, Australia  Lile.Li@student.uts.edu.au, Quan.Do@student.uts.edu.au, Wei.Liu@uts.edu.au  Abstract  Data across many business domains can be represented by two or more coupled data sets. Correlations 
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Jointly Multiple Hash Learning  Xingbo Liu,1 Xiushan Nie,2,* Yingxin Wang,1 Yilong Yin3,∗  1School of Computer Science and Technology, Shandong University, Jinan, P.R. China  2School of Computer Science and Technology, Shandong Jianzhu University, Jinan, P.R. China  3School of Software, Shandong University, Jinan, P.R. China  sclxb@mail.sdu.edu.cn, wyxwalker@163.com, niexiushan@163.com, ylyin@sdu.edu.cn  Motivation  Hashing ca
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Adaptive Optimization Framework for Control of Multi-Agent Systems  Anna Lukina  Technische Universit´at Wien  Treitlstrasse 3, 1040, Vienna, Austria  +43(677)62044339, anna.lukina@tuwien.ac.at  http://logic-cs.at/phd/students/anna-lukina/  Abstract  The main focus of this work is an optimization-based frame- work for control of multi-agent systems that synthesizes ac- tions steering a given system towards a speciﬁed state. Th
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Implementation of Boolean AND and OR Logic Gates with  Biologically Reasonable Time Constants in Spiking Neural Networks Debasrita Chakraborty, Ashish Ghosh  Lakshay Sahni  Department of Electrical & Electronics Engineering  Delhi Technological University, India  layshaysahni 2k14@dtu.ac.in  +91-995321150  Machine Intelligence Unit Indian Statistical Institute,  Kolkata, India  debasritac@gmail.com  ash@isical.ac.in  Abstract 
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Lipper: Speaker Independent Speech Synthesis Using Multi-View Lipreading  Khwaja Mohd. Salik MIDAS Lab, NSUT-Delhi khwajam.co@nsit.net.in  Swati Aggarwal  NSUT-Delhi  swati@nsit.ac.in  Yaman Kumar  MIDAS Lab, IIIT-Delhi yaman@nsitonline.in  Rajiv Ratn Shah MIDAS Lab, IIIT-Delhi  rajivratn@iiitd.ac.in  Rohit Jain  MIDAS Lab, NSUT-Delhi  rohitj.co@nsit.net.in  Roger Zimmermann  NUS, Singapore  rogerz@comp.nus.edu.sg  Abstract  L
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Towards Sequence-to-Sequence Reinforcement Learning for Constraint Solving with Constraint-Based Local Search  Helge Spieker  Simula Research Laboratory  P.O. Box 134  1325 Lysaker, Norway  helge@simula.no  Abstract  This paper proposes a framework for solving constraint prob- lems with reinforcement learning (RL) and sequence-to- sequence recurrent neural networks. We approach constraint solving as a declarative machine learn
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Identifying Bottlenecks in Practical SAT-Based Model Finding for First-Order Logic Ontologies with Datasets  Shirly Stephen, Torsten Hahmann School of Computing and Information Science, {shirly.stephen, torsten.hahmann}@maine.edu  University of Maine, Orono, Maine 04473  Abstract  Satisﬁability of ﬁrst-order logic (FOL) ontologies is typically veriﬁed by translation to propositional satisﬁability (SAT) problems, which is then 
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Improving Full-Body Pose Estimation from a Small  Sensor Set Using Artiﬁcial Neural Networks and a Kalman Filter  Frank J. Wouda,1 Matteo Giuberti,2 Giovanni Bellusci,2 Bert-Jan F. van Beijnum,1  Peter H. Veltink1  1Department of Biomedical Signals & Systems, Technical Medical Centre, University of Twente, Enschede, The Netherlands f.j.wouda@utwente.nl  2Xsens Technologies B.V., Enschede, The Netherlands  Abstract  Previous re
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Manifold Distance-Based Over-Sampling Technique for Class Imbalance Learning  Lingkai Yang, Yinan Guo, Jian Cheng  School of Information and Control Engineering, China University of  Mining and Technology, Xuzhou, 221008, Jiangsu, China, +86-15150024213  yanglk@cumt.edu.cn, nanﬂy@126.com, chengjian@cumt.edu.cn  Abstract  Over-sampling technology for handling the class imbalanced problem generates more minority samples to balan
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)  Sequence to Sequence Learning for Query Expansion  Salah Zaiem  Ecole Polytechnique  Palaiseau, France  mohamed-salah.zaiem@polytechnique.edu  Fatiha Sadat  Universite du Quebec a Montreal  201 Avenue du Pr´esident Kennedy, Montreal  sadat.fatiha@uqam.ca  Abstract  As fas as we are aware, using Sequence to Sequence algo- rithms for query expansion has not been explored yet in In- formation Retrieval literature. We tried to ﬁll
A New Granger Causal Model for Inﬂuence Evolution in  Dynamic Social Networks: The Case of DBLP  Belkacem Chikhaoui, Mauricio Chiazzaro, Shengrui Wang  Prospectus Laboratory, Department of Computer Science  {belkacem.chikhaoui, mauricio.chiazzaro, shengrui.wang}@usherbrooke.ca  University of Sherbrooke, Canada  Abstract  This paper addresses a new problem concerning the evolution of inﬂuence relationships between commu- nities in dynamic social networks. A weighted tempo- ral multigraph is emplo
An Agent-Based Model of the Emergence and Transmission of a Language System for the Expression of Logical Combinations  Joseﬁna Sierra-Santib´a˜nez  Technical University of Catalonia, Campus Nord, Ed. Omega  c/ Jordi Girona Salgado 1-3, Barcelona 08034 Spain  E-mail: Maria.Joseﬁna.Sierra@upc.edu  Abstract  This paper presents an agent-based model of the emergence and transmission of a language system for the expression of logical combinations of propositions. The model assumes the agents have so
Heuristic Induction of Rate-Based Process Models  Pat Langley and Adam Arvay  Department of Computer Science, University of Auckland,  Private Bag 92019, Auckland 1142, New Zealand  Abstract  This paper presents a novel approach to inductive process modeling, the task of constructing a quantitative account of dynamical behavior from time-series data and background knowledge. We review earlier work on this topic, noting its reliance on methods that evaluate entire model structures and use repeate
Aggregating Electric Cars to Sustainable Virtual Power Plants:  The Value of Flexibility in Future Electricity Markets  Micha Kahlen and Wolfgang Ketter  Erasmus University Rotterdam  Burgemeester Oudlaan 50, The Netherlands  {kahlen,wketter}@rsm.nl  Abstract  Electric vehicles will play a crucial role in balancing the future electrical grid, which is complicated by many intermittent renewable energy. We developed an algo- rithm that determines for a ﬂeet of electric vehicles, which EV at what p
Real-Time Predictive Optimization for Energy  Management in a Hybrid Electric Vehicle  Alexander Styler and Illah Nourbakhsh  Abstract  With increasing numbers of electric and hybrid vehicles on the road, transportation presents a unique opportunity to leverage data-driven intelligence to realize large scale impact in energy use and emissions. Energy management in these vehicles is highly sensitive to upcoming power load on the vehicle, which is not considered in conventional reactive policies c
Fair Information Sharing for Treasure Hunting  Yiling Chen Harvard SEAS  yiling@seas.harvard.edu  Ben-Gurion University and Harvard CRCS  Kobbi Nissim  kobbi@cs.bgu.ac.il  Bo Waggoner Harvard SEAS  bwaggoner@fas.harvard.edu  Abstract  In a search task, a group of agents compete to be the ﬁrst to ﬁnd the solution. Each agent has different private informa- tion to incorporate into its search. This problem is inspired by settings such as scientiﬁc research, Bitcoin hash inversion, or hunting for so
A Mechanism Design Approach to Measure Awareness∗  DI, Universit`a degli Studi di Salerno, Italy  SCM, Teesside University, UK  Diodato Ferraioli  dferraioli@unisa.it  Carmine Ventre  C.Ventre@tees.ac.uk  Gabor Aranyi  SCM, Teesside University, UK  G.Aranyi@tees.ac.uk  Abstract  In this paper, we study protocols that allow to discern con- scious and unconscious decisions of human beings; i.e., pro- tocols that measure awareness. Consciousness is a central research theme in Neuroscience and AI, w
Truthful Mechanisms without Money for  Non-Utilitarian Heterogeneous Facility Location  Paolo Seraﬁno School of Computing Teesside University p.seraﬁno@tees.ac.uk  Abstract  In this paper, we consider the facility location problem un- der a novel model recently proposed in the literature, which combines the no-money constraint (i.e. the impossibility to employ monetary transfers between the mechanism and the agents) with the presence of heterogeneous facilities, i.e. fa- cilities serving differe
Reusing Previously Found A* Paths for Fast Goal-Directed Navigation  in Dynamic Terrain  Carlos Hern´andez  Roberto As´ın  Jorge A. Baier  Depto. de Ingenier´ıa Inform´atica  Depto. de Ingenier´ıa Inform´atica  Univ. Cat´olica de la Ssma. Concepci´on  Univ. Cat´olica de la Ssma. Concepci´on  Depto. de Ciencia de la Computaci´on Pontiﬁcia Universidad Cat´olica de Chile  Concepci´on, Chile  Concepci´on, Chile  Santiago, Chile  Abstract  Generalized Adaptive A* (GAA*) is an incremental algo- rithm 
Collaboration in Social Problem-Solving: When Diversity Trumps Network Efﬁciency  Diego V. Noble, Marcelo O.R. Prates, Daniel S. Bossle and Lu´ıs C. Lamb  Institute of Informatics, Federal University of Rio Grande do Sul - Porto Alegre, 91501-970, Brazil  dvnoble@inf.ufrgs.br morprates@inf.ufrgs.br dsbossle@inf.ufrgs.br lamb@inf.ufrgs.br  Abstract  Recent studies have suggested that current agent-based models are not sufﬁciently sophisticated to reproduce results achieved by human collaborative 
CrowdWON: A Modelling Language for Crowd Processes Based on Workﬂow Nets David S´anchez-Charles and Victor Munt´es-Mulero  CA Labs, CA Technologies, Barcelona (Spain).  {David.Sanchez, Victor.Muntes}@ca.com  Universitat Polit`ecnica de Catalunya -  Marc Sol´e  BarcelonaTech,  Barcelona (Spain). msole@ac.upc.edu  Barcelona Supercomputing Center (BSC),  Universitat Polit`ecnica de Catalunya - BarcelonaTech,  Jordi Nin  Barcelona (Spain). {nin@ac.upc.edu  Abstract  Although crowdsourcing has been p
Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence  RANSAC versus CS-RANSAC   Geun-Sik Jo, Kee-Sung Lee, Devy Chandra, Chol-Hee Jang, Myung-Hyun Ga  Department of Computer & Information Engineering   gsjo@inha.ac.kr, lee.ks@outlook.kr, wiy_ch4n@hotmail.com, orange@eslab.inha.ac.kr, gagaman7777@eslab.inha.ac.kr   INHA University   Incheon, Korea   Abstract   A  homography  matrix  is  used  in  computer  vision  field  to  solve  the  correspondence  problem  between  a  p
Game-Theoretic Approach for Non-Cooperative Planning  Jaume Jord´an and Eva Onaindia  Universitat Polit`ecnica de Val`encia  Departamento de Sistemas Inform´aticos y Computaci´on  Camino de Vera s/n. 46022 Valencia, Spain  {jjordan,onaindia}@dsic.upv.es  Abstract  When two or more self-interested agents put their plans to execution in the same environment, conﬂicts may arise as a consequence, for instance, of a common uti- lization of resources. In this case, an agent can post- pone the executio
Integration and Evaluation of a Matrix  Factorization Sequencer in Large Commercial ITS  Carlotta Schatten, Ruth Janning, Lars Schmidt-Thieme Information Systems and Machine Learning Lab, University of Hildesheim  Marienburger Platz 22  31148 Hildesheim, Germany  Abstract  Correct evaluation of Machine Learning based sequencers re- quire large data availability, large scale experiments and con- sideration of different evaluation measures. Such constraints make the construction of ad-hoc Intellig
Solving and Explaining Analogy Questions  Using Semantic Networks  Adrian Boteanu, Sonia Chernova  Worcester Polytechnic Institute  100 Institute Road Worcester, MA 01609  aboteanu@wpi.edu, soniac@wpi.edu  Abstract  Analogies are a fundamental human reasoning pattern that relies on relational similarity. Understanding how analogies are formed facilitates the transfer of knowl- edge between contexts. The approach presented in this work focuses on obtaining precise interpretations of analogies. We
Parallelized Hitting Set Computation for Model-Based Diagnosis  Dietmar Jannacha, Thomas Schmitza, Kostyantyn Shchekotykhinb  aTU Dortmund, Germany  bAlpen-Adria Universit¨at Klagenfurt, Austria  {ﬁrstname.lastname}@tu-dortmund.de, kostya@iﬁt.uni-klu.ac.at  Abstract  Model-Based Diagnosis techniques have been successfully applied to support a variety of fault-localization tasks both for hardware and software artifacts. In many applications, Re- iter’s hitting set algorithm has been used to deter
The Relative Expressiveness of Abstract Argumentation and Logic Programming  Hannes Strass  Computer Science Institute Leipzig University, Germany  strass@informatik.uni-leipzig.de  Abstract  We analyze the relative expressiveness of the two-valued se- mantics of abstract argumentation frameworks, normal logic programs and abstract dialectical frameworks. By expressive- ness we mean the ability to encode a desired set of two-valued interpretations over a given propositional vocabulary A us- ing 
Exploiting Determinism to Scale Relational Inference  Mohamed-Hamza Ibrahim and Christopher Pal and Gilles Pesant Department of Computer and Software Engineering, ´Ecole Polytechnique Montr´eal 2500, Chemin de Polytechnique, Universite de Montr´eal, Montr´eal, Qu´ebec, Canada  Abstract  One key challenge in statistical relational learning (SRL) is scalable inference. Unfortunately, most real-world prob- lems in SRL have expressive models that translate into large grounded networks, representing 
Topic Segmentation with An Ordering-Based Topic Model  Lan Du, John K Pate and Mark Johnson Department of Computing, Macquarie University {Lan.Du, John.Pate, Mark.Johnson}@mq.edu.au  Sydney, NSW 2109, Australia  Abstract  Documents from the same domain usually discuss sim- ilar topics in a similar order. However, the number of topics and the exact topics discussed in each individ- ual document can vary. In this paper we present a sim- ple topic model that uses generalised Mallows models and inco
Generating Event Causality Hypotheses through Semantic Relations  Chikara Hashimoto∗, Kentaro Torisawa†, Julien Kloetzer‡, Jong-Hoon Oh§ National Institute of Information and Communications Technology, Kyoto, 619-0289, Japan  {∗ch, †torisawa, ‡julien, §rovellia}@nict.go.jp  Abstract  Event causality knowledge is indispensable for intelli- gent natural language understanding. The problem is that any method for extracting event causalities from text is insufﬁcient; it is likely that some event cau
Efﬁcient Active Learning of Halfspaces via Query Synthesis  Ibrahim Alabdulmohsin and Xin Gao and Xiangliang Zhang  Computer, Electrical and Mathematical Sciences & Engineering Division  King Abdullah University of Science & Technology (KAUST)  Thuwal, Saudi Arabia 23955  Abstract  Active learning is a subﬁeld of machine learning that has been successfully used in many applications in- cluding text classiﬁcation and bioinformatics. One of the fundamental branches of active learning is query synt
Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence  Low-Rank Multi-View Learning  in Matrix Completion for Multi-Label Image Classiﬁcation Meng Liu†, Yong Luo†§, Dacheng Tao‡, Chao Xu†, and Yonggang Wen§  †Key Laboratory of Machine Perception (MOE), School of EECS, PKU, Beijing 100871, China ‡Center for Quantum Computation and Intelligent Systems, UTS, Sydney, NSW 2007, Australia  §Division of Networks and Distributed Systems School of Computer Engineering, NTU, 639798, S
V-MIN: Efﬁcient Reinforcement Learning  through Demonstrations and Relaxed Reward Demands  David Mart´ınez and Guillem Aleny`a and Carme Torras  Institut de Rob`otica i Inform`atica Industrial (CSIC-UPC)  C/ Llorens i Artigas 4-6. 08028 Barcelona, Spain  {dmartinez,galenya,torras}@iri.upc.edu  Abstract  Reinforcement learning (RL) is a common paradigm for learning tasks in robotics. However, a lot of exploration is usually required, making RL too slow for high-level tasks. We present V-MIN, an a
Agnostic System Identiﬁcation  for Monte Carlo Planning  Erik Talvitie  Mathematics and Computer Science  Franklin & Marshall College  erik.talvitie@fandm.edu  Abstract  While model-based reinforcement learning is often studied under the assumption that a fully accurate model is contained within the model class, this is rarely true in practice. When the model class may be fundamentally limited, it can be difﬁcult to obtain theoretical guar- antees. Under some conditions the DAgger algorithm prom
Strong Temporal Planning with Uncontrollable Durations:  A State-Space Approach  Alessandro Cimatti and Andrea Micheli and Marco Roveri  Fondazione Bruno Kessler – Italy {cimatti,amicheli,roveri}@fbk.eu  Abstract  In many practical domains, planning systems are required to reason about durative actions. A common assumption in the literature is that the executor is allowed to decide the duration of each action. However, this assumption may be too restric- tive for applications. In this paper, we 
Measuring Plan Diversity: Pathologies in Existing  Approaches and a New Plan Distance Metric  Robert P. Goldman and Ugur Kuter  SIFT, LLC  Minneapolis, MN USA  {rpgoldman,ukuter}@sift.net  Abstract  In this paper we present a plan-plan distance metric based on Kolmogorov (Algorithmic) complexity. Generating di- verse sets of plans is useful for tasks such as probing user preferences and reasoning about vulnerability to cyber at- tacks. Generating diverse plans, and comparing different di- verse 
Bayesian Networks Speciﬁed Using Propositional and  Relational Constructs: Combined, Data, and Domain Complexity  Fabio Gagliardi Cozman and Denis Deratani Mau´a  Escola Polit´ecnica, Universidade de S˜ao Paulo, Brazil  Abstract  We examine the inferential complexity of Bayesian net- works speciﬁed through logical constructs. We ﬁrst con- sider simple propositional languages, and then move to relational languages. We examine both the combined complexity of inference (as network size and evidence
SMT-Based Validation of Timed Failure Propagation Graphs  Marco Bozzano, Alessandro Cimatti, Marco Gario and Andrea Micheli  Fondazione Bruno Kessler – Italy  {bozzano,cimatti,gario,amicheli}@fbk.eu—  Abstract  Timed Failure Propagation Graphs (TFPGs) are a formalism used in industry to describe failure propagation in a dynamic partially observable system. TFPGs are commonly used to perform model-based diagnosis. As in any model-based diag- nosis approach, however, the quality of the diagnosis s
Online Detection of Abnormal Events Using Incremental Coding Length  Institute for Intelligent Systems, and Department of Electrical & Computer Engineering  Jayanta K. Dutta and Bonny Banerjee  The University of Memphis Memphis, TN 38152, USA  {jkdutta, bbnerjee}@memphis.edu  Abstract  We present an unsupervised approach for abnormal event de- tection in videos. We propose, given a dictionary of features learned from local spatiotemporal cuboids using the sparse coding objective, the abnormality
      Position Assignment on an Enterprise Level Using    Combinatorial Optimization    Leonard Kinnaird-Heether   Chris Dorman  Ford Motor Company   lkinnair,cdorman1@ford.com          Abstract   We developed a tool to solve a problem of position assign- ment  within  the  IT  Ford  College  Graduate  program.    This  position  assignment  tool  was  first  developed  in  2012  and  has  been  used  successfully  since  then.    The  tool  has  since  evolved  for  use  with  several  other  p
Proceedings of the Twenty-Seventh Conference on Innovative Applications of Artificial Intelligence  Day-Ahead Hail Prediction Integrating Machine Learning  with Storm-Scale Numerical Weather Models  David John Gagne II  Center for Analysis and Prediction of Storms  University of Oklahoma  Amy McGovern  School of Computer Science  University of Oklahoma  Jerald Brotzge  Dept. of Atmospheric and Environmental  Sciences, University of Albany  Michael Coniglio  NOAA National Severe Storms Laboratory
Capturing Human Route Preferences from Track Information: New Results  Johnathan Gohde ∗ Mark Boddy Hazel Shackleton  Steve Johnston  Adventium Labs  111 3rd Ave S, Suite 100, Minneapolis MN 55401 USA  {ﬁrstname.lastname@adventiumlabs.com}  Abstract  In previous work, we described G2I2, a system that adjusts the cost function used by an off-road route planning system in order to more closely mimic the route choices made by hu- mans. In this paper, we report on an extension to G2I2, called GUIDE,
     SKILL: A System for Skill Identification and Normalization   Meng Zhao, Faizan Javed, Ferosh Jacob, Matt McNair   5550-A Peachtree Parkway, Norcross, GA 30092, USA   {Meng.Zhao, Faizan.Javed, Ferosh.Jacob, Matt.McNair}@CareerBuilder.com          Abstract   Named  Entity  Recognition  (NER)  and  Named  Entity  Nor- malization (NEN) refer to the recognition and normalization  of raw texts to known entities. From the perspective of re- cruitment innovation, professional skill characterization
On the Diagnosis of Cyber-Physical Production Systems:  State-of-the-Art and Research Agenda  Oliver Niggemann and Volker Lohweg  inIT–Institute for Industrial IT, University of Applied Sciences OWL, 32657 Lemgo, Germany  email: {oliver.niggemann, volker.lohweg}@hs-owl.de  Abstract  Cyber-Physical Production Systems (CPPSs) are in the focus of research, industry and politics: By applying new IT and new computer science solutions, production systems will become more adaptable, more resource ef- ﬁ
     GEF:  A Self-Programming Robot Using Grammatical Evolution   Charles Peabody and Jennifer Seitzer  Department of Mathematics and Computer Science   Rollins College, Winter Park, FL 32789   {cpeabody, jseitzer}@rollins.edu  http://myweb.rollins.edu/JSEITZER        Abstract  is   in  which  robots   Grammatical  Evolution  (GE)  that  area  of  genetic  algorithms  that  evolves  computer  programs  in  high-level  languages  possessing  a  BNF  grammar.    In  this  work,  we  present  GEF  
Thesis Summary:  Optimal Multi-Agent Pathﬁnding Algorithms  Guni Sharon  Abstract  The multi-agent path ﬁnding (MAPF) problem is a general- ization of the single-agent path ﬁnding problem for k > 1 agents. It consists of a graph and a number of agents. For each agent, a unique start state and a unique goal state are given, and the task is to ﬁnd paths for all agents from their start states to their goal states, under the constraint that agents cannot collide during their movements. In many cases
A Planning-Based Assistance System for Setting Up a Home Theater  Pascal Bercher and Felix Richter and Thilo H¨ornle and Thomas Geier and  Daniel H¨oller and Gregor Behnke and Florian Nothdurft and Frank Honold and  Wolfgang Minker and Michael Weber and Susanne Biundo  Faculty of Engineering and Computer Science  Ulm University, Germany  email: forename.surname@uni-ulm.de  Abstract  Modern technical devices are often too complex for many users to be able to use them to their full extent. Based o
Multi-Agent Dynamic Coupling for Cooperative Vehicles Modeling  Maxime Gu´eriau , Romain Billot , Nour-Eddin El Faouzi  Universit´e de Lyon, F-69000, Lyon, France  IFSTTAR, LICIT, F-69500, Bron  ENTPE, LICIT, F-69518, Vaulx en Velin Salima Hassas , Fr´ed´eric Armetta Universit´e de Lyon, F-69000, Lyon, France  LIRIS Laboratory, F-69000, Lyon  Abstract  Cooperative Intelligent Transportation Systems (C-ITS) are complex systems well-suited to a multi-agent mod- eling. We propose a multi-agent base
Salient Object Detection via Objectness Proposals  Tam V. Nguyen  Department for Technology, Innovation and Enterprise  Singapore Polytechnic, Singapore  Abstract  Salient object detection has gradually become a popu- lar topic in robotics and computer vision research. This paper presents a real-time system that detects salient object by integrating objectness, foreground and com- pactness measures. Our algorithm consists of four basic steps. First, our method generates the objectness map via ob
     DeepTutor: An Effective, Online Intelligent  Tutoring System that Promotes Deep Learning   Vasile Rus, Nobal B. Niraula, Rajendra Banjade  Department of Computer Science/Institute for Intelligent Systems   The University of Memphis   {vrus,nbnraula,rbanjade}@memphis.edu            Abstract   We  present  in  this  paper  an  innovative  solution  to  the  challenge of building effective educational technologies that  offer  tailored  instruction  to  each  individual  learner.  The  propose
Using Social Relationships to Control Narrative Generation  Julie Porteous and Fred Charles and Marc Cavazza  School of Computing, Teesside University {j.porteous,f.charles,m.o.cavazza}@tees.ac.uk  Middlesbrough, United Kingdom  Abstract  Narrative generation represents an application domain for AI planning where plan quality is related to prop- erties such as shape of plan trajectory. In our work we have developed a plan-based approach to narrative gen- eration that uses character relationships
Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence  Recommendation by Mining Multiple User Behaviors with Group Sparsity  Ting Yuan, Jian Cheng, Xi Zhang, Shuang Qiu, Hanqing Lu  National Lab of Pattern Recognition, Institute of Automation, Chinese Academy of Science  {tyuan, jcheng, xi.zhang, shuang.qiu, luhq}@nlpr.ia.ac.cn  Abstract  Recently, some recommendation methods try to im- prove the prediction results by integrating information from user’s multiple types of be
Huffman Coding for Storing Non-Uniformly Distributed  Messages in Networks of Neural Cliques  Bartosz Boguslawski1, Vincent Gripon2, Fabrice Seguin2, Fr´ed´eric Heitzmann1  1Univ. Grenoble Alpes, F-38000 Grenoble, France  CEA, LETI, MINATEC Campus, F-38054 Grenoble, France  {bartosz.boguslawski, frederic.heitzmann}@cea.fr, {vincent.gripon, fabrice.seguin}@telecom-bretagne.eu  2TELECOM Bretagne, Electronics Department, 29238 Brest, France  Abstract  Associative memories are data structures that a
An Agent-Based Model Studying the Acquisition of a Language System of Logical Constructions  Joseﬁna Sierra-Santib´a˜nez Technical University of Catalonia  Jordi Girona Sangado, 1–3, 08034 Barcelona, Spain  E-mail: Maria.Joseﬁna.Sierra@upc.edu  Abstract  This paper presents an agent-based model that studies the emergence and evolution of a language system of logical con- structions, i.e. a vocabulary and a set of grammatical con- structions that allows the expression of logical combinations of c
Social Planning: Achieving Goals by Altering Others’ Mental States  Chris Pearce and Ben Meadows and Pat Langley and Mike Barley  Department of Computer Science, University of Auckland  Private Bag 92019, Auckland 1142, New Zealand  Abstract  In this paper, we discuss a computational approach to the cognitive task of social planning. First, we specify a class of planning problems that involve an agent who attempts to achieve its goals by altering other agents’ mental states. Next, we describe SF
Placement of Loading Stations for Electric Vehicles: No Detours Necessary!  Stefan Funke and Andr´e Nusser  Universit¨at Stuttgart  Institut f¨ur Formale Methoden der Informatik  70569 Stuttgart, Germany  {funke,nusser}@fmi.uni-stuttgart.de  Sabine Storandt  Albert-Ludwigs-Universit¨at Freiburg  Institut f¨ur Informatik  79110 Freiburg, Germany storandt@cs.uni-freiburg.de  Abstract  Compared to conventional cars, electric vehicles still suffer from a considerably shorter cruising range. Combined
Bounding the Support Size in Extensive Form Games with Imperfect Information  Martin Schmid  Charles University In Prague  lifrordi@gmail.com  Matej Moravcik  Charles University In Prague moravcikmatej42@gmail.com  Milan Hladik  Charles University In Prague  hladik@kam.mff.cuni.cz  Abstract  It is a well known fact that in extensive form games with perfect information, there is a Nash equilibrium with support of size one. This doesn’t hold for games with imperfect information, where the size of 
Managing Change in Graph-Structured Data Using Description Logics  Shqiponja Ahmetaj  Institute of Information Systems  Vienna Univ. of Technology, Austria  Diego Calvanese KRDB Research Centre  Free Univ. of Bozen-Bolzano, Italy  Magdalena Ortiz Mantas ˇSimkus  Institute of Information Systems  Vienna Univ. of Technology, Austria  Abstract  In this paper, we consider the setting of graph-structured data that evolves as a result of operations carried out by users or applications. We study differ
Capturing Relational Schemas and Functional Dependencies in RDFS  Diego Calvanese KRDB Research Centre  Free Univ. of Bozen-Bolzano, Italy  calvanese@inf.unibz.it  Wolfgang Fischl, Reinhard Pichler, Emanuel Sallinger, Mantas ˇSimkus  Institute of Information Systems  Vienna Univ. of Technology, Austria  {wﬁschl, pichler, sallinger, simkus}@dbai.tuwien.ac.at  Abstract  Mapping relational data to RDF is an important task for the development of the Semantic Web. To this end, the W3C has recently re
Data Quality in Ontology-Based Data Access:  The Case of Consistency  Marco Console, Maurizio Lenzerini  Dipartimento di Ingegneria Informatica, Automatica e  Gestionale “Antonio Ruberti’  Sapienza Universit`a di Roma, Roma, Italy  lastname@dis.uniroma1.it  Abstract  Ontology-based data access (OBDA) is a new paradigm aim- ing at accessing and managing data by means of an ontol- ogy, i.e., a conceptual representation of the domain of inter- est in the underlying information system. In the last y
Learning Scripts as Hidden Markov Models  J. Walker Orr, Prasad Tadepalli, Janardhan Rao Doppa,  Xiaoli Fern, Thomas G. Dietterich  {orr,tadepall,doppa,xfern,tgd}@eecs.oregonstate.edu  School of EECS, Oregon State Univserity, Corvallis OR 97331  Abstract  Scripts have been proposed to model the stereotypical event sequences found in narratives. They can be ap- plied to make a variety of inferences including ﬁlling gaps in the narratives and resolving ambiguous refer- ences. This paper proposes t
(cid:38)(cid:75)(cid:76)(cid:81)(cid:72)(cid:86)(cid:72) (cid:50)(cid:89)(cid:72)(cid:85)(cid:87) (cid:51)(cid:85)(cid:82)(cid:81)(cid:82)(cid:88)(cid:81) (cid:53)(cid:72)(cid:86)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:29)  (cid:36) (cid:37)(cid:76)(cid:79)(cid:76)(cid:81)(cid:74)(cid:88)(cid:68)(cid:79) (cid:36)(cid:83)(cid:83)(cid:85)(cid:82)(cid:68)(cid:70)(cid:75) (cid:38)(cid:75)(cid:72)(cid:81) (cid:38)(cid:75)(cid:72)(cid:81) (cid:68)(cid:81)(cid:71) (cid:57)(cid:76)(
(cid:38)(cid:75)(cid:76)(cid:81)(cid:72)(cid:86)(cid:72) (cid:61)(cid:72)(cid:85)(cid:82) (cid:51)(cid:85)(cid:82)(cid:81)(cid:82)(cid:88)(cid:81) (cid:53)(cid:72)(cid:86)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:29) (cid:36)(cid:81) (cid:56)(cid:81)(cid:86)(cid:88)(cid:83)(cid:72)(cid:85)(cid:89)(cid:76)(cid:86)(cid:72)(cid:71) (cid:36)(cid:83)(cid:83)(cid:85)(cid:82)(cid:68)(cid:70)(cid:75)  (cid:38)(cid:82)(cid:80)(cid:69)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74) (cid:53)(ci
Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence  Kernelized Bayesian Transfer Learning⇤  mehmet.gonen@sagebase.org  Mehmet G¨onen† Sage Bionetworks  Seattle, WA 98109, USA  Abstract  Transfer learning considers related but distinct tasks deﬁned on heterogenous domains and tries to transfer knowledge between these tasks to improve generaliza- tion performance. It is particularly useful when we do not have sufﬁcient amount of labeled training data in some tasks, which m
Non-Convex Feature Learning via (cid:96)p,∞ Operator  Deguang Kong and Chris Ding  Department of Computer Science & Engineering,  University of Texas, Arlington,  500 UTA Blvd, Arlington, TX 76010  doogkong@gmail.com; chqding@uta.edu  Abstract  We present a feature selection method for solving sparse reg- ularization problem, which has a composite regularization of (cid:96)p norm and (cid:96)∞ norm. We use proximal gradient method to solve this (cid:96)p,∞ operator problem, where a simple but ef
Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence  Pairwise-Covariance Linear Discriminant Analysis  Deguang Kong and Chris Ding  Department of Computer Science & Engineering  University of Texas, Arlington,  500 UTA Blvd, TX 76010  doogkong@gmail.com; chqding@uta.edu  Abstract  In machine learning, linear discriminant analysis (LDA) is a popular dimension reduction method. In this paper, we ﬁrst provide a new perspective of LDA from an information the- ory perspective.
Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence  Scalable Sparse Covariance Estimation via Self-Concordance  Anastasios Kyrillidis, Rabeeh Karimi Mahabadi,  Quoc Tran Dinh and Volkan Cevher⇤  ´Ecole Polytechnique F´ed´erale de Lausanne  {anastasios.kyrillidis,rabeeh.karimimahabadi,quoc.trandinh,volkan.cevher}@epﬂ.ch  Abstract  We consider the class of convex minimization prob- lems, composed of a self-concordant function, such as the log det metric, a convex data ﬁdel
A Hybrid Grammar-Based Approach for  Learning and Recognizing Natural Hand Gestures  Amir Sadeghipour and Stefan Kopp  Faculty of Technology, Center of Excellence ‘Cognitive Interaction Technology’ (CITEC),  Bielefeld University, P.O. Box 100131, D-33501 Bielefeld, Germany  Abstract  In this paper, we present a hybrid grammar formalism designed to learn structured models of natural iconic gesture performances that allow for compressed repre- sentation and robust recognition. We analyze a dataset
Grandpa Hates Robots — Interaction Constraints for  Planning in Inhabited Environments  Uwe K¨ockemann and Federico Pecora and Lars Karlsson Center for Applied Autonomous Sensor Systems, ¨Orebro University, Sweden  {uwe.koeckemann,federico.pecora,lars.karlsson}@oru.se  Abstract  Consider a family whose home is equipped with sev- eral service robots. The actions planned for the robots must adhere to Interaction Constraints (ICs) relating them to human activities and preferences. These con- strain
Tree-Based On-Line Reinforcement Learning  Andr´e da Motta Salles Barreto  Laborat´orio Nacional de Computac¸˜ao Cient´ıﬁca  Petr´opolis, RJ, Brazil  Abstract  Fitted Q-iteration (FQI) stands out among reinforce- ment learning algorithms for its ﬂexibility and ease of use. FQI can be combined with any regression method, and this choice determines the algorithm’s statistical and computational properties. The combination of FQI with an ensemble of regression trees gives rise to an al- gorithm, FQI
Diagnosing Analogue Linear Systems  Using Dynamic Topological Reconﬁguration  Alexander Feldman  General Diagnostics  Burgwal 47  2611GG, Delft, The Netherlands  email: alex@general-diagnostics.com  Abstract  Fault diagnosis of analogue linear systems poses many chal- lenges, such as the size of the search space that must be explored and the possibility of simulation instabilities in- troduced by particular fault classes. We study a novel al- gorithm that addresses both problems. This algorithm 
A Reasoner for the RCC-5 and RCC-8  Calculi Extended with Constants  Stella Giannakopoulou, Charalampos Nikolaou, Manolis Koubarakis  National and Kapodistrian University of Athens, Greece  {sgian,charnik,koubarak}@di.uoa.gr  Abstract  The problem of checking the consistency of spatial calculi that contain both unknown and known entities (constants, i.e., real geometries) has recently been studied. Until now, all the approaches are theoretical and no implementation has been proposed. In this pap
A Support-Based Algorithm for  the Bi-Objective Pareto Constraint  Renaud Hartert∗ and Pierre Schaus  UCLouvain, ICTEAM, Place Sainte Barbe 2,  1348 Louvain-la-Neuve, Belgium  {renaud.hartert, pierre.schaus}@uclouvain.be  Abstract  Bi-Objective Combinatorial Optimization problems are ubiquitous in real-world applications and designing approaches to solve them efﬁciently is an important research area of Artiﬁcial Intelligence. In Constraint Programming, the recently introduced bi-objective Pareto
Fast Consistency Checking of Very Large Real-World RCC-8 Constraint Networks Using Graph Partitioning  Charalampos Nikolaou and Manolis Koubarakis  National and Kapodistrian University of Athens, Greece  {charnik,koubarak}@di.uoa.gr  Abstract  We present a new reasoner for RCC-8 constraint networks, called gp-rcc8, that is based on the patchwork property of path-consistent tractable RCC-8 networks and graph parti- tioning. We compare gp-rcc8 with state of the art reason- ers that are based on co
Grounding Acoustic Echoes  in Single View Geometry Estimation  Wajahat Hussain, Javier Civera, Luis Montano  Robotics, Perception and Real Time Group, I3A  University of Zaragoza, Spain  {hussain, jcivera, montano}@unizar.es  Abstract  Extracting the 3D geometry plays an important part in scene understanding. Recently, robust visual descriptors are proposed for extracting the indoor scene layout from a passive agent’s perspective, speciﬁcally from a single image. Their robustness is mainly due t
Predictive Models for Determining If and When to Display Online Lead Forms  Timothy Chan, Joseph I, Carlos Macasaet, Daniel Kang, Robert M. Hardy,  Carlos Ruiz, Rigel Porras, Brian Baron, Karim Qazi, Padraic Hannon, Tomonori Honda  Propensity Modeling Team  Edmunds.com  1620 26th St, 4th Floor Santa Monica, CA 90404  Abstract  This paper will demonstrate a machine learning appli- cation for predicting positive lead conversion events on the Edmunds.com website, an American destination for car sho
(cid:3)  Proceedings of the Twenty-Sixth Annual Conference on Innovative Applications of Artificial Intelligence  (cid:3) (cid:3)  (cid:40)(cid:81)(cid:74)(cid:76)(cid:81)(cid:72)(cid:72)(cid:85)(cid:76)(cid:81)(cid:74)(cid:3)(cid:58)(cid:82)(cid:85)(cid:78)(cid:86)(cid:3)(cid:54)(cid:70)(cid:75)(cid:72)(cid:71)(cid:88)(cid:79)(cid:76)(cid:81)(cid:74)(cid:3)(cid:73)(cid:82)(cid:85)(cid:3)(cid:3)  (cid:43)(cid:82)(cid:81)(cid:74)(cid:3)(cid:46)(cid:82)(cid:81)(cid:74)(cid:182)(cid:86)(cid:3)(cid:
      Pattern Discovery in Protein Networks Reveals High-Confidence   Predictions of Novel Interactions    Hazem Radwan Ahmed and Janice I. Glasgow   School of Computing, Queen‟s University, 25 Union Street, Kingston, Ontario K7L 3N6, Canada   {hazem, janice}@cs.queensu.ca            Abstract   technique   robust  and   low-cost  optimization   Pattern discovery in protein interaction networks can reveal  crucial  biological  knowledge  on  the  inner  workings  of  cellular  machinery.  Althoug
      A Smart Range Helping Cognitively-Impaired Persons Cooking     Bruno Bouchard, Kevin Bouchard, Abdenour Bouzouane   Université du Québec à Chicoutimi (UQAC), 555 boul. Université, Saguenay (QC) Canada, G7H 2B1   {Bruno.Bouchard, Kevin.Bouchard, Abdenour.Bouzouane}@uqac.ca            Abstract   their  cognitive   impairment.  Using   People  suffering  from  a  loss  of  autonomy  caused  by  a  cognitive  deficit  generally  have  to  perform  important  daily  tasks  (such  as  cooking)  
Swissnoise: Online Polls with Game-Theoretic Incentives  Florent Garcin and Boi Faltings  Artiﬁcial Intelligence Lab  Ecole Polytechnique F´ed´erale de Lausanne  Switzerland  {ﬁrstname.lastname}@epﬂ.ch  Abstract  There is much interest in crowdsourcing information that is distributed among many individuals, such as the likelihood of future events, election outcomes, the quality of products, or the consequence of a decision. To obtain accurate out- comes, various game-theoretic incentive schemes 
   A Schedule Optimization Tool for    Destructive and Non-Destructive Vehicle Tests   Jeremy Ludwig, Annaka Kalton, and    Robert Richards   Brian Bautsch, Craig Markusic, and    J. Schumacher   Stottler Henke Associates, Inc.   San Mateo, California   {ludwig, kalton, richards} @ stottlerhenke.com    Honda R&D Americas, Inc.   Raymond, OH   { CMarkusic, Bbautsch, JSchumacher } @ oh.hra.com  Abstract   Whenever an auto manufacturer refreshes an existing car or  truck  model  or  builds  a  new 
Proceedings of the Fifth Symposium on Educational Advances in Artificial Intelligence  Shallow Blue:  Lego-Based Embodied AI as a   Platform for Cross-Curricular Project Based Learning    Robert I. Selkowitz* and Debra T. Burhans+  Canisius College Departments of Physics* and Computer Science+   2001 Main Street, Buffalo, NY  14208  {selkowir,burhansd}@canisius.edu                 Abstract   We  report  on  Shallow  Blue  (SB),  an  autonomous  chess  agent  constructed  by  a  small  group  of 
Proceedings of the Fifth Symposium on Educational Advances in Artificial Intelligence  (cid:3) (cid:3)  (cid:55)(cid:72)(cid:68)(cid:70)(cid:75)(cid:76)(cid:81)(cid:74)(cid:3)(cid:58)(cid:76)(cid:87)(cid:75)(cid:3)(cid:58)(cid:68)(cid:87)(cid:86)(cid:82)(cid:81)(cid:3)  (cid:48)(cid:76)(cid:70)(cid:75)(cid:68)(cid:72)(cid:79)(cid:3)(cid:58)(cid:82)(cid:79)(cid:79)(cid:82)(cid:90)(cid:86)(cid:78)(cid:76)   (cid:53)(cid:82)(cid:86)(cid:72)(cid:16)(cid:43)(cid:88)(cid:79)(cid:80)(cid:68)(cid:81)(ci
Solving Semantic Problems Using  Contexts Extracted from Knowledge Graphs  Adrian Boteanu aboteanu@wpi.edu  Worcester Polytechnic Institute  Abstract  This thesis seeks to address word reasoning problems from a semantic standpoint, proposing a uniform ap- proach for generating solutions while also providing human-understandable explanations. Current state of the art solvers of semantic problems rely on traditional machine learning methods. Therefore their results are not easily reusable by algor
Proceedings of the Nineteenth AAAI/SIGAI Doctoral Consortium  (cid:3)  (cid:36)(cid:81)(cid:68)(cid:79)(cid:82)(cid:74)(cid:92)(cid:3)(cid:55)(cid:88)(cid:87)(cid:82)(cid:85)(cid:29)(cid:3)(cid:36)(cid:3)(cid:55)(cid:88)(cid:87)(cid:82)(cid:85)(cid:76)(cid:81)(cid:74)(cid:3)(cid:54)(cid:92)(cid:86)(cid:87)(cid:72)(cid:80)(cid:3)(cid:73)(cid:82)(cid:85)(cid:3)(cid:51)(cid:85)(cid:82)(cid:80)(cid:82)(cid:87)(cid:76)(cid:81)(cid:74)(cid:3)(cid:3)  (cid:38)(cid:82)(cid:81)(cid:70)(cid:72)(cid:83)(ci
Monte Carlo Simulation Adjusting  Nobuo Araki*, Masakazu Muramatsu, Kunihito Hoki, Satoshi Takahashi Graduate School of Informatics and Engineering, The University of Electro-Communications  1-5-1 Chofugaoka, Chofu, Tokyo 182-8585 Japan  *a1341001@edu.cc.uec.ac.jp  http://gi.cs.uec.ac.jp:10140/Araki complementary/ReadMe.html  Abstract  Simulation Adjusting  In this paper, we propose a new learning method sim- ulation adjusting that adjusts simulation policy to im- prove the move decisions of the
Partial Satisfaction Planning Under Time Uncertainty  with Control on When Objectives Can Be Aborted  Sylvain Labranche and ´Eric Beaudry  Universit´e du Qu´ebec `a Montr´eal  320 Sainte-Catherine Est Street, Montreal, QC H2X 1L7  (514) 987-3000 # 7890  http://gdac2.uqam.ca/~sylvain/  labranche.sylvain@courrier.uqam.ca, eric.beaudry@uqam.ca  Introduction  Classical planners consider a goal as a non-separable set of objectives, i.e. a goal can be either satisﬁed or not satisﬁed in a particular st
     LSDH: A Hashing Approach for    Large-Scale Link Prediction in Microblogs   Dawei Liu§, Yuanzhuo Wang†, Yantao Jia†, Jingyuan Li†, Zhihua Yu§,†  §Institute of Network Technology, Institute of Computing Technology(Yantai), CAS, Beijing, P.R. China   †Institute of Computing Technology, CAS, Beijing, P.R. China   liudw@int-yt.com, {wangyuanzhuo, jiayuantao, lijingyuan, yzh}@ict.ac.cn        Abstract   One challenge of link prediction in online social networks is  the large scale of many such n
Preventing Unraveling in Social Networks Gets Harder∗  Rajesh Chitnis† and Fedor V. Fomin‡ and Petr A. Golovach‡  Abstract  The behavior of users in social networks is often observed to be affected by the actions of their friends. Bhawalkar et al. (Bhawalkar et al. 2012) introduced a formal mathemati- cal model for user engagement in social networks where each individual derives a beneﬁt proportional to the number of its friends which are engaged. Given a threshold degree k the equilibrium for t
A Tractable Leader-Follower MDP  Model for Animal Disease Management  R´egis Sabbadin  INRA-MIAT, Toulouse, France Regis.Sabbadin@toulouse.inra.fr  Abstract  Sustainable animal disease management requires to design and implement control policies at the regional scale. How- ever, for diseases which are not regulated, individual farmers are responsible for the adoption and successful application of control policies at the farm scale. Organizations (groups of farmers, health institutions...) may tr
Enabling E-Mobility: Facility Location for Battery Loading Stations  Sabine Storandt  Albert-Ludwigs-Universit¨at Freiburg  Institut f¨ur Informatik  79110 Freiburg, Germany  storandt@informatik.uni-freiburg.de  Stefan Funke Universit¨at Stuttgart  Institut f¨ur Formale Methoden der Informatik  70569 Stuttgart, Germany funke@fmi.uni-stuttgart.de  Abstract  The short cruising range due to the limited battery supply of current Electric Vehicles (EVs) is one of the main obstacles for a complete tra
Multi-Target Detection and Recognition by UAVs Using Online POMDPs  Caroline P. Carvalho Chanel1,2 and Florent Teichteil-K¨onigsbuch2 and Charles Lesire2  1 Universit´e de Toulouse – ISAE – Institut Sup´erieur de l’A´eronautique et de l’Espace  2 Onera – The French Aerospace Lab, F-31055, Toulouse, France  name.surname@onera.fr  Abstract  This paper tackles high-level decision-making tech- niques for robotic missions, which involve both ac- tive sensing and symbolic goal reaching, under uncer- t
Integrating Digital Pens in Breast Imaging for Instant Knowledge Acquisition  Daniel Sonntag, Markus Weber  German Research Center for AI  Stuhlsatzenhausweg 3  66123 Saarbr¨ucken, Germany  Matthias Hammon, Alexander Cavallaro Imaging Science Institute, University Hospital Erlangen  Maximiliansplatz 1  91054 Erlangen, Germany  Abstract  Future radiology practices assume that the radiology reports should be uniform, comprehensive, and easily managed. This means that reports must be “readable” to 
     Scalable Models for Patterns of Life  J.T. Folsom-Kovarika, Sae Schatzb, Randolph M. Jonesa, Kathleen Bartlettb,  Robert E. Wray a    aSoar Technology, Inc.; bMESH Solutions, LLC, a DSCI Company   {jeremiah | rjones | wray}@soartech.com; {sschatz | kbartlett}@mesh.dsci.com          Patterns of Life    Patterns  of  life  (POL)  are  emergent  properties  of  com- plex  social  systems  such  as  neighborhoods  or  even  cities  (Schatz et al. 2012). In such systems, observable regulari- tie
     Student-Friendly Java-Based Multiagent Event Handling    Dan Tappan   Department of Computer Science  Eastern Washington University   dtappan@ewu.edu          Abstract   This  work  describes  a  student-friendly,  pedagogy-oriented  event-handling  system  for  managing  multiagent  AI  simulations in a classroom environment.   Problem     Solution   This  work  in  progress  introduces  an  expressive  Java  API  with a reasonable learning curve for handling the types of  concurrent event
   Phase Transition and Network Structure in Realistic SAT Problems    Soumya C. Kambhampati & Thomas Liu   Peggy Payne Academy, McClintock High School, 1830 E Del Rio Dr Tempe, AZ 85282    {budugu2z, thomasliu02}@gmail.com  Full report at http://arxiv.org/abs/1304.0145    Introduction     Previous  research  (Gomes  and  Selman  2005)  has  shown  that  3-SAT  problems  are  easy  to  solve  both  when  the  “constrainedness” (γ, the ratio of the number of clauses to  the  number  of  variables
     Efficient Algorithms for Strong Local Consistencies    in Constraint Satisfaction Problems   Anastasia Paparrizou   Dept. of Informatics and Telecommunications Engineering   University of Western Macedonia, Greece    apaparrizou@uowm.gr           Abstract   The existing complete methods for solving Constraint Satis- faction Problems (CSPs) are usually based on a combination  of exhaustive search and constraint propagation techniques  for  the  reduction  of  the  search  space.  Such  propa
The Wisdom of Crowds in Bioinformatics: What Can We Learn  (and Gain) from Ensemble Predictions?  Mariana R. Mendoza  PhD Thesis Supervisor: Ana L. C. Bazzan  PPGC, Instituto de Inform´atica, UFRGS, Porto Alegre, Brazil  Abstract  The combination of distinct algorithms expertise to im- prove prediction accuracy, inspired by the theory of wis- dom of crowds, has been increasingly discussed in liter- ature. However, its application to bioinformatics-related tasks is still in its infancy. This thes
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  (cid:1) (cid:1)  (cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:4)(cid:8)(cid:4)(cid:3)(cid:1)(cid:9)(cid:10)(cid:6)(cid:11)(cid:4)(cid:6)(cid:12)(cid:1)(cid:13)(cid:14)(cid:5)(cid:15)(cid:1)(cid:16)(cid:5)(cid:7)(cid:5)(cid:15)(cid:13)(cid:12)(cid:5)(cid:6)(cid:5)(cid:13)(cid:17)(cid:18)(cid:1)(cid:19)(cid:3)(cid:10)(cid:20)(cid:5)(cid:21)(cid:4)(cid:3)(cid:1)(cid:16)(cid:22)(cid:23)(cid:5)(cid:15)(cid:6)(cid:5)(
Survival Prediction by an Integrated Learning Criterion  on Intermittently Varying Healthcare Data  Jianfei Zhang1, Lifei Chen2, Alain Vanasse3, Josiane Courteau3, Shengrui Wang1  1PROSPCTUS Lab, Department of Computer Science, University of Sherbrooke, Canada  2School of Mathematics and Computer Science, Fujian Normal University, China  3PRIMUS Research Group, Department of Family Medicine, University of Sherbrooke, Canada  {jianfei.zhang, alain.vanasse, josiane.courteau, shengrui.wang}@usherbr
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  Reﬁning Subgames in Large Imperfect Information Games  Matej Moravcik, Martin Schmid,  Karel Ha, Milan Hladik Charles University In Prague  {moravcim, schmidm, karelha, hladik}  @kam.mff.cuni.cz,  Stephen J. Gaukrodger  Koypetition  stephen@koypetition.com  Abstract  The leading approach to solving large imperfect information games is to pre-calculate an approximate solution using a simpliﬁed abstraction of the fu
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  Lift-Based Bidding in Ad Selection  Jian Xu∗, Xuhui Shao, Jianjie Ma, Kuang-chih Lee, Hang Qi, Quan Lu  ∗ TouchPal Inc., 1172 Castro St, Mountain View, CA 94040  Yahoo Inc., 701 First Ave, Sunnyvale, CA 94089  jian.xu@cootek.cn, {xshao,jianma,kclee,hangqi,qlu}@yahoo-inc.com  Abstract  Real-time bidding has become one of the largest on- line advertising markets in the world. Today the bid price per ad impression is
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  Minimizing User Involvement for Learning  Human Mobility Patterns from Location Traces  Basma Alharbi, Abdulhakim Qahtan, Xiangliang Zhang Computer, Electrical and Mathematical Sciences & Engineering Division  King Abdullah University of Science & Technology (KAUST)  Thuwal, 23955, Saudi Arabia  Abstract  Utilizing trajectories for modeling human mobility often in- volves extracting descriptive features for each i
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  SDDs Are Exponentially More Succinct than OBDDs  Simone Bova  Technische Universit¨at Wien  Favoritenstraße 9–11 1040 Wien (Austria)  Abstract  Introduced by Darwiche (2011), sentential decision diagrams (SDDs) are essentially as tractable as ordered binary decision diagrams (OBDDs), but tend to be more succinct in practice. This makes SDDs a prominent representation language, with many applications in artiﬁcial i
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  Locally Adaptive Translation for Knowledge Graph Embedding  Yantao Jia1, Yuanzhuo Wang1, Hailun Lin2, Xiaolong Jin1, Xueqi Cheng1  1CAS Key Laboratory of Network Data Science and Technology,  Institute of Computing Technology, Chinese Academy of Science, Beijing, China  2Institute of Information Engineering,  Chinese Academy of Science, Beijing, China  Abstract  Knowledge graph embedding aims to represent entities
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  Causal Explanation Under Indeterminism: A Sampling Approach  Christopher A Merck and Samantha Kleinberg  Stevens Institute of Technology  Hoboken NJ  Abstract  One of the key uses of causes is to explain why things happen. Explanations of speciﬁc events, like an individual’s heart at- tack on Monday afternoon or a particular car accident, help assign responsibility and inform our future decisions. Com- putational 
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  On the Performance of GoogLeNet and AlexNet Applied to Sketches  Pedro Ballester and Ricardo Matsumura Araujo  Center for Technological Development, Federal University of Pelotas  Pelotas, RS, Brazil  pedballester@gmail.com, ricardo@inf.ufpel.edu.br  Abstract  This work provides a study on how Convolutional Neural Networks, trained to identify objects primarily in photos, per- form when applied to more abstract re
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  Semisupervised Autoencoder for Sentiment Analysis  Shuangfei Zhai, Zhongfei (Mark) Zhang Computer Science Department, Binghamton University  4400 Vestal Pkwy E, Binghamton, NY 13902  szhai2@binghamton.edu zhongfei@cs.binghamton.edu  Abstract  In this paper, we investigate the usage of autoencoders in modeling textual data. Traditional autoencoders suffer from at least two aspects: scalability with the high dimensi
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  The Ostomachion Process  Machine Learning Research Group, National ICT Australia, Eveleigh, NSW 2015, Australia  Xuhui Fan, Bin Li, Yi Wang, Yang Wang, Fang Chen {xuhui.fan, bin.li, yi.wang, yang.wang, fang.chen}@nicta.com.au  Abstract  Stochastic partition processes for exchangeable graphs pro- duce axis-aligned blocks on a product space. In relational modeling, the resulting blocks uncover the underlying inter- 
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  Reinforcement Learning with Parameterized Actions  Warwick Masson and Pravesh Ranchod School of Computer Science and Applied Mathematics  University of Witwatersrand Johannesburg, South Africa  warwick.masson@students.wits.ac.za  pravesh.ranchod@wits.ac.za  Abstract  We introduce a model-free algorithm for learning in Markov decision processes with parameterized actions—discrete ac- tions with continuous parameter
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  Viral Clustering: A Robust Method to Extract  Structures in Heterogeneous Datasets  Vahan Petrosyan and Alexandre Proutiere  Royal Institute of Technology (KTH)  {vahanp, alepro}@kth.se  Abstract  Cluster validation constitutes one of the most challenging problems in unsupervised cluster analysis. For example, iden- tifying the true number of clusters present in a dataset has been investigated for decades, and is 
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  Semi-Supervised Dictionary Learning via Structural Sparse Preserving  Di Wang, Xiaoqin Zhang∗, Mingyu Fan and Xiuzi Ye College of Mathematics & Information Science, Wenzhou University  wangdi@amss.ac.cn, zhangxiaoqinnan@gmail.com, {fanmingyu, yexiuzi}@wzu.edu.cn  Zhejiang, China  Abstract  While recent techniques for discriminative dictionary learn- ing have attained promising results on the classiﬁcation tasks, t
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  An Efﬁcient Time Series Subsequence Pattern Mining and Prediction Framework with an Application to Respiratory Motion Prediction  Shouyi Wang, Kin Ming Kam Department of Industrial, Manufacturing  & Systems Engineering, University of Texas  at Arlington, TX 76019  Cao Xiao,∗ Stephen Bowen,+ W. Chaovalitwongse∗  ∗Department of Industrial and Systems Engineering +Department of Radiation Oncology,University of  Washi
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  Constrained Submodular Minimization  for Missing Labels and Class Imbalance in Multi-Label Learning  Baoyuan Wu  KAUST, Saudi Arabia  Siwei Lyu  SUNY-Albany, NY USA  Bernard Ghanem KAUST, Saudi Arabia  Abstract  In multi-label learning, there are two main challenges: miss- ing labels and class imbalance (CIB). The former assumes that only a partial set of labels are provided for each train- ing instance while othe
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  A Proximal Alternating Direction Method  for Semi-Deﬁnite Rank Minimization  Ganzhao Yuan and Bernard Ghanem  King Abdullah University of Science and Technology (KAUST), Saudi Arabia  yuanganzhao@gmail.com, bernard.ghanem@kaust.edu.sa  Abstract  Semi-deﬁnite rank minimization problems model a wide range of applications in both signal processing and machine learning ﬁelds. This class of problem is NP-hard in genera
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  Emergence of Social Punishment and Cooperation  through Prior Commitments  The Anh Han  School of Computing and Digital Futures Institute  Teesside University, UK  Abstract  Social punishment, whereby cooperators punish defec- tors, has been suggested as an important mechanism that promotes the emergence of cooperation or main- tenance of social norms in the context of the one-shot (i.e. non-repeated) interaction.
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  Single or Multiple? Combining Word Representations  Independently Learned from Text and WordNet  Josu Goikoetxea, Eneko Agirre, and Aitor Soroa  IXA NLP Group  University of the Basque Country  Donostia, Basque Country  {josu.goikoetxea,e.agirre,a.soroa}@ehu.eus  Abstract  Text and Knowledge Bases are complementary sources of in- formation. Given the success of distributed word represen- tations learned from text,
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  Agreement on Target-Bidirectional LSTMs  for Sequence-to-Sequence Learning  Lemao Liu, Andrew Finch, Masao Utiyama, Eiichiro Sumita National Institute of Information and Communications Technology (NICT)  3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan  {lmliu,ﬁrst.last}@nict.go.jp  Abstract  Recurrent neural networks, particularly the long short-term memory networks, are extremely appealing for sequence-to- se
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  Evaluation of Semantic Dependency Labeling Across Domains  Svetlana Stoyanchev  Interactions Labs  25 Broadway, New York, NY  sstoyanchev@interactions.com  Amanda Stent  Yahoo Labs  229 W. 43rd St. New York, NY  stent@yahoo-inc.com  Srinivas Bangalore  Interactions Labs 41 Spring Street,  Murray Hill, NJ 07974  sbangalore@interactions.com  Abstract  One of the key concerns in computational semantics is to con- str
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  Minimally-Constrained Multilingual Embeddings  via Artiﬁcial Code-Switching  Michael Wick  Oracle Labs  michael.wick@oracle.com  Pallika Kanani  Oracle Labs  pallika.kanani@oracle.com  Adam Pocock  Oracle Labs  adam.pocock@oracle.com  Abstract  We present a method that consumes a large corpus of multi- lingual text and produces a single, uniﬁed word embedding in which the word vectors generalize across languages. 
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  A Semi-Supervised Learning Approach to Why-Question Answering  Jong-Hoon Oh∗ Kentaro Torisawa† Chikara Hashimoto‡ Ryu Iida§  Masahiro Tanaka¶  Julien Kloetzer(cid:3)  National Institute of Information and Communications Technology (NICT), Kyoto, 619-0289, Japan  {∗rovellia,†torisawa,‡ch,§ryu.iida,¶mtnk,(cid:3)julien}@nict.go.jp  Abstract  We propose a semi-supervised learning method for im- proving why-question an
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  Separators and Adjustment Sets in Markov Equivalent DAGs  Benito van der Zander∗ and Maciej Li´skiewicz Institute of Theoretical Computer Science, University of L¨ubeck  Ratzeburger Allee 160, 23538 L¨ubeck, Germany  {benito,liskiewi}@tcs.uni-luebeck.de  Abstract  In practice the vast majority of causal effect estimations from observational data are computed using adjustment sets which avoid confounding by adjusti
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  Multi-View 3D Human Tracking in Crowded Scenes  Xiaobai Liu  Department of Computer Science, San Diego State University  GMCS Building, Campanile Drive  San Diego, CA 92182  Abstract  This paper presents a robust multi-view method for tracking people in crowded 3D scene. Our method distinguishes it- self from previous works in two aspects. Firstly, we deﬁne a set of binary spatial relationships for individual subj
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  QART : A System for Real-Time Holistic Quality  Assurance for Contact Center Dialogues  Shourya Roy, Ragunathan Mariappan, Sandipan Dandapat, Saurabh Srivastava, Sainyam Galhotra and Balaji Peddamuthu  Xerox Research Centre India  Bangalore, India  {ﬁrstname.lastname@xerox.com}  Abstract  Quality assurance (QA) and customer satisfaction (C-Sat) analysis are two commonly used practices to measure good- ness of dial
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)       Adaptable Regression Method for Ensemble Consensus Forecasting   John K. Williams, Peter P. Neilley, Joseph P. Koval and Jeff McDonald   The Weather Company, Andover, MA   john.williams@weather.com           Abstract   Accurate weather forecasts enhance sustainability by facili- tating  decision  making  across  a  broad  range  of  endeavors  including  public  safety,  transportation,  energy  generation  a
Proceedings of the Twenty-Eighth AAAI Conference on Innovative Applications (IAAI-16)  Automated Regression Testing Using Constraint Programming∗  Arnaud Gotlieb,1 Mats Carlsson,2 Marius Liaaen,3  Dusica Marijan,1 Alexandre P´etillon1  1 SIMULA, Norway (simula.no)  2 SICS, Sweden (sics.se)  3 CISCO, Norway (cisco.com)  Abstract  In software validation, regression testing aims to check the absence of regression faults in new releases of a software system. Typically, test cases used in regression 
Proceedings of the Twenty-Eighth AAAI Conference on Innovative Applications (IAAI-16)  (cid:2) (cid:2)  (cid:3)(cid:2)(cid:4)(cid:5)(cid:6)(cid:6)(cid:7)(cid:8)(cid:2)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:2)(cid:9)(cid:13)(cid:6)(cid:7)(cid:15)(cid:2)(cid:3)(cid:16)(cid:16)(cid:11)(cid:13)(cid:10)(cid:17)(cid:18)(cid:2)(cid:19)(cid:13)(cid:2)(cid:20)(cid:8)(cid:21)(cid:7)(cid:11)(cid:2)(cid:22)(cid:5)(cid:23)(cid:7)(cid:24)(cid:17)(cid:10)(cid:15)(cid:7)(cid:24)(cid:2)(cid:21)(cid:
Proceedings of the Sixth Symposium on Educational Advances in Artificial Intelligence (EAAI-16)       BeeMo, a Monte Carlo Simulation Agent   for Playing Parameterized Poker Squares   Karo Castro-Wunsch, William Maga, Calin Anton   MacEwan University, Edmonton, Alberta, Canada   karoantonio@gmail.com, magaw@mymacewan.ca, antonc@macewan.ca         Abstract   inquiry  along   investigated  Parameterized  Poker  Squares   We  to  approximate an optimal game playing agent. We organized  our  three  
Proceedings of the Sixth Symposium on Educational Advances in Artificial Intelligence (EAAI-16)  Learning and Using Hand Abstraction Values  for Parameterized Poker Squares  Todd W. Neller and Colin M. Messinger and Zuozhi Yang  Gettysburg College  tneller@gettysburg.edu  Abstract  We describe the experimental development of an AI player that adapts to different point systems for Param- eterized Poker Squares. After introducing the game and research competition challenge, we describe our static 
Proceedings of the Sixth Symposium on Educational Advances in Artificial Intelligence (EAAI-16)  The Turing Test in the Classroom  Lisa Torrey, Karen Johnson,  Sid Sondergard, Pedro Ponce, Laura Desmond  St. Lawrence University  Abstract  This paper discusses the Turing Test as an educational activ- ity for undergraduate students. It describes in detail an exper- iment that we conducted in a ﬁrst-year non-CS course. We also suggest other pedagogical purposes that the Turing Test could serve.  In
Proceedings of the Sixth Symposium on Educational Advances in Artificial Intelligence (EAAI-16)  A.I. as an Introduction to  Research Methods in Computer Science  Raghuram Ramanujan  Dept. of Mathematics and Computer Science  Davidson College, Davidson, NC 28035  raramanujan@davidson.edu  Abstract  While many computer science programs offer courses on research methods, such classes typically tend to be aimed at graduate students. In this paper, we propose a novel means for introducing undergradu
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  Ethical Dilemmas for Adaptive Persuasion Systems  Oliviero Stock, Marco Guerini, Fabio Pianesi  FBK-Irst  Via Sommarive 18, Trento - I-38123 Italy  stock@fbk.eu, guerini@fbk.eu, pianesi@fbk.eu  Abstract  A key acceptability criterion for artiﬁcial agents will be the possible moral implications of their actions. In particular, intelligent persuasive systems (systems designed to inﬂu- ence humans via communication) 
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  Ontology Instance Linking: Towards Interlinked Knowledge Graphs  Department of Computer Science and Engineering  Jeff Heﬂin  Lehigh University  19 Memorial Drive West  Bethlehem, PA 18015, USA  Dezhao Song  Research and Development  Thomson Reuters  610 Opperman Drive  Eagan, MN 55123, USA  Abstract  Due to the decentralized nature of the Semantic Web, the same real-world entity may be described in various data so
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  MIP-Nets: Enabling Information Sharing  in Loosely-Coupled Teamwork  Ofra Amir, Barbara J. Grosz and Krzysztof Z. Gajos  Harvard School of Engineering and Applied Sciences  {oamir,grosz,kgajos}@seas.harvard.edu  Abstract  People collaborate in carrying out such complex activities as treating patients, co-authoring documents and developing soft- ware. While technologies such as Dropbox and Github enable groups to w
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  Abstraction Using Analysis of Subgames  Anjon Basak  University of Texas at El Paso  500 W University Ave, El Paso, TX 79968 915-731-3083, abasak@miners.utep.edu  Abstract  Normal form games are one of the most familiar representa- tions for modeling interactions among multiple agent. How- ever, modeling many realistic interactions between agents re- sults in games that are extremely large. In these cases com- put
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  A Comparison of Supervised Learning Algorithms  for Telerobotic Control Using Electromyography Signals  Antonio G. Sestito∗ sestitoa11@gmail.com Barry C. Husowitz∗ husowitzb@wit.edu  Craig Versek‡ c.versek@neu.edu Nate Derbinsky∗ derbinskyn@wit.edu  Tyler M. Frasca∗† tyfrasc15@gmail.com Douglas E. Dow∗  dowd@wit.edu  Abstract  Human Computer Interaction (HCI) is central for many applications, including hazardous e
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  Hierarchy Prediction in Online Communities  Denys Katerenchuk(cid:2) and Andrew Rosenberg(cid:2)† dkaterenchuk@gradcenter.cuny.edu, andrew@cs.qc.cuny.edu  365 Fifth Avenue, Room 4319, New York, NY 10016  (cid:2)CUNY Graduate Center, New York, USA †CUNY Queens College, New York, USA  65-30 Kissena Boulevard, Room A-202, Flushing, NY 11367  Abstract  With the development of the Internet, a big part of social interac
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  (cid:1) (cid:1)  (cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:6)(cid:9)(cid:10)(cid:1)(cid:11)(cid:6)(cid:9)(cid:12)(cid:13)(cid:1)(cid:14)(cid:9)(cid:5)(cid:1)(cid:15)(cid:16)(cid:4)(cid:6)(cid:3)(cid:1)(cid:17)(cid:18)(cid:6)(cid:19)(cid:5)(cid:6)(cid:9)(cid:10)(cid:1)(cid:15)(cid:6)(cid:20)(cid:4)(cid:21)(cid:1)(cid:22)(cid:1)(cid:2)(cid:14)(cid:8)(cid:16)(cid:23)(cid:17)(cid:14)(cid:13)(cid:4)(cid:5)(
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  A Word Embedding and a Josa Vector  for Korean Unsupervised Semantic Role Induction  Kyeong-Min Nam and Yu-Seop Kim  Department of Convergence Software  Hallym University 1 Hallymdaehak-gil, Chuncheon,Gangwon-do, 200-702 Korea  jkre4030@naver.com, yuseop.kim@gmail.com  Abstract  We propose an unsupervised semantic role labeling method for Korean language, one of the agglutinative languages which have complicated s
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  Mobility Sequence Extraction and Labeling Using Sparse Cell Phone Data  Yingxiang Yang Massachusetts Inst. of  Technology  77 Massachusetts Ave.  Cambridge, USA  Peter Widhalm Austrian Institute of  Technology  Gieﬁnggasse 2, Vienna, Austria  Shounak Athavale  Ford Motor Company  Palo Alto, CA  Marta C. Gonz´alez Massachusetts Inst. of  Technology  77 Massachusetts Ave.  Cambridge, USA  Abstract  Human mobility mo
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  Learning Structural Features of Nodes in Large-Scale Networks for Link Prediction  Aakas Zhiyuli, Xun Liang, Xiaoping Zhou  Department of Computer Science, School of Information, Renmin University of China,  {zhiyulee, xliang, zhouxiaoping}@ruc.edu.cn  Abstract  We present an algorithm (LsNet2Vec) that, given a large-scale network (millions of nodes), embeds the structural features of node into a lower and ﬁxed di
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)       NLU Framework for Voice Enabling    Non-Native Applications on Smart Devices    Soujanya Lanka, Deepika Pathania, Pooja Kushalappa, Pradeep Varakantham   i.am+ LLC. 809 North Cahunga Blvd, Los Angeles - 90036  {soujanya, deepika.pathania, pooja, pradeep}@iamplus.com          Abstract   Voice  is  a  critical  user  interface  on  smart  devices  (weara- bles,  phones,  speakers,  televisions)  to  access  app
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  WWDS APIs: A for Efficient Manipula Hanumant Redkar1, Sudh Neha Prabhugao 1Indian  Application Programming Interfa ation of World WordNet Database ha Bhingardive1, Kevin Patel1, Pushpak Bhat onkar2, Apurva Nagvenkar2, Ramdas Karmal n Institute of Technology Bombay, Mumbai, India   aces  e Structure  tacharyya1                              li2   2Goa University, Goa, India   {hanumantredka {nehapgaonk  ar, bhingard
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)       Artificial Swarm Intelligence,    a Human-in-the-Loop Approach to A.I.      Unanimous A.I, San Francisco, California, USA   Louis Rosenberg    Louis@UnanimousAI.com   ,          Abstract   Most research into Swarm Intelligence explores swarms of  autonomous robots or simulated agents. Little work, howev- er,  has  been  done  on  swarms  of  networked  humans.  This  paper introduces UNU, an online platform t
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)  EDDIE: An Embodied AI System for    Research and Intervention for Individuals with ASD   Robert Selkowitz1, Jonathan Rodgers2, P.J. Moskal3, Jon Mrowczynski1, Christine Colson1 {1Dept of Physics, 2Institute for Autism Research, 3Department of Communication Studies} Canisius College, Buffalo, NY   {selkowir, rodgers1, moskalp, mrowczy1, colsonc} @canisius.edu   Abstract   We report on the ongoing development of EDD
136  137  138  139  140  141  142  e v i t c e b O 0  j  20  40  60  Number of solutions  0.74 0.61 0.47  100  80  0  20  40  60  Number of solutions  0.44 0.24 0.05  100  y c a r u c c A  80  143  144  145  
231  232  233  234  235  236  237  238  239  240  
Pull the Plug? Predicting If Computers or Humans Should Segment Images  Danna Gurari  Suyog Dutt Jain  Margrit Betke  Kristen Grauman  Abstract  Foreground object segmentation is a critical step for many image analysis tasks. While automated methods can produce high-quality results, their failures disappoint users in need of practical solutions. We propose a resource al- location framework for predicting how best to allocate a ﬁxed budget of human annotation effort in order to collect higher qua
A Deeper Look at Saliency:  Feature Contrast, Semantics, and Beyond  Neil D. B. Bruce, Christopher Catton, Sasa Janjic  University of Manitoba  Winnipeg, MB  bruce@cs.umanitoba.ca  Abstract  In this paper we consider the problem of visual saliency modeling, including both human gaze prediction and salient object segmentation. The overarching goal of the paper is to identify high level considerations relevant to deriving more sophisticated visual saliency models. A deep learn- ing model based on 
Split and Match: Example-based Adaptive Patch Sampling for Unsupervised  Style Transfer  Oriel Frigo1  2  ,  Neus Sabater1  Julie Delon2  Pierre Hellier1  1Technicolor, Research&Innovation, France  2Universit´e Paris Descartes, MAP5, France  Abstract  This paper presents a novel unsupervised method to transfer the style of an example image to a source image. The complex notion of image style is here considered as a local texture transfer, eventually coupled with a global color transfer. For the 
Detection and Accurate Localization of Circular Fiducials under Highly  Challenging Conditions  Lilian Calvet1, Pierre Gurdjos2, Carsten Griwodz1, and Simone Gasparini2  1Simula Research Laboratory, Oslo, Norway ☞ {lcalvet, griff}@simula.no  2University of Toulouse, France ☞ {pgurdjos, simone.gasparini}@enseeiht.fr  Abstract  Using ﬁducial markers ensures reliable detection and identiﬁcation of planar features in images. Fiducials are used in a wide range of applications, especially when a re- l
Single-Image Crowd Counting via Multi-Column Convolutional Neural Network  Yingying Zhang Desen Zhou  Siqin Chen  Shenghua Gao Yi Ma  {zhangyy2,zhouds,chensq,gaoshh,mayi}@shanghaitech.edu.cn  Shanghaitech University  Abstract  This paper aims to develop a method than can accurately estimate the crowd count from an individual image with ar- bitrary crowd density and arbitrary perspective. To this end, we have proposed a simple but effective Multi-column Con- volutional Neural Network (MCNN) archi
Shallow and Deep Convolutional Networks for Saliency Prediction  Junting Pan∗, Elisa Sayrol and Xavier Giro-i-Nieto  Kevin McGuinness∗ and Noel E. O’Connor  Image Processing Group  Universitat Politecnica de Catalunya  Barcelona, Catalonia/Spain  Insight Center for Data Analytics  Dublin City University  Dublin, Ireland  xavier.giro@upc.edu  kevin.mcguinness@insight-centre.org  Abstract  The prediction of salient areas in images has been tra- ditionally addressed with hand-crafted features based
923  924  925  926  927  928  929  930  931  932  
1114  contrastive information [10] or triplets [27, 8]). For exam- ple, Wang et al. [39] proposes a deep ranking model to di- rectly learn the similarity metric by sampling triplets from images. However, these strategies still have several lim- itations in ﬁne-grained datasets: 1) Although the features learned from triplet constraints are effective at discovering similar instances, its classiﬁcation accuracy may be infe- rior to the ﬁne-tuned deep models that emphasize on the classiﬁcation loss
Structured Feature Similarity with Explicit Feature Map  National Institute of Advanced Industrial Science and Technology  Takumi Kobayashi  Umezono 1-1-1, Tsukuba, Japan  takumi.kobayashi@aist.go.jp  Abstract  Feature matching is a fundamental process in a variety of computer vision tasks. Beyond the standard L2 metric, var- ious methods to measure similarity between features have been proposed mainly on the assumption that the features are deﬁned in a histogram form. On the other hand, in a ﬁe
Temporal Epipolar Regions  Mor Dar and Yael Moses  Eﬁ Arazi School of Computer Science  The Interdisciplinary Center, Herzliya 46150, Israel  mor.dar@post.idc.ac.il and yael@idc.ac.il  Abstract  Dynamic events are often photographed by a number of people from different viewpoints at different times, result- ing in an unconstrained set of images. Finding the corre- sponding moving features in each of the images allows us to extract information about objects of interest in the scene. Computing cor
Recurrent Convolutional Network for Video-based Person Re-Identiﬁcation  Niall McLaughlin, Jesus Martinez del Rincon, Paul Miller  Centre for Secure Information Technologies (CSIT)  Queen’s University Belfast  n.mclaughlin@qub.ac.uk  Abstract  In this paper we propose a novel recurrent neural net- work architecture for video-based person re-identiﬁcation. Given the video sequence of a person, features are extracted from each frame using a convolutional neural network that incorporates a recurren
Improving Person Re-identiﬁcation via Pose-aware Multi-shot Matching  Yeong-Jun Cho and Kuk-Jin Yoon  Computer Vision Laboratory, GIST, South Korea  {yjcho, kjyoon}@gist.ac.kr  Abstract  Person re-identiﬁcation is the problem of recogniz- ing people across images or videos from non-overlapping views. Although there has been much progress in person re-identiﬁcation for the last decade, it still remains a chal- lenging task because of severe appearance changes of a per- son due to diverse camera v
Siamese Instance Search for Tracking  Ran Tao, Efstratios Gavves, Arnold W.M. Smeulders QUVA Lab, Science Park 904, 1098 XH Amsterdam  {r.tao,egavves,a.w.m.smeulders}@uva.nl  Abstract  In this paper we present a tracker, which is radically different from state-of-the-art trackers: we apply no model updating, no occlusion detection, no combination of track- ers, no geometric matching, and still deliver state-of-the- art tracking performance, as demonstrated on the popular online tracking benchmar
3D Part-Based Sparse Tracker with Automatic Synchronization and Registration  Adel Bibi, Tianzhu Zhang, and Bernard Ghanem  King Abdullah University of Science and Technology (KAUST), Saudi Arabia  adel.bibi@kaust.edu.sa, tianzhu.zhang@kaust.edu.sa, bernard.ghanem@kaust.edu.sa  Abstract  In this paper, we present a part-based sparse tracker in a particle ﬁlter framework where both the motion and ap- pearance model are formulated in 3D. The motion model is adaptive and directed according to a sim
3D Action Recognition from Novel Viewpoints  School of Computer Science and Software Engineering, The University of Western Australia  hossein@csse.uwa.edu.au, ajmal.mian@uwa.edu.au  Hossein Rahmani, and Ajmal Mian  Abstract  We propose a human pose representation model that transfers human poses acquired from different unknown views to a view-invariant high-level space. The model is a deep convolutional neural network and requires a large corpus of multiview training data which is very expensiv
1601       Abstract  Recent advances in neural networks have revolutionized computer vision, but these algorithms are still outperformed by humans. Could this performance gap be due to systematic differences between object representations in humans and machines? To answer this question we collected a large dataset of 26,675 perceived dissimilarity measurements from 2,801 visual objects across 269 human subjects, and used this dataset to train and test leading computational models. The best model
PSyCo: Manifold Span Reduction for Super Resolution  Eduardo Pérez-Pellitero1,2, Jordi Salvador2, Javier Ruiz-Hidalgo3 and Bodo Rosenhahn1  1TNT Lab, Leibniz Universität Hannover  2Technicolor R&I Hannover  3Image Processing Group, Universitat Politècnica de Catalunya  Abstract  The main challenge in Super Resolution (SR) is to dis- cover the mapping between the low- and high-resolution manifolds of image patches, a complex ill-posed problem which has recently been addressed through piecewise li
Aggregating Image and Text Quantized Correlated Components  Thi Quynh Nhi Tran  CEA, LIST and CEDRIC-CNAM  thiquynhnhi.tran@cea.fr  Herv´e Le Borgne  CEA, LIST  Gif-sur-Yvette, France  Michel Crucianu CEDRIC-CNAM  Paris, France  herve.le-borgne@cea.fr  michel.crucianu@cnam.fr  Abstract  Cross-modal tasks occur naturally for multimedia con- tent that can be described along two or more modalities like visual content and text. Such tasks require to “translate” in- formation from one modality to ano
Efﬁcient Globally Optimal 2D-to-3D Deformable Shape Matching  Zorah L¨ahner TU M¨unchen  Emanuele Rodol`a  TU M¨unchen  Frank R. Schmidt  TU M¨unchen  laehner@in.tum.de  Universit`a della Svizzera Italiana  f.schmidt@cs.tum.edu  emanuele.rodola@usi.ch  Michael M. Bronstein  Universit`a della Svizzera Italiana  michael.bronstein@usi.ch  Daniel Cremers  TU M¨unchen  cremers@tum.de  Abstract  We propose the ﬁrst algorithm for non-rigid 2D-to-3D shape matching, where the input is a 2D query shape as
Semantic Channels for Fast Pedestrian Detection  Arthur Daniel Costea  Sergiu Nedevschi  Image Processing and Pattern Recognition Research Center  Technical University of Cluj-Napoca, Romania  {arthur.costea, sergiu.nedevschi}@cs.utcluj.ro  Abstract  Pedestrian detection and semantic segmentation are high potential tasks for many real-time applications. However most of the top performing approaches provide state of art results at high computational costs. In this work we pro- pose a fast solutio
Combining Markov Random Fields and Convolutional Neural Networks for  Image Synthesis  Chuan Li  Mainz University  Germany  Michael Wand  Mainz University  Germany  chuli@uni-mainz.de  wandm@uni-mainz.de  Abstract  This paper studies a combination of generative Markov random ﬁeld (MRF) models and discriminatively trained deep convolutional neural networks (dCNNs) for synthesiz- ing 2D images. The generative MRF acts on higher-levels of a dCNN feature pyramid, controling the image layout at an ab
Learning to Read Chest X-Rays:  Recurrent Neural Cascade Model for Automated Image Annotation  Hoo-Chang Shin1, Kirk Roberts2, Le Lu1, Dina Demner-Fushman2, Jianhua Yao1, Ronald M Summers1  1Imaging Biomarkers and Computer- Aided Diagnosis Laboratory, Radiology and Imaging Sciences, Clinical Center  2Lister Hill National Center for Biomedical Communications, National Library of Medicine  {hoochang.shin; le.lu; kirk.roberts; rms}@nih.gov; ddemner@mail.nih.gov; JYao@cc.nih.gov  National Institutes
DeepFool: a simple and accurate method to fool deep neural networks  Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Pascal Frossard  ´Ecole Polytechnique F´ed´erale de Lausanne  {seyed.moosavi,alhussein.fawzi,pascal.frossard} at epfl.ch  Abstract  State-of-the-art deep neural networks have achieved im- pressive results on many image classiﬁcation tasks. How- ever, these same architectures have been shown to be un- stable to small, well sought, perturbations of the images. Despite the importance
Action Recognition in Video Using Sparse Coding and Relative Features  Anal´ı Alfaro  Domingo Mery  Alvaro Soto  P. Universidad Catolica de Chile  P. Universidad Catolica de Chile  P. Universidad Catolica de Chile  Santiago, Chile  Santiago, Chile  Santiago, Chile  ajalfaro@uc.cl  dmery@ing.puc.cl  asoto@ing.uc.cl  Abstract  This work presents an approach to category-based ac- tion recognition in video using sparse coding techniques. The proposed approach includes two main contributions: i) A ne
BoxCars: 3D Boxes as CNN Input  for Improved Fine-Grained Vehicle Recognition  Jakub Sochor∗, Adam Herout, Jiˇr´ı Havel  Graph@FIT, Brno University of Technology  Brno, Czech Republic  {isochor,herout,ihavel}@fit.vutbr.cz  Abstract  We are dealing with the problem of ﬁne-grained vehi- cle make & model recognition and veriﬁcation. Our contri- bution is showing that extracting additional data from the video stream – besides the vehicle image itself – and feed- ing it into the deep convolutional ne
What Sparse Light Field Coding Reveals about Scene Structure  Ole Johannsen, Antonin Sulc and Bastian Goldluecke  University of Konstanz  Abstract  In this paper, we propose a novel method for depth esti- mation in light ﬁelds which employs a speciﬁcally designed sparse decomposition to leverage the depth-orientation re- lationship on its epipolar plane images. The proposed method learns the structure of the central view and uses this information to construct a light ﬁeld dictionary for which gr
Multiple Models Fitting as a Set Coverage Problem  Luca Magri  Andrea Fusiello  Computer Science Dept. - University of Verona  DPIA - University of Udine  Strada Le Grazie, 15 - 37134 Verona, IT  Via delle Scienze, 208, - 33100 Udine, IT  magri.luca.l@gmail.com  name.surname@uniud.it  Abstract  This paper deals with the extraction of multiple mod- els from noisy or outlier-contaminated data. We cast the multi-model ﬁtting problem in terms of set coverage, deriv- ing a simple and effective method
Piecewise-planar 3D approximation from wide-baseline stereo  ICTEAM institute, Université catholique de Louvain (UCL), Louvain-la-Neuve, Belgium  {cedric.verleysen, christophe.devleeschouwer}@uclouvain.be  C. Verleysen and C. De Vleeschouwer  Abstract  This paper approximates the 3D geometry of a scene by a small number of 3D planes. The method is especially suited to man-made scenes, and only requires two calibrated wide-baseline views as inputs. It relies on the computa- tion of a dense but no
Multicamera calibration from visible and mirrored epipoles  Andrey Bushnevskiy, Lorenzo Sorgi Technicolor Research & Innovation  Karl-Wiechert Allee 74, 30625  Hannover, Germany  Bodo Rosenhahn  Leibniz University Hannover  Appelstr 9A, 30169 Hannover, Germany  Abstract  Multicamera rigs are used in a large number of 3D Vi- sion applications, such as 3D modeling, motion capture or telepresence and a robust calibration is of utmost impor- tance in order to achieve a high accuracy results. In many
Canny Text Detector: Fast and Robust Scene Text Localization Algorithm  Hojin Cho  Myungchul Sung  Bongjin Jun  {hojin.cho, myungchul.sung, bongjin.jun}@stradvision.com  Stradvision, Inc.  Abstract  This paper presents a novel scene text detection algo- rithm, Canny Text Detector, which takes advantage of the similarity between image edge and text for effective text lo- calization with improved recall rate. As closely related edge pixels construct the structural information of an object, we obse
Panoramic Stereo Videos with a Single Camera  Rajat Aggarwal∗  Amrisha Vohra*  Anoop M. Namboodiri  Kohli Center on Intelligent Systems,  International Institute of Information Technology- Hyderabad, India.  {rajat.aggarwal@research, amrisha.vohra@research, anoop@}.iiit.ac.in  Abstract  We present a practical solution for generating 360◦ stereo panoramic videos using a single camera. Current approaches either use a moving camera that captures mul- tiple images of a scene, which are then stitched
3803  3804  3805  Although the hierarchical structure of our ST-AOG is similar to those spatial-only models used in [37, 46] for car detection, we introduce the semantic part Or-nodes (to be used to deﬁne ﬂuent) for detailed part status modelling in both spatial and temporal domains.  A parse tree, pt, is an instantiation of the ST-AOG be- ing placed at a location in the spatial-temporal feature pyra- mid. It is computed by following the breadth-ﬁrst-search order of the nodes in G, selecting 
Sublabel–Accurate Relaxation of Nonconvex Energies  Thomas M¨ollenhoff∗  TU M¨unchen  Emanuel Laude∗  TU M¨unchen  Michael Moeller  TU M¨unchen  moellenh@in.tum.de  laudee@in.tum.de  moellerm@in.tum.de  Jan Lellmann  University of L¨ubeck  Daniel Cremers  TU M¨unchen  lellmann@mic.uni-luebeck.de  cremers@tum.de  Abstract  We propose a novel spatially continuous framework for  convex relaxations based on functional lifting. Our method  can be interpreted as a sublabel–accurate solution to mul-  t
Fast Algorithms for Convolutional Neural Networks  Andrew Lavin alavin@acm.org  Scott Gray  Nervana Systems  sgray@nervanasys.com  Abstract  Deep convolutional neural networks take GPU-days of computation to train on large data sets. Pedestrian detec- tion for self driving cars requires very low latency. Image recognition for mobile phones is constrained by limited pro- cessing resources. The success of convolutional neural net- works in these situations is limited by how fast we can com- pute t
Robust Light Field Depth Estimation for Noisy Scene with Occlusion  Williem and In Kyu Park  Dept. of Information and Communication Engineering, Inha University  22112195@inha.edu, pik@inha.ac.kr  Abstract  Light ﬁeld depth estimation is an essential part of many light ﬁeld applications. Numerous algorithms have been de- veloped using various light ﬁeld characteristics. However, conventional methods fail when handling noisy scene with occlusion. To remedy this problem, we present a light ﬁeld de
4527  4528  4529  y  1h  1x  2h  2x  3h  3x  4h  4x  5h  5x  6x  4530  4531  4532  4533  4534  
Unsupervised Learning from Narrated Instruction Videos  Jean-Baptiste Alayrac∗ †  Piotr Bojanowski∗  Nishant Agrawal ∗ ‡  Josef Sivic∗  Ivan Laptev∗  Simon Lacoste-Julien†  Abstract  We address the problem of automatically learning the main steps to complete a certain task, such as changing a car tire, from a set of narrated instruction videos. The con- tributions of this paper are three-fold. First, we develop a new unsupervised learning approach that takes advantage of the complementary nature
Pairwise Linear Regression Classiﬁcation for Image Set Retrieval  Qingxiang Feng, Yicong Zhou∗, Rushi Lan  Department of Computer and Information Science, University of Macau  Avenida da Universidade, Taipa, Macau, China  fengqx1988@gmail.com, yicongzhou@umac.mo, rslan2012@gmail.com  Abstract  This paper proposes the pairwise linear regression clas- siﬁcation (PLRC) for image set retrieval. In PLRC, we ﬁrst deﬁne a new concept of the unrelated subspace and intro- duce two strategies to constitut
Learnt Quasi-Transitive Similarity for Retrieval from Large Collections of Faces  Ognjen Arandjelovi´c  University of St Andrews, United Kingdom  ognjen.arandjelovic@gmail.com  Abstract  We are interested in identity-based retrieval of face sets from large unlabelled collections acquired in uncontrolled environments. Given a baseline algorithm for measuring the similarity of two face sets, the meta-algorithm intro- duced in this paper seeks to leverage the structure of the data corpus to make th
Geospatial Correspondences for Multimodal Registration  Diego Marcos  University of Zurich  Raffay Hamid  DigitalGlobe Inc.  Devis Tuia  University of Zurich  diego.marcos@geo.uzh.ch  raffay@cc.gatech.edu  devis.tuia@geo.uzh.ch  Abstract  The growing availability of very high resolution (<1 m/pixel) satellite and aerial images has opened up un- precedented opportunities to monitor and analyze the evo- lution of land-cover and land-use across the world. To do so, images of the same geographical a
5157  5158  5159  5160  5161  5162  5163  5164  
5422  Efﬁcient3DRoomShapeRecoveryfromaSinglePanoramaHaoYangHuiZhangSchoolofSoftware,TsinghuaUniversity,Beijing,ChinaTsinghuaNationalLaboratoryforInformationScienceandTechnology(TNList)yanghao14@mails.tsinghua.edu.cn,huizhang@tsinghua.edu.cnAbstractWeproposeamethodtorecovertheshapeofa3Droomfromafull-viewindoorpanorama.Ouralgorithmcanau-tomaticallyinfera3Dshapefromacollectionofpartiallyorientedsuperpixelfacetsandlinesegments.Thecorepartofthealgorithmisaconstraintgraph,whichincludeslinesandsuperpix
●❧♦❜❛❧❧② ❖♣t✐♠❛❧ ❘✐❣✐❞ ■♥t❡♥s✐t② ❇❛s❡❞ ❘❡❣✐str❛t✐♦♥✿  ❆ ❋❛st ❋♦✉r✐❡r ❉♦♠❛✐♥ ❆♣♣r♦❛❝❤  ❇❡❤r♦♦③ ◆❛s✐❤❛t❦♦♥✶✱✸✱ ❋r✐❞❛ ❋❡❥♥❡✶✱✸✱ ❛♥❞ ❋r❡❞r✐❦ ❑❛❤❧✶✱✷✱✸  ✶❉❡♣❛rt♠❡♥t ♦❢ ❙✐❣♥❛❧s ❛♥❞ ❙②st❡♠s✱ ❈❤❛❧♠❡rs ❯♥✐✈❡rs✐t② ♦❢ ❚❡❝❤♥♦❧♦❣②✱ ❙✇❡❞❡♥  ✷❈❡♥tr❡ ❢♦r ▼❛t❤❡♠❛t✐❝❛❧ ❙❝✐❡♥❝❡s✱ ▲✉♥❞ ❯♥✐✈❡rs✐t②✱ ❙✇❡❞❡♥  ✸▼❡❞❚❡❝❤ ❲❡st✱ ❙✇❡❞❡♥  ④❜❡❤r♦♦③✳♥❛s✐❤❛t❦♦♥✱❢r✐❞❛✳❢❡❥♥❡✱❢r❡❞r✐❦✳❦❛❤❧⑥❅❝❤❛❧♠❡rs✳s❡  ❆❜str❛❝t  ❍✐❣❤ ❝♦♠♣✉t❛t✐♦♥❛❧ ❝♦st ✐s t❤❡ ♠❛✐♥ ♦❜st❛❝❧❡ t♦ ❛❞❛♣t✐♥❣ ❣❧♦❜❛❧❧② ♦♣t✐♠❛❧ ❜r❛♥❝❤✲❛♥❞✲❜♦✉♥❞ ❛❧❣♦r✐t❤♠s t♦ ✐♥t❡♥s✐t②✲❜❛s❡❞ 
Traditional Saliency Reloaded:  A Good Old Model in New Shape  Simone Frintrop, Thomas Werner, and Germ´an M. Garc´ıa  Institute of Computer Science III  Rheinische Friedrich-Wilhelms-Universit¨at Bonn, Germany  frintrop@iai.uni-bonn.de  Abstract  In this paper, we show that the seminal, biologically- inspired saliency model by Itti et al. [21] is still compet- itive with current state-of-the-art methods for salient ob- ject segmentation if some important adaptions are made. We show which change
Leveraging Stereo Matching with Learning-based Conﬁdence Measures  Min-Gyu Park and Kuk-Jin Yoon  Computer Vision Laboratory, GIST, South Korea  {mpark,kjyoon}@gist.ac.kr  Abstract  We propose a new approach to associate supervised learning-based conﬁdence prediction with the stereo match- ing problem. First of all, we analyze the characteristics of various conﬁdence measures in the regression forest frame- work to select effective conﬁdence measures using training data. We then train regression
A Dynamic Programming Approach for Fast and Robust Object Pose  Recognition from Range Images  Christopher Zach  Toshiba Research Europe  Cambridge, UK  Adrian Penate-Sanchez  CSIC-UPC  Barcelona, Spain  Minh-Tri Pham  Toshiba Research Europe  Cambridge, UK  christopher.m.zach@gmail.com  apenate@iri.upc.edu  mtpham@crl.toshiba.co.uk  Abstract  Joint object recognition and pose estimation solely from range images is an important task e.g. in robotics applica- tions and in automated manufacturing 
Structured Sparse Subspace Clustering: A Uniﬁed Optimization Framework  Chun-Guang Li  SICE, Beijing University of Posts  and Telecommunications  Abstract  Subspace clustering refers to the problem of segmenting data drawn from a union of subspaces. State of the art ap- proaches for solving this problem follow a two-stage ap- proach. In the ﬁrst step, an afﬁnity matrix is learned from the data using sparse or low-rank minimization techniques. In the second step, the segmentation is found by appl
                  Abstract   a   novel   framework   computational   It  is  believed  that  eye  movements  in  free-viewing  of  natural  scenes  are  directed  by  both  bottom-up  visual  saliency  and  top-down  visual  factors.  In  this  paper,  we  propose  to  simultaneously learn these two types of visual features from  raw  image  data  using  a  multiresolution  convolutional  neural network (Mr-CNN) for predicting eye fixations. The  Mr-CNN is directly trained from image regions cen
Kernel Fusion for Better Image Deblurring  Long Mai  Feng Liu  Portland State University  Portland State University  mtlong@cs.pdx.edu  fliu@cs.pdx.edu  Abstract  Kernel estimation for image deblurring is a challenging task and a large number of algorithms have been developed. Our hypothesis is that while individual kernels estimated using different methods alone are sometimes inadequate, they often complement each other. This paper addresses the problem of fusing multiple kernels estimated usin
	 				      ()*+#,*#  ! -.#/01)"  % 23"*  !    %  "#$  ! &'  4567897:;4<=>9?@ABC@5=6>C D5EFG7H IEG7J;9>BK6=4BA FG97459B=L97:;<79M459@=4BA  NO  PQRSTUTRVWXTXTYXTZT[VRV\][Y^R_ZR[TZZT[V\R^X]^T\[ \QRSTXT`]S[\V\][R[aXT^RVTaVRZbZc deT`WXXT[VZVRVTf ]UfVeTfRXVUTRVWXT^TRX[\[SYRXRa\SQ \ZZWYTXg\ZTa^TRX[f \[SUX]Q ^RhT^TaaRVRci]jTgTXkVe\ZYRXRa\SQ XTlW\XTZ ^RXSTfZ`R^T`RVTS]X_^RhT^Zkje\`e^\Q\VZ\VZRYY^\`Rh\^\V_ V]a]QR\[ZjeT
Depth Camera Tracking with Contour Cues  Qian-Yi Zhou  Vladlen Koltun  Intel Labs  Abstract  We present an approach for tracking camera pose in real time given a stream of depth images. Existing algorithms are prone to drift in the presence of smooth surfaces that destabilize geometric alignment. We show that useful con- tour cues can be extracted from noisy and incomplete depth input. These cues are used to establish correspondence con- straints that carry information about scene geometry and c
ActivityNet: A Large-Scale Video Benchmark for Human Activity  Understanding  Fabian Caba Heilbron1,2, Victor Escorcia1,2, Bernard Ghanem2 and Juan Carlos Niebles1  2King Abdullah University of Science and Technology (KAUST), Saudi Arabia  1Universidad del Norte, Colombia  Abstract  In spite of many dataset efforts for human action recog- nition, current computer vision algorithms are still severely limited in terms of the variability and complexity of the ac- tions that they can recognize. This
Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database  Hoo-Chang Shin  Le Lu  Lauren Kim Ari Seff  Jianhua Yao  Ronald M. Summers  Imaging Biomarkers and Computer-Aided Diagnosis Laboratory  Radiology and Imaging Sciences  National Institutes of Health Clinical Center  {hoochang.shin, le.lu, lauren.kim2, ari.seff, rms}@nih.gov, jyao@cc.nih.gov  Bethesda, MD 20892-1182  Abstract  Despite tremendous progress in computer vision, effec- tive learning on very large-scale (> 100K pati
Continuous Visibility Feature  Guilin Liu  gliu2@gmu.edu  Yotam Gingold ygingold@gmu.edu  Jyh-Ming Lien∗  jmlien@cs.gmu.edu  Abstract  In this work, we propose a new type of visibility mea- surement named Continuous Visibility Feature (CVF). We say that a point q on the mesh is continuously visible from another point p if there exists a geodesic path connecting p and q that is entirely visible by p. In order to efﬁciently esti- mate the continuous visibility for all the vertices in a model, we p
A Statistical Model of Riemannian Metric Variation  for Deformable Shape Analysis  Dipartimento di Scienze Ambientali, Informatica e Statistica  Universit´a Ca’ Foscari Venezia - via Torino, 155 - 30172 Venice Italy  Andrea Gasparetto and Andrea Torsello  {andrea.gasparetto,torsello}@unive.it  Abstract  The analysis of deformable 3D shape is often cast in terms of the shape’s intrinsic geometry due to its invari- ance to a wide range of non-rigid deformations. However, object’s plasticity in non
On the Relationship between Visual Attributes and Convolutional Networks  Victor Escorcia1,2, Juan Carlos Niebles2, Bernard Ghanem1  1King Abdullah University of Science and Technology (KAUST), Saudi Arabia  2Universidad del Norte, Colombia  Abstract  One of the cornerstone principles of deep models is their abstraction capacity, i.e. their ability to learn ab- stract concepts from ‘simpler’ ones. Through extensive ex- periments, we characterize the nature of the relationship between abstract co

Large-Scale Damage Detection Using Satellite Imagery  Lionel Gueguen  Raffay Hamid  DigitalGlobe Inc.  12076 Grant Street, Thornton, Colorado, USA  {lgueguen, mhamid}@digitalglobe.com  Abstract  Satellite imagery is a valuable source of information for assessing damages in distressed areas undergoing a calamity, such as an earthquake or an armed conﬂict. How- ever, the sheer amount of data required to be inspected for this assessment makes it impractical to do it manually. To address this proble
Generalized Deformable Spatial Pyramid: Geometry-Preserving Dense  Correspondence Estimation  Junhwa Hur1, Hwasup Lim1,2, Changsoo Park1, and Sang Chul Ahn1,2  1Center for Imaging Media Research, Robot & Media Institute, KIST, Seoul, Korea  2HCI & Robotics Dept., University of Science & Technology, Korea  {hurjunhwa, hslim, winspark, asc}@imrc.kist.re.kr  Abstract  Source  Ours  SIFT Flow  We present a Generalized Deformable Spatial Pyramid (GDSP) matching algorithm for calculating the dense cor
Ü·­½®»¬» Ø§°»®ó¹®¿°¸ Ó¿¬½¸·²¹  Ö«²½¸· Ç¿²§¦ô Ý¸¿± Æ¸¿²¹¦ô Ø±²¹§«¿² Æ¸¿¨ô É»· Ô·«¦ô È·¿±µ¿²¹ Ç¿²¹§ô Í¬»°¸»² Óò Ý¸«¦  § Í¸¿²¹¸¿· Ö·¿± Ì±²¹ Ë²·ª»®­·¬§  ¦ ×ÞÓ Î»­»¿®½¸  ¨ Ù»±®¹·¿ ×²­¬·¬«¬» ±º Ì»½¸²±´±¹§  º§¿²¶«²½¸·ô¨µ§¿²¹¹à­¶¬«ò»¼«ò½²  ¾¶¦½¸¿±à½²ò·¾³ò½±³ ¦¸¿à½½ò¹¿¬»½¸ò»¼«  º©»·´·«ô­½¸«¹à«­ò·¾³ò½±³  ß¾­¬®¿½¬  Ì¸·­ °¿°»® º±½«­»­ ±² ¬¸» °®±¾´»³ ±º ¸§°»®ó¹®¿°¸ ³¿¬½¸·²¹ô ¾§ ¿½½±«²¬·²¹ º±® ¾±¬¸ «²¿®§ ¿²¼ ¸·¹¸»®ó±®¼»®  ·³¿¬» º®¿³»©±®µ ©¸·´» ¬¸» °®±¾´»³ ·­ ·¬»®¿¬·ª»´§ ­±´ª»¼ ·²  ³¿²§ »¨¬¿²¬ ½±²¬·²«±«­ ³»¬¸±
tation. Prominent examples from this line of work are re- stricted Boltzmann machines (RBMs) [12] and Deep Boltz- mann Machines (DBMs) [25], as well as the plethora of models derived from them [11, 21, 19, 28, 22]. RBMs and DBMs are undirected graphical models which aim to build a probabilistic model of the data and treat encoding and gen- eration as an (intractable) joint inference problem.  A different approach is to train directed graphical mod- els of the data distribution. This includes a 
Bayesian Inference for Neighborhood Filters with Application in Denoising  Chao-Tsung Huang  National Tsing Hua University, Taiwan  chaotsung@ee.nthu.edu.tw  Abstract  Range-weighted neighborhood ﬁlters are useful and popular for their edge-preserving property and simplicity, but they are originally proposed as intuitive tools. Previous works needed to connect them to other tools or models for indirect property reasoning or parameter estimation. In this paper, we introduce a uniﬁed empirical Bay
Real-time Joint Estimation of Camera Orientation and Vanishing Points  Jeong-Kyun Lee and Kuk-Jin Yoon  Computer Vision Laboratory  Gwangju Institute of Science and Technology  {leejk, kjyoon}@gist.ac.kr  Abstract  A widely-used approach for estimating camera orienta- tion is to use points at inﬁnity, i.e., vanishing points (VPs). By enforcing the orthogonal constraint between the VPs, called the Manhattan world constraint, a drift-free cam- era orientation estimation can be achieved. However, i
	 	      	  	  !"#$%&' ( (+,' () (*"% %-./)0"1"".23"4 '56"783"49":);$"6<23"4  07/)87b7c6".8q742/));6)$6,)q74p)8/f<06.,  "6,)1at3b)6;)$ghuj"$6)/7b;4c"d)8b)6;)$ev)  =>?@ABACDBEEDC@@C@FGH?@@IC@>?JECD?JKF>CBELBJMC@ $c"6)6,))bb).6":))$$26,););)$6"886470"8"0"66"7$" ?JBMMIDBFC?NBKCBO?KJNCJFPQ?D@FRF>CACDSHDNBJMCHS 07$6/)$.;"c67; f$)/0)6,7/$et";$62)d"$6"/)$.;"c67;$ ;)/
Pushing the Frontiers of Unconstrained Face Detection and Recognition: IARPA  Janus Benchmark A ⇤  Brendan F. Klare†       Ben Klein†        Emma Taborsky†        Austin Blanton†       Jordan Cheney†   Kristen Allen†   Patrick Grother‡         Alan Mah§          Anil K. Jain¶  Abstract  1. Introduction  Rapid progress in unconstrained face recognition has re- sulted in a saturation in recognition accuracy for current benchmark datasets. While important for early progress, a chief limitation in m
Casual Stereoscopic Panorama Stitching  Fan Zhang and Feng Liu  Department of Computer Science  Portland State University  {zhangfan,fliu}@cs.pdx.edu  Abstract  This paper presents a method for stitching stereoscopic panoramas from stereo images casually taken using a stereo camera. This method addresses three challenges of stereo- scopic image stitching: how to handle parallax, how to stitch the left- and right-view panorama consistently, and how to take care of disparity during stitching. This
Semi-supervised Learning with Explicit Relationship Regularization  Kwang In Kim  Lancaster University  James Tompkin Harvard SEAS  Hanspeter Pfister Harvard SEAS  Christian Theobalt MPI for Informatics  Abstract  In many learning tasks, the structure of the target space of a function holds rich information about the relationships between evaluations of functions on different data points. Existing ap- proaches attempt to exploit this relationship information implicitly by enforcing smoothness on
Real-time Visual Analysis of Microvascular Blood Flow for Critical Care  Chao Liu ∗  Hernando Gomez †  Srinivasa Narasimhan ∗  Artur Dubrawski ∗  Michael R. Pinsky †  Brian Zuckerbraun †  Abstract  Microcirculatory monitoring plays an important role in diagnosis and treatment of critical care patients. Sidestream Dark Field (SDF) imaging devices have been used to visualize and support interpretation of the micro- vascular blood ﬂow. However, due to subsurface scattering within the tissue that em
❙✁✂✄☎ ❉☎✆✝ ❙✞☎✂ ❘☎✄✟✠✞✆✡✟☛  ❏☞✌✍✎✏ ▲✎  ✭✌✈☞✮ ❋♦✑✒✔✓✘  ❯✏☞✈❡✑✒☞✓✔ ♦✕ ■✖✖☞✏♦☞✒ ✌✓ ❯✑✗✌✏✌ ❈✘✌✙✚✌☞✛✏  ❯✏☞✈❡✑✒☞✓✔ ♦✕ ■✖✖☞✏♦☞✒ ✌✓ ❯✑✗✌✏✌ ❈✘✌✙✚✌☞✛✏  ❥✜✢✣✤✥✦✜✜✦✧★✦✩✪✫✬✢  ✬❞✯✥✦✜✜✦✧★✦✩✪✫✬✢  ❆✰✱✲✳✴✵✲  q③♥⑦ ③❶⑧❿✐ r✉ ❷③⑩✐r ③❧❹r✉❶⑧♥③r❧➊  ❲✶ ✷✶✸✹✺✻✼✶ ❛ ♠✶✽✾✿✷ ✽✿ ♣✺✿✷❀✹✶ ✷✶✽❛✻❁✶✷ ✾✻❤✾ ✺✶✸✿❁❀❂ ✽✻✿t ✷✶♣✽✾ ♠❛♣✸ ❢✺✿♠ ❛❤❤✺✶✸✸✻❃✶❁❄ ✸❀✼✸❛♠♣❁✶✷ ✷✶♣✽✾ ♠✶❛❂ ✸❀✺✶♠✶t✽✸❅ ❖❀✺ ♠✶✽✾✿✷ ❢❀❁❁❄ ❀✸✶✸ ✽✾✶ ✺✶❁❛✽✻✿t✸✾✻♣ ✼✶✽❜✶✶t ✻♠❛❤✶ ✸✶❤♠✶t✽❛✽✻✿t ✼✿❀t✷❛✺✻✶✸ ❛t✷ ✷✶♣✽✾ ✼✿❀t✷❛✺✻✶✸❅ ❇✽ ❀✸✶✸ ❛t ✻♠❛❤✶ ✹✿♠✼✻t✶✷ ❜✻✽✾ ❛ ❁✿❜ ✺✶✸✿❁❀✽✻✿t ✷✶♣✽✾ ♠❛
Learning a Non-linear Knowledge Transfer Model  for Cross-View Action Recognition  Computer Science and Software Engineering, The University of Western Australia  Hossein Rahmani, and Ajmal Mian  hossein@csse.uwa.edu.au, ajmal.mian@uwa.edu.au  Abstract  This paper concerns action recognition from unseen and unknown views. We propose unsupervised learning of a non-linear model that transfers knowledge from mul- tiple views to a canonical view. The proposed Non-linear Knowledge Transfer Model (NKT
Random Tree Walk toward Instantaneous 3D Human Pose Estimation  Ho Yub Jung1  Soochahn Lee2  Yong Seok Heo3  Il Dong Yun4  Div. of Comp. & Elect. Sys. Eng.,  Hankuk U. of Foreign Studies  Dept. of Elect. Eng. Soonchunghyang U.  Dept. of Elect. & Comp. Eng.  Ajou University  Asan-si, Korea, 336-745  Suwon, Korea, 336-745  sclsch@sch.ac.kr  2  ysheo@ajou.ac.kr  3  Yongin, Korea, 449-791  jung.ho.yub@gmail.com  1  yun@hufs.ac.kr  4  Abstract  The availability of accurate depth cameras have made rea
Three Viewpoints Toward Exemplar SVM  National Institute of Advanced Industrial Science and Technology  Takumi Kobayashi  Umezono 1-1-1, Tsukuba, Japan takumi.kobayashi@aist.go.jp  Abstract  In contrast to category-level or cluster-level classiﬁers, exemplar SVM [17] is successfully applied to classifying (or detecting) a target object as well as transferring instance- level annotations. The method, however, is formulated in a highly biased classiﬁcation problem where only one posi- tive sample 
Visual Recognition by Learning from Web Data:  A Weakly Supervised Domain Generalization Approach  School of Computer Engineering, Nanyang Technology University (NTU), Singapore  Li Niu, Wen Li, and Dong Xu  flniu002,wli1,dongxug@ntu.edu.sg  Abstract  In this work, we formulate a new weakly supervised domain generalization approach for visual recognition by using loosely labeled web images/videos as training data. Speciﬁcally, we aim to address two challenging issues when learning robust classiﬁ
Non-rigid Registration of Images with Geometric and Photometric Deformation  by Using Local Afﬁne Fourier-Moment Matching  Institute of Information Systems and Applications  Hong-Ren Su  National Tsing Hua University  Hsinchu,Taiwan  Shang-Hong Lai  Department of Computer Science National Tsing Hua University  Hsinchu,Taiwan  suhongren@gmail.com  lai@cs.nthu.edu.tw  Abstract  Registration between images taken with different cam- eras, from different viewpoints or under different lighting conditi
Unsupervised Visual Alignment with Similarity Graphs  Fatemeh Shokrollahi Yancheshmeh, Ke Chen, and Joni-Kristian K¨am¨ar¨ainen Department of Signal Processing, Tampere University of Technology, Finland  {fatemeh.shokrollahiyancheshmeh, ke.chen, joni.kamarainen}@tut.fi  Abstract  Alignment of semantically meaningful visual patterns, such as object classes, is an important pre-processing step for a number of applications such as object detection and im- age categorization. Considering the expensi
Supervised Mid-Level Features for Word Image Representation  Albert Gordo  Computer Vision Group  Xerox Research Centre Europe albert.gordo@xrce.xerox.com  Abstract  This paper addresses the problem of learning word im- age representations: given the cropped image of a word, we are interested in ﬁnding a descriptive, robust, and compact ﬁxed-length representation. Machine learning techniques can then be supplied with these representations to produce models useful for word retrieval or recognitio
Exemplar SVMs as Visual Feature Encoders  Joaquin Zepeda and Patrick P´erez  Technicolor  Abstract  In this work, we investigate the use of exemplar SVMs (linear SVMs trained with one positive example only and a vast collection of negative examples) as encoders that turn generic image features into new, task-tailored features. The proposed feature encoding leverages the ability of the exemplar-SVM (E-SVM) classiﬁer to extract, from the orig- inal representation of the exemplar image, what is uni
Object Scene Flow for Autonomous Vehicles  Moritz Menze  Leibniz Universit¨at Hannover menze@ipi.uni-hannover.de  Andreas Geiger MPI T¨ubingen  andreas.geiger@tue.mpg.de  Abstract  This paper proposes a novel model and dataset for 3D scene ﬂow estimation with an application to autonomous driving. Taking advantage of the fact that outdoor scenes often decompose into a small number of independently mov- ing objects, we represent each element in the scene by its rigid motion parameters and each sup
requiring two weeks on 300 cores to process a dataset with 200,000 images.  We introduce a framework for Cryo-EM density estima- tion, formulating the problem as one of stochastic optimiza- tion to perform maximum-a-posteriori (MAP) estimation in a probabilistic model. The approach is remarkably efﬁcient, providing useful low resolution density estimates in an hour. We also show that our stochastic optimization technique is insensitive to initialization, allowing the use of random ini- tializat
Interaction Part Mining: A Mid-Level Approach for Fine-Grained Action  Recognition  Yang Zhou1, Bingbing Ni2, Richang Hong3, Meng Wang3, and Qi Tian1  1University of Texas at San Antonio, US  2Advanced Digital Sciences Center, Singapore  3HeFei University of Technology, China  myh511@my.utsa.edu bingbing.ni@adsc.com.sg hongrc@hfut.edu.cn  eric.mengwang@gmail.com qi.tian@utsa.edu  Abstract  Modeling human-object interactions and manipulating motions lies in the heart of ﬁne-grained action recogni
Matching Bags of Regions in RGBD images  Hao Jiang  Boston College, USA  hjiang@cs.bc.edu  Abstract  We study the new problem of matching regions between a pair of RGBD images given a large set of overlapping region proposals. These region proposals do not have a tree hierarchy and are treated as bags of regions. Match- ing RGBD images using bags of region candidates with un- structured relations is a challenging combinatorial prob- lem. We propose a linear formulation, which optimizes the regio
Scene Labeling with LSTM Recurrent Neural Networks  Wonmin Byeon1 2  Marcus Liwicki1  Thomas M. Breuel1 1 University of Kaiserslautern, Germany.  Federico Raue1 2  2 German Research Center for Artiﬁcial Intelligence (DFKI), Germany.  {wonmin.byeon,federico.raue}@dfki.de  {tmb,liwicki}@cs.uni-kl.de  Abstract  This paper addresses the problem of pixel-level segmen- tation and classiﬁcation of scene images with an entirely learning-based approach using Long Short Term Mem- ory (LSTM) recurrent neur
Automatically Discovering Local Visual Material Attributes  Gabriel Schwartz  Ko Nishino  Department of Computer Science, Drexel University  {gbs25,kon}@drexel.edu  Abstract  Shape cues play an important role in computer vision, but shape is not the only information available in images. Materials, such as fabric and plastic, are discernible in images even when shapes, such as those of an object, are not. We argue that it would be ideal to recognize materials without relying on object cues such a
Bayesian Sparse Representation for Hyperspectral Image Super Resolution  Naveed Akhtar, Faisal Shafait and Ajmal Mian  naveed.akhtar@research.uwa.edu.au, {faisal.shafait & ajmal.mian} @ uwa.edu.au  The University of Western Australia  35 Stirling Highway, Crawley, 6009. WA  Abstract  Despite the proven efﬁcacy of hyperspectral imaging in many computer vision tasks, its widespread use is hindered by its low spatial resolution, resulting from hardware lim- itations. We propose a hyperspectral imag
Fisher Vectors Meet Neural Networks: A Hybrid Classiﬁcation Architecture  Florent Perronnin and Diane Larlus  Computer Vision Group, Xerox Research Centre Europe  Abstract  Fisher Vectors (FV) and Convolutional Neural Networks (CNN) are two image classiﬁcation pipelines with different strengths. While CNNs have shown superior accuracy on a number of classiﬁcation tasks, FV classiﬁers are typically less costly to train and evaluate. We propose a hybrid archi- tecture that combines their strengths
(cid:212)•„‚‹(cid:218)•»·…”ﬁ–‡(cid:211)•‰ﬁ–(cid:243)(cid:190)¿›»·•†»(cid:215)‡¿„»—¿•ﬁ  (cid:222)»•¶•†„(cid:213)»§(cid:212)¿(cid:190)–ﬁ¿‹–ﬁ§–”(cid:211)«·‹•(cid:243)…•‡»†›•–†œ(cid:211)«·‹•(cid:243)›‰¿·»(cid:221)–‡(cid:176)«‹¿‹•–†¿·—‚–‹–„ﬁ¿(cid:176)‚§ł(cid:211)(cid:211)(cid:221)—(cid:247)(cid:244)  (cid:204)›•†„‚«¿¸†•“»ﬁ›•‹§(cid:244)(cid:222)»•¶•†„(cid:239)(cid:240)(cid:240)(cid:240)Ł(cid:236)(cid:221)‚•†¿  ˘‚–«‹–†„˘‚¿†„(cid:244)˙»(cid:190)•†(cid:212)•«(cid:244)ˇ•–†„‚¿•(cid:220)¿•  (cid:223)(cid:19
Hierarchical-PEP Model for Real-world Face Recognition  Haoxiang Li, Gang Hua  Stevens Institute of Technology  Hoboken, NJ 07030  {hli18, ghua}@stevens.edu  Abstract  Pose variation remains one of the major factors ad- versely affect the accuracy of real-world face recognition systems. Inspired by the recently proposed probabilistic elastic part (PEP) model and the success of the deep hi- erarchical architecture in a number of visual tasks, we pro- pose the Hierarchical-PEP model to approach th
)  %  (   e  t  a r   r o r r e    t s e  t    e g a r e v A  100  90  80  70  60  50  40  30  20    0     100     100     naive CLPL DLHD DLSD MCar−SVM  )  %  (   e  t  a r   r o r r e    t s e  t    e g a r e v A  0.2  0.4  0.6  0.8  1  Portion of ambiguously labeled samples ( )  90  80  70  60  50  40  30  20    0  naive CLPL DLHD DLSD MCar−SVM  0.2  0.4  0.6  0.8  1  )  %  (   e  t  a r   r o r r e     g n  i l  e b a  l    e g a r e v A  80  60  40  20  0    0  naive CLPL DLHD DLSD MC
Line-Based Multi-Label Energy Optimization for Fisheye Image Rectiﬁcation  and Calibration  Mi Zhang  Jian Yao† Menghan Xia Kai Li Yi Zhang Yaping Liu  School of Remote Sensing and Information Engineering, Wuhan University, Hubei, China  †  EMail:  jian.yao@whu.edu.cn  Web:  http://cvrs.whu.edu.cn  Abstract  Fisheye image rectiﬁcation and estimation of intrinsic parameters for real scenes have been addressed in the lit- erature by using line information on the distorted images. In this paper, we
Displets: Resolving Stereo Ambiguities using Object Knowledge  Fatma G¨uney MPI T¨ubingen  Andreas Geiger MPI T¨ubingen  fatma.guney@tue.mpg.de  andreas.geiger@tue.mpg.de  Abstract  Stereo techniques have witnessed tremendous progress over the last decades, yet some aspects of the problem still remain challenging today. Striking examples are reﬂect- ing and textureless surfaces which cannot easily be recov- ered using traditional local regularizers. In this paper, we therefore propose to regular
Face Alignment using Cascade Gaussian Process Regression Trees  Donghoon Lee Chang D. Yoo Korea Advanced institute of Science and Technology  Hyunsin Park  291 Daehak-ro, Yuseong-gu, Daejeon, Korea  {iamdh, hs.park, cd yoo}@kaist.ac.kr  Abstract  In this paper, we propose a face alignment method that uses cascade Gaussian process regression trees (cG- PRT) constructed by combining Gaussian process regres- sion trees (GPRT) in a cascade stage-wise manner. Here, GPRT is a Gaussian process with a k
A Fast Algorithm for Elastic Shape Distances between Closed Planar Curves  G¨unay Do˘gan1  ,  2  Javier Bernal2 Charles R. Hagwood2  1 Theiss Research 2 National Institute of Standards and Technology (NIST)  {gunay.dogan,javier.bernal,charles.hagwood}@nist.gov  Abstract  Effective computational tools for shape analysis are needed in many areas of science and engineering. We ad- dress this and propose a new fast iterative algorithm to com- pute the elastic geodesic distance between shapes of clos
Reﬂection Removal for In-Vehicle Black Box Videos  Christian Simon and In Kyu Park  Department of Information and Communication Engineering, Inha University  {sen.christiansimon@gmail.com, pik@inha.ac.kr}  Incheon 402-751, Korea  Abstract  The in-vehicle black box camera (dashboard camera) has become a popular device in many countries for security monitoring and event capturing. The readability of video content is the most critical matter, however, the content is often degraded due to the windsc
Effective Face Frontalization in Unconstrained Images  Tal Hassner1  Shai Harel1 †  Eran Paz1 †  1 The open University of Israel  2 Adience  Roee Enbar2  Figure 1: Frontalized faces. Top: Input photos; bottom: our frontalizations, obtained without estimating 3D facial shapes.  Abstract  “Frontalization” is the process of synthesizing frontal facing views of faces appearing in single unconstrained photos. Recent reports have suggested that this process may substantially boost the performance of f
Weakly Supervised Localization of Novel Objects Using Appearance Transfer  Mrigank Rochan  Department of Computer Science University of Manitoba, Canada  Yang Wang  Department of Computer Science University of Manitoba, Canada  mrochan@cs.umanitoba.ca  ywang@cs.umanitoba.ca  Abstract  We consider the problem of localizing unseen objects in weakly labeled image collections. Given a set of images an- notated at the image level, our goal is to localize the object in each image. The novelty of our p
Shape-based Automatic Detection of a Large Number of 3D Facial Landmarks  Syed Zulqarnain Gilani Ajmal Mian School of Computer Science and Software Engineering,  Faisal Shafait  The University of Western Australia  {zulqarnain.gilani,faisal.shafait,ajmal.mian}@uwa.edu.au  Abstract  We present an algorithm for automatic detection of a large number of anthropometric landmarks on 3D faces. Our approach does not use texture and is completely shape based in order to detect landmarks that are morpholo
Superpixel-based Video Object Segmentation using Perceptual Organization and  Location Prior  Daniela Giordano  Francesca Murabito  Simone Palazzo  Concetto Spampinato  Department of Electrical, Electronic and Computer Engineering  {dgiordan, palazzosim, cspampin}@dieei.unict.it, francescamurabito@gmail.com  University of Catania  Abstract  In this paper we present an approach for segmenting ob- jects in videos taken in complex scenes with multiple and different targets. The method does not make
CameraIntrinsicBlurKernelEstimation:AReliableFrameworkAliMosleh1PaulGreen2EmmanuelOnzon2IsabelleBegin2J.M.PierreLanglois11´EcolePolytechniquedeMontre´al,Montr´eal,QC,Canada2AlgoluxInc.,Montre´al,QC,Canada{ali.mosleh,pierre.langlois}@polymtl.ca{paul.green,emmanuel.onzon,isabelle.begin}@algolux.comAbstractThispaperpresentsareliablenon-blindmethodtomea-sureintrinsiclensblur.Weﬁrstintroduceanaccuratecamera-scenealignmentframeworkthatavoidserroneoushomographyestimationandcameratonecurveestimation.Thi

TVSum: Summarizing Web Videos Using Titles  Yale Song, Jordi Vallmitjana, Amanda Stent, Alejandro Jaimes  Yahoo Labs, New York  {yalesong,jvallmi,stent,ajaimes}@yahoo-inc.com  Abstract  Video summarization is a challenging problem in part because knowing which part of a video is important requires prior knowledge about its main topic. We present TVSum, an unsupervised video summarization framework that uses title-based image search results to ﬁnd visually important shots. We observe that a video
Constrained Planar Cuts - Object Partitioning for Point Clouds  Markus Schoeler, Jeremie Papon and Florentin W¨org¨otter Bernstein Center for Computational Neuroscience (BCCN)  III Physikalisches Institut - Biophysik, Georg-August University of G¨ottingen  {mschoeler,jpapon,worgott}@gwdg.de  Abstract  ples [5, 9, 15, 18].  While humans can easily separate unknown objects into meaningful parts, recent segmentation methods can only achieve similar partitionings by training on human- annotated grou
Inverting RANSAC: Global Model Detection via Inlier Rate Estimation  Roee Litman∗  Simon Korman∗ Alex Bronstein  School of Electrical Engineering, Tel-Aviv university  Shai Avidan  Abstract  This work presents a novel approach for detecting in- liers in a given set of correspondences (matches). It does so without explicitly identifying any consensus set, based on a method for inlier rate estimation (IRE). Given such an estimator for the inlier rate, we also present an algorithm that detects a gl
Local High-order Regularization on Data Manifolds  Kwang In Kim  Lancaster University  James Tompkin Harvard SEAS  Hanspeter Pfister Harvard SEAS  Christian Theobalt MPI for Informatics  Abstract  The common graph Laplacian regularizer is well-established in semi-supervised learning and spectral dimensionality reduction. However, as a first-order regularizer, it can lead to degenerate functions in high-dimensional manifolds. The iterated graph Laplacian enables high-order regularization, but it 
Robust Reconstruction of Indoor Scenes  Sungjoon Choi∗ †  Qian-Yi Zhou∗ ‡  Vladlen Koltun‡  Abstract  We present an approach to indoor scene reconstruction from RGB-D video. The key idea is to combine geomet- ric registration of scene fragments with robust global opti- mization based on line processes. Geometric registration is error-prone due to sensor noise, which leads to alias- ing of geometric detail and inability to disambiguate dif- ferent surfaces in the scene. The presented optimization
Generating Holistic 3D Scene Abstractions for Text-based Image Retrieval  Ang Li  Jin Sun Joe Yue-Hei Ng Ruichi Yu Vlad I. Morariu Larry S. Davis  Institution for Advanced Computer Studies  Univesrity of Maryland, College Park, MD 20742  {angli,jinsun,yhng,richyu,morariu,lsd}@umiacs.umd.edu  Abstract  Spatial relationships between objects provide important information for text-based image retrieval. As users are more likely to describe a scene from a real world perspec- tive, using 3D spatial re
One-Shot Video Object Segmentation  S. Caelles1,* K.-K. Maninis1,∗  J. Pont-Tuset1 1ETH Z¨urich  L. Leal-Taix´e2 D. Cremers2 L. Van Gool1  2TU M¨unchen  Figure 1. Example result of our technique: The segmentation of the ﬁrst frame (red) is used to learn the model of the speciﬁc object to track, which is segmented in the rest of the frames independently (green). One every 20 frames shown of 90 in total.  Abstract  ...to One(cid:173)Shot Video Object Segmentation  This paper tackles the task of se
Polyhedral Conic Classiﬁers for Visual Object Detection and Classiﬁcation  Hakan Cevikalp  Bill Triggs  Eskisehir Osmangazi University  Laboratoire Jean Kuntzmann  Meselik Kampusu, 26480, Eskisehir, Turkey  B.P. 53, 38041 Grenoble Cedex 9, France  hakan.cevikalp@gmail.com  bill.triggs@imag.fr  Abstract  We propose a family of quasi-linear discriminants that outperform current large-margin methods in sliding win- dow visual object detection and open set recognition tasks. In these tasks the class
Robust Interpolation of Correspondences for Large Displacement Optical Flow  Yinlin Hu1  Yunsong Li1  Rui Song1,2, ∗  huyinlin@gmail.com  ysli@mail.xidian.edu.cn  rsong@xidian.edu.cn  Abstract  The interpolation of correspondences (EpicFlow) was widely used for optical ﬂow estimation in most-recent works. It has the advantage of edge-preserving and efﬁ- ciency. However, it is vulnerable to input matching noise, which is inevitable in modern matching techniques. In this paper, we present a Robust
IRINA: Iris Recognition (even) in Inaccurately Segmented Data  Hugo Proenc¸a and Jo˜ao C. Neves IT - Instituto de Telecomunicac¸ ˜oes University of Beira Interior, Portugal  {hugomcp,jcneves}@di.ubi.pt  Abstract  The effectiveness of current iris recognition systems de- pends on the accurate segmentation and parameterisation of the iris boundaries, as failures at this point misalign the coefﬁcients of the biometric signatures. This paper de- scribes IRINA, an algorithm for Iris Recognition that 
Video Frame Interpolation via Adaptive Convolution  Simon Niklaus∗  Long Mai∗  Feng Liu  Portland State University  Portland State University  Portland State University  sniklaus@pdx.edu  mtlong@cs.pdx.edu  fliu@cs.pdx.edu  Abstract  Video frame interpolation typically involves two steps: motion estimation and pixel synthesis. Such a two-step ap- proach heavily depends on the quality of motion estima- tion. This paper presents a robust video frame interpo- lation method that combines these two s
PolyNet: A Pursuit of Structural Diversity in Very Deep Networks  Xingcheng Zhang∗  Zhizhong Li∗  Chen Change Loy  Dahua Lin  {zx016, lz015, ccloy, dhlin}@ie.cuhk.edu.hk  Abstract  Output  A number of studies have shown that increasing the depth or width of convolutional networks is a rewarding ap- proach to improve the performance of image recognition. In our study, however, we observed difﬁculties along both di- rections. On one hand, the pursuit for very deep networks is met with a diminishin
Accurate Optical Flow via Direct Cost Volume Processing  Jia Xu  Ren´e Ranftl  Vladlen Koltun  Intel Labs  Abstract  We present an optical ﬂow estimation approach that op- erates on the full four-dimensional cost volume. This direct approach shares the structural beneﬁts of leading stereo matching pipelines, which are known to yield high accu- racy. To this day, such approaches have been considered impractical due to the size of the cost volume. We show that the full four-dimensional cost volume
Context-Aware Correlation Filter Tracking  Matthias Mueller, Neil Smith, Bernard Ghanem  King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia  {matthias.mueller.2, neil.smith, bernard.ghanem}@kaust.edu.sa  Abstract  Correlation ﬁlter (CF) based trackers have recently gained a lot of popularity due to their impressive per- formance on benchmark datasets, while maintaining high frame rates. A signiﬁcant amount of recent research focuses on the incorporation of stronger 
SCC: Semantic Context Cascade for Efﬁcient Action Detection  Fabian Caba Heilbron, Wayner Barrios, Victor Escorcia and Bernard Ghanem  King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia  http://www.cabaf.net/scc  Abstract  Despite the recent advances in large-scale video analy- sis, action detection remains as one of the most challeng- ing unsolved problems in computer vision. This snag is in part due to the large volume of data that needs to be ana- lyzed to detect
Deep Learning of Human Visual Sensitivity in Image Quality Assessment  Framework  Department of Electrical and Electronic Engineering, Yonsei Universiy, Seoul, Korea  Jongyoo Kim  Sanghoon Lee∗  {jongky, slee}@yonsei.ac.kr  Abstract  Since human observers are the ultimate receivers of dig- ital images, image quality metrics should be designed from a human-oriented perspective. Conventionally, a number of full-reference image quality assessment (FR-IQA) meth- ods adopted various computational mod
FFTLasso: Large-Scale LASSO in the Fourier Domain  Adel Bibi, Hani Itani, Bernard Ghanem  King Abdullah University of Science and Technology (KAUST), Saudi Arabia adel.bibi@kaust.edu.sa, hmi10@mail.aub.edu, bernard.ghanem@kaust.edu.sa  Abstract  In this paper, we revisit the LASSO sparse representa- tion problem, which has been studied and used in a variety of different areas, ranging from signal processing and in- formation theory to computer vision and machine learning. In the vision community
Universal adversarial perturbations  Seyed-Mohsen Moosavi-Dezfooli∗ †  Alhussein Fawzi∗ †  seyed.moosavi@epfl.ch  hussein.fawzi@gmail.com  Omar Fawzi‡  Pascal Frossard†  omar.fawzi@ens-lyon.fr  pascal.frossard@epfl.ch  Abstract  Given a state-of-the-art deep neural network classiﬁer, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassiﬁed with high probability. We propose a sys- tematic algorithm for computing univers
Action Unit Detection with Region Adaptation, Multi-labeling Learning and  Optimal Temporal Fusing  Wei Li  Farnaz Abtahi  Zhigang Zhu  Dept of Electrical Engineering  Dept of Computer Science  Dept of Computer Science  CUNY City College  New York, USA  CUNY Graduate Center  CUNY Graduate Center and City College  New York, USA  New York, USA  wli3@ccny.cuny.edu  fabtahi@gradcenter.cuny.edu  zhu@cs.ccny.cuny.edu  Abstract  Action Unit (AU) detection becomes essential for fa- cial analysis. Many p
Deep MANTA: A Coarse-to-ﬁne Many-Task Network for joint 2D and 3D vehicle  analysis from monocular image  Florian Chabot1 , Mohamed Chaouch1 , Jaonary Rabarisoa1 , C´eline Teuli`ere2 , Thierry Chateau2  1 CEA-LIST Vision and Content Engineering Laboratory, 2 Pascal Institute, Blaise Pascal University  1{florian.chabot, mohamed.chaouch, jaonary.rabarisoa}@cea.fr  2{celine.teuliere, thierry.chateau}@univ-bpclermont.fr  Abstract  In this paper, we present a novel approach, called Deep MANTA (Deep M
ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on  Weakly-Supervised Classiﬁcation and Localization of Common Thorax Diseases  Xiaosong Wang1, Yifan Peng 2, Le Lu 1, Zhiyong Lu 2, Mohammadhadi Bagheri 1, Ronald M. Summers 1  1Department of Radiology and Imaging Sciences, Clinical Center,  2 National Center for Biotechnology Information, National Library of Medicine,  National Institutes of Health, Bethesda, MD 20892  {xiaosong.wang,yifan.peng,le.lu,luzh,mohammad.bagheri,rms}@nih
Loss Max-Pooling for Semantic Image Segmentation  Samuel Rota Bul`o⋆,†  Gerhard Neuhold†  Peter Kontschieder†  †Mapillary - Graz, Austria - {samuel,gerhard,pkontschieder}@mapillary.com  ⋆FBK - Trento, Italy - rotabulo@fbk.eu  Abstract  We introduce a novel loss max-pooling concept for han- dling imbalanced training data distributions, applicable as alternative loss layer in the context of deep neural net- works for semantic image segmentation. Most real-world semantic segmentation datasets exhib
Learned Contextual Feature Reweighting for Image Geo-Localization  Hyo Jin Kim  Enrique Dunn  UNC Chapel Hill  Stevens Institute of Technology  Jan-Michael Frahm UNC Chapel Hill  hyojin@cs.unc.edu  edunn@stevens.edu  jmf@cs.unc.edu  Abstract  We address the problem of  large scale image geo- localization where the location of an image is estimated by identifying geo-tagged reference images depicting the same place. We propose a novel model for learning image rep- resentations that integrates con
CLKN: Cascaded Lucas-Kanade Networks for Image Alignment  Che-Han Chang  Chun-Nan Chou HTC Research  Edward Y. Chang  {CheHan Chang,Jason.CN Chou,Edward Chang}@htc.com  Abstract  This paper proposes a data-driven approach for image alignment. Our main contribution is a novel network archi- tecture that combines the strengths of convolutional neural networks (CNNs) and the Lucas-Kanade algorithm. The main component of this architecture is a Lucas-Kanade layer that performs the inverse composition
Tail  Trunk  M ouse  Eleph- ant  Elp.  Shrew  Small  Large  B ush  PredicPion:   E lepOMnP S OreR   Detection H as H as attribute Looks like Found in  2673  2674  2675  C OC O  Detections: P erson C ar B icycle  Detected  Nodes  P ropagation   Step  Reorder  and Zero-  pad  0  To C lassification   Net  t=1  t=2  Output  Network  Nodes  Expanded  by  Importance  P ropagation   Step  0  t=T  xinit xinit  0 0  P rop. Net  h(1) h(1)  init init  h(2) h(2)  init init  h(1) h(1)  ad j1 ad j1  h(2) h
UltraStereo: Efﬁcient Learning-based Matching for Active Stereo Systems  Sean Ryan Fanello∗  Julien Valentin∗ Christoph Rhemann∗  Adarsh Kowdle∗ Vladimir Tankovich∗ Philip Davidson  Shahram Izadi  perceptiveIO  Abstract  Efﬁcient estimation of depth from pairs of stereo images is one of the core problems in computer vision. We efﬁciently solve the specialized problem of stereo matching under ac- tive illumination using a new learning-based algorithm. This type of ‘active’ stereo i.e. stereo matc
Accurate Depth and Normal Maps  from Occlusion-Aware Focal Stack Symmetry  Michael Strecke, Anna Alperovich, and Bastian Goldluecke  University of Konstanz  firstname.lastname@uni-konstanz.de  Abstract  We introduce a novel approach to jointly estimate con- sistent depth and normal maps from 4D light ﬁelds, with two main contributions. First, we build a cost volume from focal stack symmetry. However, in contrast to previous ap- proaches, we introduce partial focal stacks in order to be able to r
3D Human Pose Estimation from a Single Image via Distance Matrix Regression  Institut de Rob`otica i Inform`atica Industrial (CSIC-UPC), 08028, Barcelona, Spain  Francesc Moreno-Noguer  Abstract  This paper addresses the problem of 3D human pose esti- mation from a single image. We follow a standard two-step pipeline by ﬁrst detecting the 2D position of the N body joints, and then using these observations to infer 3D pose. For the ﬁrst step, we use a recent CNN-based detector. For the second ste
A New Representation of Skeleton Sequences for 3D Action Recognition  Qiuhong Ke1, Mohammed Bennamoun1, Senjian An1, Ferdous Sohel2, Farid Boussaid1  1The University of Western Australia  2Murdoch University  qiuhong.ke@research.uwa.edu.au  {mohammed.bennamoun,senjian.an,farid.boussaid}@uwa.edu.au  f.sohel@murdoch.edu.au  Abstract  This paper presents a new method for 3D action recogni- tion with skeleton sequences (i.e., 3D trajectories of human skeleton joints). The proposed method ﬁrst transf
Gated Feedback Reﬁnement Network for Dense Image Labeling  Md Amirul Islam, Mrigank Rochan, Neil D. B. Bruce, and Yang Wang  Department of Computer Science, University of Manitoba  Winnipeg, MB, Canada  {amirul, mrochan, bruce, ywang}@cs.umanitoba.ca  Abstract  Effective integration of local and global contextual infor- mation is crucial for dense labeling problems. Most existing methods based on an encoder-decoder architecture simply concatenate features from earlier layers to obtain higher- fr
3826  Scotopic vision studies the tradeoff between accuracy and exposure time. It is compelling in situations such as 1) au- tonomous driving [11] and competitive robotics [41], where the desired response time does not guarantee good quality pictures, and 2) medical imaging / classiﬁcation [38] and as- trophysics [27], where photons are expensive due to photo- toxicity or prolonged acquisition times.  Scotopic vision also gains prominence thanks to the re- cent development of photon-counting im
Minimum Delay Moving Object Detection  Dong Lao and Ganesh Sundaramoorthi  King Abdullah University of Science & Technology (KAUST), Saudi Arabia  {dong.lao, ganesh.sundaramoorthi}@kaust.edu.sa  Abstract  We present a general framework and method for detec- tion of an object in a video based on apparent motion. The object moves relative to background motion at some un- known time in the video, and the goal is to detect and seg- ment the object as soon it moves in an online manner. Due to unrelia
Ñ²´·²» ß­§³³»¬®·½ Í·³·´¿®·¬§ Ô»¿®²·²¹ º±® Ý®±­­óÓ±¼¿´ Î»¬®·»ª¿´  Ç·´·²¹ É«ïåîô Í¸«¸«· É¿²¹ïô Ï·²¹³·²¹ Ø«¿²¹ïåî  ïÕ»§ Ô¿¾ ±º ×²¬»´´ò ×²º±ò Ð®±½»­­òô ×²­¬ò ±º Ý±³°«¬ò Ì»½¸ô Ý¸·²»­» ß½¿¼»³§ ±º Í½·»²½»­ô Ý¸·²¿  îË²·ª»®­·¬§ ±º Ý¸·²»­» ß½¿¼»³§ ±º Í½·»²½»­ô Ý¸·²¿  §·´·²¹ò©«àª·°´ò·½¬ò¿½ò½²ô ©¿²¹­¸«¸«·à·½¬ò¿½ò½²ô ¯³¸«¿²¹à«½¿­ò¿½ò½²  ß¾­¬®¿½¬  Ý®±­­ó³±¼¿´ ®»¬®·»ª¿´ ¸¿­ ¿¬¬®¿½¬»¼ ·²¬»²­·ª» ¿¬¬»²¬·±² ·² ®»½»²¬ §»¿®­ò Ó»¿­«®·²¹ ¬¸» ­»³¿²¬·½ ­·³·´¿®·¬§ ¾»¬©»»² ¸»¬»®±¹»²»±«­ ¼¿¬¿ ±¾¶»½¬­ ·­ ¿² »­­»²¬·¿´ §»¬ ½¸
Learning non-maximum suppression  Jan Hosang  Rodrigo Benenson  Bernt Schiele  Max Planck Institut für Informatik  Saarbrücken, Germany  firstname.lastname@mpi-inf.mpg.de  Abstract  Object detectors have hugely proﬁted from moving to- wards an end-to-end learning paradigm: proposals, fea- tures, and the classiﬁer becoming one neural network improved results two-fold on general object detection. One indispensable component is non-maximum suppression (NMS), a post-processing algorithm responsible 
POSEidon: Face-from-Depth for Driver Pose Estimation  Guido Borghi Marco Venturelli Roberto Vezzani Rita Cucchiara  University of Modena and Reggio Emilia  {name.surname}@unimore.it  Abstract  Fast and accurate upper-body and head pose estima- tion is a key task for automatic monitoring of driver at- tention, a challenging context characterized by severe il- lumination changes, occlusions and extreme poses. In this work, we present a new deep learning framework for head localization and pose est
Convex Global 3D Registration with Lagrangian Duality  Jesus Briales  Javier Gonzalez-Jimenez  MAPIR-UMA Group  University of Malaga, Spain  {jesusbriales,javiergonzalez}@uma.es  Abstract  The registration of 3D models by a Euclidean transfor- mation is a fundamental task at the core of many applica- tion in computer vision. This problem is non-convex due to the presence of rotational constraints, making traditional local optimization methods prone to getting stuck in lo- cal minima. This paper 
Illuminant-Camera Communication to Observe Moving Objects under Strong  External Light by Spread Spectrum Modulation  Ryusuke Sagawa and Yutaka Satoh  The National Institute of Advanced Industrial Science and Technology Tsukuba Central 1, 1-1-1 Umezono, Tsukuba, Ibaraki 305-8560 Japan  {ryusuke.sagawa,yu.satou}@aist.go.jp  Abstract  Many algorithms of computer vision use light sources to illuminate objects to actively create situation appropri- ate to extract their characteristics. For example, 
Using Ranking-CNN for Age Estimation  Shixing Chen1 Caojin Zhang2 Ming Dong1  Jialiang Le3 Mike Rao3  1Department of Computer Science  2Department of Mathematics  3Research & Innovation Center  Wayne State University  Wayne State University  Ford Motor Company  {schen, czhang, mdong}@wayne.edu  {jle1, mrao}@ford.com  Abstract  Human age is considered an important biometric trait for human identiﬁcation or search. Recent research shows that the aging features deeply learned from large-scale data 
Hyperspectral image super-resolution via non-local sparse tensor factorization  Renwei Dian, Leyuan Fang, Shutao Li  College of Electrical and Information Engineering, Hunan University  drw@hnu.edu.cn, fangleyuan@gmail.com, shutao li@hnu.edu.cn  Abstract  Hyperspectral image (HSI) super-resolution, which fuses a low-resolution (LR) HSI with a high-resolution (HR) mul- tispectral image (MSI), has recently attracted much atten- tion. Most of the current HSI super-resolution approaches are based on
Event-based Visual Inertial Odometry  Alex Zihao Zhu, Nikolay Atanasov, Kostas Daniilidis  University of Pennsyvlania  {alexzhu, atanasov, kostas}@seas.upenn.edu  Abstract  Event-based cameras provide a new visual sensing model by detecting changes in image intensity asyn- chronously across all pixels on the camera. By providing these events at extremely high rates (up to 1MHz), they al- low for sensing in both high speed and high dynamic range situations where traditional cameras may fail. In t
Reﬂection Removal Using Low-Rank Matrix Completion  Byeong-Ju Han  Jae-Young Sim  School of Electrical Engineering  Ulsan National Institute of Science and Technology, Ulsan, Korea  {bjhan, jysim}@unist.ac.kr  Abstract  The images taken through glass often capture a target transmitted scene as well as undesired reﬂected scenes. In this paper, we propose a low-rank matrix completion algo- rithm to remove reﬂection artifacts automatically from mul- tiple glass images taken at slightly different ca
A Minimal Solution for Two-view Focal-length Estimation using  Two Afﬁne Correspondences  Daniel Barath, Tekla Toth, and Levente Hajder  Machine Perception Research Laboratory  MTA SZTAKI, Budapest, Hungary  {barath.daniel,hajder.levente}@sztaki.mta.hu  Abstract  A minimal solution using two afﬁne correspondences is presented to estimate the common focal length and the fun- damental matrix between two semi-calibrated cameras – known intrinsic parameters except a common focal length. To the best 
Deep Affordance-grounded Sensorimotor Object Recognition  Spyridon Thermos1  ,  2 Georgios Th. Papadopoulos1  Petros Daras1 Gerasimos Potamianos2  1Information Technologies Institute, Centre for Research and Technology Hellas, Greece  2Department of Electrical and Computer Engineering, University of Thessaly, Greece  {spthermo,papad,daras}@iti.gr  gpotam@ieee.org  Abstract  It is well-established by cognitive neuroscience that hu- man perception of objects constitutes a complex process, where ob
All You Need is Beyond a Good Init: Exploring Better Solution for Training Extremely Deep Convolutional Neural Networks with Orthonormality and  Modulation  Di Xie  Jiang Xiong  Shiliang Pu  xiedi@hikvision.com  xiongjiang@hikvision.com Hikvision Research Institute  Hangzhou, China  pushiliang@hikvision.com  Abstract  Deep neural network is difﬁcult to train and this predica- ment becomes worse as the depth increases. The essence of this problem exists in the magnitude of backpropagated errors t
Alternating Direction Graph Matching  D. Khuˆe Lˆe-Huu  Nikos Paragios  CentraleSup´elec, Universit´e Paris-Saclay, France {khue.le, nikos.paragios}@centralesupelec.fr  Abstract  In this paper, we introduce a graph matching method that can account for constraints of arbitrary order, with ar- bitrary potential functions. Unlike previous decomposition approaches that rely on the graph structures, we introduce a decomposition of the matching constraints. Graph match- ing is then reformulated as a n
DUST: Dual Union of Spatio-Temporal Subspaces for Monocular  Multiple Object 3D Reconstruction  Antonio Agudo  Francesc Moreno-Noguer  Institut de Rob`otica i Inform`atica Industrial (CSIC-UPC), 08028, Barcelona, Spain  Abstract  I nput :  2D t raj ect ories  Spat ial Sim ilarit y  We present an approach to reconstruct the 3D shape of multiple deforming objects from incomplete 2D trajectories acquired by a single camera. Additionally, we simultane- ously provide spatial segmentation (i.e., we id
Multi-Task Clustering of Human Actions by Sharing Information  Xiaoqiang Yan, Shizhe Hu, Yangdong Ye*  School of Information Engineering  Zhengzhou University, 450000, Zhengzhou, China  iexqyan@gmail.com, ieszhu@gs.zzu.edu.cn, ieydye@zzu.edu.cn  Abstract  Sharing information between multiple tasks can enhance the accuracy of human action recognition systems. Howev- er, using shared information to improve multi-task human action clustering has never been considered before, and cannot be achieved 
Deeply Aggregated Alternating Minimization for Image Restoration  Youngjung Kim1∗, Hyungjoo Jung1∗, Dongbo Min2, and Kwanghoon Sohn1†  1Yonsei University  2Chungnam National University  Abstract  age decomposition [8].  Regularization-based image restoration has remained an active research topic in image processing and computer vi- sion. It often leverages a guidance signal captured in dif- ferent ﬁelds as an additional cue. In this work, we present a general framework for image restoration, cal
Leveraging captions to learn a global visual representation for semantic retrieval  Beyond instance-level image retrieval:  Albert Gordo and Diane Larlus  Computer Vision group, Xerox Research Center Europe  firstname.name@xrce.xerox.com  Abstract  Querying with an example image is a simple and intuitive interface to retrieve information from a visual database. Most of the research in image retrieval has focused on the task of instance-level image retrieval, where the goal is to retrieve images 
Fast Boosting based Detection using Scale Invariant  Multimodal Multiresolution Filtered Features  Arthur Daniel Costea, Robert Varga and Sergiu Nedevschi  Image Processing and Pattern Recognition Research Center  Technical University of Cluj-Napoca, Romania  {arthur.costea, robert.varga, sergiu.nedevschi}@cs.utcluj.ro  Abstract  In this paper we propose a novel boosting-based slid- ing window solution for object detection which can keep up with the precision of the state-of-the art deep learnin
DOPE: Distributed Optimization for Pairwise Energies  Jose Dolz  Ismail Ben Ayed  jose.dolz@livia.etsmtl.ca  ismail.benayed@etsmtl.ca  Christian Desrosiers  Laboratory for Imagery, Vision and Artiﬁcial Intelligence  Ecole de Technologie Superieure, Montreal, Canada  christian.desrosiers@etsmtl.ca  Abstract  We formulate an Alternating Direction Method of Mul- tipliers (ADMM) that systematically distributes the compu- tations of any technique for optimizing pairwise functions, including non-submo
Temporal Action Co-segmentation in 3D Motion Capture Data and Videos  Konstantinos Papoutsakis1,2 , Costas Panagiotakis1,3 , Antonis A. Argyros1,2  1 Computational Vision and Robotics Laboratory, Institute of Computer Science, FORTH, Greece  2 Computer Science Department, University of Crete, Greece  3 Business Administration Department (Agios Nikolaos), TEI of Crete, Greece  {papoutsa,cpanag,argyros}@ics.forth.gr  Abstract  Given two action sequences, we are interested in spotting/co-segmenting
Comprehension-guided referring expressions  Ruotian Luo TTI-Chicago  rluo@ttic.edu  Gregory Shakhnarovich  TTI-Chicago  greg@ttic.edu  Abstract  We consider generation and comprehension of natural language referring expression for objects in an image. Un- like generic “image captioning” which lacks natural stan- dard evaluation criteria, quality of a referring expression may be measured by the receiver’s ability to correctly infer which object is being described. Following this intuition, we pro
Modeling Sub-Event Dynamics in First-Person Action Recognition  Hasan F. M. Zaki†∗, Faisal Shafait‡ and Ajmal Mian†  † School of Computer Science and Software Engineering, The University of Western Australia  ‡ National University of Sciences and Technology, Pakistan  ∗ Department of Mechatronics Engineering, International Islamic University Malaysia  hasan.mohdzaki@research.uwa.edu.au, faisal.shafait@seeks.edu.pk, ajmal.mian@uwa.edu.au  Abstract  Frame 6  Frame 20  Frame 72  Frame 90  First-per
Speed/accuracy trade-offs for modern convolutional object detectors  Jonathan Huang Alireza Fathi  Vivek Rathod  Chen Sun  Menglong Zhu  Ian Fischer  Zbigniew Wojna  Yang Song  Anoop Korattikara Sergio Guadarrama  Kevin Murphy  Abstract  The goal of this paper is to serve as a guide for se- lecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usa
Semi-supervised Spectral Clustering for Image Set Classiﬁcation  Arif Mahmood, Ajmal Mian, Robyn Owens  School of Computer Science and Software Engineering, The University of Western Australia  {arif.mahmood, ajmal.mian, robyn.owens}@ uwa.edu.au  Abstract  We present an image set classiﬁcation algorithm based on unsupervised clustering of labeled training and unla- beled test data where labels are only used in the stopping criterion. The probability distribution of each class over the set of clu
Measuring Distance Between Unordered Sets of Different Sizes  Andrew Gardner,∗ Jinko Kanno∗ {abg010, jkanno}@latech.edu  Christian A. Duncan†  Rastko Selmic∗  christian.duncan@quinnipiac.edu  rselmic@latech.edu  ∗Louisiana Tech University, Ruston, LA 71270 †Quinnipiac University, Hamden, CT 06518  Abstract  where  We present a distance metric based upon the notion of minimum-cost injective mappings between sets. Our func- tion satisﬁes metric properties as long as the cost of the minimum mapping
Non-rigid Segmentation using Sparse Low Dimensional Manifolds and  Deep Belief Networks ∗  Jacinto C. Nascimento  Instituto de Sistemas e Rob´otica  Instituto Superior T´ecnico, Portugal  Abstract  In this paper, we propose a new methodology for seg- menting non-rigid visual objects, where the search pro- cedure is conducted directly on a sparse low-dimensional manifold, guided by the classiﬁcation results computed from a deep belief network. Our main contribution is the fact that we do not rely
Object Partitioning using Local Convexity  Simon Christoph Stein, Markus Schoeler, Jeremie Papon and Florentin W¨org¨otter  Bernstein Center for Computational Neuroscience (BCCN)  III Physikalisches Institut - Biophysik, Georg-August University of G¨ottingen  {scstein,mschoeler,jpapon,worgott}@physik3.gwdg.de  Abstract  The problem of how to arrive at an appropriate 3D- segmentation of a scene remains difﬁcult. While current state-of-the-art methods continue to gradually improve in benchmark per
Simultaneous Localization and Calibration: Self-Calibration of Consumer Depth Cameras  Qian-Yi Zhou∗  Vladlen Koltun†  Abstract  We describe an approach for simultaneous localization and calibration of a stream of range images. Our approach jointly optimizes the camera trajectory and a calibration function that corrects the camera’s unknown nonlinear dis- tortion. Experiments with real-world benchmark data and synthetic data show that our approach increases the accu- racy of camera trajectories 
Very Fast Solution to the PnP Problem with Algebraic Outlier Rejection  Luis Ferraz1  Xavier Binefa1  Francesc Moreno-Noguer2  1Department of Information and Communication Technologies, UPF, Barcelona, Spain  2Institut de Rob`otica i Inform`atica Industrial, CSIC-UPC, Barcelona, Spain  luis.ferraz@upf.edu, xavier.binefa@upf.edu, fmoreno@iri.upc.edu  Abstract  We propose a real-time, robust to outliers and accurate solution to the Perspective-n-Point (PnP) problem. The main advantages of our solu
Human Body Shape Estimation Using a Multi-Resolution Manifold Forest  Frank Perbet  Sam Johnson  Minh-Tri Pham  {frank.perbet,sam.johnson,minhtri.pham,bjorn.stenger}@crl.toshiba.co.uk  Toshiba Research Europe, Cambridge, UK  Bj¨orn Stenger  Abstract  This paper proposes a method for estimating the 3D body shape of a person with robustness to clothing. We formulate the problem as optimization over the manifold of valid depth maps of body shapes learned from synthetic training data. The manifold i
Discriminative Hierarchical Modeling of  Spatio-Temporally Composable Human Activities  Ivan Lillo, Alvaro Soto  P. Universidad Catolica de Chile. ialillo@uc.cl, asoto@ing.puc.cl  Juan Carlos Niebles  Universidad del Norte. Colombia.  njuan@uninorte.edu.co  Abstract  This paper proposes a framework for recognizing com- plex human activities in videos. Our method describes hu- man activities in a hierarchical discriminative model that operates at three semantic levels. At the lower level, body po
Fantope Regularization in Metric Learning  Marc T. Law  Nicolas Thome  Matthieu Cord  Sorbonne Universit´es, UPMC Univ Paris 06, UMR 7606, LIP6, F-75005, Paris, France  Abstract  Query  Top 5 Image Retrieval Results  This paper introduces a regularization method to ex- plicitly control the rank of a learned symmetric positive semideﬁnite distance matrix in distance metric learning. To this end, we propose to incorporate in the objective function a linear regularization term that minimizes the k 
Robust Online Multi-Object Tracking based on Tracklet Conﬁdence  and Online Discriminative Appearance Learning  Seung-Hwan Bae and Kuk-Jin Yoon  Computer Vision Laboratory, GIST, Korea  fbshwan, kjyoong@gist.ac.kr  Abstract  Online multi-object tracking aims at producing complete tracks of multiple objects using the information accumu- lated up to the present moment. It still remains a difﬁcult problem in complex scenes, because of frequent occlusion by clutter or other objects, similar appearan
	 	   !"#$!"%#&"'#()(!*  "+"),)-".")/!0+"11"+23.+"4.  56").+3, 1,)#-6").7"  89:;<=>?>@ ABC=>>?DE:FGH:IG<JH@ KL=>>MEH:IG<JH@ N;L=HE:ICFGH:IG<JH  OP  (cid:146)  QRSTUVWXYZR[TWT\STR[]]VW[Z^UW\R[VT]W__S`\RR\Ra bRTU[VcdVWY]_edVWY]_U^[UZWTU[STWT\cUfWU[VdRU_ghWV SThRVVSTd^Sd^\RiR\ZWTURjUU^[UZ[T`RY_RXUWSb]VWiR bY\USaU[VdRUUV[ZkSTdST[X[U[a[__WZS[USWT`[_RXhV[bRa fWVklmT\SkRbW_URjS_USTd[__WZS[USWTa`[_RXUV[Z
                                                                  
On the quotient representation for the essential manifold  Roberto Tron and Kostas Daniilidis∗  Abstract  The essential matrix, which encodes the epipolar con- straint between points in two projective views, is a corner- stone of modern computer vision. Previous works have pro- posed different characterizations of the space of essential matrices as a Riemannian manifold. However, they either do not consider the symmetric role played by the two views, or do not fully take into account the geometr
IncrementalFaceAlignmentintheWildAkshayAsthana1StefanosZafeiriou1ShiyangCheng1MajaPantic1,21DepartmentofComputing,ImperialCollegeLondon,UnitedKingdom2EEMCS,UniversityofTwente,Netherlands{a.asthana,s.zafeiriou,shiyang.cheng11,m.pantic}@imperial.ac.ukAbstractThedevelopmentoffacialdatabaseswithanabundanceofannotatedfacialdatacapturedunderunconstrained’in-the-wild’conditionshavemadediscriminativefacialde-formablemodelsthedefactochoiceforgenericfacialland-marklocalization.Eventhoughverygoodperformanc
One Millisecond Face Alignment with an Ensemble of Regression Trees  Vahid Kazemi and Josephine Sullivan KTH, Royal Institute of Technology  Computer Vision and Active Perception Lab  Teknikringen 14, Stockholm, Sweden  {vahidk,sullivan}@csc.kth.se  Abstract  This paper addresses the problem of Face Alignment for a single image. We show how an ensemble of regression trees can be used to estimate the face’s landmark positions directly from a sparse subset of pixel intensities, achieving super-rea
Automatic Feature Learning for Robust Shadow Detection  S. H. Khan, M. Bennamoun, F. Sohel, R. Togneri  The University of Western Australia  {salman.khan@research.,mohammed.bennamoun@,ferdous.sohel@,roberto.togneri@}uwa.edu.au  Abstract  We present a practical framework to automatically de- tect shadows in real world scenes from a single photograph. Previous works on shadow detection put a lot of effort in designing shadow variant and invariant hand-crafted fea- tures. In contrast, our framework
Simplex-Based 3D Spatio-Temporal Feature Description for Action Recognition  Hao Zhang, Wenjun Zhou, Christopher Reardon, Lynne E. Parker  University of Tennessee, Knoxville, TN 37996, USA  {haozhang,wzhou4,creardon,leparker}@utk.edu  Abstract  We present a novel feature description algorithm to de- scribe 3D local spatio-temporal features for human action recognition. Our descriptor avoids the singularity and lim- ited discrimination power issues of traditional 3D descrip- tors by quantizing an
In Search of Inliers: 3D Correspondence by Local and Global Voting  Anders Glent Buch1 Yang Yang2 Norbert Kr¨uger1 Henrik Gordon Petersen1  Maersk Mc-Kinney Moller Institute, University of Southern Denmark  1{anbu,norbert,hgp}@mmmi.sdu.dk  2yayan13@student.sdu.dk  Abstract  We present a method for ﬁnding correspondence between 3D models. From an initial set of feature correspondences, our method uses a fast voting scheme to separate the in- liers from the outliers. The novelty of our method lies
Multiview Shape and Reﬂectance from Natural Illumination  Geoffrey Oxholm and Ko Nishino  Department of Computing  Drexel University, Philadelphia, PA 19104, USA  {gao25,kon}@drexel.edu  Abstract  The world is full of objects with complex reﬂectances, situated in complex illumination environments. Past work on full 3D geometry recovery, however, has tried to han- dle this complexity by framing it into simplistic models of reﬂectance (Lambetian, mirrored, or diffuse plus specular) or illumination
Locally Optimized Product Quantization for Approximate Nearest Neighbor Search  Yannis Kalantidis and Yannis Avrithis  National Technical University of Athens  {ykalant, iavr}@image.ntua.gr  Abstract  We present a simple vector quantizer that combines low distortion with fast search and apply it to approximate near- est neighbor (ANN) search in high dimensional spaces. Leveraging the very same data structure that is used to pro- vide non-exhaustive search, i.e., inverted lists or a multi- index,
      Word Channel Based Multiscale Pedestrian Detection  Without Image Resizing and Using Only One Classifier   Arthur Daniel Costea       and       Sergiu Nedevschi   Image Processing and Pattern Recognition Group  (http://cv.utcluj.ro)   Computer Science Department, Technical University of Cluj-Napoca, Romania   {arthur.costea, sergiu.nedevschi}@cs.utcluj.ro   pedestrian or non-pedestrian based on image features. The  image features should capture the required information for  classification,
Generating object segmentation proposals using global and local search  Pekka Rantalankila, Juho Kannala, and Esa Rahtu  University of Oulu  Abstract  We present a method for generating object segmentation proposals from groups of superpixels. The goal is to pro- pose accurate segmentations for all objects of an image. The proposed object hypotheses can be used as input to object detection systems and thereby improve efﬁciency by replacing exhaustive search. The segmentations are gener- ated in 
Generalized Max Pooling  Naila Murray and Florent Perronnin  Computer Vision Group, Xerox Research Centre Europe  Abstract  State-of-the-art patch-based image representations in- volve a pooling operation that aggregates statistics com- puted from local descriptors. Standard pooling operations include sum- and max-pooling. Sum-pooling lacks discrim- inability because the resulting representation is strongly in- ﬂuenced by frequent yet often uninformative descriptors, but only weakly inﬂuenced by
Video Classiﬁcation using Semantic Concept Co-occurrences  Shayan Modiri Assari  Amir Roshan Zamir  Center for Research in Computer Vision, UCF  Mubarak Shah  Abstract  We address the problem of classifying complex videos based on their content. A typical approach to this problem is performing the classiﬁcation using semantic attributes, commonly termed concepts, which occur in the video. In this paper, we propose a contextual approach to video clas- siﬁcation based on Generalized Maximum Clique
Unsupervised Trajectory Modelling using Temporal Information via Minimal  Paths.  B. Cancela, A. Iglesias, M. Ortega, M. G. Penedo  VARPA Group, Universidade da Coru˜na Campus de Elvi˜na s/n, A Coru˜na, Spain  http://www.varpa.org/  Abstract  This paper presents a novel methodology for modelling pedestrian trajectories over a scene, based in the hypothe- sis that, when people try to reach a destination, they use the path that takes less time, taking into account environmen- tal information like 
Human Action Recognition Based on Context-Dependent Graph Kernels  Baoxin Wu, Chunfeng Yuan, and Weiming Hu  National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China  {bxwu,cfyuan,wmhu}@nlpr.ia.ac.cn  Abstract  Graphs are a powerful tool to model structured object- s, but it is nontrivial to measure the similarity between two graphs. In this paper, we construct a two-graph model to represent human actions by recording the spatial and tem- poral relationships among
First, we consider a new problem that is fundamentally dif- ferent from traditional video retrieval tasks. Most previous work on video retrieval ﬁnd relevant videos given a query, while we focus on matching individual words in the query to speciﬁc objects (i.e. a tracklet of 2D boxes) participating in sub-events in highly dynamic scenes. Second, we intro- duce a new dataset for semantic retrieval, which not only provides textual descriptions for each video segment, but also includes detailed as
Dual Linear Regression Based Classiﬁcation for Face Cluster Recognition  Liang Chen  University of Northern British Columbia Prince George, BC, Canada V2N 4Z9  chen.liang.97@gmail.com  Abstract  We are dealing with the face cluster recognition prob- lem where there are multiple images per subject in both gallery and probe sets. It is never guaranteed to have a clear spatio-temporal relation among the multiple images of each subject. Considering that the image vectors of each subject, either in g
Scale-space Processing Using Polynomial Representations  Gou Koutaki  Kumamoto University  2-39-1 Kurokami Kumamoto, Japan  Keiichi Uchimura  Kumamoto University  2-39-1 Kurokami Kumamoto, Japan  koutaki@cs.kumamoto-u.ac.jp  uchimura@cs.kumamoto-u.ac.jp  Abstract  In this study, we propose the application of principal components analysis (PCA) to scale-spaces. PCA is a stan- dard method used in computer vision. The translation of an input image into scale-space is a continuous operation, which r
Easy and Efﬁcient Solution of Joint Multi-Label and Estimation Problems  FAST LABEL:  Ganesh Sundaramoorthi KAUST, Saudi Arabia  Byung-Woo Hong  Chung-Ang University, Seoul, Korea  ganesh.sundaramoorthi@kaust.edu.sa  hong@cau.ac.kr  Abstract  We derive an easy-to-implement and efﬁcient algorithm for solving multi-label image partitioning problems in the form of the problem addressed by Region Competition [19]. These problems jointly determine a parameter for each of the regions in the partition.
Parallax-tolerant Image Stitching  Fan Zhang and Feng Liu  Department of Computer Science  Portland State University  {zhangfan,fliu}@cs.pdx.edu  Abstract  Parallax handling is a challenging task for image stitch- ing. This paper presents a local stitching method to handle parallax based on the observation that input images do not need to be perfectly aligned over the whole overlapping re- gion for stitching. Instead, they only need to be aligned in a way that there exists a local region where t
Dirichlet-based Histogram Feature Transform for Image Classiﬁcation  National Institute of Advanced Industrial Science and Technology  Takumi Kobayashi  Umezono 1-1-1, Tsukuba, Japan takumi.kobayashi@aist.go.jp  Abstract  Histogram-based features have signiﬁcantly contributed to recent development of image classiﬁcations, such as by SIFT local descriptors. In this paper, we propose a method to efﬁciently transform those histogram features for improv- ing the classiﬁcation performance. The (L1-no
Evolutionary Quasi-random Search for Hand Articulations Tracking  Iason Oikonomidis1,2 , Manolis I.A. Lourakis1 , Antonis A. Argyros1,2  1 Computational Vision and Robotics Laboratory, Institute of Computer Science, FORTH, Greece  2 Computer Science Department, University of Crete  {oikonom,lourakis,argyros}@ics.forth.gr  Abstract  We present a new method for tracking the 3D position, global orientation and full articulation of human hands. Following recent advances in model-based, hypothesize- 
Scalable 3D Tracking of Multiple Interacting Objects  Institute of Computer Science, FORTH and Computer Science Department, University of Crete  Nikolaos Kyriazis, Antonis Argyros  {kyriazis, argyros}@ics.forth.gr  Abstract  We consider the problem of tracking multiple interact- ing objects in 3D, using RGBD input and by considering a hypothesize-and-test approach. Due to their interaction, objects to be tracked are expected to occlude each other in the field of view of the camera observing them
Better Feature Tracking Through Subspace Constraints  Bryan Poling  University Of Minnesota  Gilad Lerman  University Of Minnesota  Arthur Szlam  City College of New York  poli0048@umn.edu  lerman@umn.edu  aszlam@ccny.cuny.edu  Abstract  Feature tracking in video is a crucial task in computer vi- sion. Usually, the tracking problem is handled one feature at a time, using a single-feature tracker like the Kanade- Lucas-Tomasi algorithm, or one of its derivatives. While this approach works quite w
Region-based particle ﬁlter for video object segmentation  David Varas and Ferran Marques  Universitat Politecnica de Catalunya Barcelona Tech  {david.varas,ferran.marques}@upc.edu ∗  Abstract  We present a video object segmentation approach that extends the particle ﬁlter to a region-based image repre- sentation. Image partition is considered part of the parti- cle ﬁlter measurement, which enriches the available infor- mation and leads to a re-formulation of the particle ﬁlter. The prediction s
Understanding Objects in Detail with Fine-grained Attributes  Andrea Vedaldi1  Siddharth Mahendran2  Stavros Tsogkas3  Subhransu Maji4  Ross Girshick5  Juho Kannala6  Esa Rahtu6  Matthew B. Blaschko3  David Weiss7  Ben Taskar8  Naomi Saphra2  Sammy Mohamed9  Iasonas Kokkinos3  Karen Simonyan1  Abstract  We study the problem of understanding objects in detail, intended as recognizing a wide array of ﬁne-grained object attributes. To this end, we introduce a dataset of 7,413 air- planes annotated 
Active Flattening of Curved Document Images via Two Structured Beams  Gaofeng MENG Ying WANG Shenquan QU Shiming XIANG Chunhong PAN  National Laboratory of Pattern Recognition (NLPR)  Institute of Automation, Chinese Academy of Sceiences (CASIA) No.95, Zhongguancun East Road, Beijing 100190, P.R. China {gfmeng, ywang, shenquan.qu, smxiang, chpan}@nlpr.ia.ac.cn  Abstract—Document images captured by a digital camera often suffer from serious geometric distortions. In this paper, we propose an acti
Fast MRF Optimization with Application to Depth Reconstruction  Qifeng Chen∗  Vladlen Koltun†  Abstract  We describe a simple and fast algorithm for optimizing Markov random ﬁelds over images. The algorithm performs block coordinate descent by optimally updating a horizon- tal or vertical line in each step. While the algorithm is not as accurate as state-of-the-art MRF solvers on traditional benchmark problems, it is trivially parallelizable and pro- duces competitive results in a fraction of a 
LATEX Author Guidelines for CVPR Proceedings  First Author Institution1  Institution1 address firstauthor@i1.org  Abstract  The ABSTRACT is to be in fully-justiﬁed italicized text, at the top of the left-hand column, below the author and afﬁliation information. Use the word “Abstract” as the title, in 12-point Times, boldface type, centered relative to the column, initially capitalized. The abstract is to be in 10- point, single-spaced type. Leave two blank lines after the Abstract, then begin t
Dual-Space Decomposition of 2D Complex Shapes ∗  Guilin Liu  gliu2@gmu.edu  Zhonghua Xi zxi@gmu.edu  Jyh-Ming Lien†  jmlien@cs.gmu.edu  Abstract  While techniques that segment shapes into visually meaningful parts have generated impressive results, these techniques also have only focused on relatively simple shapes, such as those composed of a single object either without holes or with few simple holes. In many applica- tions, shapes created from images can contain many over- lapping objects and
Noising versus Smoothing for Vertex Identiﬁcation in Unknown Shapes  Konstantinos A. Raftopoulos  ∗  and Marin Ferecatu  †  Centre d’Etudes et De Recherche en Informatique et Communications(CEDRIC)  ∗  Conservatoire National des Arts et M´etiers (CNAM), 292 Rue St Martin FR-75141 Paris Cedex 03  †  Centre d’Etudes et De Recherche en Informatique et Communications(CEDRIC)  Email:konstantinos.raftopoulos@cnam.fr  Conservatoire National des Arts et M´etiers (CNAM), 292 Rue St Martin FR-75141 Paris 
Dense Non-Rigid Shape Correspondence using Random Forests  Emanuele Rodol`a  TU M¨unchen  rodola@in.tum.de  Samuel Rota Bul`o  Fondazione Bruno Kessler  rotabulo@fbk.eu  Thomas Windheuser  TU M¨unchen  windheus@in.tum.de  Matthias Vestner  TU M¨unchen  vestner@in.tum.de  Daniel Cremers TU M¨unchen cremers@tum.de  Abstract  We propose a shape matching method that produces dense correspondences tuned to a speciﬁc class of shapes and deformations. In a scenario where this class is rep- resented by 
Automatic Face Reenactment  Pablo Garrido1  Thorsten Thorm¨ahlen2  Levi Valgaerts1 Patrick P´erez3  Ole Rehmsen1 Christian Theobalt1 2Philipps-Universit¨at Marburg  3Technicolor  1MPI for Informatics  Abstract  We propose an image-based, facial reenactment system that replaces the face of an actor in an existing target video with the face of a user from a source video, while preserv- ing the original target performance. Our system is fully au- tomatic and does not require a database of source ex
Remote Heart Rate Measurement From Face Videos Under Realistic Situations  Xiaobai Li, Jie Chen, Guoying Zhao, Matti Pietik¨ainen  CMV, University of Oulu, Finland  {lxiaobai, jchen, gyzhao, mkp}@ee.oulu.fi  Abstract  Heart rate is an important indicator of people’s physio- logical state. Recently, several papers reported methods to measure heart rate remotely from face videos. Those meth- ods work well on stationary subjects under well controlled conditions, but their performance signiﬁcantly d
2014 IEEE Conference on Computer Vision and Pattern Recognition 2014 IEEE Conference on Computer Vision and Pattern Recognition  GPS-Tag Reﬁnement using Random Walks with an Adaptive Damping Factor  Amir Roshan Zamir  Shervin Ardeshir  Center for Research in Computer Vision, UCF  Mubarak Shah  Abstract  The number of GPS-tagged images available on the web is increasing at a rapid rate. The majority of such location tags are speciﬁed by the users, either through manual tag- ging or localization-c
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  A Non-Parametric Framework for Document Bleed-Through Removal  R´ois´ın Rowley-Brooke  Franc¸ois Piti´e  Anil Kokaram  Department of Electronic and Electrical Engineering  Trinity College Dublin, Ireland  {rowleybr@, fpitie@mee., anil.kokaram@}tcd.ie  Abstract  This paper presents recent work on a new fram
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  Fast multiple-part based object detection using KD-Ferns  Dan Levi  Shai Silberstein  Aharon Bar-Hillel  General Motors R&D, Advanced Technical Center - Israel  dan.levi@gm.com  s
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  Dictionary Learning from Ambiguously Labeled Data  Yi−Chen Chen, Vishal M. Patel, Jaishanker K. Pillai, Rama Chellappa ∗ and P. Jonathon Phillips †  Abstract  We propose a novel dictionary-based learning method for ambiguously labeled multiclass classiﬁcation, where each training sample has multiple labels
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  $ &RQYH[ 5HJXODUL]HU IRU 5HGXFLQJ &RORU $UWLIDFW LQ &RORU ,PDJH 5HFRYHU\  6KXQVXNH 2QR ,VDR <DPDGD 7RN\R ,QVWLWXWH RI 7HFKQRORJ\ -DSDQ  ono/isao@sp.ce.titech.ac.jp  $EVWUDFW  :H SURSRVH D QHZ FRQYH[ UHJXODUL]HU QDPHG WKH ORFDO FRORU QXFOHDU QRUP /&11 IRU FRORU LPDJH UHFRYHU\ 7KH /&11 LV GHVLJQHG WR S
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  A New Model and Simple Algorithms for Multi-Label Mumford-Shah Problems  Byung-Woo Hong  Chung-Ang University  hong@cau.ac.kr  Abstract  In this work, we address the multi-label M
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  Bayesian Grammar Learning for Inverse Procedural Modeling  An ¯delo Martinovi´c and Luc Van Gool  Abstract  Within the ﬁelds of urban reconstruction and city model- ing, shape grammars have emerged as a powerful tool for both synthesizing novel designs and reconstructing build- ings. Traditionally, a human
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  Learning SURF Cascade for Fast and Accurate Object Detection  Jianguo Li, Yimin Zhang  Intel Labs China  Abstract  This paper presents a novel learning framework for training boosting cascade based object detector from large scale dataset. The framework is derived from the well- known Viola-Jones (VJ) fram
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  Cross-View Action Recognition via a Continuous Virtual Path  Zhong Zhang, Chunheng Wang, Baihua Xiao, Wen Zhou, Shuang Liu and Cunzhao Shi  State Key Laboratory of Management and Control for Complex Systems, CASIA  {zhong.zhang,chunheng.wang,baihua.xiao,wen.zhou,shuang.liu,cunzhao.shi}@ia.ac.cn  Abstract  
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  Non-Rigid Structure from Motion with Diffusion Maps Prior  Lili Tao  Bogdan J. Matuszewski  Applied Digital Signal and Image Processing Research Centre  University of Central Lancashire, UK {lltao,bmatuszewski1}@uclan.ac.uk  Abstract  In this paper, a novel approach based on a non-linear manifold learning 
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  Rotation, Scaling and Deformation Invariant Scattering for Texture  Discrimination  Laurent Sifre  CMAP, Ecole Polytechnique  91128 Palaiseau  Abstract  An afﬁne invariant represe
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  A Minimum Error Vanishing Point Detection Approach for Uncalibrated  Monocular Images of Man-made Environments  Yiliang Xu  Sangmin Oh Anthony Hoogs  Kitware Inc.  28 Corporate Dr
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  Class Generative Models based on Feature Regression for Pose Estimation of  Object Categories  Michele Fenzi, Laura
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  Blur Processing Using Double Discrete Wavelet Transform  Yi Zhang  Keigo Hirakawa  Department of Electrical & Computer Engineering, University of Dayton  http://campus.udayton.edu
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  1063-6919/13 $26.00 © 2013 IEEE 1063-6919/13 $26.00 © 2013 IEEE 1063-6919/13 $26.00 © 2013 IEEE DOI 10.1109/CVPR.2013.206 DOI 10.1109/CVPR.2013.206 DOI 10.1109/CVPR.2013.206  1568 1568 1570  1569 1569 1571  1570 1570 1572  1571 1571 1573  1572 1572 1574  1573 1573 1575  1574 1574 1576  1575 1575 157
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  Correspondence-Less Non-Rigid Registration of Triangular Surface Meshes  Zsolt S´anta and Zoltan Kato  Image Processing and Computer Graphics Dept., University of Szeged  H-6701 Szeged, PO. Box 652., Hungary, Fax: +36 62 546-397  {santazs,kato}@inf.u-szeged.hu  Abstract  A novel correspondence-less approac
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  Globally Consistent Multi-Label Assignment on the Ray Space of 4D Light Fields  Sven Wanner  Christoph Straehle  Bastian Goldluecke  Heidelberg Collaboratory for Image Processing 
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  Five Shades of Grey for Fast and Reliable Camera Pose Estimation  Adam Herout, Istv´an Szentandr´asi, Michal Zachari´aˇs, Mark´eta Dubsk´a, Rudolf Kajan  Graph@FIT, Brno Universit
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  Long-term Occupancy Analysis using Graph-Based Optimisation in Thermal  Imagery  Rikke Gade, Anders Jørgensen and Thomas B. Moeslund  Visual Analysis of People Lab Aalborg University, Denmark  {rg, andjor, tbm}@create.aau.dk  Abstract  This paper presents a robust occupancy analysis system for thermal imag
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  BoF meets HOG: Feature Extraction based on  Histograms of Oriented p.d.f Gradients for Image Classiﬁcation  Nationa
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  Saliency Aggregation: A Data-driven Approach  Long Mai  Yuzhen Niu  Feng Liu  Department of Computer Science, Portland State University  Portland, OR, 97207 USA  {mtlong,yuzhen,fl
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  Physically Plausible 3D Scene Tracking:  The Single Actor Hypothesis  Institute of Computer Science, FORTH and Computer Science Department, University of Crete  Nikolaos Kyriazis, Antonis Argyros  {kyriazis, argyros}@ics.forth.gr  Abstract  In several hand-object(s) interaction scenarios,  the change in th
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  Can a Fully Unconstrained Imaging Model be  Applied Effectively to Central Cameras?  Filippo Bergamasco, Andrea Albarelli, Emanuele Rodol´a, Andrea Torsello  Dipartimento di Scien
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  A Joint Model for 2D and 3D Pose Estimation from a Single Image  E. Simo-Serra  1  , A. Quattoni  2  , C. Torras  1  , F. Moreno-Noguer  1  Institut de Rob`otica i Inform`atica Industrial (CSIC-UPC), Barcelona, Spain  1  2  Universitat Polit`ecnica de Catalunya (UPC), Barcelona, Spain  Raw  2D with Detecto
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  Measures and Meta-Measures for the  Supervised Evaluation of Image Segmentation  Jordi Pont-Tuset and Ferran Marques  Universitat Polit`ecnica de Catalunya BarcelonaTech∗  http://imatge.upc.edu  Abstract  This paper tackles the supervised evaluation of image segmentation algorithms. First, it surveys and s
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  Scene Text Recognition using Part-based Tree-structured Character Detection  Cunzhao Shi, Chunheng Wang, Baihua Xiao, Yang Zhang, Song Gao and Zhong Zhang  State Key Laboratory of Management and Control for Complex Systems, CASIA, Beijing, China  {cunzhao.shi,chunheng.wang,baihua.xiao,yang.zhang,song.gao,z
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  Hierarchical Video Representation with Trajectory Binary Partition Tree  Guillem Palou and Philippe Salembier  Technical University of Catalonia  guillem.palou@upc.edu, philippe.salembier@upc.edu  Abstract  As early stage of video processing, we introduce an iter- ative trajectory merging algorithm that pr
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  1063-6919/13 $26.00 © 2013 IEEE 1063-6919/13 $26.00 © 2013 IEEE 1063-6919/13 $26.00 © 2013 IEEE DOI 10.1109/CVPR.2013.236 DOI 10.1109/CVPR.2013.236 DOI 10.1109/CVPR.2013.236  1804 1804 1806  up being faster by heuristically randomly sampling bound- ing boxes without absorbing the cost of ﬁring the detecto
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  A higher-order CRF model for road network extraction  Jan D. Wegner, Javier A. Montoya-Zegarra, Konrad Schindler  Photogrammetry and Remote Sensing, ETH Z¨urich, Switzerland  Abstract  The aim of this work is to extract the road network from aerial images. What makes the problem challenging is the complex 
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  The Variational Structure of Disparity and Regularization of 4D Light Fields  Bastian Goldluecke  Sven Wanner  Heidelberg Collaboratory for Image Processing  Abstract  Unlike trad
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  Learning without Human Scores for Blind Image Quality Assessment  Wufeng Xue  Inst. of Im. Pro. & Pat. Rec  Xi’an Jiaotong Univ. xwolfs@hotmail.com  Lei Zhang  Dept. of Computing 
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  	    		    !"#$%  ?(.$%   A$$H),  &'()*+!,-.$*%+, /01234567058912:;16<=>  @A,B(C(*($)D E><701230=;F<64;G  &'()*+!,-.$*%+, 580I9J;34567058912:;16<=>  KL   ,+B(A(*)(A{  MNOPQRQSTQTUQUPSPRVSW PXQSTYOPSZPS[RV\O]SZ^S RV
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  Local Fisher Discriminant Analysis for Pedestrian Re-identiﬁcation  Sateesh Pedagadi, James Orwell  Kingston University London  Sergio Velastin  Universidad de Santiago de Chile  Boghos Boghossian  Ipsotek Ltd, UK  Abstract  Metric learning methods, for person re-identiﬁcation, es- timate a scaling for dis
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  Learning Separable Filters∗  CVLab, ´Ecole Polytechnique F´ed´erale de Lausanne  Vincent Lepetit  Pascal Fua  Roberto Rigamonti  Amos Sironi  Lausanne, Switzerland name.surname@epfl.ch  Abstract  Learning ﬁlters to produce sparse image representations in terms of overcomplete dictionaries has emerged as a 
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  Mesh Based Semantic Modelling for Indoor and Outdoor Scenes  Julien P. C. Valentin1,3  Sunando Sengupta1,3  Jonathan Warrell1 Ali Shahrokni2  Philip H. S. Torr1  {julien.valentin-2010, ssengupta, jwarrell, philiptorr}@brookes.ac.uk,  ali.Shahrokni@2d3sensing.co.uk  1Oxford Brookes University  22d3 Ltd. ∗  
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds  Jeremie Papon  Alexey Abramov Florentin W¨org¨otter  Markus Schoeler  Bernstein Center for Computational Neuroscience (BCCN)  III Physikalisches Institut - Biophysik, Georg-August University of G¨ottingen  {jpapon,abramov,mschoeler,worgo
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  Seeking the strongest rigid detector  Rodrigo Benenson∗†‡ Markus Mathias∗†  † ESAT-PSI-VISICS/IBBT,  Katholieke Universiteit Leuven, Belgium firstname.lastname@esat.kuleuven.be  Tinne Tuytelaars† Luc Van Gool† ‡ Max Planck Institut für Informatik  Saarbrücken, Germany benenson@mpi-inf.mpg.de  Abstract  The
2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition 2013 IEEE Conference on Computer Vision and Pattern Recognition  PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance  Contrast Measures with Spatial Priors  Keyang Shi1, Keze Wang1, Jiangbo Lu2, Liang Lin1∗  1Sun Yat-Sen University, Guangzhou, China 2Advanced Digital Sciences Center, Singapore  Abstract  Driven by recent vision and graphics applicatio
What and How Well You Performed? A Multitask Learning Approach to  Action Quality Assessment  Paritosh Parmar  Brendan Tran Morris  University of Nevada, Las Vegas  parmap1@unlv.nevada.edu, brendan.morris@unlv.edu  Abstract  Can performance on the task of action quality assess- ment (AQA) be improved by exploiting a description of the action and its quality? Current AQA and skills assessment approaches propose to learn features that serve only one task - estimating the ﬁnal score. In this paper,
Latent Space Autoregression for Novelty Detection  Davide Abati Angelo Porrello  Simone Calderara Rita Cucchiara  University of Modena and Reggio Emilia  {name.surname}@unimore.it  Abstract  Novelty detection is commonly referred to as the discrim- ination of observations that do not conform to a learned model of regularity. Despite its importance in different ap- plication settings, designing a novelty detector is utterly complex due to the unpredictable nature of novelties and its inaccessibil
Attending to Discriminative Certainty for Domain Adaptation  Vinod Kumar Kurmi∗,  Shanu Kumar∗, Vinay P. Namboodiri  Indian Institute of Technology Kanpur  India  {vinodkk,sshanu,vinaypn}@iitk.ac.in  Abstract  In this paper, we aim to solve for unsupervised domain adaptation of classiﬁers where we have access to label in- formation for the source domain while these are not avail- able for a target domain. While various methods have been proposed for solving these including adversarial discrimi- 
On Implicit Filter Level Sparsity in Convolutional Neural Networks  Dushyant Mehta1  ,  3 Kwang In Kim2 Christian Theobalt1  3  ,  1MPI For Informatics 2UNIST 3Saarland Informatics Campus  Abstract  We investigate ﬁlter level sparsity that emerges in convo- lutional neural networks (CNNs) which employ Batch Nor- malization and ReLU activation, and are trained with adap- tive gradient descent techniques and L2 regularization or weight decay. We conduct an extensive experimental study casting our 
860  0.8  0.7  0.6  0.5  QF = 50  QF = 75  QF = 90  Original  Accuracy  (a)  Attack Success Rate  1.2 1 0.8 0.6 0.4 0.2 0  0.004  0.002  0  FC-1  FC-13  FC-25 Mean  (b)  FC-37  FC-49  FC-61  Std  861  One Pass Process  Enhanced JPEG Decompression  JPEG  Huffman/ Arithmetic Decoding  Dequantization  Crafted   Quantization  Crafted   Dequantization  IDCT  Enhanced JPEG Compression  Two Pass Process  Raw data  DCT  Crafted   Quantization  Huffman/arithmetic  Encoding  JPEG  JPEG  Adversarial  Exa
HorizonNet: Learning Room Layout with 1D Representation and Pano Stretch  Data Augmentation  Cheng Sun  Chi-Wei Hsiao  Min Sun  Hwann-Tzong Chen  National Tsing Hua University  {chengsun, chiweihsiao}@gapp.nthu.edu.tw  sunmin@ee.nthu.edu.tw  htchen@cs.nthu.edu.tw  Figure 1: Some examples of 3D reconstructed room layouts by our HorizonNet.  Abstract  We present a new approach to the problem of estimating the 3D room layout from a single panoramic image. We rep- resent room layout as three 1D vect
Self-Supervised Learning of 3D Human Pose using Multi-view Geometry  Muhammed Kocabas ∗  Salih Karagoz ∗  Emre Akbas  Department of Computer Engineering, Middle East Technical University  {muhammed.kocabas,e234299,eakbas}@metu.edu.tr  Abstract  Training accurate 3D human pose estimators requires large amount of 3D ground-truth data which is costly to collect. Various weakly or self supervised pose estima- tion methods have been proposed due to lack of 3D data. Nevertheless, these methods, in add
1217  1218  1219  1220  1221  1222  1223  1224  1225  1226  
Leveraging Shape Completion for 3D Siamese Tracking  Silvio Giancola*, Jesus Zarzar*, and Bernard Ghanem  King Abdullah University of Science and Technology (KAUST), Saudi Arabia  {silvio.giancola,jesusalejandro.zarzartorano,bernard.ghanem}@kaust.edu.sa  Abstract  Point clouds are challenging to process due to their spar- sity, therefore autonomous vehicles rely more on appear- ance attributes than pure geometric features. However, 3D LIDAR perception can provide crucial information for ur- ban 
Latent Filter Scaling for Multimodal Unsupervised Image-to-Image Translation  King Abdullah University for Science and Technology (KAUST)  KAUST  KAUST  Yazeed Alharbi  Neil Smith  Peter Wonka  yazeed.alharbi@kaust.edu.sa  Figure 1: An illustration of latent ﬁlter scaling. Input noise codes are mapped to the ﬁlters of the network, instead of injection into the input image.  Abstract  1. Introduction  In multimodal unsupervised image-to-image translation tasks, the goal is to translate an image f
Learning to Calibrate Straight Lines for Fisheye Image Rectiﬁcation  Zhucun Xue, Nan Xue, Gui-Song Xia∗, Weiming Shen  CAPTAIN-LIESMARS, Wuhan University, Wuhan, 430079, China  {zhucun.xue, xuenan, guisong.xia, shenwm}@whu.edu.cn  Abstract  This paper presents a new deep-learning based method to simultaneously calibrate the intrinsic parameters of ﬁsh- eye lens and rectify the distorted images. Assuming that the distorted lines generated by ﬁsheye projection should be straight after rectiﬁcation
Predicting Future Frames using Retrospective Cycle GAN  Yong-Hoon Kwon  Min-Gyu Park  Advanced Camera Lab, LG Electronics, Korea  Korea Electronics Technology Institute  yonghoon.kwon@lge.com  mpark@keti.re.kr  Abstract  Recent advances in deep learning have signiﬁcantly im- proved the performance of video prediction, however, top- performing algorithms start to generate blurry predictions as they attempt to predict farther future frames. In this pa- per, we propose a uniﬁed generative adversari
All about Structure: Adapting Structural Information across Domains for  Boosting Semantic Segmentation  Wei-Lun Chang* Hui-Po Wang* Wen-Hsiao Peng Wei-Chen Chiu  National Chiao Tung University, Taiwan  {luckchang.ee06g, a88575847.cs06g, wpeng, walon}@nctu.edu.tw  Abstract  Source Domain  In this paper we tackle the problem of unsupervised domain adaptation for the task of semantic segmentation, where we attempt to transfer the knowledge learned upon synthetic datasets with ground-truth labels t
Noise2Void - Learning Denoising from Single Noisy Images  Alexander Krull1  ,  2, Tim-Oliver Buchholz2, Florian Jug  1  krull@mpi-cbg.de  2  Authors contributed equally  MPI-CBG/PKS (CSBD), Dresden, Germany  Abstract  noisy  clean  noisy  noisy  noisy  void  Instead,  The ﬁeld of image denoising is currently dominated by discriminative deep learning methods that are trained on pairs of noisy input and clean target images. Re- cently it has been shown that such methods can also independent be tra
Gradient Matching Generative Networks for Zero-Shot Learning  Mert Bulent Sariyildiz  Bilkent University  Ramazan Gokberk Cinbis  Middle East Technical University (METU)  Department of Computer Engineering  Department of Computer Engineering  mert.sariyildiz@bilkent.edu.tr  gcinbis@metu.edu.tr  Abstract  Zero-shot learning (ZSL) is one of the most promising problems where substantial progress can potentially be achieved through unsupervised learning, due to distribu- tional differences between s
Zero-Shot Task Transfer  Arghya Pal, Vineeth N Balasubramanian  Department of Computer Science and Engineering Indian Institute of Technology, Hyderabad, INDIA  {cs15resch11001, vineethnb}@iith.ac.in  Abstract  In this work, we present a novel meta-learning algo- rithm that regresses model parameters for novel tasks for which no ground truth is available (zero-shot tasks). In order to adapt to novel zero-shot tasks, our meta-learner learns from the model parameters of known tasks (with ground tr
Attention-based Dropout Layer for Weakly Supervised Object Localization  Junsuk Choe, and Hyunjung Shim∗  School of Integrated Technology, Yonsei University, South Korea  {junsukchoe, kateshim}@yonsei.ac.kr  Abstract  Weakly Supervised Object Localization (WSOL) tech- niques learn the object location only using image-level la- bels, without location annotations. A common limitation for these techniques is that they cover only the most discrimi- native part of the object, not the entire object. T
2337  2338  2339  2340  Label  Ground Truth  CRN [8]  pix2pixHD [45]  Ours  Figure 5: Visual comparison of semantic image synthesis results on the COCO-Stuff dataset. Our method successfully synthesizes realistic details from semantic labels.  Label  Ground Truth  CRN [8]  SIMS [40]  pix2pixHD [45]  Ours  Figure 6: Visual comparison of semantic image synthesis results on the ADE20K outdoor and Cityscapes datasets. Our method produces realistic images while respecting the spatial semantic lay
ADVENT: Adversarial Entropy Minimization for Domain Adaptation  in Semantic Segmentation  Tuan-Hung Vu1  Himalaya Jain1  Maxime Bucher1  Matthieu Cord1,2  Patrick P´erez1  1valeo.ai, Paris, France  2Sorbonne University, Paris, France  Abstract  Semantic segmentation is a key problem for many com- puter vision tasks. While approaches based on convolu- tional neural networks constantly break new records on dif- ferent benchmarks, generalizing well to diverse testing en- vironments remains a major 
Learning RoI Transformer for Oriented Object Detection in Aerial Images  Jian Ding, Nan Xue, Yang Long, Gui-Song Xia∗, Qikai Lu  LIESMARS-CAPTAIN, Wuhan University, Wuhan, 430079, China  {jian.ding, xuenan, longyang, guisong.xia, qikai lu}@whu.edu.cn  Abstract  Object detection in aerial images is an active yet chal- lenging task in computer vision because of the bird’s-eye view perspective, the highly complex backgrounds, and the variant appearances of objects. Especially when detecting densely
Learning with Batch-wise Optimal Transport Loss for 3D Shape Recognition  Lin Xu1  ,  2 ∗  Han Sun1  2  ,  Yuai Liu1  2  ,  1Nanjing Institute of Advanced Artiﬁcial Intelligence 2Horizon Robotics  {lin01.xu, han.sun, yuai.liu}@horizon.ai  Abstract  Deep metric learning is essential for visual recognition. The widely used pair-wise (or triplet) based loss objectives cannot make full use of semantical information in training samples or give enough attention to those hard samples dur- ing optimizat
DuLa-Net: A Dual-Projection Network for Estimating Room Layouts from a  Single RGB Panorama  Shang-Ta Yang1  2  ,  Fu-En Wang1  sundadenny@gapp.nthu.edu.tw  fulton84717@gapp.nthu.edu.tw  Chi-Han Peng2 pchihan@asu.edu  Peter Wonka2  Min Sun1  Hung-Kuo Chu1  pwonka@gmail.com  sunmin@ee.nthu.edu.tw  hkchu@cs.nthu.edu.tw  1National Tsing Hua University  2KAUST  Abstract  We present a deep learning framework, called DuLa-Net, to predict Manhattan-world 3D room layouts from a sin- gle RGB panorama. To
Robust Facial Landmark Detection via Occlusion-adaptive Deep Networks  Meilu Zhu, Daming Shi∗, Mingjie Zheng, Muhammad Sadiq  College of Computer Science and Software Engineering, Shenzhen University, China  {zhumeilu2016,zhengmingjie}@email.szu.edu.cn; dshi@szu.edu.cn; sadiq paec@yahoo.com  Abstract  In this paper, we present a simple and effective frame- work called Occlusion-adaptive Deep Networks (ODN) with the purpose of solving the occlusion problem for facial landmark detection. In this m
Cross-task weakly supervised learning from instructional videos  Dimitri Zhukov1,2  Jean-Baptiste Alayrac1,3  Ramazan Gokberk Cinbis4  David Fouhey5  Ivan Laptev1,2  Josef Sivic1,2,6  Abstract  In this paper we investigate learning visual models for the steps of ordinary tasks using weak supervision via in- structional narrations and an ordered list of steps instead of strong supervision via temporal annotations. At the heart of our approach is the observation that weakly su- pervised learning m
Events-to-Video: Bringing Modern Computer Vision to Event Cameras  Henri Rebecq †  Ren´e Ranftl ‡  Vladlen Koltun ‡  Davide Scaramuzza †  Abstract  Event cameras are novel sensors that report brightness changes in the form of asynchronous “events” instead of intensity frames. They have signiﬁcant advantages over conventional cameras: high temporal resolution, high dy- namic range, and no motion blur. Since the output of event cameras is fundamentally different from conventional cam- eras, it is 
Networks for Joint Afﬁne and Non-parametric Image Registration  Zhengyang Shen UNC Chapel Hill  Xu Han  Zhenlin Xu  UNC Chapel Hill  UNC Chapel Hill  Marc Niethammer UNC Chapel Hill  zyshen@cs.unc.edu  xhs400@cs.unc.edu  zhenlinx@cs.unc.edu  mn@cs.unc.edu  Abstract  We introduce an end-to-end deep-learning framework for 3D medical image registration. In contrast to ex- isting approaches, our framework combines two regis- tration methods: an afﬁne registration and a vector momentum-parameterized 
4273  4274  4275  4276  Table 2: Architectures used in Kinetics experiments in Table 3(d).  C2D  baseline 7 × 7, 64,  stride 2, 2(, 1)  CPNet (Ours) 6 CP modules  7 × 7, 64 stride 2, 2  (cid:20)3 × 3  3 × 3  64  64(cid:21) × 2  ,  ,  ,  ,  3 × 3  (cid:20)3 × 3 (cid:20)3 × 3  3 × 3  ,  ,  (cid:20)3 × 3  3 × 3  ,  ,  (cid:20)3 × 3  3 × 3  ,  ,  64  128  64(cid:21) × 2 128(cid:21) × 2   256(cid:21) × 2   512(cid:21) × 2    256  512  3 × 3  3 × 3  128  128  ,  ,  CP module 3 × 3 256  ,  3 
Sphere Generative Adversarial Network Based on Geometric Moment Matching  Sung Woo Park and Junseok Kwon  School of Computer Science and Engineering, Chung-Ang University, Seoul, Korea  pswkiki@gmail.com  jskwon@cau.ac.kr  Abstract  We propose sphere generative adversarial network (GAN), a novel integral probability metric (IPM)-based GAN. Sphere GAN uses the hypersphere to bound IPMs in the objective function. Thus, it can be trained sta- bly. On the hypersphere, sphere GAN exploits the in- for
Decoupling Direction and Norm for Efﬁcient Gradient-Based L2  Adversarial Attacks and Defenses  Jérôme Rony∗ 1  Luiz G. Hafemann∗ 1 Robert Sabourin1  Luiz S. Oliveira2 Eric Granger1  Ismail Ben Ayed1  1Laboratoire d’imagerie, de vision et d’intelligence artiﬁcielle (LIVIA), ÉTS Montreal, Canada  2Department of Informatics, Federal University of Paraná, Curitiba, Brazil  jerome.rony@gmail.com  luiz.gh@mailbox.org  lesoliveira@inf.ufpr.br  {ismail.benayed, robert.sabourin, eric.granger}@etsmtl.ca 
Soft Labels for Ordinal Regression  Ra´ul D´ıaz, Amit Marathe  HP Inc.  {raul.diaz.garcia, amit.marathe}@hp.com  Abstract  Ordinal regression attempts to solve classiﬁcation prob- lems in which categories are not independent, but rather follow a natural order. It is crucial to classify each class correctly while learning adequate interclass ordinal rela- tionships. We present a simple and effective method that constrains these relationships among categories by seam- lessly incorporating metric p
What does it mean to learn in deep networks? And, how does one detect  adversarial attacks?  Ciprian A. Corneanu  Meysam Madadi  Univ. Barcelona  CVC, UAB  Sergio Escalera Univ. Barcelona  Aleix M. Martinez  OSU  Abstract  INPUT  DNN  FUNCTIONAL GRAPH  v4<  v5<  v3< v2<  v0<  v1<  The ﬂexibility and high-accuracy of Deep Neural Net- works (DNNs) has transformed computer vision. But, the fact that we do not know when a speciﬁc DNN will work and when it will fail has resulted in a lack of trust. A
On Learning Density Aware Embeddings  Soumyadeep Ghosh, Richa Singh, Mayank Vatsa  IIIT-Delhi, India  {soumyadeepg, rsingh, mayank}@iiitd.ac.in  Abstract  Deep metric learning algorithms have been utilized to learn discriminative and generalizable models which are effective for classifying unseen classes. In this paper, a novel noise tolerant deep metric learning algorithm is pro- posed. The proposed method, termed as Density Aware Metric Learning, enforces the model to learn embeddings that are
LP-3DCNN: Unveiling Local Phase in 3D Convolutional Neural Networks  Sudhakar Kumawat and Shanmuganathan Raman  Indian Institute of Technology Gandhinagar  Gandhinagar, Gujarat, India  {sudhakar.kumawat, shanmuga}@iitgn.ac.in  Abstract  Traditional 3D Convolutional Neural Networks (CNNs) are computationally expensive, memory intensive, prone to overﬁt, and most importantly, there is a need to improve their feature learning capabilities. To address these issues, we propose Rectiﬁed Local Phase Vo
Multi-Similarity Loss with General Pair Weighting  for Deep Metric Learning  Xun Wang, Xintong Han, Weiling Huang∗, Dengke Dong, Matthew R. Scott  Malong Technologies, Shenzhen, China  Shenzhen Malong Artiﬁcial Intelligence Research Center, Shenzhen, China  {xunwang,xinhan,whuang,dongdk,mscott}@malong.com  Abstract  A family of loss functions built on pair-based computa- tion have been proposed in the literature which provide a myriad of solutions for deep metric learning. In this pa- per, we pr
5147  structure, we analyze the gradient propagation of recursive block and ﬁnd that the gradient explosion of a recursive net- work is caused by BN layer. Therefore, we propose Loopy Variable Batch Normalization to stabilize the gradients and beneﬁt feature re-usage in a more general and easier way than LSTM [10]. In particular, we employ different BN layer for each looping. By applying LVBN, the recursive network can be trained better with fewer parameters.  To incorporate the discrete decisi
Ranked List Loss for Deep Metric Learning  Xinshao Wang1,2, Yang Hua1, Elyor Kodirov2, Guosheng Hu2,1, Romain Garnier2, Neil M. Robertson1,2 1 School of Electronics, Electrical Engineering and Computer Science, Queen’s University Belfast, UK  {xwang39, y.hua, n.robertson}@qub.ac.uk, {elyor, guosheng.hu, romaing}@anyvision.co  2 Anyvision Research Team, UK  Abstract  The objective of deep metric learning (DML) is to learn embeddings that can capture semantic similarity informa- tion among data po
RVOS: End-to-End Recurrent Network for Video Object Segmentation  Carles Ventura1, Miriam Bellver2, Andreu Girbau3, Amaia Salvador3,  Ferran Marques3 and Xavier Giro-i-Nieto3  1Universitat Oberta de Catalunya  2Barcelona Supercomputing Center  3Universitat Polit`ecnica de Catalunya  cventuraroy@uoc.edu, miriam.bellver@bsc.es, {andreu.girbau, amaia.salvador, ferran.marques, xavier.giro}@upc.edu  Abstract  Multiple object video object segmentation is a challeng- ing task, specially for the zero-sh
5434  5435  5436  5437  5438  5439  5440  5441  5442  
9 1 0 2    r a     M 8 1      ]  V C . s c [      2 v 3 2 3 6 0  .  3 0 9 1 : v i X r a  5504  5505  5506  5507  (a)  (b)  (c)  (d)  5508  5509  5510  5511  5512  5513  5514  
Art2Real: Unfolding the Reality of Artworks  via Semantically-Aware Image-to-Image Translation  Matteo Tomei Marcella Cornia Lorenzo Baraldi Rita Cucchiara  University of Modena and Reggio Emilia  {name.surname}@unimore.it  Abstract  The applicability of computer vision to real paintings and artworks has been rarely investigated, even though a vast heritage would greatly beneﬁt from techniques which can understand and process data from the artistic domain. This is partially due to the small amou
Arbitrary Style Transfer with Style-Attentional Networks  Dae Young Park1,* and Kwang Hee Lee2,*,**  Artiﬁcial Intelligence Research Institute, Korea  1likebullet86@gmail.com, 2lkwanghee@gmail.com  Figure 1: Result of the proposed SANet. We can transfer various styles to content images with high-quality.  Abstract  Arbitrary style transfer aims to synthesize a content image with the style of an image to create a third image that has never been seen before. Recent arbitrary style transfer algorit
Knowledge-Embedded Routing Network for Scene Graph Generation  Tianshui Chen†  Sun Yat-Sen University DarkMatter AI Research tianshuichen@gmail.com  Weihao Yu†  Riquan Chen  Sun Yat-Sen University weihaoyu6@gmail.com  Sun Yat-Sen University  sysucrq@gmail.com  Liang Lin∗  Sun Yat-Sen University DarkMatter AI Research  linliang@ieee.org  Abstract  To understand a scene in depth not only involves locat- ing/recognizing individual objects, but also requires to in- fer the relationships and interact
Sampling Techniques for Large-Scale Object Detection  from Sparsely Annotated Objects  Yusuke Niitani Takuya Akiba  Tommi Kerola Toru Ogawa Preferred Networks, Inc.  Shotaro Sano  Shuji Suzuki  {niitani,akiba,tommi,ogawa,sano,ssuzuki}@preferred.jp  Abstract  missing annotations.  Efﬁcient and reliable methods for training of object de- tectors are in higher demand than ever, and more and more data relevant to the ﬁeld is becoming available. How- ever, large datasets like Open Images Dataset v4 (
Barrage of Random Transforms for Adversarially Robust Defense  Edward Raﬀ1,2,4  Jared Sylvester1,2,4  Steven Forsyth3  Mark McLean1  1 Laboratory for Physical Sciences  2 Booz Allen Hamilton 3 nvidia 4 u.m.b.c.  Abstract  Defenses against adversarial examples, when using the ImageNet dataset, are historically easy to defeat. The common understanding is that a combination of simple image transformations and other various de- fenses are insuﬃcient to provide the necessary protec- tion when the obf
6868  6869  6870  6871  6872  6873  6874  6875  6876  6877  
OICSR: Out-In-Channel Sparsity Regularization for  Compact Deep Neural Networks  Jiashi Li1,2,*, Qi Qi1,2,*, Jingyu Wang1,2,†, Ce Ge1,2, Yujian Li1,2, Zhangzhang Yue1, and Haifeng Sun1,2  1State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and  Telecommunications, Beijing 100876, P.R. China  2EBUPT Information Technology Co., Ltd., Beijing 100191, P.R. China  Abstract  Channel pruning can signiﬁcantly accelerate and com- press deep neural networks. Many chan
Semantically Aligned Bias Reducing Zero Shot Learning  Akanksha Paul  Narayanan C. Krishnan  Prateek Munjal  Indian Institute of Technology  Indian Institute of Technology  Indian Institute of Technology  Ropar  Ropar  Ropar  akanksha.paul@iitrpr.ac.in  ckn@iitrpr.ac.in  2017csm1009@iitrpr.ac.in  Abstract  Zero shot  learning (ZSL) aims to recognize unseen classes by exploiting semantic relationships between seen and unseen classes. Two major problems faced by ZSL al- gorithms are the hubness pr
All You Need is a Few Shifts: Designing Efﬁcient Convolutional Neural Networks  for Image Classiﬁcation  Weijie Chen, Di Xie, Yuan Zhang, Shiliang Pu Hikvision Research Institute, Hangzhou, China  {chenweijie5, xiedi, zhangyuan, pushiliang}@hikvision.com  Abstract  Shift operation is an efﬁcient alternative over depthwise separable convolution. However, it is still bottlenecked by its implementation manner, namely memory movement. To put this direction forward, a new and novel basic compo- nent 
Exploring the Bounds of the Utility of Context for Object Detection  Ehud Barnea and Ohad Ben-Shahar  Dept. of Computer Science, Ben-Gurion University  Beer-Sheva, Israel  {barneaeh, ben-shahar}@cs.bgu.ac.il  Abstract  The recurring context in which objects appear holds valuable information that can be employed to predict their existence. This intuitive observation indeed led many re- searchers to endow appearance-based detectors with ex- plicit reasoning about context. The underlying thesis sug
A-CNN: Annularly Convolutional Neural Networks on Point Clouds  Artem Komarichev  Zichun Zhong  Jing Hua  Department of Computer Science, Wayne State University  {artem.komarichev,zichunzhong,jinghua}@wayne.edu  Abstract  Input  Analyzing the geometric and semantic properties of 3D point clouds through the deep networks is still challenging due to the irregularity and sparsity of samplings of their ge- ometric structures. This paper presents a new method to deﬁne and compute convolution directly
Fitting Multiple Heterogeneous Models by Multi-class Cascaded T-linkage  Luca Magri Andrea Fusiello  DPIA - Universit`a degli Studi di Udine, Italy  name.surname@uniud.it  Abstract  This paper addresses the problem of multiple model ﬁt- ting in the general context where the sought structures can be described by a mixture of heterogeneous paramet- ric models drawn from different classes. To this end, we conceive a multi-model selection framework that extends T- linkage to cope with different nest
Left Frame  Conv  Right Frame  Conv  Monocular Baseline  RoIAlign  Left RoI  3D Bounding Boxes  3D RPN  with TLNet  Classification Regression with TLNet  RoIAlign  Right RoI  with stereo Triangulation Learning Network (TLNet)  7615  7616  (a) Input image  (b) 2D objectness confidence map  Left Camera  Right Camera  Target  Reference Anchor  (c) 3D anchor candidates  (d) Potential anchors  Left RoI  Right RoI  7617  (cid:1861)=24(cid:1871)(cid:3036)=0.898  (cid:1861)=29(cid:1871)(cid:3036)=0.8
Learning Representations for Active Monocular Depth Estimation  Connecting the Dots:  Gernot Riegler1,∗ Yiyi Liao2,∗  Simon Donne2 Vladlen Koltun1 Andreas Geiger2  1Intel Intelligent Systems Lab  2Autonomous Vision Group, MPI-IS / University of T¨ubingen  {firstname.lastname}@intel.com  {firstname.lastname}@tue.mpg.de  Abstract  We propose a technique for depth estimation with a monocular structured-light camera, i.e., a calibrated stereo set-up with one camera and one laser projector. Instead o
FA-RPN: Floating Region Proposals for Face Detection  Mahyar Najibi ∗  Bharat Singh ∗  Larry S. Davis  {najibi,bharat,lsd}@cs.umd.edu  Abstract  We propose a novel approach for generating region pro- posals for performing face detection. Instead of classifying anchor boxes using features from a pixel in the convolu- tional feature map, we adopt a pooling-based approach for generating region proposals. However, pooling hundreds of thousands of anchors which are evaluated for generating proposals 
RepNet: Weakly Supervised Training of an Adversarial Reprojection Network  for 3D Human Pose Estimation  Bastian Wandt and Bodo Rosenhahn  Leibniz Universit¨at Hannover  Hannover, Germany  wandt@tnt.uni-hannover.de  Abstract  This paper addresses the problem of 3D human pose es- timation from single images. While for a long time human skeletons were parameterized and ﬁtted to the observation by satisfying a reprojection error, nowadays researchers di- rectly use neural networks to infer the 3D p
Collaborative Spatiotemporal Feature Learning for Video Action Recognition  Chao Li Qiaoyong Zhong Di Xie  Shiliang Pu  Hikvision Research Institute  {lichao15,zhongqiaoyong,xiedi,pushiliang}@hikvision.com  Abstract  Spatiotemporal feature learning is of central importance for action recognition in videos. Existing deep neural net- work models either learn spatial and temporal features in- dependently (C2D) or jointly with unconstrained parame- ters (C3D). In this paper, we propose a novel neura
Video Summarization by Learning from Unpaired Data  Mrigank Rochan and Yang Wang  University of Manitoba  {mrochan, ywang}@cs.umanitoba.ca  Abstract  We consider the problem of video summarization. Given an input raw video, the goal is to select a small subset of key frames from the input video to create a shorter sum- mary video that best describes the content of the original video. Most of the current state-of-the-art video summa- rization approaches use supervised learning and require labeled
      Assessing Personally Perceived Image Quality via Image Features and   Collaborative Filtering   Jari Korhonen   School of Computer Science and Software Engineering, Shenzhen University, China   jari.t.korhonen@ieee.org         Abstract   During  the  past  few  years,  different  methods  for  optimizing  the  camera  settings  and  post-processing  techniques  to  improve  the  subjective  quality  of  consumer  photos have been studied extensively. However, most of the  research in the p
Seamless Scene Segmentation  Lorenzo Porzi, Samuel Rota Bul`o, Aleksander Colovic, Peter Kontschieder  Mapillary Research  research@mapillary.com  Abstract  In this work we introduce a novel, CNN-based archi- tecture that can be trained end-to-end to deliver seam- less scene segmentation results. Our goal is to predict consistent semantic segmentation and detection results by means of a panoptic output format, going beyond the sim- ple combination of independently trained segmentation and detect
A Framework for Generating Controllable and Grounded Captions  Show, Control and Tell:  Marcella Cornia Lorenzo Baraldi Rita Cucchiara  University of Modena and Reggio Emilia  {name.surname}@unimore.it  Abstract  Current captioning approaches can describe images us- ing black-box architectures whose behavior is hardly con- trollable and explainable from the exterior. As an image can be described in inﬁnite ways depending on the goal and the context at hand, a higher degree of controllability is 
Deep ChArUco: Dark ChArUco Marker Pose Estimation  Danying Hu, Daniel DeTone, and Tomasz Malisiewicz  {dhu,ddetone,tmalisiewicz}@magicleap.com  Magic Leap, Inc.  Abstract  ChArUco boards are used for camera calibration, monocular pose estimation, and pose veriﬁcation in both robotics and augmented reality. Such ﬁducials are de- tectable via traditional computer vision methods (as found in OpenCV) in well-lit environments, but classical meth- ods fail when the lighting is poor or when the image u
Holistic and Comprehensive Annotation of Clinically Signiﬁcant Findings on Diverse CT Images: Learning from Radiology Reports and Label Ontology  Ke Yan1, Yifan Peng2, Veit Sandfort1, Mohammadhadi Bagheri1, Zhiyong Lu2, Ronald M. Summers1  1 Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Clinical Center  2 National Center for Biotechnology Information, National Library of Medicine  1  ,  2 National Institutes of Health, Bethesda, MD 20892  {ke.yan, yifan.peng, veit.sandfort, mohamma
Robust Subspace Clustering with Independent and Piecewise Identically  Distributed Noise Modeling  Yuanman Li, Jiantao Zhou, Xianwei Zheng, Jinyu Tian, Yuan Yan Tang  Department of Computer and Information Science, University of Macau, Macau, China  {yuanmanli, jtzhou, yb47430, yb77405, yytang}@um.edu.mo  Abstract  Most of the existing subspace clustering (SC) frame- works assume that the noise contaminating the data is gen- erated by an independent and identically distributed (i.i.d.) source, w
A Flexible Convolutional Solver for Fast Style Transfers  Gilles Puy Technicolor  Patrick P´erez  Valeo.ai  975 Avenue des Champs Blancs  15 Rue de la Baume  F-35576 Cesson-S´evign´e  F-75008 Paris  gilles.puy@technicolor.com  patrick.perez@valeo.com  Abstract  We propose a new ﬂexible deep convolutional neural net- work (convnet) to perform fast neural style transfers. Our network is trained to solve approximately, but rapidly, the artistic style transfer problem of [15] for arbritary styles. W
SparseFool: a few pixels make a big difference  Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard  ´Ecole Polytechnique F´ed´erale de Lausanne  {apostolos.modas, seyed.moosavi, pascal.frossard}@epfl.ch  Abstract  Deep Neural Networks have achieved extraordinary re- sults on image classiﬁcation tasks, but have been shown to be vulnerable to attacks with carefully crafted perturbations of the input data. Although most attacks usually change val- ues of many image’s pixels, it has bee
MBS: Macroblock Scaling for CNN Model Reduction  Yu-Hsun Lin, Chun-Nan Chou, and Edward Y. Chang  HTC Research & Healthcare (DeepQ)  {lyman lin, jason.cn chou, edward chang}@htc.com  Abstract  In this paper we propose the macroblock scaling (MBS) algorithm, which can be applied to various CNN architec- tures to reduce their model size. MBS adaptively reduces each CNN macroblock depending on its information redun- dancy measured by our proposed effective ﬂops. Empiri- cal studies conducted with I
Rare Event Detection using Disentangled Representation Learning  Ryuhei Hamaguchi, Ken Sakurada, and Ryosuke Nakamura  National Institute of Advanced Industrial Science and Technology (AIST)  {ryuhei.hamaguchi, k.sakurada, r.nakamura}@aist.go.jp  Abstract  This paper presents a novel method for rare event de- tection from an image pair with class-imbalanced datasets. A straightforward approach for event detection tasks is to train a detection network from a large-scale dataset in an end-to-end m
Effective Aesthetics Prediction with Multi-level Spatially Pooled Features  Vlad Hosu  Bastian Goldl¨ucke  Dietmar Saupe  University of Konstanz, Germany  {vlad.hosu, bastian.goldluecke, dietmar.saupe}@uni-konstanz.de  Abstract  We propose an effective deep learning approach to aes- thetics quality assessment that relies on a new type of pre- trained features, and apply it to the AVA data set, the cur- rently largest aesthetics database. While all previous ap- proaches miss some of the informati
Explicit Spatial Encoding for Deep Local Descriptors  Arun Mukundan  Giorgos Tolias  Ondˇrej Chum  Visual Recognition Group, FEE, CTU in Prague  Abstract  We propose a kernelized deep local-patch descriptor based on efﬁcient match kernels of neural network activa- tions. Response of each receptive ﬁeld is encoded together with its spatial location using explicit feature maps. Two location parametrizations, Cartesian and polar, are used to provide robustness to a different types of canonical patc
On zero-shot recognition of generic objects  Tristan Hascoet  Yasuo Ariki  Tetsuya Takiguchi  tristan.hascoet@gmail.com  ariki@kobe-u.ac.jp  takigu@kobe-u.ac.jp  Graduate School of System Informatics, Kobe University, Japan  Abstract  Many recent advances in computer vision are the result of a healthy competition among researchers on high qual- ity, task-speciﬁc, benchmarks. After a decade of active re- search, zero-shot learning (ZSL) models accuracy on the Imagenet benchmark remains far too lo
MVTec AD — A Comprehensive Real-World Dataset for Unsupervised  Anomaly Detection  Paul Bergmann  Michael Fauser  David Sattlegger  Carsten Steger  MVTec Software GmbH  www.mvtec.com  {paul.bergmann, fauser, sattleger, steger}@mvtec.com  Abstract  The detection of anomalous structures in natural image data is of utmost importance for numerous tasks in the ﬁeld of computer vision. The development of methods for unsu- pervised anomaly detection requires data on which to train and evaluate new appr
Octree guided CNN with Spherical Kernels for 3D Point Clouds  Huan Lei Ajmal Mian Computer Science and Software Engineering  Naveed Akhtar  The University of Western Australia  huan.lei@research.uwa.edu.au, {naveed.akhtar, ajmal.mian}@uwa.edu.au  Abstract  We propose an octree guided neural network architec- ture and spherical convolutional kernel for machine learn- ing from arbitrary 3D point clouds. The network archi- tecture capitalizes on the sparse nature of irregular point clouds, and hier
VITAMIN-E: VIsual Tracking And MappINg  with Extremely Dense Feature Points  Masashi Yokozuka, Shuji Oishi, Thompson Simon, Atsuhiko Banno  Robot Innovation Research Center,  National Institute of Advanced Industrial Science and Technology (AIST), Japan  {yokotsuka-masashi, shuji.oishi, simon.thompson, atsuhiko.banno}@aist.go.jp  Abstract  In this paper, we propose a novel indirect monocular SLAM algorithm called “VITAMIN-E,” which is highly ac- curate and robust as a result of tracking extremel
BeautyGlow: On-Demand Makeup Transfer Framework with Reversible  Generative Network  Hung-Jen Chen, Ka-Ming Hui, Szu-Yu Wang, Li-Wu Tsao, Hong-Han Shuai, Wen-Huang Cheng  National Chiao Tung University, Taiwan  {hjc.eed07g,ming2242.cm06g,joey840404.eed03,leo159753.me05,hhshuai,whcheng}@nctu.edu.tw  Abstract  As makeup has been widely-adopted for beautiﬁcation, ﬁnding suitable makeup by virtual makeup applications be- comes popular. Therefore, a recent line of studies proposes to transfer the mak
Nesti-Net: Normal Estimation for Unstructured 3D Point Clouds  using Convolutional Neural Networks  Yizhak Ben-Shabat  Michael Lindenbaum  Anath Fischer  Mechanical Engineering  Computer Science  Mechanical Engineering  Techion IIT Haifa, Israel  Techion IIT Haifa, Israel  Techion IIT Haifa, Israel  sitzikbs@gmail.com  mic@cs.technion.ac.il  meranath@technion.ac.il  Abstract  tion [21] and surface reconstruction [11].  In this paper, we propose a normal estimation method for unstructured 3D poin
Speed Invariant Time Surface  for Learning to Detect Corner Points with Event-Based Cameras  Jacques Manderscheid1, Amos Sironi1, Nicolas Bourdis1, Davide Migliore1 and Vincent Lepetit2  1Prophesee, Paris, France, 2University of Bordeaux, Bordeaux, France  {jmanderscheid, asironi, nbourdis, dmigliore}@prophesee.ai, vincent.lepetit@u-bordeaux.fr  Abstract  We propose a learning approach to corner detection for event-based cameras that is stable even under fast and abrupt motions. Event-based came
Training deep learning based image denoisers from undersampled  measurements without ground truth and without image prior  Magauiya Zhussip,  Shakarim Soltanayev,  Se Young Chun  Ulsan National Institute of Science and Technology (UNIST), Republic of Korea  {mzhussip,shakarim,sychun}@unist.ac.kr  Abstract  Compressive sensing is a method to recover the origi- nal image from undersampled measurements. In order to overcome the ill-posedness of this inverse problem, image priors are used such as sp
Privacy Protection in Street-View Panoramas using Depth  and Multi-View Imagery  Ries Uittenbogaard1,  ∗Clint Sebastian2,  Julien Vijverberg3,  Bas Boom3, Dariu M. Gavrila1, Peter H.N. de With2  1Intelligent Vehicles Group, TU Delft,  2VCA Group, TU Eindhoven,  3Cyclomedia B.V  c.sebastian@tue.nl  ∗  corresponding author  Abstract  The current paradigm in privacy protection in street- view images is to detect and blur sensitive information. In this paper, we propose a framework that is an altern
Attention Branch Network:  Learning of Attention Mechanism for Visual Explanation  Hiroshi Fukui, Tsubasa Hirakawa, Takayoshi Yamashita, Hironobu Fujiyoshi  Chubu University  1200 Matsumotocho, Kasugai, Aichi, Japan  {fhiro@mprg.cs, hirakawa@mprg.cs, yamashita@isc, fujiyoshi@isc}.chubu.ac.jp  Abstract  Visual explanation enables humans to understand the de- cision making of deep convolutional neural network (CNN), but it is insufﬁcient to contribute to improving CNN perfor- mance. In this paper,
Explainability Methods for Graph Convolutional Neural Networks  Phillip E. Pope*  Soheil Kolouri*  Mohammad Rostami  HRL Laboratories, LLC  HRL Laboratories, LLC  HRL Laboratories, LLC  pepope@hrl.com  skolouri@hrl.com  mrostami@hrl.com  Charles E. Martin  Heiko Hoffmann  HRL Laboratories, LLC  HRL Laboratories, LLC  cemartin@hrl.com  hhoffmann@hrl.com  Abstract  With the growing use of graph convolutional neural net- works (GCNNs) comes the need for explainability. In this paper, we introduce e
HoloPose: Holistic 3D Human Reconstruction In-The-Wild  Rıza Alp G¨uler  Iasonas Kokkinos  Ariel AI  Figure 1: We introduce HoloPose, a method for holistic monocular 3D body reconstruction in-the-wild. We start with an accurate, part-based estimate of 3D model parameters θ, and decoupled, FCN-based estimates of DensePose, 2D and 3D joints. We then efﬁciently optimize a misalignment loss Ltotal(θ) between the top-down 3D model predictions to the bottom- up pose estimates, thereby largely improvin
A convex relaxation for multi-graph matching  Paul Swoboda∗,1,2, Dagmar Kainm¨uller3,4, Ashkan Mokarian3,4, Christian Theobalt1,2, Florian Bernard1,2  1MPI Informatics 2Saarland Informatics Campus 3Berlin Institute of Health 4MDC Berlin  Abstract  1  2  We present a convex relaxation for the multi-graph matching problem. Our formulation allows for partial pair- wise matchings, guarantees cycle consistency, and our ob- jective incorporates both linear and quadratic costs. More- over, we also pres
Meta-learning Convolutional Neural Architectures for Multi-target Concrete  Defect Classiﬁcation with the COncrete DEfect BRidge IMage Dataset  Martin Mundt1∗, Sagnik Majumder1, Sreenivas Murali1∗, Panagiotis Panetsos2, Visvanathan Ramesh1∗  1. Goethe University  2. Egnatia Odos A. E.  {mmundt, vramesh}@em.uni-frankfurt.de  {majumder, murali}@ccc.cs.uni-frankfurt.de  ppane@egnatia.gr  Abstract  Recognition of defects in concrete infrastructure, espe- cially in bridges, is a costly and time consu
PEPSI : Fast Image Inpainting with Parallel Decoding Network  Min-cheol Sagong1, Yong-goo Shin1, Seung-wook Kim1, Seung Park1, Sung-jea Ko2  Korea university  1{mcsagong, ygshin, swkim, spark}@dali.korea.ac.kr  2  sjko@korea.ac.kr  Abstract  Recently, a generative adversarial network (GAN)-based method employing the coarse-to-ﬁne network with the con- textual attention module (CAM) has shown outstanding re- sults in image inpainting. However, this method requires numerous computational resources
FastDraw: Addressing the Long Tail of Lane Detection  by Adapting a Sequential Prediction Network  Jonah Philion  ISEE.AI  jonahphilion@isee.ai  Abstract  The search for predictive models that generalize to the long tail of sensor inputs is the central difﬁculty when devel- oping data-driven models for autonomous vehicles. In this paper, we use lane detection to study modeling and train- ing techniques that yield better performance on real world test drives. On the modeling side, we introduce a 
Learning Active Contour Models for Medical Image Segmentation  Xu Chen1, Bryan M. Williams1, Srinivasa R. Vallabhaneni1,2, Gabriela Czanner1,3, Rachel Williams1,  and Yalin Zheng1  1Department of Eye and Vision Science, Institute of Ageing and Chronic Disease, University of  2Liverpool Vascular & Endovascular Service, Royal Liverpool University Hospital, L7 8XP, UK  3Department of Applied Mathematics, Liverpool John Moores University, L3 3AF, UK  {xuchen, bryan, fempop, g.czanner, rlw, yzheng}@l
Perturbation Analysis of the 8-Point Algorithm: a Case Study for Wide FoV  Cameras  Thiago L. T. da Silveira and Claudio R. Jung  Institute of Informatics, Federal University of Rio Grande do Sul, Brazil  {tltsilveira,crjung}@inf.ufrgs.br  Abstract  This paper presents a perturbation analysis for the esti- mate of epipolar matrices using the 8-Point Algorithm (8- PA). Our approach explores existing bounds for singular subspaces and relates them to the 8-PA, without assuming any kind of error dis
Deep Single Image Camera Calibration with Radial Distortion  Manuel L´opez-Antequera*  Roger Mar´ı † Javier Gonzalez-Jimenez‡  Pau Gargallo* Gloria Haro§  Yubin Kuang*  Abstract  Single image calibration is the problem of predicting the camera parameters from one image. This problem is of importance when dealing with images collected in un- controlled conditions by non-calibrated cameras, such as crowd-sourced applications. In this work we propose a method to predict extrinsic (tilt and roll) an
Attribute-aware Face Aging with Wavelet-based Generative Adversarial  Networks  Yunfan Liu1 ∗  Qi Li1  2  ,  3 ∗  ,  Zhenan Sun1  2  4  ,  ,  1 Center for Research on Intelligent Perception and Computing, CASIA  2 National Laboratory of Pattern Recognition, CASIA  3 Artiﬁcial Intelligence Research, CAS, Jiaozhou, Qingdao, China  4 Center for Excellence in Brain Science and Intelligence Technology, CAS  yunfan.liu@cripac.ia.ac.cn, {qli, znsun}@nlpr.ia.ac.cn  Abstract   Test Face       Output   Te
Noise-Tolerant Paradigm for Training Face Recognition CNNs  Wei Hu1 Yangyu Huang2∗  Fan Zhang1 Ruirui Li1  1Beijing University of Chemical Technology, China  2Yunshitu Corporation, China  1{huwei,zhangf,liruirui}@mail.buct.edu.cn  2yangyu.huang.1990@gmail.com  Abstract  Beneﬁt from large-scale training datasets, deep Convo- lutional Neural Networks(CNNs) have achieved impressive results in face recognition(FR). However, tremendous scale of datasets inevitably lead to noisy data, which obviously 
12126  network that can be trained in an end-to-end fashion to rep- resent all the relevant information about the social and scene context. The Multi-Agent Tensor representation, illustrated in Fig. 1, spatially aligns an encoding of the scene with en- codings of the past trajectory of every agent in the scene, which maintains the spatial relationships between agents and scene features. Next, a fused Multi-Agent Tensor en- coding is formed via a fully convolutional mapping (see Fig. 2), which n
Focus Is All You Need: Loss Functions For Event-based Vision  Guillermo Gallego †  Mathias Gehrig †  Davide Scaramuzza †  Input Events  Parameters  θ  Warped Events  Focus score  ] x i  p  [  Y  Warping along point trajectories  time [s]  X [pix]  ] x i  p  [  Y  X [pix]  I(x; θ)  Measure  event alignment  θ  Figure 1: Motion Compensation Framework. Events in a space-time window are warped according to point trajectories described by motion parameters θ, resulting in an image of warped events (I
Convolutional Neural Networks Can Be Deceived by Visual Illusions  A. Gomez-Villa∗, A. Mart´ın∗, J. Vazquez-Corral†, M. Bertalm´ıo∗  {alexander.gomez, adrian.martin, marcelo.bertalmio}@upf.edu, j.vazquez@uea.ac.uk  Abstract  Visual illusions teach us that what we see is not always what is represented in the physical world. Their special na- ture make them a fascinating tool to test and validate any new vision model proposed. In general, current vision mod- els are based on the concatenation of l
Variational Autoencoders Pursue PCA Directions (by Accident)  Michal Rol´ınek∗, Dominik Zietlow∗ and Georg Martius  Max-Planck-Institute for Intelligent Systems, T¨ubingen, Germany  {mrolinek, dzietlow, gmartius}@tue.mpg.de  Abstract  The Variational Autoencoder (VAE) is a powerful archi- tecture capable of representation learning and generative modeling. When it comes to learning interpretable (disen- tangled) representations, VAE and its variants show unpar- alleled performance. However, the r
PCAN: 3D Attention Map Learning Using Contextual Information for Point  Cloud Based Retrieval  Wenxiao Zhang, Chunxia Xiao  School of Computer Science, Wuhan University, China  wenxxiao.zhang@gmail.com cxxiao@whu.edu.cn  Abstract  Point cloud based retrieval for place recognition is an emerging problem in vision ﬁeld. The main challenge is how to ﬁnd an efﬁcient way to encode the local features into a discriminative global descriptor. In this paper, we propose a Point Contextual Attention Networ
12446  12447  ✠✝  ✟✞  ✟✝  ✞  ✝  ✌✍  ☛  ☞✌✍  ☞  ☞✌✍  ✁✂✄☎✁✆  ✡☛  ☞  ☛  ✠✝  ✟✞  ✟✝  ✞  ✝  ✠✝  ✟✞  ✟✝  ✞  ✝  ✠✝  ✟✞  ✟✝  ✞  ✝  ✠✝  ✟✞  ✟✝  ✞  ✝  ✣  ✢  ✏  ✑  ✒  ✤✥✦ ✤✧✦  ✑✒  ✩  ★  ✣  ✏  ✒  ✤✥✦ ✤✧✦  ✫✬✭  ✪  ✮✪  ✫✯✭  ✁✂✄☎✁✆  ✓✔✕✖✗ ✓✘✙✙✚ ✘✛ ✜  ✓✔✕✖✗ ✘✛ ✜  ✎✏  ✎✑  ✒  ✑  ✏  ✒  ✏  ✣  ★  ✩  ✑✒  ✁✂✄☎✁✆  12448  12449  ✒✄  ✒✂  ✒✁      ✁  ✂  ✄  ☎  ✆  ✝  ✞  ✟  ✠  ✁  ✡☛☞✌✍ ✎✏✑  ✓ ✔✕✖✕ ✗✘✙✚✛✗ ✘✜✚  ✠  ✟✬✞  ✟  ✝✬✞  ✝  ✭✮✯✰ ✭✱✮✲✰  ✄✠  ✄✟  ✝  ✟  ✠  ✢✣✤✥✦ ✢✧★★✩ ✧✪ ✫  ✠  ✟✬✞  ✟  ✝✬✞  ✝  ✝  ✭✮✯✰ ✭✱✮✲✰ ✶✷✸✹✺✶✷✻✹  ✠  
Good News, Everyone! Context driven entity-aware captioning for news images  Ali Furkan Biten, Lluis Gomez, Marc¸al Rusi˜nol, Dimosthenis Karatzas  Computer Vision Center, UAB, Spain  {abiten, lgomez, marcal, dimos}@cvc.uab.es  Abstract  Current image captioning systems perform at a merely descriptive level, essentially enumerating the objects in the scene and their relations. Humans, on the contrary, inter- pret images by integrating several sources of prior knowl- edge of the world. In this wo
Spatio-Temporal Dynamics and Semantic Attribute Enriched  Visual Encoding for Video Captioning  Nayyer Aafaq  Naveed Akhtar Wei Liu  Syed Zulqarnain Gilani  Ajmal Mian  Computer Science and Software Engineering,  The University of Western Australia.  nayyer.aafaq@research.uwa.edu.au,{naveed.akhtar, wei.liu, syed.gilani, ajmal.mian}@uwa.edu.au  Abstract  Automatic generation of video captions is a fundamen- tal challenge in computer vision. Recent techniques typi- cally employ a combination of Co
In Defense of Pre-trained ImageNet Architectures  for Real-time Semantic Segmentation of Road-driving Images  Marin Orˇsi´c∗  Ivan Kreˇso∗  Petra Bevandi´c  Siniˇsa ˇSegvi´c  Faculty of Electrical Engineering and Computing  University of Zagreb, Croatia  name.surname@fer.hr  Abstract  Recent success of semantic segmentation approaches on demanding road driving datasets has spurred interest in many related application ﬁelds. Many of these applications involve real-time prediction on mobile platfo
Context-Aware Visual Compatibility Prediction  Guillem Cucurull  Element AI  Perouz Taslakian  Element AI  David Vazquez  Element AI  gcucurull@elementai.com  perouz@elementai.com  dvazquez@elementai.com  Abstract  How do we determine whether two or more clothing items are compatible or visually appealing? Part of the answer lies in understanding of visual aesthetics, and is biased by personal preferences shaped by social attitudes, time, and place. In this work we propose a method that predicts
An Alternative Deep Feature Approach to Line Level Keyword Spotting  George Retsinas1  ,  2, Georgios Louloudis1, Nikolaos Stamatopoulos1, Giorgos Sﬁkas1  ,  3, Basilis Gatos1  1 Institute of Informatics and Telecommunications, NCSR “Demokritos”, Greece  2 School of Electrical and Computer Engineering, National Technical University of Athens, Greece  3 Department of Computer Science and Engineering, University of Ioannina, Greece  {georgeretsi,louloud,nstam,sfikas,bgat}@iit.demokritos.gr  Abstra
12687  ✁✂✄✂☎✆✝ ✞✟  ✏✁ ✑✒✓✔✕✖  ✗✚✙  ✠✡☛☞✌✍✎✎✎  ✁✂✄✂☎✆✝ ✞✟  ✏✁ ✑✒✓✔✕✖  ✗✘✙  ✠✡☛☞✌✍✎✎✎  ✁✂✄✂☎✆✝ ✞✟  ✏✁ ✑✒✓✔✕✖  ✗✛✙  ✠✡☛☞✌✍✎✎✎  ✁✂✄✂☎✆✝ ✞✟  ✠✡☛☞✌✍✎✎✎  ✏✁ ✑✒✓✔✕✖  ✗✜✙  12688  12689  ❀❂  ❀❁  ❀❃  ☛☞✌☞✍✎  ✴✵✶  ✴✵✶  ✶✷✸✹ ✺✽  ✶✷✸✹ ✺✾  ✴✵✶  ✴✵✶  ✶✷✸✹ ✺✻  ✶✷✸✹ ✺✼  ✫✬✭✭✮✯✰ ✱✿  ✏✑✒✓✔✕✖✒✔✕✗✓  ✫✬✭✭✮✯✰ ✱✲✳ ✫✬✭✭✮✯✰ ✱✲✲  ✘✕✔✌✙✚✒✓✛ ✜✗✢✕✓✣ ✤✎✥✦  ✁✂✄☎✆✝✂☎✆✞✄ ✟☎✠✡✟  ✧✓✔★✗✙✩ ✜✗✢✕✓✣ ✤✎✪✦  12690  ❖✘★▲★✜✧✯  ✚✛✧✜✦★❆✧✦★✩✜ ✱✦✙❇✱ ✲❈✰✴  ❂✺❃❯❃✽✼❙ ♥◗❦♦✔♠♣♣♣ q❙✵r  ✒✓✔✓✕✖  P✿✼✽✻❃❄✼✻❃✵✽  ❘❃✻✔✹❙✼✽❅ ❚✵❁❃✽❯ ❱✖❲❳  ❦✽✻✺✵✹❧ ❚✵❁❃✽❯ ❱✖
PointPillars: Fast Encoders for Object Detection from Point Clouds  Alex H. Lang  Sourabh Vora  Holger Caesar Oscar Beijbom  Lubing Zhou  Jiong Yang  {alex, sourabh, holger, lubing, jiong.yang, oscar}@nutonomy.com  nuTonomy: an APTIV company  Abstract  Object detection in point clouds is an important aspect of many robotics applications such as autonomous driving. In this paper, we consider the problem of encoding a point cloud into a format appropriate for a downstream detection pipeline. Recen
From Coarse to Fine: Robust Hierarchical Localization at Large Scale  Paul-Edouard Sarlin1 Cesar Cadena1 Roland Siegwart1 Marcin Dymczyk1  2  ,  1Autonomous Systems Lab, ETH Z¨urich  2Sevensense Robotics AG  Abstract  Query  Robust and accurate visual localization is a fundamental capability for numerous applications, such as autonomous driving, mobile robotics, or augmented reality. It remains, however, a challenging task, particularly for large-scale environments and in presence of signiﬁcant 
Neighbors Searching based   on K eypoints Similarity  human body part  parsing dataset  . . .  image with keypoints  morphological   constraints  part-level prior  Pose-Guided   K nowledge Transfer  70  71  train image with  gt keypoints  . . .  . . .  . . .  average  existing part  parsing dataset  morphed masks  (a) Discovering   Pose-Similar Cluster  (b) Generating  Part-Level Prior  concat  refinement  network  (c) Image-guided  Prior Refinement  synthetic part   segmentation label  72  Q
A Certiﬁably Globally Optimal Solution  to the Non-Minimal Relative Pose Problem  Jesus Briales  MAPIR-UMA Group  University of Malaga, Spain  Laurent Kneip  Mobile Perception Lab  SIST ShanghaiTech  Javier Gonzalez-Jimenez  MAPIR-UMA Group  University of Malaga, Spain  jesusbriales@uma.es  lkneip@shanghaitech.edu.cn  javiergonzalez@uma.es  Abstract  Finding the relative pose between two calibrated views ranks among the most fundamental geometric vision prob- lems. It therefore appears as somewh
Glimpse Clouds: Human Activity Recognition from Unstructured Feature Points  Fabien Baradel1 , Christian Wolf1  ,  2, Julien Mille3, Graham W. Taylor4  5  ,  {fabien.baradel,christian.wolf}@liris.cnrs.fr, julien.mille@insa-cvl.fr, gwtaylor@uoguelph.ca  Abstract  We propose a method for human activity recognition from RGB data that does not rely on any pose information dur- ing test time, and does not explicitly calculate pose infor- mation internally. Instead, a visual attention module learns to
Interactive Image Segmentation with Latent Diversity  Zhuwen Li  Qifeng Chen  Intel Labs  Vladlen Koltun  Abstract  Interactive image segmentation is characterized by mul- timodality. When the user clicks on a door, do they intend to select the door or the whole house? We present an end-to- end learning approach to interactive image segmentation that tackles this ambiguity. Our architecture couples two convolutional networks. The ﬁrst is trained to synthesize a diverse set of plausible segmentat
Deep Extreme Cut: From Extreme Points to Object Segmentation  K.-K. Maninis* S. Caelles∗  J. Pont-Tuset  L. Van Gool  Computer Vision Lab, ETH Z¨urich, Switzerland  Figure 1. Example results of DEXTR: The user provides the extreme clicks for an object, and the CNN produces the segmented masks.  Abstract  This paper explores the use of extreme points in an object (left-most, right-most, top, bottom pixels) as input to obtain precise object segmentation for images and videos. We do so by adding an
Learned Shape-Tailored Descriptors for Segmentation  Naeemullah Khan and Ganesh Sundaramoorthi  King Abdullah University of Science and Technology (KAUST), Saudi Arabia  {naeemullah.khan,ganesh.sundaramoorthi}@kaust.edu.sa  Abstract  We address the problem of  texture segmentation by grouping dense pixel-wise descriptors. We introduce and construct learned Shape-Tailored Descriptors that aggre- gate image statistics only within regions of interest to avoid mixing statistics of different textures
Towards a Mathematical Understanding of the Difﬁculty in Learning with  Feedforward Neural Networks  fortiss - The Research Institute of the Free State of Bavaria, Germany  Hao Shen  Guerickestr. 25, 80805 Munich, Germany  hao.shen@fortiss.org  Abstract  Training deep neural networks for solving machine learning problems is one great challenge in the ﬁeld, mainly due to its associated optimisation problem being highly non-convex. Recent developments have suggested that many training algorithms d
840  841  842  843  844  lx  lx  lx  1lx  (a) Wide-Resnet  basic block  1lx  (b) Wide-Resnet  bottleneck  1lx  (c) MCN basic  845  MP: Max Pooling  BN: BatchNormlization  D: Dropout  R: ReLU  CNN  MCN  846  847  848  
SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels  Matthias Fey∗, Jan Eric Lenssen∗, Frank Weichert, Heinrich M¨uller  Department of Computer Graphics  TU Dortmund University  {matthias.fey,janeric.lenssen}@udo.edu  ∗ Both authors contributed equally to this work.  Abstract  We present Spline-based Convolutional Neural Networks (SplineCNNs), a variant of deep neural networks for irreg- ular structured and geometric input, e.g., graphs or meshes. Our main contribution is a 
❖ t✁✂ ■✄☎✆✝✞✟✠✡☛ ♦☞ ▲✌✍✎✏ ◗✑✒✓✔✕✖ ❢✗✘ ❙✙✚✛✜✢✣✤ ✥✦✧★✩✪✫✬✭✮✯✰  ❆✱✲✳✴✵✶✷✸✹ ❩✺✻✼✽✾✿❀❁ ❘❂❃❄❅❇❈❉❊ ❏❛❋●❍❑▼◆P❚ ❯❱❲❳❨❬❭ ❪❫❴❵❜❝  ❞❡❣❤✐ ❥❦❧♠♥♣  qrs✉✈✇①②③④⑤⑥⑦ ⑧⑨⑩❶❷❸❹❺❻ ❼❽ ❾❿➀➁➂➃➄➅➆➇  ➈➉➊➋➌➍➎➏ ➐➑➒➓➔→➣↔↕➙➛  ➜➝➞➟➠➡➢➤  ➥➦➧➨➩➫➭➯➲➳➵➸➺ ➻➼➽➾➚➪➶➹ ➘➴➷➬➮➱✃❐❒❮ ❰ÏÐÑ ÒÓÔÕÖ× ØÙÚ ÛÜÝÞßàáâ ãäåæçèéê ëì íîïðñòóô õö÷øù úûüýþÿ❣✁✂✄☎✆ P✝♦✞ ❞✟✠✡☛☞ ❛✌✍✎✏✑✒✓✱ ♣✔✕✖✗✘✙✚✈✛✜ ❧✢✣✤✥✦ r❡✧★✩✪✫✬ ❢✭✮ t✯✰✲ ✳✴✵✶ ✐✷  ✸ ✹✺✻✼✽✾✿ ❀❁❂ ❃❄❅❆ ❝❇❈❉❊❋●❍■ ❏❑▲▼◆❖◗❘ ❤❙❚❯❱❲❳❨ ❩❬❭❪❫❴❵❜❥  ❦♠♥qs✉✇①②③④⑤ ⑥⑦⑧⑨⑩❶ ❷❸❹❺❻❼ ❽❾❿➀➁ ➂➃➄➅ ➆➇➈➉ ➊ ➋➌➍➎➏➐➑➒ ➓➔  →➣↔ ↕➙➛➜ ➝➞➟ 
Adversarial Data Programming: Using GANs to Relax the Bottleneck of Curated  Labeled Data  Arghya Pal, Vineeth N Balasubramanian  Department of Computer Science and Engineering Indian Institute of Technology, Hyderabad, INDIA  {cs15resch11001,vineethnb}@iith.ac.in  Abstract  Paucity of  large curated hand-labeled training data forms a major bottleneck in the deployment of machine learning models in computer vision and other ﬁelds. Re- cent work (Data Programming) has shown how distant su- pervis
Context-aware Synthesis for Video Frame Interpolation  Simon Niklaus  Feng Liu  Portland State University  Portland State University  sniklaus@pdx.edu  fliu@cs.pdx.edu  Abstract  Video frame interpolation algorithms typically estimate optical ﬂow or its variations and then use it to guide the synthesis of an intermediate frame between two consecu- tive original frames. To handle challenges like occlusion, bidirectional ﬂow between the two input frames is often estimated and used to warp and blen
ISTA-Net: Interpretable Optimization-Inspired Deep Network for Image  Compressive Sensing  Jian Zhang, Bernard Ghanem  King Abdullah University of Science and Technology (KAUST), Saudi Arabia  jian.zhang@kaust.edu.sa, bernard.ghanem@kaust.edu.sa  Abstract  With the aim of developing a fast yet accurate algorith- m for compressive sensing (CS) reconstruction of natural images, we combine in this paper the merits of two exist- ing categories of CS methods: the structure insights of tra- ditional o
Learning from Millions of 3D Scans for Large-scale 3D Face Recognition  Syed Zulqarnain Gilani  Ajmal Mian  Computer Science and Software Engineering,  The University of Western Australia  {zulqarnain.gilani,ajmal.mian}@uwa.edu.au  Abstract  Deep networks trained on millions of facial images are believed to be closely approaching human-level perfor- mance in face recognition. However, open world face recognition still remains a challenge. Although, 3D face recognition has an inherent edge over i
CBMV: A Coalesced Bidirectional Matching Volume for Disparity Estimation  Konstantinos Batsos kbatsos@stevens.edu  Changjiang Cai  Philippos Mordohai  ccai1@stevens.edu  mordohai@cs.stevens.edu  Stevens Institute of Technology  Abstract  Recently, there has been a paradigm shift in stereo match- ing with learning-based methods achieving the best results on all popular benchmarks. The success of these methods is due to the availability of training data with ground truth; training learning-based s
Facial Expression Recognition by De-expression Residue Learning  Huiyuan Yang, Umur Ciftci and Lijun Yin  Department of Computer Science  State University of New York at Binghamton, USA  {hyang51, uciftci}@binghamton.edu; lijun@cs.binghamton.edu  Abstract  A facial expression is a combination of an expressive component and a neutral component of a person. In this paper, we propose to recognize facial expressions by ex- tracting information of the expressive component through a de-expression lear
Memory Based Online Learning of Deep Representations from Video Streams  Federico Pernici, Federico Bartoli, Matteo Bruni and Alberto Del Bimbo  MICC – Media Integration and Communcation Center  University Of Florence – Italy  {federico.pernici, federico.bartoli, matteo.bruni, alberto.delbimbo}@unifi.it  Abstract  We present a novel online unsupervised method for face identity learning from video streams. The method exploits deep face descriptors together with a memory based learn- ing mechanism
Multi-Level Fusion based 3D Object Detection from Monocular Images  School of Remote Sensing and Information Engineering, Wuhan University, China  {ysfalo,zzchen}@whu.edu.cn  Bin Xu, Zhenzhong Chen∗  Abstract  In this paper, we present an end-to-end multi-level fu- sion based framework for 3D object detection from a sin- gle monocular image. The whole network is composed of two parts: one for 2D region proposal generation and an- other for simultaneously predictions of objects’ 2D loca- tions, o
A Weighted Sparse Sampling and Smoothing Frame Transition Approach for  Semantic Fast-Forward First-Person Videos  Michel Silva  Washington Ramos  Joao Ferreira  Felipe Chamone  Mario Campos  Erickson R. Nascimento  Universidade Federal de Minas Gerais (UFMG), Brazil  {michelms, washington.ramos, joaoklock, cadar, mario, erickson}@dcc.ufmg.br  Abstract  Thanks to the advances in the technology of low-cost dig- ital cameras and the popularity of the self-recording cul- ture, the amount of visual 
Image Collection Pop-up: 3D Reconstruction and Clustering  of Rigid and Non-Rigid Categories  Antonio Agudo  Melcior Pijoan  Francesc Moreno-Noguer  Institut de Rob`otica i Inform`atica Industrial, CSIC-UPC, 08028, Barcelona, Spain  Abstract  This paper introduces an approach to simultaneously estimate 3D shape, camera pose, and object and type of deformation clustering, from partial 2D annotations in a multi-instance collection of images. Furthermore, we can indistinctly process rigid and non-r
Natural lighting                                         Directional lighting  2936  Env. map  1l  1N  ...  1  1ONˆ 1  2  2ONˆ 2  k  ˆ ONk P  Normal space  2n  Pn  1n  Patch  extraction  SVD,  Constant  albedo  Shortest   path  0  1 nn ,  2  2  1 nn , 0  ...  ...  1 nn , 2 nn ,  P  P  Nˆ  N  Matrix   completion  Integrability  constraint  Input images  Equivalent directional lighting  Local ambiguity  Angular distance matrix  Global ambiguity  Output  1 nn ,  P  2 nn ,  P  ...  0  2937  Patch 
Curve Reconstruction via the Global Statistics of Natural Curves  Ehud Barnea and Ohad Ben-Shahar  Dept. of Computer Science, Ben-Gurion University  Beer-Sheva, Israel  {barneaeh, ben-shahar}@cs.bgu.ac.il  Abstract  Reconstructing the missing parts of a curve has been the subject of much computational research, with applica- tions in image inpainting, object synthesis, etc. Different approaches for solving that problem are typically based on processes that seek visually pleasing or perceptually 
What do Deep Networks Like to See?  Sebastian Palacio∗  Joachim Folz∗ German Research Center for Artiﬁcial Intelligence (DFKI)  J¨orn Hees  Federico Raue Damian Borth Andreas Dengel  TU Kaiserslautern  first.last@dfki.de  Abstract  encoder (ﬁxed)  decoder  (ﬁne-tuned)  classiﬁer (ﬁxed)  We propose a novel way to measure and understand con- volutional neural networks by quantifying the amount of in- put signal they let in. To do this, an autoencoder (AE) was ﬁne-tuned on gradients from a pre-trai
Deep Video Super-Resolution Network Using  Dynamic Upsampling Filters Without Explicit Motion Compensation  Younghyun Jo  Seoung Wug Oh  Jaeyeon Kang  Seon Joo Kim  Yonsei University  Abstract  Video super-resolution (VSR) has become even more im- portant recently to provide high resolution (HR) contents for ultra high deﬁnition displays. While many deep learn- ing based VSR methods have been proposed, most of them rely heavily on the accuracy of motion estimation and com- pensation. We introduc
Defense against Universal Adversarial Perturbations  Naveed Akhtar*  Jian Liu*  Ajmal Mian  *The authors contributed equally to this work.  Computer Science and Software Engineering  The University of Western Australia  naveed.akhtar@uwa.edu.au, jian.liu@research.uwa.edu.au, ajmal.mian@uwa.edu.au  Abstract  Recent advances in Deep Learning show the existence of image-agnostic quasi-imperceptible perturbations that when applied to ‘any’ image can fool a state-of-the-art net- work classiﬁer to cha
DenseASPP for Semantic Segmentation in Street Scenes  Maoke Yang  Kun Yu  Chi Zhang DeepMotion  Zhiwei Li  Kuiyuan Yang  {maokeyang, kunyu, chizhang, zhiweili, kuiyuanyang}@deepmotion.ai  Abstract  Semantic image segmentation is a basic street scene un- derstanding task in autonomous driving, where each pixel in a high resolution image is categorized into a set of seman- tic labels. Unlike other scenarios, objects in autonomous driving scene exhibit very large scale changes, which poses great ch
Towards dense object tracking in a 2D honeybee hive  Katarzyna Bozek  Laetitia Hebert  Alexander S. Mikheyev  OIST Graduate University  Okinawa, Japan  kasia.bozek@oist.jp  laetitia.hebert@oist.jp  alexander.mikheyev@oist.jp  Greg J. Stephens  OIST Graduate University and  VU University - Faculty of Sciences  Amsterdam, The Netherlands greg.stephens@oist.jp  Abstract  Introduction  From human crowds to cells in tissue, the detection and efﬁcient tracking of multiple objects in dense conﬁguration
Coupled End-to-end Transfer Learning with Generalized Fisher Information  Shixing Chen1  Caojin Zhang2  Ming Dong1  1Department of Computer Science  2Department of Mathematics  Wayne State University  {schen, czhang, mdong}@wayne.edu  Abstract  In transfer learning, one seeks to transfer related infor- mation from source tasks with sufﬁcient data to help with the learning of target task with only limited data. In this paper, we propose a novel Coupled End-to-end Transfer Learning (CETL) framewor
Residual Parameter Transfer for Deep Domain Adaptation  Artem Rozantsev  Mathieu Salzmann  Pascal Fua  Computer Vision Laboratory, ´Ecole Polytechnique F´ed´erale de Lausanne  Lausanne, Switzerland  {firstname.lastname}@epfl.ch  Abstract  Source  Target  The goal of Deep Domain Adaptation is to make it pos- sible to use Deep Nets trained in one domain where there is enough annotated training data in another where there is little or none. Most current approaches have focused on learning feature r
Geometric robustness of deep networks: analysis and improvement  Can Kanbak, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard  ´Ecole Polytechnique F´ed´erale de Lausanne  {can.kanbak, seyed.moosavi, pascal.frossard} @epfl.ch  Abstract  Deep convolutional neural networks have been shown to be vulnerable to arbitrary geometric transformations. How- ever, there is no systematic method to measure the invari- ance properties of deep networks to such transformations. We propose ManiFool as a simple yet
Trust your Model: Light Field Depth Estimation with inline Occlusion Handling  Hendrik Schilling, Maximilian Diebold, Carsten Rother, Bernd J¨ahne  Heidelberg Collaboratory for Image Processing (HCI)  hendrik.schilling@posteo.de  Abstract  We address the problem of depth estimation from light- ﬁeld images. Our main contribution is a new way to handle occlusions which improves general accuracy and quality of object borders. In contrast to all prior work we work with a model which directly incorpo
Reﬂection Removal for Large-Scale 3D Point Clouds  Jae-Seong Yun and Jae-Young Sim  School of Electrical and Computer Engineering  Ulsan National Institute of Science and Technology, Ulsan, Korea  jsyun@unist.ac.kr and jysim@unist.ac.kr  Abstract  Large-scale 3D point clouds (LS3DPCs) captured by ter- restrial LiDAR scanners often exhibit reﬂection artifacts by glasses, which degrade the performance of related com- puter vision techniques. In this paper, we propose an ef- ﬁcient reﬂection remova
Geometry-Aware Network for Non-Rigid Shape Prediction from a Single View  A. Pumarola1  A. Agudo1  L. Porzi2  A. Sanfeliu1  V. Lepetit3  F. Moreno-Noguer1  1Institut de Rob`otica i Inform`atica Industrial, CSIC-UPC, Barcelona, Spain  3Laboratoire Bordelais de Recherche en Informatique, Universit´e de Bordeaux, France  2Mapillary Research, Graz, Austria  Abstract  We propose a method for predicting the 3D shape of a deformable surface from a single view. By contrast with previous approaches, we d
Multimodal Visual Concept Learning with Weakly Supervised Techniques  Giorgos Bouritsas, Petros Koutras, Athanasia Zlatintsi and Petros Maragos  School of E.C.E., National Technical University of Athens, Greece gbouritsas@gmail.com, {pkoutras, nzlat, maragos}@cs.ntua.gr  Abstract  Despite the availability of a huge amount of video data accompanied by descriptive texts, it is not always easy to exploit the information contained in natural language in or- der to automatically recognize video conce
Pyramid Stereo Matching Network  Jia-Ren Chang  Yong-Sheng Chen  Department of Computer Science, National Chiao Tung University, Taiwan  {followwar.cs00g, yschen}@nctu.edu.tw  Abstract  Recent work has shown that depth estimation from a stereo pair of images can be formulated as a supervised learning task to be resolved with convolutional neural net- works (CNNs). However, current architectures rely on patch-based Siamese networks, lacking the means to ex- ploit context information for ﬁnding co
Mix and match networks: encoder-decoder alignment for zero-pair image  translation  Yaxing Wang, Joost van de Weijer, Luis Herranz  Computer Vision Center, Universitat Aut`onoma de Barcelona  Barcelona, Spain  {wang,joost,lherranz}@cvc.uab.es  Abstract  We address the problem of image translation between domains or modalities for which no direct paired data is available (i.e. zero-pair translation). We propose mix and match networks, based on multiple encoders and decoders aligned in such a way 
Analyzing Filters Toward Efﬁcient ConvNet  National Institute of Advanced Industrial Science and Technology, Japan  takumi.kobayashi@aist.go.jp  Takumi Kobayashi  Abstract  Deep convolutional neural network (ConvNet) is a promising approach for high-performance image classiﬁ- cation. The behavior of ConvNet is analyzed mainly based on the neuron activations, such as by visualizing them. In this paper, in contrast to the activations, we focus on ﬁlters which are main components of ConvNets. Throu
In-Place Activated BatchNorm for Memory-Optimized Training of DNNs  Samuel Rota Bulò, Lorenzo Porzi, Peter Kontschieder  Mapillary Research  research@mapillary.com  Abstract  In this work we present In-Place Activated Batch Nor- malization (INPLACE-ABN) – a novel approach to drasti- cally reduce the training memory footprint of modern deep neural networks in a computationally efﬁcient way. Our solution substitutes the conventionally used succession of BatchNorm + Activation layers with a single 
Fast Monte-Carlo Localization on Aerial Vehicles using  Approximate Continuous Belief Representations  Aditya Dhawale∗  Kumar Shaurya Shankar∗  Nathan Michael  Abstract  Size, weight, and power constrained platforms im- pose constraints on computational resources that introduce unique challenges in implementing localization algorithms. We present a framework to perform fast localization on such platforms enabled by the compressive capabilities of Gaus- sian Mixture Model representations of point
AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions  Chunhui Gu∗ Yeqing Li∗  Chen Sun∗ David A. Ross∗ Sudheendra Vijayanarasimhan∗  Carl Vondrick∗ George Toderici∗  Caroline Pantofaru∗  Susanna Ricco∗  Rahul Sukthankar∗  Cordelia Schmid† ∗  Jitendra Malik‡ ∗  Abstract  This paper introduces a video dataset of spatio- temporally localized Atomic Visual Actions (AVA). The AVA dataset densely annotates 80 atomic visual actions in 437 15-minute video clips, where actions are lo
Lose The Views: Limited Angle CT Reconstruction via Implicit Sinogram  Completion  Rushil Anirudh, Hyojin Kim, Jayaraman J. Thiagarajan, K. Aditya Mohan,  Kyle Champley, Timo Bremer  Lawrence Livermore National Laboratory.  Abstract  Computed Tomography (CT) reconstruction is a funda- mental component to a wide variety of applications ranging from security, to healthcare. The classical techniques re- quire measuring projections, called sinograms, from a full 180° view of the object. However, obt
AMNet: Memorability Estimation with Attention  Jiri Fajtl1, Vasileios Argyriou1, Dorothy Monekosso2, Paolo Remagnino1  1Kingston University, London, UK  2Leeds Beckett University, Leeds, UK  Abstract  In this paper we present the design and evaluation of an end-to-end trainable, deep neural network with a visual at- tention mechanism for memorability estimation in still im- ages. We analyze the suitability of transfer learning of deep models from image classiﬁcation to the memorability task. Fur
Blind Predicting Similar Quality Map for Image Quality Assessment  Da Pan, Ping Shi, Ming Hou, Zefeng Ying, Sizhe Fu, Yuan Zhang  Communication University of China  No.1 Dingfuzhuang East Street Chaoyang District, Beijing, China  {pdmeng, shiping, houming, yingzf, fusizhe, yzhang}@cuc.edu.cn  Abstract  A key problem in blind image quality assessment (BIQA) is how to effectively model the properties of human visual system in a data-driven manner. In this paper, we propose a simple and efﬁcient BI
Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?  Kensho Hara, Hirokatsu Kataoka, Yutaka Satoh  National Institute of Advanced Industrial Science and Technology (AIST)  Tsukuba, Ibaraki, Japan  {kensho.hara, hirokatsu.kataoka, yu.satou}@aist.go.jp  Abstract  The purpose of this study is to determine whether current video datasets have suﬃcient data for training very deep convolutional neural networks (CNNs) with spatio-temporal three-dimensional (3D) kernels. Recently, the
Dynamic Video Segmentation Network  Yu-Syuan Xu, Tsu-Jui Fu∗, Hsuan-Kung Yang∗, Student Member, IEEE and Chun-Yi Lee, Member, IEEE  Elsa Lab, Department of Computer Science, National Tsing Hua Uiversity  {yusean0118, rayfu1996ozig, hellochick}@gapp.nthu.edu.tw, cylee@cs.nthu.edu.tw  Abstract  In this paper, we present a detailed design of dynamic video segmentation network (DVSNet) for fast and efﬁcient semantic video segmentation. DVSNet consists of two con- volutional neural networks: a segmen
Video Rain Streak Removal By Multiscale Convolutional Sparse Coding  Minghan Li1, Qi Xie1, Qian Zhao1, Wei Wei1, Shuhang Gu2, Jing Tao1, Deyu Meng1 ∗  1National Engineering Laboratory for Algorithm and Analysis Technologiy on Big Data and Ministry  of Education Key Lab of Intelligent Networks and Network Security, Xian Jiaotong University  2computer vision lab, eth, zurich  {liminghan, xq.liwu}@stu.xjtu.edu.cn, timmy.zhaoqian@gmail.com,  weiweiwe@stu.xjtu.edu.cn, shuhanggu@gmail.com,{jtao, dymen
2    r e t s u  l  C  (a) Cluster 1  Cluster 4 (c)  3    r e t s u  l  C  (b)  (d)  6936  6937  Bounding Boxes  Object Clusters  Stochastic Flow Graph  Input Image  Post  processing  Markov Clustering  MC N  f0(i,j)  f3(i,j)  f2(i,j)  f1(i,j)  6938  Y  l0  Y  l0  l2  D Pattr  78 88 77 87 76 86 75 85 71 84 71 71 71 71 81 71 71Pattr 70 80 69 79  98 97 96 95 94 93 92 91 90 89  108 107 106 105 104 103 102 101 100 99  l1  (b)  68 67 66 65 64 63 62 61 60 59  48 47 46 45 44 43 42 41 40 39  58 57 56 
Simultaneous Detection, Ranking, and Subitizing of Multiple Salient Objects  Revisiting Salient Object Detection:  Md Amirul Islam*  Mahmoud Kalash*  Neil D. B. Bruce  University of Manitoba  University of Manitoba  Ryerson University  amirul@cs.umanitoba.ca  kalashm@cs.umanitoba.ca  bruce@ryerson.ca  Abstract  Salient object detection is a problem that has been con- sidered in detail and many solutions proposed. In this paper, we argue that work to date has addressed a problem that is relativel
È Ç  Ã  ½ Ê É  Á¾ÿ ¾ÀÄÅ¾Æ À¾¿ ¾¿  	 	  !"#$%&'()(*#! '#+,$&-./01)(02(*3345+ *0$6#07 -6#07'890%'&*/3 :;<=>?@ABCD?>AEFF?GHAIJFAFKLMNMOPEQR=STUV=QRVISAW;BQXUYBOPEQRSZEREY [\]  ¡ÿ£¤¥¡¦§¥¨©ÿª ¤©ÿ«¡¬¥ÿ¦¡¥¡ÿ¬­¬®¤¯°±ÿ²§­¡ÿ¦¡¥¡ÿ³¤¨¡ÿ´§³§¥£µ³ÿ ^_`ab`bc_de_efghbg_efe_hibajkbalmcndomoonchadhmbe ©ª¬¥¶£³§¦¡¥©ÿ£¬³³¤¯°ÿ£¥§­ÿ£¤¥¡ÿ·µ¥©ª©±ÿ²§­¡ÿ¦¡¯ªÿ­µ³ª¤´§³§¥¡¸ÿ bkdc_pn_eq_bkc_eh_eq_ckbarmes
Bidirectional Retrieval Made Simple  Jˆonatas Wehrmann  School of Technology  Rodrigo C. Barros  School of Technology  Pontif´ıcia Universidade Cat´olica  Pontif´ıcia Universidade Cat´olica  do Rio Grande do Sul  do Rio Grande do Sul  jonatas.wehrmann@acad.pucrs.br  rodrigo.barros@pucrs.br  Abstract  This paper provides a very simple yet effective character- level architecture for learning bidirectional retrieval mod- els. Aligning multimodal content is particularly challeng- ing considering the
clcNet: Improving the Efﬁciency of Convolutional Neural Network using  Channel Local Convolutions  Dong-Qing Zhang ImaginationAI LLC  dongqing@gmail.com  Abstract  Depthwise convolution and grouped convolution has been successfully applied to improve the efﬁciency of convo- lutional neural network (CNN). We suggest that these mod- els can be considered as special cases of a generalized con- volution operation, named channel local convolution(CLC), where an output channel is computed using a subs
Unsupervised Domain Adaptation with Similarity Learning  Pedro O. Pinheiro  Element AI  Montreal, QC, Canada  pedro@elementai.com  Abstract  µ  µ  µ  µ  The objective of unsupervised domain adaptation is to leverage features from a labeled source domain and learn a classiﬁer for an unlabeled target domain, with a simi- lar but different data distribution. Most deep learning ap- proaches to domain adaptation consist of two steps: (i) learn features that preserve a low risk on labeled samples (sou
DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks  Orest Kupyn1,3, Volodymyr Budzan1,3, Mykola Mykhailych1, Dmytro Mishkin2, Jiˇri Matas2  1 Ukrainian Catholic University, Lviv, Ukraine  {kupyn, budzan, mykhailych}@ucu.edu.ua  2 Visual Recognition Group, Center for Machine Perception, FEE, CTU in Prague  {mishkdmy, matas}@cmp.felk.cvut.cz  3 ELEKS Ltd.  Abstract  We present DeblurGAN, an end-to-end learned method for motion deblurring. The learning is based on a condi- ti
Unsupervised Person Image Synthesis in Arbitrary Poses  Albert Pumarola  Antonio Agudo  Alberto Sanfeliu  Francesc Moreno-Noguer  Institut de Rob`otica i Inform`atica Industrial (CSIC-UPC)  08028, Barcelona, Spain  Figure 1: Given an original image of a person (left) and a desired body pose deﬁned by a 2D skeleton (bottom-row), our model generates new photo-realistic images of the person under that pose (top-row). The main contribution of our work is to train this generative model with unlabeled
Semi-parametric Image Synthesis  Xiaojuan Qi  CUHK  Qifeng Chen  Intel Labs  Jiaya Jia CUHK  Vladlen Koltun  Intel Labs  Abstract  We present a semi-parametric approach to photographic image synthesis from semantic layouts. The approach com- bines the complementary strengths of parametric and non- parametric techniques. The nonparametric component is a memory bank of image segments constructed from a train- ing set of images. Given a novel semantic layout at test time, the memory bank is used to
9039  9040  age. During training, the input images interact through our novel loss function (Sec. 5), which evaluates the predicted decompositions jointly for the entire sequence.  For our network, we use a variant of the U-Net archi- tecture [30, 16] (Figure 2). Our network has one encoder and two decoders, one for log-reﬂectance and one for log- shading, with skip connections for both decoders. Each layer of the encoder consists mainly of a 4 × 4 stride-2 convolu- tional layer followed by ba
TieNet: Text-Image Embedding Network for Common Thorax Disease  Classiﬁcation and Reporting in Chest X-rays  Xiaosong Wang∗1, Yifan Peng∗2, Le Lu1, Zhiyong Lu2, Ronald M. Summers1  1Department of Radiology and Imaging Sciences, Clinical Center,  2 National Center for Biotechnology Information, National Library of Medicine,  National Institutes of Health, Bethesda, MD 20892  {xiaosong.wang,yifan.peng,luzh,rms}@nih.gov, lel@nvidia.com  Abstract  Chest X-rays are one of the most common radiologi- c
Analytic Expressions for Probabilistic Moments of PL-DNN with Gaussian Input  Adel Bibi∗, Modar Alfadly∗, and Bernard Ghanem  King Abdullah University of Science and Technology (KAUST), Saudi Arabia  {adel.bibi,modar.alfadly,bernard.ghanem}@kaust.edu.sa  Abstract  The outstanding performance of deep neural networks (DNNs), for the visual recognition task in particular, has been demonstrated on several large-scale benchmarks. This performance has immensely strengthened the line of re- search that
Light ﬁeld intrinsics with a deep encoder-decoder network  Anna Alperovich, Ole Johannsen, Michael Strecke and Bastian Goldluecke  University of Konstanz  Konstanz, Germany  anna.alperovich@uni-konstanz.de  Abstract  We present a fully convolutional autoencoder for light ﬁelds, which jointly encodes stacks of horizontal and verti- cal epipolar plane images through a deep network of resid- ual layers. The complex structure of the light ﬁeld is thus re- duced to a comparatively low-dimensional rep
Manifold Learning in Quotient Spaces  ´Eloi Mehr1, Andr´e Lieutier , Fernando Sanchez Bermudez , Vincent Guitteny , Nicolas Thome1, and  Matthieu Cord1  1LIP6, UPMC Sorbonne Universit´es  Abstract  When learning 3D shapes we are usually interested in their intrinsic geometry rather than in their orientation. To deal with the orientation variations the usual trick consists in augmenting the data to exhibit all possible variability, and thus let the model learn both the geometry as well as the rot
Deep Lesion Graphs in the Wild: Relationship Learning and Organization of  Signiﬁcant Radiology Image Findings in a Diverse Large-scale Lesion Database  Ke Yan, Xiaosong Wang, Le Lu, Ling Zhang, Adam P. Harrison  Mohammadhadi Bagheri, Ronald M. Summers  Imaging Biomarkers and Computer-Aided Diagnosis Laboratory  National Institutes of Health Clinical Center, 10 Center Drive, Bethesda, MD 20892  {ke.yan, xiaosong.wang, ling.zhang3, mohammad.bagheri, rms}@nih.gov,  {lel, aharrison}@nvidia.com  Abs
CNN Driven Sparse Multi-Level B-spline Image Registration  Pingge Jiang  James A. Shackleford  Department of Electrical and Computer Engineering  Drexel University  {pingge,shack}@drexel.edu  Abstract  Traditional single-grid and pyramidal B-spline param- eterizations used in deformable image registration require users to specify control point spacing conﬁgurations capa- ble of accurately capturing both global and complex local deformations. In many cases, such grid conﬁgurations are non-obvious
The power of ensembles for active learning in image classiﬁcation  William H. Beluch  Tim Genewein  BCAI∗  BCAI  Andreas N¨urnberger University Magdeburg  Jan M. K¨ohler  BCAI  Abstract  Deep learning methods have become the de-facto stan- dard for challenging image processing tasks such as im- age classiﬁcation. One major hurdle of deep learning ap- proaches is that large sets of labeled data are necessary, which can be prohibitively costly to obtain, particularly in medical image diagnosis app
5 1 0 2    n u J    2 2      ]  R  I . s c [      5 v 6 5 1 7  .  2 1 4 1 : v i X r a  Accepted as workshop contribution at ICLR 2015  REPRESENTATION LEARNING FOR COLD-START RECOMMENDATION  Gabriella Contardo∗, Ludovic Denoyer∗,Thierry Arti`eresφ ∗ Sorbonne Universit´es, UPMC Univ Paris 06, UMR 7606, LIP6, F-75005, Paris, France φ Ecole Centrale Marseille - Laboratoire d’Informatique Fondamentale (Aix-Marseille Univ.), France  ABSTRACT  A standard approach to Collaborative Filtering (CF), i.e. p
5 1 0 2    r p A 0 1         ] L C . s c [      2 v 4 0 0 7  .  2 1 4 1 : v i X r a  Accepted as a workshop contribution at ICLR 2015  TAILORING WORD EMBEDDINGS FOR BILEXICAL PREDICTIONS: AN EXPERIMENTAL COMPARISON  Pranava Swaroop Madhyastha Universitat Polit`ecnica de Catalunya Campus Nord UPC, 08034 Barcelona pranava@cs.upc.edu  Xavier Carreras Xerox Research Centre Europe 38240 Meylan, France xavier.carreras@xrce.xerox.com  Ariadna Quattoni Xerox Research Centre Europe 38240 Meylan, France a
Data Representation using the Weyl Transform  Qiang Qiu∗, Andrew Thompson†, Robert Calderbank∗, and Guillermo Sapiro∗  1  5 1 0 2    l u J    1 2      ]  V C . s c [      5 v 4 3 1 6  .  2 1 4 1 : v i X r a  Abstract—The Weyl transform is introduced as a rich frame- work for data representation. Transform coefﬁcients are con- nected to the Walsh-Hadamard transform of multiscale au- tocorrelations, and different forms of dyadic periodicity in a signal are shown to appear as different features in 
6 1 0 2     y a M 9         ]  V C . s c [      4 v 4 7 5 6  .  2 1 4 1 : v i X r a  Paper Visual Instance Retrieval with Deep Convolutional Networks  Ali S. Razavian †, Josephine Sullivan †, Stefan Carlsson †,  Atsuto Maki †  Abstract This paper provides an extensive study on the availability of image representations based on convolutional networks (ConvNets) for the task of visual instance retrieval. Besides the choice of convolutional layers, we present an efﬁcient pipeline exploiting multi-s
                  The Diagonalized New ton Algorithm for Non-  negative Matrix Factorization   Hugo Van hamme   University of Leuven, dept. ESAT   Kasteelpark Arenberg 10 – bus 2441, 3001 Leuven, Belgium   hugo.vanhamme@esat.kuleuven.be   Abstract   Non-negative  matrix  factorization  (NMF)  has  become  a  popular  machine  learning  approach  to  many  problems  in  text  mining,  speech  and  image  processing,  bio-informatics  and  seismic  data  analysis  to  name  a  few.  In  NMF,  a  m
3 1 0 2     n a J    6 1      ]  V C . s c [      1 v 5 5 7 3  .  1 0 3 1 : v i X r a  Gradient Driven Learning for Pooling in Visual  Pipeline Feature Extraction Models  Derek Rose and Itamar Arel  Deptartment of Electrical Engineering and Computer Science  University of Tennessee  derek@utk.edu, itamar@ieee.org  Abstract  Hyper-parameter selection remains a daunting task when building a pattern recog- nition architecture which performs well, particularly in recently constructed visual pipeline
3 1 0 2     n a J    6 1      ] T I . s c [      1 v 0 9 5 3  .  1 0 3 1 : v i X r a  Tree structured sparse coding on cubes  Arthur Szlam  City College of New York  aszlam@ccny.cuny.edu  Several recent works have discussed tree structured sparse coding [8, 10, 7, 3], where N data points in Rd written as the d × N matrix X are approximately decomposed into the product of matrices W Z. Here W is a d×K dictionary matrix, and Z is a K ×N matrix of coefﬁcients. In tree structured sparse coding, the 
Published as a conference paper at ICLR 2018  MIXED PRECISION TRAINING OF CONVOLUTIONAL NEURAL NETWORKS USING INTEGER OPERATIONS  Dipankar Das∗, Naveen Mellempudi∗, Dheevatsa Mudigere∗, Dhiraj Kalamkar∗ {dipankar.das,naveen.k.mellempudi, dheevatsa.mudigere,dhiraj.d.kalamkar}@intel.com Parallel Computing Lab Intel Labs, India  Sasikanth Avancha, Kunal Banerjee, Srinivas Sridharan, Karthik Vaidyanathan, Bharat Kaul Parallel Computing Lab Intel Labs, India  Evangelos Georganas, Alexander Heinecke, 
Published as a conference paper at ICLR 2018  SEMI-PARAMETRIC TOPOLOGICAL MEMORY FOR NAVIGATION  Nikolay Savinov∗ ETH Z¨urich  Alexey Dosovitskiy∗ Intel Labs  Vladlen Koltun Intel Labs  ABSTRACT  We introduce a new memory architecture for navigation in previously unseen envi- ronments, inspired by landmark-based navigation in animals. The proposed semi- parametric topological memory (SPTM) consists of a (non-parametric) graph with nodes corresponding to locations in the environment and a (parame
Published as a conference paper at ICLR 2018  APPRENTICE: USING KNOWLEDGE DISTILLATION TECHNIQUES TO IMPROVE LOW-PRECISION NET- WORK ACCURACY  Asit Mishra & Debbie Marr Accelerator Architecture Lab Intel Labs {asit.k.mishra,debbie.marr}@intel.com  ABSTRACT  Deep learning networks have achieved state-of-the-art accuracies on computer vi- sion workloads like image classiﬁcation and object detection. The performant systems, however, typically involve big models with numerous parameters. Once traine
Published as a conference paper at ICLR 2018  WRPN: WIDE REDUCED-PRECISION NETWORKS  Asit Mishra, Eriko Nurvitadhi, Jeffrey J Cook & Debbie Marr Accelerator Architecture Lab Intel Labs {asit.k.mishra,eriko.nurvitadhi,jeffrey.j.cook,debbie.marr}@intel.com  ABSTRACT  For computer vision applications, prior works have shown the efﬁcacy of re- ducing numeric precision of model parameters (network weights) in deep neu- ral networks. Activation maps, however, occupy a large memory footprint dur- ing b
Under review as a conference paper at ICLR 2018  IMPLICIT CAUSAL MODELS FOR GENOME-WIDE ASSOCIATION STUDIES  Anonymous authors Paper under double-blind review  ABSTRACT  Progress in probabilistic generative models has accelerated, developing richer models with neural architectures, implicit densities, and with scalable algorithms for their Bayesian inference. However, there has been limited progress in mod- els that capture causal relationships, for example, how individual genetic factors cause 
Published as a conference paper at ICLR 2017  LEARNING TO ACT BY PREDICTING THE FUTURE  Alexey Dosovitskiy Intel Labs  Vladlen Koltun Intel Labs  ABSTRACT  We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by in- teracting with the environment. T
Published as a conference paper at ICLR 2017  REGULARIZING CNNS WITH LOCALLY CONSTRAINED DECORRELATIONS  Pau Rodr´ıguez†, Jordi Gonz`alez†,‡, Guillem Cucurull†, Josep M. Gonfaus‡, Xavier Roca†,‡ †Computer Vision Center - Univ. Aut`onoma de Barcelona (UAB), 08193 Bellaterra, Catalonia Spain ‡Visual Tagging Services, Campus UAB, 08193 Bellaterra, Catalonia Spain  ABSTRACT  Regularization is key for deep learning since it allows training more complex mod- els while keeping lower levels of overﬁttin
Published as a conference paper at ICLR 2017  INCREMENTAL NETWORK QUANTIZATION: TOWARDS LOSSLESS CNNS WITH LOW-PRECISION WEIGHTS  Aojun Zhou∗, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen Intel Labs China {aojun.zhou, anbang.yao, yiwen.guo, lin.x.xu, yurong.chen}@intel.com  ABSTRACT  This paper presents incremental network quantization (INQ), a novel method, tar- geting to efﬁciently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version wh
UnderreviewasaconferencepaperatICLR2017SHIFTAGGREGATEEXTRACTNETWORKSFrancescoOrsini12,DanieleBaracchi2andPaoloFrasconi21DepartmentofComputerScience2DepartmentofInformationEngineeringKatholiekeUniversiteitLeuvenUniversit`adegliStudidiFirenzeCelestijnenlaan200AViadiSantaMarta33001Heverlee,BelgiumI-50139Firenze,Italyfrancesco.orsini@kuleuven.bedaniele.baracchi@unifi.itpaolo.frasconi@unifi.itABSTRACTTheShiftAggregateExtractNetwork(SAEN)isanarchitectureforlearningrepre-sentationsonsocialnetworkdata.S
Under review as a conference paper at ICLR 2017  CHARGED POINT NORMALIZATION  AN EFFICIENT SOLUTION TO THE SADDLE POINT PROBLEM  Armen Aghajanyan Bellevue, WA 98007, USA armen.ag@live.com  ABSTRACT  Recently, the problem of local minima in very high dimensional non-convex op- timization has been challenged and the problem of saddle points has been intro- duced. This paper introduces a dynamic type of normalization that forces the system to escape saddle points. Unlike other saddle point escaping
Deep learning for class-generic object detection  3 1 0 2   c e D 4 2         ]  V C . s c [      1 v 5 8 8 6  .  2 1 3 1 : v i X r a  Brody Huval Adam Coates Andrew Ng  Abstract  We investigate the use of deep neural networks for the novel task of class-generic object detec- tion. We show that neural networks originally designed for image recognition can be trained to detect objects within images, regardless of their class, including objects for which no bounding box labels have been provided. 
3 1 0 2   c e D 0 2         ]  V C . s c [      1 v 4 2 0 6  .  2 1 3 1 : v i X r a  Occupancy Detection in Vehicles Using Fisher Vector  Image Representation  Yusuf Artan  Xerox Research Center  Webster, NY 14580  Yusuf.Artan@xerox.com  Peter Paul  Xerox Research Center  Webster, NY 14580  Peter.Paul@xerox.com  Abstract  Due to the high volume of trafﬁc on modern roadways, transportation agencies have proposed High Occupancy Vehicle (HOV) lanes and High Occupancy Tolling (HOT) lanes to promote 
000 001 002 003 004 005 006 007 008 009 010 011 012 4 013 1 014 0 015 2 016   b 017 e 018 F 019   2 020     021 ] G 022 L 023 024 . s 025 c [ 026     027 2 028 v 9 029 6 030 1 031 6 032 . 2 033 1 034 3 035 1 036 : v 037 i X 038 r 039 a 040 041 042 043 044 045 046 047 048 049 050 051 052 053 054  Predict Information Diﬀusion using a Latent Representation Space  Abstract  2. Notations  Information propagation is a hard task where the goal is to predict users behavior. We in- troduce an extension o
3 1 0 2    c e D 0 2         ]  G L . s c [      1 v 3 8 7 5  .  2 1 3 1 : v i X r a  Unsupervised Feature Learning by Deep Sparse  Coding  Yunlong He∗  Koray Kavukcuoglu†  Yun Wang‡  Arthur Szlam §  Yanjun Qi¶  Abstract  In this paper, we propose a new unsupervised feature learning framework, namely Deep Sparse Coding (DeepSC), that extends sparse coding to a multi-layer architecture for vi- sual object recognition tasks. The main innovation of the framework is that it connects the sparse-encod
4 1 0 2     b e F 8 1         ] L C . s c [      3 v 9 5 5 5  .  2 1 3 1 : v i X r a  Distributional Models and Deep Learning  Embeddings: Combining the Best of Both Worlds  Irina Sergienya and Hinrich Sch¨utze  Center for Information and Language Processing  University of Munich  Germany  irina@cis.lmu.de  Abstract  There are two main approaches to the distributed representation of words: low- dimensional deep learning embeddings and high-dimensional distributional mod- els, in which each dimen
3 1 0 2   c e D 9 1         ] L C . s c [      2 v 9 2 1 5  .  2 1 3 1 : v i X r a  Deep Learning Embeddings for Discontinuous  Linguistic Units  Wenpeng Yin and Hinrich Sch¨utze  Center for Information and Language Processing  University of Munich  Germany  wenpeng@cis.lmu.de  Abstract  Deep learning embeddings have been successfully used for many natural language processing problems. Embeddings are mostly computed for word forms although a number of recent papers have extended this to other li
Deep Belief Networks for Image Denoising  Mohammad Ali Keyvanrad  Dept. of Computer Engineering and IT, Amirkabir University of Technology,  Mohammad Pezeshki  Dept. of Computer Engineering and IT, Amirkabir University of Technology,  Tehran, Iran  Tehran, Iran  keyvanrad@aut.ac.ir  m.pezeshki@aut.ac.ir  4 1 0 2     n a J    2      ]  G L . s c [      2 v 8 5 1 6  .  2 1 3 1 : v i X r a  Mohammad Mehdi Homayounpour Dept. of Computer Engineering and IT, Amirkabir University of Technology,  Tehran
4 1 0 2     n a J    2      ]  G L . s c [      2 v 7 5 1 6  .  2 1 3 1 : v i X r a  Distinction between features extracted using  Deep Belief Networks  Mohammad Pezeshki  Department of Computer Eng. and IT Amirkabir University of Technology  Tehran, Iran  Sajjad Gholami  Department of Computer Eng. and IT Amirkabir University of Technology  Tehran, Iran  m.pezeshki@aut.ac.ir  s.gholami@aut.ac.ir  Ahmad Nickabadi  Department of Computer Eng. and IT Amirkabir University of Technology  Tehran, Ira
4 1 0 2     n a J    0 1      ] E N . s c [      2 v 1 7 1 6  .  2 1 3 1 : v i X r a  Learning Paired-associate Images with An Unsupervised Deep Learning Architecture  Ti Wang and Daniel L. Silver  Jodrey School of Computer Science  Acadia University  Wolfville, NS, Canada B4P 2R6 danny.silver@acadiau.ca  Abstract  This paper presents an unsupervised multi-modal learning system that learns as- sociative representation from two input modalities, or channels, such that input on one channel will co
Group-sparse Embeddings in Collective Matrix Factorization  Arto Klami Helsinki Institute for Information Technology HIIT, Department of Information and Computer Science, Univer- sity of Helsinki  arto.klami@cs.helsinki.fi  4 1 0 2     b e F 8 1         ] L M  . t a t s [      2 v 1 2 9 5  .  2 1 3 1 : v i X r a  Guillaume Bouchard Xerox Research Centre Europe  Abhishek Tripathi Xerox Research Centre India  Abstract  CMF is a technique for simultaneously learn- ing low-rank representations based
4 1 0 2     y a M 8 2         ]  G L . s c [      2 v 0 9 1 6  .  2 1 3 1 : v i X r a  Adaptive Feature Ranking for Unsupervised Transfer  Learning  Son N. Tran  Department of Computer Science  City University London London, UK, EC1V 0HB  Son.Tran.1@city.ac.uk  Artur d’Avila Garcez  Department of Computer Science  City University London London, UK, EC1V 0HB aag@soi.city.ac.uk  Abstract  Transfer Learning is concerned with the application of knowledge gained from solving a problem to a different 
An Empirical Investigation of Catastrophic Forgetting in  Gradient-Based Neural Networks  Ian J. Goodfellow Mehdi Mirza Da Xiao Aaron Courville Yoshua Bengio  Abstract  Catastrophic forgetting is a problem faced by many machine learning models and al- gorithms. When trained on one task, then trained on a second task, many machine learning models “forget” how to perform the ﬁrst task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the
Stopping Criteria in Contrastive Divergence: Alternatives to the  Reconstruction Error  4 1 0 2    r p A 9         ]  G L . s c [      2 v 2 6 0 6  .  2 1 3 1 : v i X r a  David Buchaca Prats Departament de Llenguatges i Sistemes Inform`atics, Universitat Polit`ecnica de Catalunya, Barcelona, Spain Enrique Romero Merino Departament de Llenguatges i Sistemes Inform`atics, Universitat Polit`ecnica de Catalunya, Barcelona, Spain Ferran Mazzanti Castrillejo Departament de F´ısica i Enginyeria Nuclea
4 1 0 2     b e F 8 1         ]  G L . s c [      2 v 9 6 8 5  .  2 1 3 1 : v i X r a  Principled Non-Linear Feature Selection  Dimitrios Athanasakis∗  dathanasakis@cs.ucl.ac.uk  John Shawe-Taylor* jst@cs.ucl.ac.uk  Delmiro Fernandez-Reyes† dfernan@nimr.mrc.ac.uk  Abstract  Recent non-linear feature selection approaches employing greedy optimisation of Centred Kernel Target Alignment (KTA) exhibit strong results in terms of gen- eralisation accuracy and sparsity. However, they are computationall
4 1 0 2     b e F 7 1         ] E N . s c [      3 v 8 0 1 6  .  2 1 3 1 : v i X r a  Modeling correlations in spontaneous activity of visual cortex with centered Gaussian-binary deep  Boltzmann machines  Nan Wang  Institut f¨ur Neuroinformatik Ruhr-Universit¨at Bochum Bochum, 44780, Germany nan.wang@ini.rub.de  Dirk Jancke  Institut f¨ur Neuroinformatik Ruhr-Universit¨at Bochum Bochum, 44780, Germany  jancke@neurobiologie.rub.de  Laurenz Wiskott  Institut f¨ur Neuroinformatik Ruhr-Universit¨at 
Sparse similarity-preserving hashing  4 1 0 2     b e F 6 1         ]  V C . s c [      3 v 9 7 4 5  .  2 1 3 1 : v i X r a  Jonathan Masci Alex M. Bronstein Michael M. Bronstein Pablo Sprechmann Guillermo Sapiro  Abstract  In recent years, a lot of attention has been de- voted to eﬃcient nearest neighbor search by means of similarity-preserving hashing. One of the plights of existing hashing techniques is the intrinsic trade-oﬀ between performance and computational complexity: while longer hash
     Some Improvements on Deep Convolutional  Neural Netw ork Based Image Classification           Andrew G. Howard   Andrew Howard Consulting   Ventura, CA 93003   andrewgeraldhoward@gmail.com   Abstract   We investigate multiple techniques to improve upon the current state of the  art  deep  convolutional  neural  network  based  image  classification  pipeline.  The  techniques  include  adding  more  image  transformations  to  the  training  data, adding  more  transformations to  generate 
4 1 0 2     b e F 7 1         ]  G L . s c [      2 v 8 9 3 5  .  2 1 3 1 : v i X r a  Continuous Learning: Engineering Super Features  With Feature Algebras  Michael Tetelman  Advanced Magic Technologies  Los Gatos, CA 95032, USA  michael.tetelman@gmail.com  Abstract  In this paper we consider a problem of searching a space of predictive models for a given training data set. We propose an iterative procedure for deriving a sequence of improving models and a corresponding sequence of sets of non
4 1 0 2     n a J    8 2      ]  G L . s c [      4 v 1 6 4 4  .  2 1 3 1 : v i X r a  Low-Rank Approximations for Conditional  Feedforward Computation in Deep Neural Networks  Department of Electrical Engineering and Computer Science  Andrew S. Davis  University of Tennessee  andrew.davis@utk.edu  Department of Electrical Engineering and Computer Science  Itamar Arel  University of Tennessee itamar@ieee.org  Abstract  Scalability properties of deep neural networks raise key research questions, 
     Reference Distance Estimator         Yanpeng Li   Department of Computer Science   Dalian University Technology   Dalian, China 116024   liyanpeng.lyp@gmail.com   Abstract   Abstract:  A  theoretical  study  is  presented  for  a  simple  linear  classifier  called reference distance estimator (RDE), which assigns the weight of each  feature  j  as  P(r|j)-P(r),  where  r  is  a  reference  feature  relevant  to  the  target  class  y.  The  analysis  shows  that  if  r  performs  better  t
4 1 0 2     b e F 8 1         ]  G L . s c [      3 v 5 9 6 4  .  2 1 3 1 : v i X r a  Sparse, complex-valued representations of natural sounds learned with phase and amplitude continuity  priors  Max-Planck Institute for Mathematics in the Sciences  Wiktor Młynarski  Leipzig, Germany  mlynar@mis.mpg.de  Abstract  Complex-valued sparse coding is a data representation which employs a dictionary of two-dimensional subspaces, while imposing sparse, factorial prior on complex amplitudes. When traine
3 1 0 2    c e D 5 1         ]  G L . s c [      1 v 9 0 2 4  .  2 1 3 1 : v i X r a  000 001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053  Feature Graph Architectures  Anonymous Author(s)  Afﬁliation Address email  Abstract  In this article we propose feature graph architectures (FGA), which are deep learn- ing systems employing a str
Published as a conference paper at ICLR 2019  RECALL TRACES: BACKTRACKING MODELS FOR EFFICIENT REINFORCEMENT LEARNING  Anirudh Goyal1, Philemon Brakel2, William Fedus1, Soumye Singhal1,4, Timothy Lillicrap2, Sergey Levine5, Hugo Larochelle3, Yoshua Bengio1  ABSTRACT  In many environments only a tiny subset of all states yield high reward. In these cases, few of the interactions with the environment provide a relevant learning signal. Hence, we may want to preferentially train on those high-rewar
Published as a conference paper at ICLR 2019  IMPROVING GENERALIZATION AND STABILITY OF GENERATIVE ADVERSARIAL NETWORKS  Hoang Thanh-Tung hoangtha@deakin.edu.au  Truyen Tran truyen.tran@deakin.edu.au  Svetha Venkatesh svetha.venkatesh@deakin.edu.au  ABSTRACT  Generative Adversarial Networks (GANs) are one of the most popular tools for learning complex high dimensional distributions. However, generalization prop- erties of GANs have not been well understood. In this paper, we analyze the generali
Under review as a conference paper at ICLR 2019  ARE ADVERSARIAL EXAMPLES INEVITABLE?  Anonymous authors Paper under double-blind review  ABSTRACT  A wide range of defenses have been proposed to harden neural networks against adversarial attacks. However, a pattern has emerged in which the majority of adversarial defenses are quickly broken by new attacks. Given the lack of success at generating robust defenses, we are led to ask a fundamental question: Are adversarial attacks inevitable? This p
Published as a conference paper at ICLR 2019  DISTRIBUTIONAL CONCAVITY REGULARIZATION FOR GANS  Shoichiro Yamaguchi, Masanori Koyama Preferred Networks {guguchi, masomatics}@preferred.jp  ABSTRACT  We propose Distributional Concavity (DC) regularization for Generative Adver- sarial Networks (GANs), a functional gradient-based method that promotes the entropy of the generator distribution and works against mode collapse. Our DC regularization is an easy-to-implement method that can be used in com
Published as a conference paper at ICLR 2019  FFJORD: FREE-FORM CONTINUOUS DYNAMICS FOR SCALABLE REVERSIBLE GENERATIVE MODELS  Will Grathwohl∗†‡ , Ricky T. Q. Chen∗†, Jesse Bettencourt†, Ilya Sutskever‡ , David Duvenaud†  ABSTRACT  Reversible generative models map points from a simple distribution to a complex distribution through an easily invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determin
Published as a conference paper at ICLR 2019  LEARNING IMPROVED DYNAMICS MODEL IN RE- INFORCEMENT LEARNING BY INCORPORATING THE LONG TERM FUTURE  Nan Rosemary Ke* ♦, Amanpreet Singh* ♦ ‡, Ahmed Touati♥♦, Anirudh Goyal♥ Yoshua Bengio♥, Devi Parikh♣♦ & Dhruv Batra¶ ♦  ABSTRACT  In model-based reinforcement learning, the agent interleaves between model learning and planning. These two components are inextricably intertwined. If the model is not able to provide sensible long-term prediction, the exe
Published as a conference paper at ICLR 2019  LEARNING DEEP REPRESENTATIONS BY MUTUAL IN- FORMATION ESTIMATION AND MAXIMIZATION  R Devon Hjelm MSR Montreal, MILA, UdeM, IVADO devon.hjelm@microsoft.com  Alex Fedorov MRN, UNM  Samuel Lavoie-Marchildon MILA, UdeM  Karan Grewal U Toronto  Phil Bachman MSR Montreal  Adam Trischler MSR Montreal  Yoshua Bengio MILA, UdeM, IVADO, CIFAR  ABSTRACT  This work investigates unsupervised learning of representations by maximizing mutual information between an 
Under review as a conference paper at ICLR 2019  AUTOLOSS: LEARNING DISCRETE SCHEDULE FOR AL- TERNATE OPTIMIZATION  Anonymous authors Paper under double-blind review  ABSTRACT  Many machine learning problems involve iteratively and alternately optimizing different task objectives with respect to different sets of parameters. Appropri- ately scheduling the optimization of a task objective or a set of parameters is usually crucial to the quality of convergence. In this paper, we present AutoLoss, 
Under review as a conference paper at ICLR 2019  EXPLORATION BY RANDOM NETWORK DISTILLATION  Anonymous authors Paper under double-blind review  ABSTRACT  We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a ﬁxed randomly initialized neural network. We also introduce a method to ﬂexibly combine intrinsi
Under review as a conference paper at ICLR 2019  LARGE-SCALE STUDY OF CURIOSITY-DRIVEN LEARNING  Anonymous authors Paper under double-blind review  ABSTRACT  Reinforcement learning algorithms rely on carefully engineered rewards from the environment that are extrinsic to the agent. However, annotating each environ- ment with hand-designed, dense rewards is difﬁcult and not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is such intrinsic r
Published as a conference paper at ICLR 2019  ON RANDOM DEEP WEIGHT-TIED AUTOENCODERS: EXACT ASYMPTOTIC ANALYSIS, PHASE TRANSI- TIONS, AND IMPLICATIONS TO TRAINING  Ping Li∗, Phan-Minh Nguyen†  ABSTRACT  We study the behavior of weight-tied multilayer vanilla autoencoders under the assumption of random weights. Via an exact characterization in the limit of large dimensions, our analysis reveals interesting phase transition phenomena when the depth becomes large. This, in particular, provides qua
Published as a conference paper at ICLR 2019  DEEP LAYERS AS STOCHASTIC SOLVERS  Adel Bibi ∗ KAUST  Bernard Ghanem KAUST  Vladlen Koltun Intel Labs  Ren´e Ranftl Intel Labs  ABSTRACT  We provide a novel perspective on the forward pass through a block of layers in a deep network. In particular, we show that a forward pass through a standard dropout layer followed by a linear layer and a non-linear activation is equivalent to optimizing a convex objective with a single iteration of a τ-nice Proxim
Published as a conference paper at ICLR 2019  SPARSE DICTIONARY LEARNING BY DYNAMICAL NEURAL NETWORKS  Tsung-Han Lin, Ping Tak Peter Tang Intel Corporation Santa Clara, CA {tsung-han.lin,peter.tang}@intel.com  ABSTRACT  A dynamical neural network consists of a set of interconnected neurons that inter- act over time continuously. It can exhibit computational properties in the sense that the dynamical system’s evolution and/or limit points in the associated state space can correspond to numerical 
Published as a conference paper at ICLR 2019  DATA-DEPENDENT CORESETS FOR COMPRESSING NEURAL NETWORKS WITH APPLICATIONS TO GENER- ALIZATION BOUNDS  Cenk Baykal†∗, Lucas Liebenwein†∗, Igor Gilitschenski†, Dan Feldman‡, Daniela Rus†  ABSTRACT  We present an efﬁcient coresets-based neural network compression algorithm that sparsiﬁes the parameters of a trained fully-connected neural network in a manner that provably approximates the network’s output. Our approach is based on an im- portance samplin
Published as a conference paper at ICLR 2019  LEARNING SPARSE RELATIONAL TRANSITION MODELS  Victoria Xia∗  Zi Wang∗  Kelsey Allen  Tom Silver  Leslie Pack Kaelbling  ABSTRACT  We present a representation for describing transition models in complex uncertain domains using relational rules. For any action, a rule selects a set of relevant objects and computes a distribution over properties of just those objects in the resulting state given their properties in the previous state. An iterative greed
Under review as a conference paper at ICLR 2019  UNSUPERVISED HYPERALIGNMENT FOR MULTILIN- GUAL WORD EMBEDDINGS  Anonymous authors Paper under double-blind review  ABSTRACT  We consider the problem of aligning continuous word representations, learned in multiple languages, to a common space. It was recently shown that, in the case of two languages, it is possible to learn such a mapping without supervision. This paper extends this line of work to the problem of aligning multiple languages to a c
Published as a conference paper at ICLR 2019  h-DETACH: MODIFYING THE LSTM GRADIENT TO- WARDS BETTER OPTIMIZATION  Bhargav Kanuparthi∗1, Devansh Arpit∗1, Giancarlo Kerg1, Nan Rosemary Ke1, Ioannis Mitliagkas1 & Yoshua Bengio1,2 1Montreal Institute for Learning Algorithms (MILA), Canada 2CIFAR Senior Fellow *Authors contributed equally {bhargavkanuparthi25,devansharpit}@gmail.com  ABSTRACT  Recurrent neural networks are known for their notorious exploding and vanishing gradient problem (EVGP). Th
Published as a conference paper at ICLR 2019  DON’T SETTLE FOR AVERAGE, GO FOR THE MAX: FUZZY SETS AND MAX-POOLED WORD VECTORS  Vitalii Zhelezniak, Aleksandar Savkov, April Shen, Francesco Moramarco, Jack Flann & Nils Y. Hammerla Babylon Health {vitali.zhelezniak, sasho.savkov, firstname.lastname} @babylonhealth.com  ABSTRACT  Recent literature suggests that averaged word vectors followed by simple post- processing outperform many deep learning methods on semantic textual similarity tasks. Furth
Published as a conference paper at ICLR 2019  VARIATIONAL BAYESIAN PHYLOGENETIC INFERENCE  Cheng Zhang, Frederick A. Matsen IV Computational Biology Program Fred Hutchinson Cancer Research Center Seattle, WA 98109, USA {czhang23,matsen}@fredhutch.org  ABSTRACT  Bayesian phylogenetic inference is currently done via Markov chain Monte Carlo with simple mechanisms for proposing new states, which hinders exploration efﬁ- ciency and often requires long runs to deliver accurate posterior estimates. In
Under review as a conference paper at ICLR 2019  UNSUPERVISED CONTROL THROUGH NON-PARAMETRIC DISCRIMINATIVE REWARDS  Anonymous authors Paper under double-blind review  ABSTRACT  Learning to control an environment without hand-crafted rewards or expert data remains challenging and is at the frontier of reinforcement learning research. We present an unsupervised learning algorithm to train agents to achieve perceptually- speciﬁed goals using only a stream of observations and actions. Our agent sim
Published as a conference paper at ICLR 2019  BEYOND GREEDY RANKING: SLATE OPTIMIZATION VIA LIST-CVAE  Ray Jiang∗  Sven Gowal∗  Yuqiu Qian†  Timothy A. Mann∗  Danilo J. Rezende∗  ABSTRACT  The conventional solution to the recommendation problem greedily ranks individual document candidates by prediction scores. However, this method fails to optimize the slate as a whole, and hence, often struggles to capture biases caused by the page layout and document interdepedencies. The slate recommendation
6 1 0 2     b e F 9 1         ] E N . s c [      4 v 1 6 5 4 0  .  1 1 5 1 : v i X r a  Published as a conference paper at ICLR 2016  8-BIT APPROXIMATIONS FOR PARALLELISM IN DEEP LEARNING  Tim Dettmers The Faculty of Informatics Universi`a della Svizzera italiana Via Giuseppe Bufﬁ 13, CH-6904 Lugano, Switzerland tim.dettmers@gmail.com  ABSTRACT  The creation of practical deep learning data-products often requires paralleliza- tion across processors and computers to make deep learning feasible on
A Deep Learning Approach to Unsupervised Ensemble Learning  Uri Shaham Xiuyuan Cheng Omer Dror Ariel Jaffe Boaz Nadler Joseph Chang Yuval Kluger  Abstract  We show how deep learning methods can be ap- plied in the context of crowdsourcing and unsu- pervised ensemble learning. First, we prove that the popular model of Dawid and Skene, which assumes that all classiﬁers are conditionally in- dependent, is equivalent to a Restricted Boltz- mann Machine (RBM) with a single hidden node. Hence, under t
Data-drivenRankBreakingforEfﬁcientRankAggregationAshishKhetanKHETAN2@ILLINOIS.EDUISEDepartment,UniversityofIllinoisatUrbana-Champaign,Urbana,IL61801USASewoongOhSWOH@ILLINOIS.EDUISEDepartment,UniversityofIllinoisatUrbana-Champaign,Urbana,IL61801USAAbstractRankaggregationsystemscollectordinalprefer-encesfromindividualstoproduceaglobalrank-ingthatrepresentsthesocialpreference.Toreducethecomputationalcomplexityoflearn-ingtheglobalranking,acommonpracticeistouserank-breaking.Individuals’preferencesare
A Self-Correcting Variable-Metric Algorithm for Stochastic Optimization  Frank E. Curtis Department of ISE, Lehigh University, 200 W. Packer Ave., Bethlehem, PA 18015 USA  FRANK.E.CURTIS@GMAIL.COM  Abstract  An algorithm for stochastic (convex or noncon- vex) optimization is presented. The algorithm is variable-metric in the sense that, in each itera- tion, the step is computed through the product of a symmetric positive deﬁnite scaling matrix and a stochastic (mini-batch) gradient of the object
Analysis of Deep Neural Networks with the Extended Data Jacobian Matrix  Shengjie Wang Abdel-rahman Mohamed Rich Caruana Jeff Bilmes Matthai Plilipose Matthew Richardson Krzysztof Geras Gregor Urban Ozlem Aslan  WANGSJ@CS.WASHINGTON.EDU ASAMIR@MICROSOFT.COM RCARUANA@MICROSOFT.COM BILMES@UW.EDU MATTHAIP@MICROSOFT.COM MATTRI@MICROSOFT.COM K.J.GERAS@SMS.ED.AC.UK GURBAN@UCI.EDU OZLEM@CS.UALBERTA.CA  Abstract  Deep neural networks have achieved great suc- cess on a variety of machine learning tasks. 
A Neural Autoregressive Approach to Collaborative Filtering  Yin Zheng Bangsheng Tang Wenkui Ding Hanning Zhou Hulu LLC., Beijing, 100084  Abstract  This paper proposes CF-NADE, a neural autore- gressive architecture for collaborative ﬁltering (CF) tasks, which is inspired by the Restricted Boltzmann Machine (RBM) based CF model and the Neural Autoregressive Distribution Estimator (NADE). We ﬁrst describe the basic CF-NADE model for CF tasks. Then we propose to improve the model by sharing param
Anytime Exploration for Multi-armed Bandits using Conﬁdence Information  Kwang-Sung Jun Wisconsin Institutes for Discovery, UW-Madison, 330 N. Orchard St., Madison, WI 53715 USA Robert Nowak Wisconsin Institutes for Discovery, UW-Madison, 330 N. Orchard St., Madison, WI 53715 USA  KJUN@DISCOVERY.WISC.EDU  RDNOWAK@WISC.EDU  Abstract  We introduce anytime Explore-m, a pure explo- ration problem for multi-armed bandits (MAB) that requires making a prediction of the top- m arms at every time step. A
Train faster, generalize better: Stability of stochastic gradient descent  Moritz Hardt Benjamin Recht Yoram Singer  Abstract  We show that parametric models trained by a stochastic gradient method (SGM) with few it- erations have vanishing generalization error. We prove our results by arguing that SGM is algo- rithmically stable in the sense of Bousquet and Elisseeff. Our analysis only employs elemen- tary tools from convex and continuous optimiza- tion. We derive stability bounds for both con-
ConservativeBanditsYifanWuYWU12@UALBERTA.CARoshanShariffROSHAN.SHARIFF@UALBERTA.CATorLattimoreTOR.LATTIMORE@GMAIL.COMCsabaSzepesv´ariSZEPESVA@CS.UALBERTA.CADepartmentofComputingScience,UniversityofAlberta,Edmonton,Alberta,CanadaAbstractWestudyanovelmulti-armedbanditproblemthatmodelsthechallengefacedbyacompanywishingtoexplorenewstrategiestomaximizerevenuewhilstsimultaneouslymaintainingtheirrevenueaboveaﬁxedbaseline,uniformlyovertime.Whilepreviousworkaddressedtheprob-lemundertheweakerrequirementof
Strongly-Typed Recurrent Neural Networks  David Balduzzi1 Muhammad Ghifary1,2 1Victoria University of Wellington, New Zealand 2Weta Digital, New Zealand  DBALDUZZI@GMAIL.COM MGHIFARY@GMAIL.COM  Abstract  Recurrent neural networks are increasing pop- ular models for sequential learning. Unfortu- nately, although the most effective RNN archi- tectures are perhaps excessively complicated, ex- tensive searches have not found simpler alterna- tives. This paper imports ideas from physics and functiona
Stochastic Block BFGS: Squeezing More Curvature out of Data  Robert M. Gower Donald Goldfarb Peter Richt´arik  GOWERROBERT@GMAIL.COM GOLDFARB@COLUMBIA.EDU PETER.RICHTARIK@ED.AC.UK  Abstract  We propose a novel limited-memory stochastic block BFGS update for incorporating enriched curvature information in stochastic approxima- tion methods. In our method, the estimate of the inverse Hessian matrix that is maintained by it, is updated at each iteration using a sketch of the Hessian, i.e., a random
Recycling Randomness with Structure for Sublinear time Kernel Expansions  Krzysztof Choromanski Vikas Sindhwani  KCHORO@GOOGLE.COM SINDHWANI@GOOGLE.COM  Abstract  We propose a scheme for recycling Gaussian random vectors into structured matrices to ap- proximate various kernel functions in sublin- ear time via random embeddings. Our frame- work includes the Fastfood construction of Le et al. (2013) as a special case, but also ex- tends to Circulant, Toeplitz and Hankel matri- ces, and the broade
Pricing a Low-regret Seller  Hoda Heidari Mohammad Mahdian Umar Syed Sergei Vassilvitskii Sadra Yazdanbod  Abstract  As the number of ad exchanges has grown, pub- lishers have turned to low regret learning algo- rithms to decide which exchange offers the best price for their inventory. This in turn opens the following question for the exchange: how to set prices to attract as many sellers as possible and maximize revenue. In this work we formulate this precisely as a learning problem, and presen
Speeding up k-means by approximating Euclidean distances via block vectors  Thomas Bottesch1,2 Thomas B¨uhler1 Markus K¨achele2  THOMAS.BOTTESCH@AVIRA.COM THOMAS.BUEHLER@AVIRA.COM MARKUS.KAECHELE@UNI-ULM.DE  1Avira Operations GmbH & Co. KG, Tettnang, Germany 2Institute of Neural Information Processing, Ulm University, Ulm, Germany  Abstract  This paper introduces a new method to approx- imate Euclidean distances between points using block vectors in combination with the H¨older in- equality. By 
Fixed Point Quantization of Deep Convolutional Networks  Darryl D. Lin Qualcomm Research, San Diego, CA 92121, USA Sachin S. Talathi Qualcomm Research, San Diego, CA 92121, USA V. Sreekanth Annapureddy NetraDyne Inc., San Diego, CA 92121, USA  Abstract  In recent years increasingly complex architec- tures for deep convolution networks (DCNs) have been proposed to boost the performance on image recognition tasks. However, the gains in performance have come at a cost of substantial increase in com
Energetic Natural Gradient Descent  Philip S. Thomas Bruno Castro da Silva Christoph Dann Emma Brunskill  Abstract  We propose a new class of algorithms for min- imizing or maximizing functions of parametric probabilistic models. These new algorithms are natural gradient algorithms that leverage more information than prior methods by using a new metric tensor in place of the commonly used Fisher information matrix. This new metric ten- sor is derived by computing directions of steep- est ascent 
PHOG: Probabilistic Model for Code  Pavol Bielik Veselin Raychev Martin Vechev Department of Computer Science, ETH Z¨urich, Switzerland  PAVOL.BIELIK@INF.ETHZ.CH VESELIN.RAYCHEV@INF.ETHZ.CH MARTIN.VECHEV@INF.ETHZ.CH  Abstract  We introduce a new generative model for code called probabilistic higher order grammar (PHOG). PHOG generalizes probabilistic con- text free grammars (PCFGs) by allowing condi- tioning of a production rule beyond the parent non-terminal, thus capturing rich contexts rele- 
Stochastic Discrete Clenshaw-Curtis Quadrature  Nico Piatkowski Artiﬁcial Intelligence Group, TU Dortmund, Germany Katharina Morik Artiﬁcial Intelligence Group, TU Dortmund, Germany  NICO.PIATKOWSKI@TU-DORTMUND.DE  KATHARINA.MORIK@TU-DORTMUND.DE  Abstract  The partition function is fundamental for proba- bilistic graphical models—it is required for infer- ence, parameter estimation, and model selection. Evaluating this function corresponds to discrete integration, namely a weighted sum over an e
OnlineBayesianPassive-AggressiveLearningTianlinShi†STL501@GMAIL.COMJunZhu‡DCSZJ@MAIL.TSINGHUA.EDU.CN†InstituteforInterdisciplinaryInformationSciences,TsinghuaUniversity,China‡Dept.ofComp.Sci.&Tech.,TNListLab,StateKeyLabofIntell.Tech.&Sys.,TsinghuaUniversity,ChinaAbstractOnlinePassive-Aggressive(PA)learningisaneffectiveframeworkforperformingmax-marginonlinelearning.Butthedeterministicformu-lationandestimatedsinglelarge-marginmodelcouldlimititscapabilityindiscoveringdescrip-tivestructuresunderlyin
Ensemble-Based Tracking:  Aggregating Crowdsourced Structured Time Series Data  Naiyan Wang Dit-Yan Yeung Department of Computer Science and Engineering, Hong Kong Univeristy of Science and Technology Clear Water Bay, Hong Kong  WINSTY@GMAIL.COM DYYEUNG@CSE.UST.HK  Abstract  We study the problem of aggregating the con- tributions of multiple contributors in a crowd- sourcing setting. The data involved is in a form not typically considered in most crowdsourcing tasks, in that the data is structur
Effective Bayesian Modeling of Groups of Related Count Time Series  Nicolas Chapados ApSTAT Technologies Inc., 408-4200 Boul. St-Laurent, Montral, QC, H2W 2R2, CANADA  CHAPADOS@APSTAT.COM  Abstract  1.1. Motivating Applications  Time series of counts arise in a variety of fore- casting applications, for which traditional mod- els are generally inappropriate. This paper in- troduces a hierarchical Bayesian formulation ap- plicable to count time series that can easily ac- count for explanatory var
On learning to localize objects with minimal supervision  Hyun Oh Song Ross Girshick Stefanie Jegelka Julien Mairal Zaid Harchaoui Trevor Darrell  Abstract  Learning to localize objects with minimal super- vision is an important problem in computer vi- sion, since large fully annotated datasets are ex- tremely costly to obtain. In this paper, we pro- pose a new method that achieves this goal with only image-level labels of whether the objects are present or not. Our approach combines a dis- crim
Improved Regret Bounds for Thompson Sampling  in Linear Quadratic Control Problems  Marc Abeille 1 Alessandro Lazaric 2  Abstract  Thompson sampling (TS) is an effective approach to trade off exploration and exploration in rein- forcement learning. Despite its empirical success and recent advances, its theoretical analysis is often limited to the Bayesian setting, ﬁnite state- action spaces, or ﬁnite-horizon problems. In this paper, we study an instance of TS in the challeng- ing setting of the 
State Abstractions for Lifelong Reinforcement Learning  David Abel 1 Dilip Arumugam 1 Lucas Lehnert 1 Michael L. Littman 1  Abstract  In lifelong reinforcement learning, agents must effectively transfer knowledge across tasks while simultaneously addressing exploration, credit as- signment, and generalization. State abstraction can help overcome these hurdles by compressing the representation used by an agent, thereby re- ducing the computational and statistical burdens of learning. To this end,
Policy and Value Transfer in Lifelong Reinforcement Learning  David Abel † 1 Yuu Jinnai † 1 Yue Guo 1 George Konidaris 1 Michael L. Littman 1  Abstract  We consider the problem of how best to use prior experience to bootstrap lifelong learning, where an agent faces a series of task instances drawn from some task distribution. First, we identify the initial policy that optimizes expected performance over the distribution of tasks for increasingly com- plex classes of policy and task distributions
INSPECTRE: Privately Estimating the Unseen  Jayadev Acharya * 1 Gautam Kamath * 2 Ziteng Sun * 1 Huanyu Zhang * 1  Abstract  We develop differentially private methods for esti- mating various distributional properties. Given a sample from a discrete distribution p, some func- tional f, and accuracy and privacy parameters α and ε, the goal is to estimate f(p) up to accu- racy α, while maintaining ε-differential privacy of the sample. We prove almost-tight bounds on the sample size required for th
Learning Representations and Generative Models for 3D Point Clouds  Panos Achlioptas 1 Olga Diamanti 1 Ioannis Mitliagkas 2 Leonidas Guibas 1  Abstract  Three-dimensional geometric data offer an excel- lent domain for studying representation learning and generative modeling. In this paper, we look at geometric data represented as point clouds. We introduce a deep AutoEncoder (AE) network with state-of-the-art reconstruction quality and gen- eralization ability. The learned representations outper
Discovering Interpretable Representations for Both  Deep Generative and Discriminative Models  Tameem Adel 1 2 Zoubin Ghahramani 1 2 3 Adrian Weller 1 2 4  Abstract  Interpretability of representations in both deep generative and discriminative models is highly desirable. Current methods jointly optimize an objective combining accuracy and interpretabil- ity. However, this may reduce accuracy, and is not applicable to already trained models. We propose two interpretability frameworks. First, we 
A Reductions Approach to Fair Classiﬁcation  Alekh Agarwal 1 Alina Beygelzimer 2 Miroslav Dud´ık 1 John Langford 1 Hanna Wallach 1  Abstract  We present a systematic approach for achieving fairness in a binary classiﬁcation setting. While we focus on two well-known quantitative deﬁni- tions of fairness, our approach encompasses many other previously studied deﬁnitions as special cases. The key idea is to reduce fair classiﬁcation to a sequence of cost-sensitive classiﬁcation problems, whose solu
Accelerated Spectral Ranking  Arpit Agarwal 1 Prathamesh Patil 1 Shivani Agarwal 1  Abstract  The problem of rank aggregation from pairwise and multiway comparisons has a wide range of im- plications, ranging from recommendation systems to sports rankings to social choice. Some of the most popular algorithms for this problem come from the class of spectral ranking algorithms; these include the rank centrality algorithm for pair- wise comparisons, which returns consistent esti- mates under the Br
MISSION: Ultra Large-Scale Feature Selection using Count-Sketches  Amirali Aghazadeh * 1 Ryan Spring * 2 Daniel LeJeune 3 Gautam Dasarathy 3 Anshumali Shrivastava 2  Richard G. Baraniuk 3  Abstract  Feature selection is an important challenge in ma- chine learning. It plays a crucial role in the ex- plainability of machine-driven decisions that are rapidly permeating throughout modern society. Unfortunately, the explosion in the size and di- mensionality of real-world datasets poses a se- vere c
Minimal I-MAP MCMC  for Scalable Structure Discovery in Causal DAG Models  Raj Agrawal 1 2 3 Tamara Broderick 1 2 Caroline Uhler 2 3  Abstract  Learning a Bayesian network (BN) from data can be useful for decision-making or discovering causal relationships. However, traditional meth- ods often fail in modern applications, which ex- hibit a larger number of observed variables than data points. The resulting uncertainty about the underlying network as well as the desire to incor- porate prior info
Simple, Distributed, and Diverse Matching with High Entropy  Proportional Allocation:  Shipra Agrawal * 1 Vahab Mirrokni 2 Morteza Zadimoghaddam 2  Abstract  Inspired by many applications of bipartite match- ing in online advertising and machine learning, we study a simple and natural iterative propor- tional allocation algorithm: Maintain a priority score βa for each node a ∈ A on one side of the bipartition, initialized as βa = 1. Iteratively allo- cate the nodes i ∈ I on the other side to eli
Bucket Renormalization for Approximate Inference  Sungsoo Ahn 1 Michael Chertkov 2 3 Adrian Weller 4 5 Jinwoo Shin 1 6  Abstract  Probabilistic graphical models are a key tool in machine learning applications. Computing the partition function, i.e., normalizing constant, is a fundamental task of statistical inference but it is generally computationally intractable, leading to extensive study of approximation methods. Itera- tive variational methods are a popular and success- ful family of approa
oi-VAE: Output Interpretable VAEs for Nonlinear Group Factor Analysis  Samuel K. Ainsworth 1 Nicholas J. Foti 1 Adrian K. C. Lee 2 Emily B. Fox 1  Abstract  Deep generative models have recently yielded en- couraging results in producing subjectively realis- tic samples of complex data. Far less attention has been paid to making these generative models inter- pretable. In many scenarios, ranging from scien- tiﬁc applications to ﬁnance, the observed variables have a natural grouping. It is often o
Limits of Estimating Heterogeneous Treatment Effects:  Guidelines for Practical Algorithm Design  Ahmed M. Alaa 1 Mihaela van der Schaar 1 2 3  Abstract  Estimating heterogeneous treatment effects from observational data is a central problem in many domains. Because counterfactual data is inac- cessible, the problem differs fundamentally from supervised learning, and entails a more com- plex set of modeling choices. Despite a vari- ety of recently proposed algorithmic solutions, a principled gui
AutoPrognosis: Automated Clinical Prognostic Modeling  via Bayesian Optimization with Structured Kernel Learning  Ahmed M. Alaa 1 Mihaela van der Schaar 1 2 3  Abstract  Clinical prognostic models derived from large- scale healthcare data can inform critical diagnos- tic and therapeutic decisions. To enable off-the- shelf usage of machine learning (ML) in prog- nostic research, we developed AUTOPROGNOSIS: a system for automating the design of predic- tive modeling pipelines tailored for clinical
Information Theoretic Guarantees for Empirical Risk Minimization with Applications to Model Selection and Large-Scale Optimization  Ibrahim Alabdulmohsin 1  Abstract  In this paper, we derive bounds on the mu- tual information of the empirical risk minimiza- tion (ERM) procedure for both 0-1 and strongly- convex loss classes. We prove that under the Ax- iom of Choice, the existence of an ERM learn- ing rule with a vanishing mutual information is equivalent to the assertion that the loss class ha
Fixing a Broken ELBO  Alexander A. Alemi 1 Ben Poole 2 * Ian Fischer 1 Joshua V. Dillon 1 Rif A. Saurous 1 Kevin Murphy 1  Abstract  Recent work in unsupervised representation learn- ing has focused on learning deep directed latent- variable models. Fitting these models by maxi- mizing the marginal likelihood or evidence is typ- ically intractable, thus a common approximation is to maximize the evidence lower bound (ELBO) instead. However, maximum likelihood training (whether exact or approximat
Differentially Private Identity and Equivalence Testing of Discrete Distributions  Maryam Aliakbarpour 1 Ilias Diakonikolas 2 Ronitt Rubinfeld 1 3  Abstract  We study the fundamental problems of identity and equivalence testing over a discrete popula- tion from random samples. Our goal is to develop efﬁcient testers while guaranteeing differential pri- vacy to the individuals of the population. We pro- vide sample-efﬁcient differentially private testers for these problems. Our theoretical result
Make the Minority Great Again:  First-Order Regret Bound for Contextual Bandits  Zeyuan Allen-Zhu * 1 S´ebastien Bubeck * 1 Yuanzhi Li * 2  Abstract  √  √  L∗, which may be much smaller than  Regret bounds in online learning compare the player’s performance to L∗, the optimal perfor- mance in hindsight with a ﬁxed strategy. Typi- cally such bounds scale with the square root of the time horizon T . The more reﬁned concept of ﬁrst-order regret bound replaces this with a scal- T . ing It is well kn
Augmented CycleGAN: Learning Many-to-Many Mappings  from Unpaired Data  Amjad Almahairi 1 † Sai Rajeswar 1 Alessandro Sordoni 2 Philip Bachman 2 Aaron Courville 1 3  Abstract  Learning inter-domain mappings from unpaired data can improve performance in structured pre- diction tasks, such as image segmentation, by re- ducing the need for paired data. CycleGAN was recently proposed for this problem, but critically assumes the underlying inter-domain mapping is approximately deterministic and one-t
Meta-Learning by Adjusting Priors  Based on Extended PAC-Bayes Theory  Ron Amit 1 Ron Meir 1  Abstract  In meta-learning an agent extracts knowledge from observed tasks, aiming to facilitate learn- ing of novel future tasks. Under the assumption that future tasks are ‘related’ to previous tasks, the accumulated knowledge should be learned in a way which captures the common structure across learned tasks, while allowing the learner sufﬁcient ﬂexibility to adapt to novel aspects of new tasks. We p
MAGAN: Aligning Biological Manifolds  Matthew Amodio 1 Smita Krishnaswamy 2 1  Abstract  It is increasingly common in many types of nat- ural and physical systems (especially biological systems) to have different types of measurements performed on the same underlying system. In such settings, it is important to align the man- ifolds arising from each measurement in order to integrate such data and gain an improved pic- ture of the system; we tackle this problem using generative adversarial netwo
Subspace Embedding and Linear Regression with Orlicz Norm  Alexandr Andoni 1 Chengyu Lin 1 Ying Sheng 1 Peilin Zhong 1 Ruiqi Zhong 1  Abstract  as (cid:107)x(cid:107)G = inf {α > 0 |(cid:80)n  We consider a generalization of the classic lin- ear regression problem to the case when the loss is an Orlicz norm. An Orlicz norm is parameterized by a non-negative convex func- tion G : R+ → R+ with G(0) = 0: the Orlicz norm of a vector x ∈ Rn is deﬁned i=1 G(|xi|/α) ≤ 1} . We consider the cases where t
On the Optimization of Deep Networks:  Implicit Acceleration by Overparameterization  Sanjeev Arora 1 2 Nadav Cohen 2 Elad Hazan 1 3  Abstract  Conventional wisdom in deep learning states that increasing depth improves expressiveness but complicates optimization. This paper suggests that, sometimes, increasing depth can speed up optimization. The effect of depth on optimization is decoupled from expressiveness by focusing on settings where additional layers amount to over- parameterization – lin
Stronger Generalization Bounds for Deep Nets via a Compression Approach  Sanjeev Arora 1 Rong Ge 2 Behnam Neyshabur 3 Yi Zhang 1  Abstract  Deep nets generalize well despite having more parameters than the number of training samples. Recent works try to give an explanation using PAC-Bayes and Margin-based analyses, but do not as yet result in sample complexity bounds better than naive parameter counting. The cur- rent paper shows generalization bounds that are orders of magnitude better in pract
Lipschitz Continuity in Model-based Reinforcement Learning  Kavosh Asadi * 1 Dipendra Misra * 2 Michael L. Littman 1  Abstract  We examine the impact of learning Lipschitz continuous models in the context of model-based reinforcement learning. We provide a novel bound on multi-step prediction error of Lipschitz models where we quantify the error using the Wasserstein metric. We go on to prove an error bound for the value-function estimate arising from Lipschitz models and show that the estimated
Obfuscated Gradients Give a False Sense of Security:  Circumventing Defenses to Adversarial Examples  Anish Athalye * 1 Nicholas Carlini * 2 David Wagner 2  Abstract  We identify obfuscated gradients, a kind of gradi- ent masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization- based attacks, we ﬁnd defenses relying on this effect can be circumvented. We descr
Synthesizing Robust Adversarial Examples  Anish Athalye * 1 2 Logan Engstrom * 1 2 Andrew Ilyas * 1 2 Kevin Kwok 2  Abstract  Standard methods for generating adversarial ex- amples for neural networks do not consistently fool neural network classiﬁers in the physical world due to a combination of viewpoint shifts, camera noise, and other natural transformations, limiting their relevance to real-world systems. We demonstrate the existence of robust 3D adversar- ial objects, and we present the ﬁrs
Contextual Graph Markov Model:  A Deep and Generative Approach to Graph Processing  Davide Bacciu 1 Federico Errica 1 Alessio Micheli 1  Abstract  We introduce the Contextual Graph Markov Model, an approach combining ideas from gen- erative models and neural networks for the pro- cessing of graph data. It founds on a constructive methodology to build a deep architecture com- prising layers of probabilistic models that learn to encode the structured information in an incre- mental fashion. Contex
Greed is Still Good: Maximizing Monotone Submodular+Supermodular (BP)  Functions  Wenruo Bai 1 Jeffrey A. Bilmes 1 2  Abstract  We analyze the performance of the greedy algo- rithm, and also a discrete semi-gradient based al- gorithm, for maximizing the sum of a suBmodular and suPermodular (BP) function (both of which are non-negative monotone non-decreasing) un- der two types of constraints, either a cardinality constraint or p ≥ 1 matroid independence con- straints. These problems occur natura
Comparing Dynamics: Deep Neural Networks versus Glassy Systems  Marco Baity-Jesi 1 Levent Sagun 2 3 Mario Geiger 3 Stefano Spigler 3 2 G´erard Ben Arous 4  Chiara Cammarota 5 Yann LeCun 4 6 7 Matthieu Wyart 3 Giulio Biroli 2 8  Abstract  We analyze numerically the training dynamics of deep neural networks (DNN) by using methods developed in statistical physics of glassy systems. The two main issues we address are (1) the com- plexity of the loss landscape and of the dynam- ics within it, and (2)
SMAC: Simultaneous Mapping and Clustering Using Spectral Decompositions  Chandrajit Bajaj 1 Tingran Gao 2 Zihang He 3 Qixing Huang 1 Zhenxiao Liang 3  Abstract  We introduce a principled approach for simultane- ous mapping and clustering (SMAC) for establish- ing consistent maps across heterogeneous object collections (e.g., 2D images or 3D shapes). Our approach takes as input a heterogeneous object collection and a set of maps computed between some pairs of objects, and outputs a homogeneous ob
A Boo(n) for Evaluating Architecture Performance  Ondrej Bajgar 1 Rudolf Kadlec 1 Jan Kleindienst 1  Abstract  We point out important problems with the com- mon practice of using the best single model perfor- mance for comparing deep learning architectures, and we propose a method that corrects these ﬂaws. Each time a model is trained, one gets a differ- ent result due to random factors in the training process, which include random parameter initial- ization and random data shufﬂing. Reporting t
Learning to Branch  Maria-Florina Balcan 1 Travis Dick 1 Tuomas Sandholm 1 Ellen Vitercik 1  Abstract  Tree search algorithms, such as branch-and- bound, are the most widely used tools for solv- ing combinatorial problems. These algorithms recursively partition the search space to ﬁnd an optimal solution. To keep the tree small, it is cru- cial to carefully decide, when expanding a tree node, which variable to branch on at that node to partition the remaining space. Many partitioning techniques 
The Mechanics of n-Player Differentiable Games  David Balduzzi 1 S´ebastien Racani`ere 1 James Martens 1 Jakob Foerster 2 Karl Tuyls 1 Thore Graepel 1  Abstract  The cornerstone underpinning deep learning is the guarantee that gradient descent on an objective converges to local minima. Unfortunately, this guarantee fails in settings, such as generative ad- versarial nets, where there are multiple interacting losses. The behavior of gradient-based methods in games is not well understood – and is 
Spline Filters For End-to-End Deep Learning  Randall Balestriero * 1 Romain Cosentino * 1 Herv´e Glotin 2 Richard Baraniuk 1  Abstract  We propose to tackle the problem of end-to-end learning for raw waveform signals by introduc- ing learnable continuous time-frequency atoms. The derivation of these ﬁlters is achieved by deﬁning a functional space with a given smooth- ness order and boundary conditions. From this space, we derive the parametric analytical ﬁlters. Their differentiability property
A Spline Theory of Deep Networks  Randall Balestriero 1  Richard G. Baraniuk 1  Abstract  We build a rigorous bridge between deep net- works (DNs) and approximation theory via spline functions and operators. Our key result is that a large class of DNs can be written as a composition of max-afﬁne spline operators (MASOs), which provide a powerful portal through which to view and analyze their inner workings. For instance, conditioned on the input signal, the output of a MASO DN can be written as 
Approximation Guarantees for Adaptive Sampling  Eric Balkanski 1 Yaron Singer 1  Abstract  In this paper we analyze an adaptive sampling approach for submodular maximization. Adap- tive sampling is a technique that has recently been shown to achieve a constant factor approximation guarantee for submodular maximization under a cardinality constraint with exponentially fewer adaptive rounds than any previously studied con- stant factor approximation algorithm for this prob- lem. Adaptivity quantiﬁ
ImprovingtheGaussianMechanismforDifferentialPrivacy:AnalyticalCalibrationandOptimalDenoisingBorjaBalle1Yu-XiangWang23AbstractTheGaussianmechanismisanessentialbuildingblockusedinmultitudeofdifferentiallyprivatedataanalysisalgorithms.Inthispaperwere-visittheGaussianmechanismandshowthattheoriginalanalysishasseveralimportantlimitations.Ouranalysisrevealsthatthevarianceformulafortheoriginalmechanismisfarfromtightinthehighprivacyregime(ε→0)anditcannotbeextendedtothelowprivacyregime(ε→∞).Weaddressthese
Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients  Lukas Balles 1 Philipp Hennig 1  Abstract  The ADAM optimizer is exceedingly popular in the deep learning community. Often it works very well, sometimes it doesn’t. Why? We inter- pret ADAM as a combination of two aspects: for each weight, the update direction is determined by the sign of stochastic gradients, whereas the update magnitude is determined by an estimate of their relative variance. We disentangle these two as
Differentially Private Database Release via Kernel Mean Embeddings  Matej Balog 1 2 Ilya Tolstikhin 1 Bernhard Sch¨olkopf 1  Abstract  We lay theoretical foundations for new database release mechanisms that allow third-parties to con- struct consistent estimators of population statis- tics, while ensuring that the privacy of each indi- vidual contributing to the database is protected. The proposed framework rests on two main ideas. First, releasing (an estimate of) the kernel mean embedding of t
Improving Optimization for Models With Continuous Symmetry Breaking  Robert Bamler 1 Stephan Mandt 1  Abstract  Many loss functions in representation learning are invariant under a continuous symmetry transfor- mation. For example, the loss function of word embeddings (Mikolov et al., 2013b) remains un- changed if we simultaneously rotate all word and context embedding vectors. We show that repre- sentation learning models for time series possess an approximate continuous symmetry that leads to 
Improved Training of Generative Adversarial Networks  using Representative Features  Duhyeon Bang 1 Hyunjung Shim 1  Abstract  Despite the success of generative adversarial net- works (GANs) for image generation, the trade-off between visual quality and image diversity re- mains a signiﬁcant issue. This paper achieves both aims simultaneously by improving the sta- bility of training GANs. The key idea of the pro- posed approach is to implicitly regularize the dis- criminator using representative
Using Inherent Structures to design  Lean 2-layer RBMs  Abhishek Bansal 1 Abhinav Anand 2 Chiranjib Bhattacharyya 2  Abstract  Understanding the representational power of Re- stricted Boltzmann Machines (RBMs) with mul- tiple layers is an ill-understood problem and is an area of active research. Motivated from the approach of Inherent Structure formalism (Stillinger & Weber, 1982), extensively used in analysing Spin Glasses, we propose a novel mea- sure called Inherent Structure Capacity (ISC), 
Classiﬁcation from Pairwise Similarity and Unlabeled Data  Han Bao 1 2 Gang Niu 2 Masashi Sugiyama 2 1  Abstract  Supervised learning needs a huge amount of la- beled data, which can be a big bottleneck under the situation where there is a privacy concern or la- beling cost is high. To overcome this problem, we propose a new weakly-supervised learning setting where only similar (S) data pairs (two examples belong to the same class) and unlabeled (U) data points are needed instead of fully labele
Bayesian Optimization of Combinatorial Structures  Ricardo Baptista 1 Matthias Poloczek 2  Abstract  The optimization of expensive-to-evaluate black- box functions over combinatorial structures is an ubiquitous task in machine learning, engineer- ing and the natural sciences. The combinatorial explosion of the search space and costly evalu- ations pose challenges for current techniques in discrete optimization and machine learning, and critically require new algorithmic ideas. This article propo
Geodesic Convolutional Shape Optimization  Pierre Baque * 1 Edoardo Remelli * 1 Franc¸ois Fleuret 2 1 Pascal Fua 1  Abstract  Aerodynamic shape optimization has many indus- trial applications. Existing methods, however, are so computationally demanding that typical engi- neering practices are to either simply try a limited number of hand-designed shapes or restrict one- self to shapes that can be parameterized using only few degrees of freedom. In this work, we introduce a new way to optimize co
Learning to Coordinate with Coordination Graphs  in Repeated Single-Stage Multi-Agent Decision Problems  Eugenio Bargiacchi 1 Timothy Verstraeten 1 Diederik M. Roijers 1 2 Ann Now´e 1 Hado van Hasselt 3  Abstract  Learning to coordinate between multiple agents is an important problem in many reinforcement learning problems. Key to learning to coordi- nate is exploiting loose couplings, i.e., condi- tional independences between agents. In this paper we study learning in repeated fully coop- erati
Testing Sparsity over Known and Unknown Bases  Siddharth Barman 1 Arnab Bhattacharyya 1 Suprovat Ghoshal 1  Abstract  Sparsity is a basic property of real vectors that is exploited in a wide variety of machine learning applications. In this work, we describe property testing algorithms for sparsity that observe a low- dimensional projection of the input. We consider two settings. In the ﬁrst setting, we test sparsity with respect to an unknown basis: given input vectors y1, . . . , yp ∈ Rd whose
Transfer in Deep Reinforcement Learning Using  Successor Features and Generalised Policy Improvement  Andr´e Barreto 1 Diana Borsa 1 John Quan 1 Tom Schaul 1 David Silver 1  Matteo Hessel 1 Daniel Mankowitz 1 Augustin ˇZ´ıdek 1 R´emi Munos 1  Abstract  The ability to transfer skills across tasks has the potential to scale up reinforcement learning (RL) agents to environments currently out of reach. Re- cently, a framework based on two ideas, successor features (SFs) and generalised policy improv
Measuring abstract reasoning in neural networks  David G.T. Barrett * 1 Felix Hill * 1 Adam Santoro * 1 Ari S. Morcos 1 Timothy Lillicrap 1  Abstract  Whether neural networks can learn abstract rea- soning or whether they merely rely on superﬁcial statistics is a topic of recent debate. Here, we propose a dataset and challenge designed to probe abstract reasoning, inspired by a well-known hu- man IQ test. To succeed at this challenge, models must cope with various generalisation ‘regimes’ in whi
Gradient descent with identity initialization  efﬁciently learns positive deﬁnite linear transformations  by deep residual networks  Peter L. Bartlett 1 David P. Helmbold 2 Philip M. Long 3  Abstract  We analyze algorithms for approximating a func- tion f (x) = Φx mapping (cid:60)d to (cid:60)d using deep linear neural networks, i.e. that learn a function h parameterized by matrices Θ1, ..., ΘL and deﬁned by h(x) = ΘLΘL−1...Θ1x. We focus on algo- rithms that learn through gradient descent on the
Mutual Information Neural Estimation  Mohamed Ishmael Belghazi 1 Aristide Baratin 1 2 Sai Rajeswar 1 Sherjil Ozair 1 Yoshua Bengio 1 3 4  Aaron Courville 1 3 R Devon Hjelm 1 4  Abstract  We argue that the estimation of mutual informa- tion between high dimensional continuous ran- dom variables can be achieved by gradient descent over neural networks. We present a Mutual Infor- mation Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, trainable through 
To Understand Deep Learning We Need to Understand Kernel Learning†  Mikhail Belkin 1 Siyuan Ma 1 Soumik Mandal 1  Abstract  deep learning will be difﬁcult until more tractable “shallow” kernel methods are better understood.  Generalization performance of classiﬁers in deep learning has recently become a subject of intense study. Deep models, which are typically heav- ily over-parametrized, tend to ﬁt the training data exactly. Despite this “overﬁtting”, they perform well on test data, a phenomen
Understanding and Simplifying One-Shot Architecture Search  Gabriel Bender 1 Pieter-Jan Kindermans 1 Barret Zoph 1 Vijay Vasudevan 1 Quoc Le 1  Abstract  There is growing interest in automating neural network architecture design. Existing architecture search methods can be computationally expen- sive, requiring thousands of different architectures to be trained from scratch. Recent work has ex- plored weight sharing across models to amortize the cost of training. Although previous methods reduce
SIGNSGD: Compressed Optimisation for Non-Convex Problems  Jeremy Bernstein 1 2 Yu-Xiang Wang 2 3 Kamyar Azizzadenesheli 4 Anima Anandkumar 1 2  Abstract  Training large neural networks requires distribut- ing learning across multiple workers, where the cost of communicating gradients can be a signif- icant bottleneck. SIGNSGD alleviates this prob- lem by transmitting just the sign of each minibatch stochastic gradient. We prove that it can get the best of both worlds: compressed gradients and SG
Distributed Clustering via LSH Based Data Partitioning  Aditya Bhaskara 1 Maheshakya Wijewardena 1  Abstract  Given the importance of clustering in the analysis of large scale data, distributed algorithms for for- mulations such as k-means, k-median, etc. have been extensively studied. A successful approach here has been the “reduce and merge” paradigm, in which each machine reduces its input size to  (cid:101)O(k), and this data reduction continues (possibly  iteratively) until all the data ﬁts
Autoregressive Convolutional Neural  Networks for Asynchronous Time Series  Mikołaj Bi´nkowski 1 2 Gautier Marti 2 3 Philippe Donnat 2  Abstract  We propose Signiﬁcance-Offset Convolutional Neural Network, a deep convolutional network architecture for regression of multivariate asyn- chronous time series. The model is inspired by standard autoregressive (AR) models and gating mechanisms used in recurrent neural networks. It involves an AR-like weighting system, where the ﬁnal predictor is obtain
Adaptive Sampled Softmax with Kernel Based Sampling  Guy Blanc 1 Steffen Rendle 2  Abstract  Softmax is the most commonly used output func- tion for multiclass problems and is widely used in areas such as vision, natural language processing, and recommendation. A softmax model has lin- ear costs in the number of classes which makes it too expensive for many real-world problems. A common approach to speed up training involves sampling only some of the classes at each train- ing step. It is known 
Optimizing the Latent Space of Generative Networks  Piotr Bojanowski 1 Armand Joulin 1 David Lopez Paz 1 Arthur Szlam 1  Abstract  Generative Adversarial Networks (GANs) have achieved remarkable results in the task of gener- ating realistic natural images. In most successful applications, GAN models share two common aspects: solving a challenging saddle point opti- mization problem, interpreted as an adversarial game between a generator and a discriminator functions; and parameterizing the gener
NetGAN: Generating Graphs via Random Walks  Aleksandar Bojchevski * 1 Oleksandr Shchur * 1 Daniel Z¨ugner * 1 Stephan G¨unnemann 1  Abstract  We propose NetGAN – the ﬁrst implicit genera- tive model for graphs able to mimic real-world networks. We pose the problem of graph genera- tion as learning the distribution of biased random walks over the input graph. The proposed model is based on a stochastic neural network that gener- ates discrete output samples and is trained using the Wasserstein GA
A Progressive Batching L-BFGS Method for Machine Learning  Raghu Bollapragada 1 Dheevatsa Mudigere 2 Jorge Nocedal 1 Hao-Jun Michael Shi 1 Ping Tak Peter Tang 3  Abstract  The standard L-BFGS method relies on gradient approximations that are not dominated by noise, so that search directions are descent directions, the line search is reliable, and quasi-Newton up- dating yields useful quadratic models of the objec- tive function. All of this appears to call for a full batch approach, but since sm
Prediction Rule Reshaping  Matt Bonakdarpour 1 Sabyasachi Chatterjee 2 Rina Foygel Barber 1 John Lafferty 3  Abstract  Two methods are proposed for high-dimensional shape-constrained regression and classiﬁcation. These methods reshape pre-trained prediction rules to satisfy shape constraints like monotonic- ity and convexity. The ﬁrst method can be applied to any pre-trained prediction rule, while the sec- ond method deals speciﬁcally with random forests. In both cases, efﬁcient algorithms are d
QuantTree: Histograms for Change Detection in Multivariate Data Streams  Giacomo Boracchi 1 Diego Carrera 1 Cristiano Cervellera 2 Danilo Macci`o 2  Abstract  We address the problem of detecting distribution changes in multivariate data streams by means of histograms. Histograms are very general and ﬂexible models, which have been relatively ig- nored in the change-detection literature as they often require a number of bins that grows un- feasibly with the data dimension. We present QuantTree, a
Matrix Norms in Data Streams: Faster, Multi-Pass and Row-Order  Vladimir Braverman 1 Stephen Chestnut 2 Robert Krauthgamer 3 Yi Li 4 David Woodruff 5 Lin Yang 6  Abstract  Given the prevalence of large scale linear algebra problems in machine learning, recently there has been considerable effort in characterizing which functions can be approximated efﬁciently of a matrix in the data stream model. We study a number of aspects of estimating matrix norms – an important class of matrix functions – i
Predict and Constrain: Modeling Cardinality in Deep Structured Prediction  Nataly Brukhim 1 Amir Globerson 1  Abstract  Many machine learning problems require the pre- diction of multi-dimensional labels. Such struc- tured prediction models can beneﬁt from model- ing dependencies between labels. Recently, sev- eral deep learning approaches to structured pre- diction have been proposed. Here we focus on capturing cardinality constraints in such models. Namely, constraining the number of non-zero 
Quasi-Monte Carlo Variational Inference  Alexander Buchholz * 1 Florian Wenzel * 2 Stephan Mandt 3  Abstract  Many machine learning problems involve Monte Carlo gradient estimators. As a prominent ex- ample, we focus on Monte Carlo variational in- ference (mcvi) in this paper. The performance of mcvi crucially depends on the variance of its stochastic gradients. We propose variance reduc- tion by means of Quasi-Monte Carlo (qmc) sam- pling. qmc replaces N i.i.d. samples from a uni- form probabil
Path-Level Network Transformation for Efﬁcient Architecture Search  Han Cai 1 Jiacheng Yang 1 Weinan Zhang 1 Song Han 2 Yong Yu 1  Abstract  We introduce a new function-preserving trans- formation for efﬁcient neural architecture search. This network transformation allows reusing pre- viously trained networks and existing success- ful architectures that improves sample efﬁciency. We aim to address the limitation of current net- work transformation operations that can only per- form layer-level a
Improved Large-Scale Graph Learning through Ridge Spectral Sparsiﬁcation  Daniele Calandriello 1 2 Ioannis Koutis 3 Alessandro Lazaric 4 Michal Valko 1  Abstract  The representation and learning beneﬁts of meth- ods based on graph Laplacians, such as Lapla- cian smoothing or harmonic function solution for semi-supervised learning (SSL), are empir- ically and theoretically well supported. Nonethe- less, the exact versions of these methods scale poorly with the number of nodes n of the graph. In t
Bayesian Coreset Construction via Greedy Iterative Geodesic Ascent  Trevor Campbell 1 Tamara Broderick 1  Abstract  Coherent uncertainty quantiﬁcation is a key strength of Bayesian methods. But modern algo- rithms for approximate Bayesian posterior infer- ence often sacriﬁce accurate posterior uncertainty estimation in the pursuit of scalability. This work shows that previous Bayesian coreset construction algorithms—which build a small, weighted subset of the data that approximates the full data
Adversarial Learning with Local Coordinate Coding  Jiezhang Cao * 1 Yong Guo * 1 Qingyao Wu * 1 Chunhua Shen 2 Junzhou Huang 3 Mingkui Tan 1  Abstract  Generative adversarial networks (GANs) aim to generate realistic data from some prior distribu- tion (e.g., Gaussian noises). However, such prior distribution is often independent of real data and thus may lose semantic information (e.g., geo- metric structure or content in images) of data. In practice, the semantic information might be represent
Fair and Diverse DPP-Based Data Summarization  L. Elisa Celis 1 Vijay Keswani 1 Damian Straszak 1 Amit Deshpande 2 Tarun Kathuria 3 Nisheeth K. Vishnoi 1  Abstract  Sampling methods that choose a subset of the data proportional to its diversity in the feature space are popular for data summarization. How- ever, recent studies have noted the occurrence of bias – e.g., under or over representation of a particular gender or ethnicity – in such data sum- marization methods. In this paper we initiate
Conditional Noise-Contrastive Estimation of Unnormalised Models  Ciwan Ceylan * 1 Michael U. Gutmann * 2  Abstract  Many parametric statistical models are not prop- erly normalised and only speciﬁed up to an in- tractable partition function, which renders pa- rameter estimation difﬁcult. Examples of unnor- malised models are Gibbs distributions, Markov random ﬁelds, and neural network models in unsu- pervised deep learning. In previous work, the es- timation principle called noise-contrastive es
Adversarial Time-to-Event Modeling  Paidamoyo Chapfuwa 1 Chenyang Tao 1 Chunyuan Li 1 Courtney Page 1 Benjamin Goldstein 1  Lawrence Carin 1 Ricardo Henao 1  Abstract  Modern health data science applications leverage abundant molecular and electronic health data, providing opportunities for machine learning to build statistical models to support clinical prac- tice. Time-to-event analysis, also called survival analysis, stands as one of the most representative examples of such statistical models
Stability and Generalization of Learning Algorithms  that Converge to Global Optima  Zachary Charles 1 Dimitris Papailiopoulos 1  Abstract  We establish novel generalization bounds for learning algorithms that converge to global min- ima. We derive black-box stability results that only depend on the convergence of a learning al- gorithm and the geometry around the minimizers of the empirical risk function. The results are shown for non-convex loss functions satisfying the Polyak-Łojasiewicz (PL)
Learning and Memorization  Satrajit Chatterjee 1  Abstract  In the machine learning research community, it is generally believed that there is a tension be- tween memorization and generalization. In this work, we examine to what extent this tension ex- ists, by exploring if it is possible to generalize by memorizing alone. Although direct memo- rization with a lookup table obviously does not generalize, we ﬁnd that introducing depth in the form of a network of support-limited lookup ta- bles lea
On the Theory of Variance Reduction for Stochastic Gradient Monte Carlo  Niladri S. Chatterji 1 Nicolas Flammarion 2 Yi-An Ma 2 Peter L. Bartlett 2 3 Michael I. Jordan 2 3  Abstract  We provide convergence guarantees in Wasser- stein distance for a variety of variance-reduction methods: SAGA Langevin diffusion, SVRG Langevin diffusion and control-variate under- damped Langevin diffusion. We analyze these methods under a uniform set of assumptions on the log-posterior distribution, assuming it to
Hierarchical Clustering with Structural Constraints  Vaggos Chatziafratis * 1 Rad Niazadeh * 1 Moses Charikar 1  Abstract  Hierarchical clustering is a popular unsupervised data analysis method. For many real-world ap- plications, we would like to exploit prior infor- mation about the data that imposes constraints on the clustering hierarchy, and is not captured by the set of features available to the algorithm. This gives rise to the problem of hierarchical clustering with structural constraint
Hierarchical Deep Generative Models for Multi-Rate Multivariate Time Series  Zhengping Che * 1 Sanjay Purushotham * 1 Guangyu Li * 1 Bo Jiang 1 Yan Liu 1  Abstract  Multi-Rate Multivariate Time Series (MR-MTS) are the multivariate time series observations which come with various sampling rates and en- code multiple temporal dependencies. State-space models such as Kalman ﬁlters and deep learning models such as deep Markov models are mainly designed for time series data with the same sam- pling r
GradNorm: Gradient Normalization for Adaptive  Loss Balancing in Deep Multitask Networks  Zhao Chen 1 Vijay Badrinarayanan 1 Chen-Yu Lee 1 Andrew Rabinovich 1  Abstract  Deep multitask networks, in which one neural net- work produces multiple predictive outputs, can offer better speed and performance than their single-task counterparts but are challenging to train properly. We present a gradient normaliza- tion (GradNorm) algorithm that automatically bal- ances training in deep multitask models 
Weakly Submodular Maximization Beyond Cardinality Constraints: Does  Randomization Help Greedy?  Lin Chen 1 2 Moran Feldman 3 Amin Karbasi 1 2  Abstract  Submodular functions are a broad class of set functions that naturally arise in many machine learning applications. Due to their combinatorial structures, there has been a myriad of algorithms for maximizing such functions under various con- straints. Unfortunately, once a function deviates from submodularity (even slightly), the known al- gori
Projection-Free Online Optimization with Stochastic Gradient: From  Convexity to Submodularity  Lin Chen 1 2 Christopher Harshaw 1 3 Hamed Hassani 4 Amin Karbasi 1 2  Abstract  Online optimization has been a successful frame- work for solving large-scale problems under com- putational constraints and partial information. Current methods for online convex optimization require either a projection or exact gradient com- putation at each step, both of which can be pro- hibitively expensive for large
Continuous-Time Flows for Efﬁcient Inference and Density Estimation  Changyou Chen 1 Chunyuan Li 2 Liqun Chen 2 Wenlin Wang 2 Yunchen Pu 2 Lawrence Carin 2  Abstract  Two fundamental problems in unsupervised learn- ing are efﬁcient inference for latent-variable mod- els and robust density estimation based on large amounts of unlabeled data. Algorithms for the two tasks, such as normalizing ﬂows and gen- erative adversarial networks (GANs), are often developed independently. In this paper, we pro
Scalable Bilinear π Learning Using State and Action Features  Yichen Chen 1 Lihong Li 2 Mengdi Wang 3  Abstract  Approximate linear programming (ALP) rep- resents one of the major algorithmic families to solve large-scale Markov decision processes (MDP). In this work, we study a primal-dual formulation of the ALP, and develop a scalable, model-free algorithm called bilinear π learning for reinforcement learning when a sampling or- acle is provided. This algorithm enjoys a num- ber of advantages.
Stein Points  Wilson Ye Chen 1 Lester Mackey 2 Jackson Gorham 3 Franc¸ois-Xavier Briol 4 5 6 Chris. J. Oates 7 6  Abstract  An important task in computational statistics and machine learning is to approximate a posterior  distribution p(x) with an empirical measure sup- ported on a set of representative points{xi}n i=1.  This paper focuses on methods where the selec- tion of points is essentially deterministic, with an emphasis on achieving accurate approximation when n is small. To this end, we
Learning K-way D-dimensional Discrete Codes for Compact Embedding  Representations  Ting Chen 1 Martin Renqiang Min 2 Yizhou Sun 1  Abstract  Conventional embedding methods directly asso- ciate each symbol with a continuous embedding vector, which is equivalent to applying a linear transformation based on a “one-hot” encoding of the discrete symbols. Despite its simplicity, such approach yields the number of parameters that grows linearly with the vocabulary size and can lead to overﬁtting. In t
PixelSNAIL: An Improved Autoregressive Generative Model  Xi Chen 1 2 Nikhil Mishra 1 2 Mostafa Rohaninejad 1 2 Pieter Abbeel 1 2  Abstract  Autoregressive generative models achieve the best results in density estimation tasks involving high dimensional data, such as images or audio. They pose density estimation as a sequence modeling task, where a recurrent neural network (RNN) models the conditional distribution over the next element conditioned on all previous elements. In this paradigm, the b
Dynamical Isometry and a Mean Field Theory of RNNs:  Gating Enables Signal Propagation in Recurrent Neural Networks  Minmin Chen * 1 Jeffrey Pennington * 2 Samuel S. Schoenholz * 2  Abstract networks  have  neural  Recurrent gained widespread use in modeling sequence data across various domains. While many successful recurrent architectures employ a notion of gating, the exact mechanism that enables such remarkable performance is not well understood. We develop a theory for signal propagation in
Learning to Explain: An Information-Theoretic Perspective  on Model Interpretation  Jianbo Chen 1 2 Le Song 3 4 Martin J. Wainwright 1 5 Michael I. Jordan 1  Abstract  We introduce instancewise feature selection as a methodology for model interpretation. Our method is based on learning a function to ex- tract a subset of features that are most informative for each given example. This feature selector is trained to maximize the mutual information be- tween selected features and the response varia
Variational Inference and Model Selection  with Generalized Evidence Bounds  Chenyang Tao * Liqun Chen * Ruiyi Zhang Ricardo Henao Lawrence Carin  Abstract  Recent advances on the scalability and ﬂexibility of variational inference have made it successful at unravelling hidden patterns in complex data. In this work we propose a new variational bound formulation, yielding an estimator that extends beyond the conventional variational bound. It naturally subsumes the importance-weighted and R´enyi 
DRACO: Byzantine-resilient Distributed Training via Redundant Gradients  Lingjiao Chen 1 Hongyi Wang 1 Zachary Charles 1 Dimitris Papailiopoulos 1  Abstract  Distributed model training is vulnerable to byzan- tine system failures and adversarial compute nodes, i.e., nodes that use malicious updates to corrupt the global model stored at a parameter server (PS). To guarantee some form of robust- ness, recent work suggests using variants of the ge- ometric median as an aggregation rule, in place of
SADAGRAD: Strongly Adaptive Stochastic Gradient Methods  Zaiyi Chen * 1 2 Yi Xu * 2 Enhong Chen 1 Tianbao Yang 2  Abstract  Although the convergence rates of existing vari- ants of ADAGRAD have a better dependence on the number of iterations under the strong convex- ity condition, their iteration complexities have a explicitly linear dependence on the dimensionality of the problem. To alleviate this bad dependence, we propose a simple yet novel variant of ADA- GRAD for stochastic (weakly) strong
Covariate Adjusted Precision Matrix Estimation via Nonconvex Optimization  Jinghui Chen 1 Pan Xu 2 Lingxiao Wang 2 Jian Ma 3 Quanquan Gu 2  Abstract  We propose a nonconvex estimator for the covari- ate adjusted precision matrix estimation problem in the high dimensional regime, under sparsity constraints. To solve this estimator, we propose an alternating gradient descent algorithm with hard thresholding. Compared with existing methods along this line of research, which lack theoretical guarant
End-to-End Learning for the Deep Multivariate Probit Model  Di Chen 1 Yexiang Xue 1 Carla Gomes 1  Abstract  The multivariate probit model (MVP) is a popu- lar classic model for studying binary responses of multiple entities. Nevertheless, the compu- tational challenge of learning the MVP model, given that its likelihood involves integrating over a multidimensional constrained space of latent vari- ables, signiﬁcantly limits its application in prac- tice. We propose a ﬂexible deep generalization
Stochastic Training of Graph Convolutional Networks with Variance Reduction  Jianfei Chen 1 Jun Zhu 1 Le Song 2 3  Abstract  Graph convolutional networks (GCNs) are power- ful deep neural networks for graph-structured data. However, GCN computes the representation of a node recursively from its neighbors, making the receptive ﬁeld size grow exponentially with the number of layers. Previous attempts on reducing the receptive ﬁeld size by subsampling neighbors do not have convergence guarantee, an
Extreme Learning to Rank via Low Rank Assumption  Minhao Cheng 1 Ian Davidson 1 Cho-Jui Hsieh 1 2  Abstract  We consider the setting where we wish to perform ranking for hundreds of thousands of users which is common in recommender systems and web search ranking. Learning a single ranking func- tion is unlikely to capture the variability across all users while learning a ranking function for each person is time-consuming and requires large amounts of data from each user. To address this situatio
Learning a Mixture of Two Multinomial Logits  Flavio Chierichetti 1 Ravi Kumar 2 Andrew Tomkins 2  Abstract  The classical Multinomial Logit (MNL) is a be- havioral model for user choice. In this model, a user is offered a slate of choices (a subset of a ﬁ- nite universe of n items), and selects exactly one item from the slate, each with probability pro- portional to its (positive) weight. Given a set of observed slates and choices, the likelihood- maximizing item weights are easy to learn at sc
Path Consistency Learning in Tsallis Entropy Regularized MDPs  Oﬁr Nachum * 1 Yinlam Chow * 2 Mohamamd Ghavamzadeh * 2  Abstract  We study the sparse entropy-regularized rein- forcement learning (ERL) problem in which the entropy term is a special form of the Tsallis en- tropy. The optimal policy of this formulation is sparse, i.e., at each state, it has non-zero proba- bility for only a small number of actions. This addresses the main drawback of the standard Shannon entropy-regularized RL (sof
An Iterative, Sketching-based Framework for Ridge Regression  Agniva Chowdhury 1 Jiasen Yang 1 Petros Drineas 2  Abstract  Ridge regression is a variant of regularized least squares regression that is particularly suitable in settings where the number of predictor variables greatly exceeds the number of observations. We present a simple, iterative, sketching-based algo- rithm for ridge regression that guarantees high- quality approximations to the optimal solution vector. Our analysis builds upo
Stochastic Wasserstein Barycenters  Sebastian Claici 1 Edward Chien 1 Justin Solomon 1  Abstract  We present a stochastic algorithm to compute the barycenter of a set of probability distributions un- der the Wasserstein metric from optimal transport. Unlike previous approaches, our method extends to continuous input distributions and allows the support of the barycenter to be adjusted in each iteration. We tackle the problem without regu- larization, allowing us to recover a much sharper output.
Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning  with Trajectory Embeddings  John D. Co-Reyes * 1 YuXuan Liu * 1 Abhishek Gupta * 1 Benjamin Eysenbach 2 Pieter Abbeel 1 Sergey Levine 1  Abstract  In this work, we take a representation learning per- spective on hierarchical reinforcement learning, where the problem of learning lower layers in a hierarchy is transformed into the problem of learn- ing trajectory-level generative models. We show that we can learn continuo
On Acceleration with Noise-Corrupted Gradients  Michael B. Cohen 1 Jelena Diakonikolas 2 Lorenzo Orecchia 2  Abstract  Accelerated algorithms have broad applications in large-scale optimization, due to their generality and fast convergence. However, their stability in the practical setting of noise-corrupted gradient oracles is not well-understood. This paper pro- vides two main technical contributions: (i) a new accelerated method AGD+ that generalizes Nes- terov’s AGD and improves on the recen
Online Linear Quadratic Control  Alon Cohen 1 2 Avinatan Hassidim 1 3 Tomer Koren 4 Nevena Lazic 4 Yishay Mansour 1 5 Kunal Talwar 4  Abstract  We study the problem of controlling linear time- invariant systems with known noisy dynamics and adversarially chosen quadratic losses. We present the ﬁrst eﬃcient online learning algorithms in this setting that guarantee O(√ T) regret under mild assumptions, where T is the time horizon. Our algorithms rely on a novel SDP relaxation for the steady-state 
GEP-PG: Decoupling Exploration and Exploitation  in Deep Reinforcement Learning Algorithms  C´edric Colas 1 Olivier Sigaud 1 2 Pierre-Yves Oudeyer 1  Abstract  In continuous action domains, standard deep rein- forcement learning algorithms like DDPG suffer from inefﬁcient exploration when facing sparse or deceptive reward problems. Conversely, evo- lutionary and developmental methods focusing on exploration like Novelty Search, Quality- Diversity or Goal Exploration Processes explore more robust
Efﬁcient Model–Based Deep Reinforcement Learning with  Variational State Tabulation  Dane Corneil 1 Wulfram Gerstner 1 Johanni Brea 1  Abstract  Modern reinforcement learning algorithms reach super–human performance on many board and video games, but they are sample inefﬁcient, i.e. they typically require signiﬁcantly more playing experience than humans to reach an equal per- formance level. To improve sample efﬁciency, an agent may build a model of the environment and use planning methods to up
Online Learning with Abstention  Corinna Cortes 1 Giulia DeSalvo 1 Claudio Gentile 1 2 Mehryar Mohri 3 1 Scott Yang⇤ 4  Abstract  We present an extensive study of a key problem in online learning where the learner can opt to ab- stain from making a prediction, at a certain cost. In the adversarial setting, we show how existing online algorithms and guarantees can be adapted to this problem. In the stochastic setting, we ﬁrst point out a bias problem that limits the straightfor- ward extension of
Constrained Interacting Submodular Groupings  Andrew Cotter 1 Mahdi Milani Fard 1 Seungil You 2 Maya Gupta 1 Jeff Bilmes 3  Abstract  We introduce the problem of grouping a ﬁnite set V into m blocks where each block is a sub- set of V and where: (i) the blocks are individu- ally highly valued by a submodular function f (both robustly and in the average case) while sat- isfying block-speciﬁc matroid constraints; and (ii) block scores interact where blocks are jointly scored highly via f, thus mak
Inference Suboptimality in Variational Autoencoders  Chris Cremer 1 Xuechen Li 1 David Duvenaud 1  Abstract  Amortized inference allows latent-variable mod- els trained via variational learning to scale to large datasets. The quality of approximate inference is determined by two factors: a) the capacity of the variational distribution to match the true poste- rior and b) the ability of the recognition network to produce good variational parameters for each datapoint. We examine approximate infer
Mix & Match – Agent Curricula for Reinforcement Learning  Wojciech Marian Czarnecki * 1 Siddhant M. Jayakumar * 1 Max Jaderberg 1 Leonard Hasenclever 1  Yee Whye Teh 1 Simon Osindero 1 Nicolas Heess 1 Razvan Pascanu 1  Abstract  We introduce Mix & Match (M&M) – a train- ing framework designed to facilitate rapid and effective learning in RL agents, especially those that would be too slow or too challenging to train otherwise. The key innovation is a procedure that allows us to automatically form
Implicit Quantile Networks for Distributional Reinforcement Learning  Will Dabney * 1 Georg Ostrovski * 1 David Silver 1 R´emi Munos 1  Abstract  In this work, we build on recent advances in dis- tributional reinforcement learning to give a gener- ally applicable, ﬂexible, and state-of-the-art dis- tributional variant of DQN. We achieve this by using quantile regression to approximate the full quantile function for the state-action return distri- bution. By reparameterizing a distribution over t
Learning Steady-States of Iterative Algorithms over Graphs  Hanjun Dai 1 Zornitsa Kozareva 2 Bo Dai 1 Alexander J. Smola 2 Le Song 1 3  Abstract  Many graph analytics problems can be solved via iterative algorithms where the solutions are often characterized by a set of steady-state conditions. Different algorithms respect to different set of ﬁxed point constraints, so instead of using these tradi- tional algorithms, can we learn an algorithm which can obtain the same steady-state solutions auto
Adversarial Attack on Graph Structured Data  Hanjun Dai 1 Hui Li 2 Tian Tian 3 Xin Huang 2 Lin Wang 2 Jun Zhu 3 Le Song 1 2  Abstract  Deep learning on graph structures has shown exciting results in various applications. However, few attentions have been paid to the robustness of such models, in contrast to numerous research work for image or text adversarial attack and defense. In this paper, we focus on the adver- sarial attacks that fool deep learning models by modifying the combinatorial str
SBEED: Convergent Reinforcement Learning  with Nonlinear Function Approximation  Bo Dai 1 Albert Shaw 1 Lihong Li 2 Lin Xiao 3 Niao He 4 Zhen Liu 1 Jianshu Chen 5 Le Song 1  Abstract  When function approximation is used, solving the Bellman optimality equation with stability guarantees has remained a major open problem in reinforcement learning for decades. The fun- damental difﬁculty is that the Bellman operator may become an expansion in general, resulting in oscillating and even divergent beh
Compressing Neural Networks using the Variational Information Bottleneck  Bin Dai 1 Chen Zhu 2 Baining Guo 3 David Wipf 3  Abstract  Neural networks can be compressed to reduce memory and computational requirements, or to in- crease accuracy by facilitating the use of a larger base architecture. In this paper we focus on prun- ing individual neurons, which can simultaneously trim model size, FLOPs, and run-time memory. To improve upon the performance of existing com- pression algorithms we utili
Asynchronous Byzantine Machine Learning (the case of SGD)  Georgios Damaskinos 1 El Mahdi El Mhamdi 1 Rachid Guerraoui 1 Rhicheek Patra 1 Mahsa Taziki 1  Abstract  Asynchronous distributed machine learning so- lutions have proven very effective so far, but al- ways assuming perfectly functioning workers. In practice, some of the workers can however ex- hibit Byzantine behavior, caused by hardware fail- ures, software bugs, corrupt data, or even mali- cious attacks. We introduce Kardam, the ﬁrst 
Escaping Saddles with Stochastic Gradients  Hadi Daneshmand * 1 Jonas Kohler * 1 Aurelien Lucchi 1 Thomas Hofmann 1  Abstract  We analyze the variance of stochastic gradients along negative curvature directions in certain non- convex machine learning models and show that stochastic gradients exhibit a strong component along these directions. Furthermore, we show that - contrary to the case of isotropic noise - this variance is proportional to the magnitude of the corresponding eigenvalues and no
Minibatch Gibbs Sampling on Large Graphical Models  Christopher De Sa 1 Vincent Chen 1 Wing Wong 2  Abstract  Gibbs sampling is the de facto Markov chain Monte Carlo method used for inference and learn- ing on large scale graphical models. For com- plicated factor graphs with lots of factors, the performance of Gibbs sampling can be limited by the computational cost of executing a single update step of the Markov chain. This cost is pro- portional to the degree of the graph, the number of factor
Stochastic Video Generation with a Learned Prior  Emily Denton 1 Rob Fergus 1 2  Abstract  Generating video frames that accurately predict future world states is challenging. Existing ap- proaches either fail to capture the full distribu- tion of outcomes, or yield blurry generations, or both. In this paper we introduce a video gener- ation model with a learned prior over stochastic latent variables at each time step. Video frames are generated by drawing samples from this prior and combining th
Decomposition of Uncertainty in Bayesian Deep Learning  for Efﬁcient and Risk-sensitive Learning  Stefan Depeweg 1 2 Jos´e Miguel Hern´andez-Lobato 3 Finale Doshi-Velez 4 Steffen Udluft 1  Abstract  Bayesian neural networks with latent variables are scalable and ﬂexible probabilistic models: they account for uncertainty in the estimation of the network weights and, by making use of latent vari- ables, can capture complex noise patterns in the data. Using these models we show how to per- form and
Accurate Inference for Adaptive Linear Models  Yash Deshpande 1 Lester Mackey 2 Vasilis Syrgkanis 2 Matt Taddy 2 3  Abstract  Estimators computed from adaptively collected data do not behave like their non-adaptive brethren. Rather, the sequential dependence of the collection policy can lead to severe distri- butional biases that persist even in the inﬁnite data limit. We develop a general method – W - decorrelation – for transforming the bias of adap- tive linear regression estimators into vari
Variational Network Inference: Strong and Stable with Concrete Support  Amir Dezfouli 1 2 Edwin V. Bonilla 1 Richard Nock 3  Abstract  Traditional methods for the discovery of latent network structures are limited in two ways: they either assume that all the signal comes from the network (i.e. there is no source of signal outside the network) or they place constraints on the net- work parameters to ensure model or algorithmic stability. We address these limitations by propos- ing a model that in
Modeling Sparse Deviations for Compressed Sensing using Generative Models  Manik Dhar 1 Aditya Grover 1 Stefano Ermon 1  Abstract  In compressed sensing, a small number of lin- ear measurements can be used to reconstruct an unknown signal. Existing approaches leverage assumptions on the structure of these signals, such as sparsity or the availability of a genera- tive model. A domain-speciﬁc generative model can provide a stronger prior and thus allow for recovery with far fewer measurements. Ho
Alternating Randomized Block Coordinate Descent  Jelena Diakonikolas 1 Lorenzo Orecchia 1  Abstract  Block-coordinate descent algorithms and alter- nating minimization methods are fundamen- tal optimization algorithms and an important primitive in large-scale optimization and ma- chine learning. While various block-coordinate- descent-type methods have been studied exten- sively, only alternating minimization – which ap- plies to the setting of only two blocks – is known to have convergence time
Learning to Act in Decentralized Partially Observable MDPs  Jilles S. Dibangoye 1 Olivier Buffet 2  Abstract  We address a long-standing open problem of rein- forcement learning in decentralized partially ob- servable Markov decision processes. Previous attempts focussed on different forms of gener- alized policy iteration, which at best led to lo- cal optima. In this paper, we restrict attention to plans, which are simpler to store and update than policies. We derive, under certain conditions, 
LeveragingWell-ConditionedBases:StreamingandDistributedSummariesinMinkowskip-NormsGrahamCormode*1CharlieDickens*1DavidP.Woodruff*2AbstractWorkonapproximatelinearalgebrahasledtoefﬁcientdistributedandstreamingalgorithmsforproblemssuchasapproximatematrixmultipli-cation,lowrankapproximation,andregression,primarilyfortheEuclideannorm‘2.Westudyother‘pnorms,whicharemorerobustforp<2,andcanbeusedtoﬁndoutliersforp>2.Un-likepreviousalgorithmsforsuchnorms,wegivealgorithmsthatare(1)deterministic,(2)worksi-mu
Noisin: Unbiased Regularization for Recurrent Neural Networks  Adji B. Dieng 1 Rajesh Ranganath 2 Jaan Altosaar 3 David M. Blei 1  Abstract  Recurrent neural networks (RNNs) are power- ful models of sequential data. They have been successfully used in domains such as text and speech. However, RNNs are susceptible to over- ﬁtting; regularization is important. In this paper we develop Noisin, a new method for regulariz- ing RNNs. Noisin injects random noise into the hidden states of the RNN and th
Discovering and Removing Exogenous State Variables and Rewards for  Reinforcement Learning  Thomas Dietterich 1 George Trimponias 2 Zhitang Chen 2  Abstract  Exogenous state variables and rewards can slow down reinforcement learning by injecting uncon- trolled variation into the reward signal. We for- malize exogenous state variables and rewards and identify conditions under which an MDP with exogenous state can be decomposed into an ex- ogenous Markov Reward Process involving only the exogenous
Coordinated Exploration in Concurrent Reinforcement Learning  Maria Dimakopoulou 1 Benjamin Van Roy 1  Abstract  We consider a team of reinforcement learning agents that concurrently learn to operate in a com- mon environment. We identify three properties – adaptivity, commitment, and diversity – which are necessary for efﬁcient coordinated exploration and demonstrate that straightforward extensions to single-agent optimistic and posterior sampling approaches fail to satisfy them. As an alterna-
Probabilistic Recurrent State-Space Models  Andreas Doerr 1 2 Christian Daniel 1 Martin Schiegg 1 Duy Nguyen-Tuong 1 Stefan Schaal 2 3 Marc Toussaint 4  Sebastian Trimpe 2  Abstract  State-space models (SSMs) are a highly expres- sive model class for learning patterns in time series data and for system identiﬁcation. Deter- ministic versions of SSMs (e.g., LSTMs) proved extremely successful in modeling complex time series data. Fully probabilistic SSMs, however, are often found hard to train, ev
Randomized Block Cubic Newton Method  Nikita Doikov 1 Peter Richt´arik 2 3 4  Abstract  We study the problem of minimizing the sum of three convex functions—a differentiable, twice- differentiable and a non-smooth term—in a high dimensional setting. To this effect we propose and analyze a randomized block cubic Newton (RBCN) method, which in each iteration builds a model of the objective function formed as the sum of the natural models of its three compo- nents: a linear model with a quadratic r
Low-Rank Riemannian Optimization on Positive Semideﬁnite Stochastic Matrices with Applications to Graph Clustering  Abstract  Ahmed Douik 1 Babak Hassibi 1  This paper develops a Riemannian optimization framework for solving optimization problems on the set of symmetric positive semideﬁnite stochas- tic matrices. The paper ﬁrst reformulates the prob- lem by factorizing the optimization variable as X = YYT and deriving conditions on p, i.e., the number of columns of Y, under which the factorizati
Essentially No Barriers in Neural Network Energy Landscape  Felix Draxler 1 2 Kambis Veschgini 2 Manfred Salmhofer 2 Fred A. Hamprecht 1  Abstract  Training neural networks involves ﬁnding min- ima of a high-dimensional non-convex loss func- tion. Relaxing from linear interpolations, we con- struct continuous paths between minima of re- cent neural network architectures on CIFAR10 and CIFAR100. Surprisingly, the paths are essen- tially ﬂat in both the training and test landscapes. This implies t
Weakly Consistent Optimal Pricing Algorithms  in Repeated Posted-Price Auctions with Strategic Buyer  Alexey Drutsa 1 2  Abstract  We study revenue optimization learning algo- rithms for repeated posted-price auctions where a seller interacts with a single strategic buyer that holds a ﬁxed private valuation for a good and seeks to maximize his cumulative discounted sur- plus. We propose a novel algorithm that never decreases offered prices and has a tight strategic regret bound of Θ(log log T ).
On the Power of Over-parametrization in Neural Networks with Quadratic Activation  Simon S. Du 1 Jason D. Lee 2  Abstract  show as long as k ≥ √  We provide new theoretical insights on why over- parametrization is effective in learning neural net- works. For a k hidden node shallow network with quadratic activation and n training data points, we 2n, over-parametrization enables local search algorithms to ﬁnd a globally optimal solution for general smooth and convex loss functions. Further, despi
Gradient Descent Learns One-hidden-layer CNN:  Don’t be Afraid of Spurious Local Minima  Simon S. Du 1 Jason D. Lee 2 Yuandong Tian 3 Barnab´as P´oczos 1 Aarti Singh 1  Abstract  We consider the problem of learning a one-hidden- layer neural network with non-overlapping con- i.e., volutional  layer and ReLU activation,  f (Z, w, a) =Pj aj(wT Zj), in which both the  convolutional weights w and the output weights a are parameters to be learned. When the labels are the outputs from a teacher networ
Investigating Human Priors for Playing Video Games  Rachit Dubey 1 Pulkit Agrawal 1 Deepak Pathak 1 Thomas L. Grifﬁths 1 Alexei A. Efros 1  Abstract  What makes humans so good at solving seem- ingly complex video games? Unlike computers, humans bring in a great deal of prior knowledge about the world, enabling efﬁcient decision mak- ing. This paper investigates the role of human priors for solving video games. Given a sample game, we conduct a series of ablation studies to quantify the importanc
A Distributed Second-Order Algorithm You Can Trust  Celestine Dünner 1 Aurelien Lucchi 2 Matilde Gargiani 3 An Bian 2 Thomas Hofmann 2 Martin Jaggi 4  Abstract  Due to the rapid growth of data and computational resources, distributed optimization has become an active research area in recent years. While ﬁrst- order methods seem to dominate the ﬁeld, second- order methods are nevertheless attractive as they potentially require fewer communication rounds to converge. However, there are signiﬁcant 
Computational Optimal Transport: Complexity by Accelerated Gradient  Descent Is Better Than by Sinkhorn’s Algorithm  Pavel Dvurechensky 1 Alexander Gasnikov 2 3 4 Alexey Kroshnin 2 3 4  Abstract  ε2  (cid:16) n2 (cid:17) (cid:110) n9/4  the complexity bound (cid:101)O (cid:16) complexity bound (cid:101)O by (cid:101)O  We analyze two algorithms for approximating the general optimal transport (OT) distance between two discrete distributions of size n, up to accu- racy ε. For the ﬁrst algorithm, w
Entropy-SGD optimizes the prior of a PAC-Bayes bound:  Generalization properties of Entropy-SGD and data-dependent priors  Gintare Karolina Dziugaite 1 2 Daniel M. Roy 3 2  Abstract  We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning algorithm, optimizes a PAC-Bayes bound on the risk of a Gibbs (posterior) classiﬁer, i.e., a random- ized classiﬁer obtained by a risk-sensitive per- turbation of the weights of a learned classiﬁer. Entropy-SGD works by optimizing the bound
Beyond the One-Step Greedy Approach in Reinforcement Learning  Yonathan Efroni 1 Gal Dalal 1 Bruno Scherrer 2 Shie Mannor 1  Abstract  The famous Policy Iteration algorithm alternates between policy improvement and policy evalu- ation. Implementations of this algorithm with several variants of the latter evaluation stage, e.g, n-step and trace-based returns, have been ana- lyzed in previous works. However, the case of multiple-step lookahead policy improvement, de- spite the recent increase in e
Parallel and Streaming Algorithms for K-Core Decomposition  Hossein Esfandiari 1 Silvio Lattanzi 1 Vahab Mirrokni 1  Abstract  The k-core decomposition is a fundamental primi- tive in many machine learning and data mining ap- plications. We present the ﬁrst distributed and the ﬁrst streaming algorithms to compute and main- tain an approximate k-core decomposition with provable guarantees. Our algorithms achieve rig- orous bounds on space complexity while bound- ing the number of passes or number
IMPALA: Scalable Distributed Deep-RL with Importance Weighted  Actor-Learner Architectures  Lasse Espeholt * 1 Hubert Soyer * 1 Remi Munos * 1 Karen Simonyan 1 Volodymyr Mnih 1 Tom Ward 1  Yotam Doron 1 Vlad Firoiu 1 Tim Harley 1 Iain Dunning 1 Shane Legg 1 Koray Kavukcuoglu 1  Abstract  In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and ex- tended trai
Scalable Gaussian Processes with Grid-Structured Eigenfunctions (GP-GRIEF)  Trefor W. Evans 1 Prasanth B. Nair 1  Abstract  We introduce a kernel approximation strategy that enables computation of the Gaussian process log marginal likelihood and all hyperparameter deriva- tives in Oppq time. Our GRIEF kernel consists of p eigenfunctions found using a Nyström ap- proximation from a dense Cartesian product grid of inducing points. By exploiting algebraic prop- erties of Kronecker and Khatri-Rao te
The Limits of Maxing, Ranking, and Preference Learning  Moein Falahatgar 1 Ayush Jain 1 Alon Orlitsky 1 Venkatadheeraj Pichapati 1 Vaishakh Ravindrakumar 1  Abstract  We present a comprehensive understanding of three important problems in PAC preference learning: maximum selection (maxing), ranking, and estimating all pairwise preference probabil- ities, in the adaptive setting. With just Weak Stochastic Transitivity, we show that maxing re- quires Ω(n2) comparisons and with slightly more restri
BOHB: Robust and Efﬁcient Hyperparameter Optimization at Scale  Stefan Falkner 1 Aaron Klein 1 Frank Hutter 1  Abstract  Modern deep learning methods are very sensitive to many hyperparameters, and, due to the long training times of state-of-the-art models, vanilla Bayesian hyperparameter optimization is typi- cally computationally infeasible. On the other hand, bandit-based conﬁguration evaluation ap- proaches based on random search lack guidance and do not converge to the best conﬁgurations as
More Robust Doubly Robust Off-policy Evaluation  Mehrdad Farajtabar * 1 Yinlam Chow * 2 Mohammad Ghavamzadeh 2  Abstract  We study the problem of off-policy evaluation (OPE) in reinforcement learning (RL), where the goal is to estimate the performance of a policy from the data generated by another policy(ies). In particular, we focus on the doubly robust (DR) estimators that consist of an importance sampling (IS) component and a performance model, and utilize the low (or zero) bias of IS and low
Efﬁcient and Consistent Adversarial Bipartite Matching  Rizal Fathony * 1 Sima Behpour * 1 Xinhua Zhang 1 Brian D. Ziebart 1  Abstract  to  rank  learning  including  Many important structured prediction prob- lems, items, correspondence-based natural language process- ing, and multi-object tracking, can be formulated as weighted bipartite matching optimizations. Existing structured prediction approaches have signiﬁcant drawbacks when applied under the constraints of perfect bipartite matchings.
Global Convergence of Policy Gradient Methods  for the Linear Quadratic Regulator  Maryam Fazel * 1 Rong Ge * 2 Sham M. Kakade * 1 Mehran Mesbahi * 1  Abstract  Direct policy gradient methods for reinforcement learning and continuous control problems are a popular approach for a variety of reasons: 1) they are easy to implement without explicit knowledge of the underlying model, 2) they are an “end- to-end” approach, directly optimizing the perfor- mance metric of interest, 3) they inherently al
CRVI: Convex Relaxation for Variational Inference  Ghazal Fazelnia 1 John Paisley 1  Abstract  We present a new technique for solving non- convex variational inference optimization prob- lems. Variational inference is a widely used method for posterior approximation in which the inference problem is transformed into an optimiza- tion problem. For most models, this optimization is highly non-convex and so hard to solve. In this paper, we introduce a new approach to solv- ing the variational infer
Fourier Policy Gradients  Matthew Fellows * 1 Kamil Ciosek * 1 Shimon Whiteson 1  Abstract  We propose a new way of deriving policy gradi- ent updates for reinforcement learning. Our tech- nique, based on Fourier analysis, recasts integrals that arise with expected policy gradients as convo- lutions and turns them into multiplications. The obtained analytical solutions allow us to capture the low variance beneﬁts of EPG in a broad range of settings. For the critic, we treat trigonometric and rad
Nonparametric variable importance  using an augmented neural network with multi-task learning  Jean Feng * 1 Brian D. Williamson * 1 Marco Carone 1 2 Noah Simon 1  Abstract  In predictive modeling applications, it is often of interest to determine the relative contribution of subsets of features in explaining the variability of an outcome. It is useful to consider this variable importance as a function of the unknown, under- lying data-generating mechanism rather than the speciﬁc predictive algo
Closed-form Marginal Likelihood in Gamma-Poisson Matrix Factorization  Louis Filstroff 1 Alberto Lumbreras 1 C´edric F´evotte 1  Abstract  We present novel understandings of the Gamma- Poisson (GaP) model, a probabilistic matrix fac- torization model for count data. We show that GaP can be rewritten free of the score/activation matrix. This gives us new insights about the es- timation of the topic/dictionary matrix by maxi- mum marginal likelihood estimation. In partic- ular, this explains the r
Automatic Goal Generation for Reinforcement Learning Agents  Carlos Florensa * 1 David Held * 2 Xinyang Geng * 1 Pieter Abbeel 1 3  Abstract  Reinforcement learning (RL) is a powerful tech- nique to train an agent to perform a task; however, an agent that is trained using RL is only capable of achieving the single task that is speciﬁed via its reward function. Such an approach does not scale well to settings in which an agent needs to per- form a diverse set of tasks, such as navigating to varyi
DiCE: The Inﬁnitely Differentiable Monte Carlo Estimator  Jakob Foerster 1 Gregory Farquhar * 1 Maruan Al-Shedivat * 2  Tim Rockt¨aschel 1 Eric P. Xing 2 Shimon Whiteson 1  Abstract  The score function estimator is widely used for estimating gradients of stochastic objectives in stochastic computation graphs (SCG), e.g., in re- inforcement learning and meta-learning. While deriving the ﬁrst order gradient estimators by dif- ferentiating a surrogate loss (SL) objective is computationally and conc
Practical Contextual Bandits with Regression Oracles  Dylan J. Foster 1 Alekh Agarwal 2 Miroslav Dud´ık 2 Haipeng Luo 3 Robert E. Schapire 2  Abstract  A major challenge in contextual bandits is to design general-purpose algorithms that are both practically useful and theoretically well-founded. We present a new technique that has the empiri- cal and computational advantages of realizability- based approaches combined with the ﬂexibility of agnostic methods. Our algorithms leverage the availabil
Generative Temporal Models with Spatial Memory  for Partially Observed Environments  Marco Fraccaro 1 * Danilo Jimenez Rezende 2 Yori Zwols 2 Alexander Pritzel 2 S. M. Ali Eslami 2 Fabio Viola 2  Abstract  In model-based reinforcement learning, genera- tive and temporal models of environments can be leveraged to boost agent performance, either by tuning the agent’s representations during train- ing or via use as part of an explicit planning mechanism. However, their application in prac- tice has
ADMM and Accelerated ADMM as Continuous Dynamical Systems  Guilherme Franc¸a 1 Daniel P. Robinson 1 Ren´e Vidal 1  Abstract  1.1. Related work  Recently, there has been an increasing interest in using tools from dynamical systems to analyze the behavior of simple optimization algorithms such as gradient descent and accelerated variants. This paper strengthens such connections by deriving the differential equations that model the contin- uous limit of the sequence of iterates generated by the alt
Bilevel Programming for Hyperparameter Optimization and Meta-Learning  Luca Franceschi 1 2 Paolo Frasconi 3 Saverio Salzo 1 Riccardo Grazzi 1 Massimiliano Pontil 1 2  Abstract  We introduce a framework based on bilevel pro- gramming that uniﬁes gradient-based hyperparam- eter optimization and meta-learning. We show that an approximate version of the bilevel prob- lem can be solved by taking into explicit account the optimization dynamics for the inner objective. Depending on the speciﬁc setting,
Efﬁcient Bias-Span-Constrained Exploration-Exploitation  in Reinforcement Learning  Ronan Fruit * 1 Matteo Pirotta * 1 Alessandro Lazaric 2 Ronald Ortner 3  Abstract  We introduce SCAL, an algorithm designed to per- form efﬁcient exploration-exploitation in any un- known weakly-communicating Markov decision process (MDP) for which an upper bound c on the span of the optimal bias function is known. For an MDP with S states, A actions and Γ ≤ S possible next states, we prove a regret bound  of (ci
Clipped Action Policy Gradient  Yasuhiro Fujita 1 Shin-ichi Maeda 1  Abstract  Many continuous control tasks have bounded ac- tion spaces. When policy gradient methods are applied to such tasks, out-of-bound actions need to be clipped before execution, while policies are usually optimized as if the actions are not clipped. We propose a policy gradient estimator that ex- ploits the knowledge of actions being clipped to reduce the variance in estimation. We prove that our estimator, named clipped 
Born-Again Neural Networks  Tommaso Furlanello 1 Zachary C. Lipton 2 3 Michael Tschannen 4 Laurent Itti 1 Anima Anandkumar 5 3  Abstract  Knowledge Distillation (KD) consists of trans- ferring “knowledge” from one machine learn- ing model (the teacher) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to beneﬁt from the student’s com- pactness, without sacriﬁcing too much p
The Generalization Error of Dictionary Learning with Moreau Envelopes  Alexandros Georgogiannis 1  Abstract  This is a theoretical study on the sample com- plexity of dictionary learning with general type of reconstruction losses. The goal is to estimate a m × d matrix D of unit-norm columns when the only available information is a set of train- ing samples. Points x in Rm are subsequently approximated by the linear combination Da after solving the problem mina∈Rd Φ(x − Da) + g(a) with function 
Local Private Hypothesis Testing: Chi-Square Tests  Marco Gaboardi 1 Ryan Rogers 2  Abstract  The local model for differential privacy is emerg- ing as the reference model for practical applica- tions of collecting and sharing sensitive informa- tion while satisfying strong privacy guarantees. In the local model, there is no trusted entity which is allowed to have each individual’s raw data as is assumed in the traditional curator model. Indi- viduals’ data are usually perturbed before sharing t
Inductive Two-layer Modeling with Parametric Bregman Transfer  Vignesh Ganapathiraman 1 Zhan Shi 1 Xinhua Zhang 1 Yaoliang Yu 2  Abstract  Latent prediction models, exempliﬁed by multi- layer networks, employ hidden variables that au- tomate abstract feature discovery. They typically pose nonconvex optimization problems and ef- fective semi-deﬁnite programming (SDP) relax- ations have been developed to enable global solu- tions (Aslan et al., 2014). However, these models rely on nonparametric tr
Hyperbolic Entailment Cones for Learning Hierarchical Embeddings  Octavian-Eugen Ganea 1 Gary B´ecigneul 1 Thomas Hofmann 1  Abstract representations  via  graph  Learning low- dimensional embeddings that preserve relevant network properties is an important class of problems in machine learning. We here present a novel method to embed directed acyclic graphs. Following prior work, we ﬁrst advocate for using hyperbolic spaces which provably model tree-like structures better than Euclidean geometr
Parameterized Algorithms for the Matrix Completion Problem  Robert Ganian 1 Iyad Kanj 2 Sebastian Ordyniak 3 Stefan Szeider 1  Abstract  We consider two matrix completion problems, in which we are given a matrix with missing entries and the task is to complete the matrix in a way that (1) minimizes the rank, or (2) minimizes the number of distinct rows. We study the param- eterized complexity of the two aforementioned problems with respect to several parameters of in- terest, including the minim
Synthesizing Programs for Images using Reinforced Adversarial Learning  Yaroslav Ganin 1 Tejas Kulkarni 2 Igor Babuschkin 2 S. M. Ali Eslami 2 Oriol Vinyals 2  Abstract  Advances in deep generative networks have led to impressive results in recent years. Neverthe- less, such models can often waste their capacity on the minutiae of datasets, presumably due to weak inductive biases in their decoders. This is where graphics engines may come in handy since they abstract away low-level details and re
Spotlight: Optimizing Device Placement for Training Deep Neural Networks  Yuanxiang Gao 1 2 Li Chen 1 Baochun Li 1  Abstract  Training deep neural networks (DNNs) requires an increasing amount of computation resources, and it becomes typical to use a mixture of GPU and CPU devices. Due to the heterogeneity of these devices, a recent challenge is how each oper- ation in a neural network can be optimally placed on these devices, so that the training process can take the shortest amount of time pos
Parallel Bayesian Network Structure Learning  Tian Gao* 1 Dennis Wei* 1  Abstract  Recent advances in Bayesian Network (BN) struc- ture learning have focused on local-to-global learning, where the graph structure is learned via one local subgraph at a time. As a natural progres- sion, we investigate parallel learning of BN struc- tures via multiple learning agents simultaneously, where each agent learns one local subgraph at a time. We ﬁnd that parallel learning can reduce the number of subgraph
Structured Output Learning with Abstention: Application to Accurate Opinion  Prediction  Alexandre Garcia 1 Slim Essid 1 Chloé Clavel 1 Florence d’Alché-Buc 1  Abstract  Motivated by Supervised Opinion Analysis, we propose a novel framework devoted to Structured Output Learning with Abstention (SOLA). The structure prediction model is able to abstain from predicting some labels in the structured output at a cost chosen by the user in a ﬂexible way. For that purpose, we decompose the problem into
Conditional Neural Processes  Marta Garnelo 1 Dan Rosenbaum 1 Chris J. Maddison 1 Tiago Ramalho 1 David Saxton 1 Murray Shanahan 1 2  Yee Whye Teh 1 Danilo J. Rezende 1 S. M. Ali Eslami 1  Abstract  Deep neural networks excel at function approxi- mation, yet they are typically trained from scratch for each new function. On the other hand, Bayesian methods, such as Gaussian Processes (GPs), exploit prior knowledge to quickly infer the shape of a new function at test time. Yet GPs are computationa
Temporal Poisson Square Root Graphical Models  Sinong Geng* 1 Zhaobin Kuang* 1 Peggy Peissig 2 David Page 1  Abstract  We propose temporal Poisson square root graphi- cal models (TPSQRs), a generalization of Poisson square root graphical models (PSQRs) speciﬁcally designed for modeling longitudinal event data. By estimating the temporal relationships for all possi- ble pairs of event types, TPSQRs can offer a holis- tic perspective about whether the occurrences of any given event type could exci
Budgeted Experiment Design for Causal Structure Learning  AmirEmad Ghassami 1 Saber Salehkaleybar 2 Negar Kiyavash 1 Elias Bareinboim 3  Abstract  We study the problem of causal structure learning when the experimenter is limited to perform at most k non-adaptive experiments of size 1. We formulate the problem of ﬁnding the best interven- tion target set as an optimization problem, which aims to maximize the average number of edges whose directions are resolved. We prove that the corresponding o
Linear Spectral Estimators and an Application to Phase Retrieval  Ramina Ghods 1 Andrew S. Lan 2 Tom Goldstein 3 Christoph Studer 1  Abstract  Phase retrieval refers to the problem of recovering real- or complex-valued vectors from magnitude measurements. The best-known algorithms for this problem are iterative in nature and rely on so-called spectral initializers that provide accu- rate initialization vectors. We propose a novel class of estimators suitable for general nonlinear measurement sys
Structured Variational Learning of Bayesian Neural Networks with Horseshoe  Priors  Soumya Ghosh 1 2 Jiayu Yao 3 Finale Doshi-Velez 3  Abstract  Bayesian Neural Networks (BNNs) have recently received increasing attention for their ability to provide well-calibrated posterior uncertainties. However, model selection—even choosing the number of nodes—remains an open question. Re- cent work has proposed the use of a horseshoe prior over node pre-activations of a Bayesian neu- ral network, which effe
Learning Maximum-A-Posteriori Perturbation Models for Structured  Prediction in Polynomial Time  Asish Ghoshal 1 Jean Honorio 1  Abstract  MAP perturbation models have emerged as a pow- erful framework for inference in structured pre- diction. Such models provide a way to efﬁciently sample from the Gibbs distribution and facilitate predictions that are robust to random noise. In this paper, we propose a provably polynomial time randomized algorithm for learning the parameters of perturbed MAP pr
Robust and Scalable Models of Microbiome Dynamics  Travis E. Gibson 1 Georg K. Gerber 1  Abstract  Microbes are everywhere, including in and on our bodies, and have been shown to play key roles in a variety of prevalent human diseases. Con- sequently, there has been intense interest in the design of bacteriotherapies or “bugs as drugs,” which are communities of bacteria administered to patients for speciﬁc therapeutic applications. Central to the design of such therapeutics is an understanding o
Non-Linear Motor Control by Local Learning in Spiking Neural Networks  Aditya Gilra 1 2 Wulfram Gerstner 1  Abstract  Learning weights in a spiking neural network with hidden neurons, using local, stable and online rules, to control non-linear body dynamics is an open problem. Here, we employ a supervised scheme, Feedback-based Online Local Learning Of Weights (FOLLOW), to train a heterogeneous network of spiking neurons with hidden layers, to control a two-link arm so as to reproduce a desired 
Learning One Convolutional Layer with Overlapping Patches  Surbhi Goel 1 Adam Klivans 1 Raghu Meka 2  Abstract  We give the ﬁrst provably efﬁcient algorithm for learning a one hidden layer convolutional network with respect to a general class of (potentially over- lapping) patches under mild conditions on the underlying distribution. We prove that our frame- work captures commonly used schemes from com- puter vision, including one-dimensional and two- dimensional “patch and stride” convolutions.
Visualizing and Understanding Atari Agents  Sam Greydanus 1 Anurag Koul 1 Jonathan Dodge 1 Alan Fern 1  Abstract  While deep reinforcement learning (deep RL) agents are effective at maximizing rewards, it is often unclear what strategies they use to do so. In this paper, we take a step toward explaining deep RL agents through a case study using Atari 2600 environments. In particular, we focus on using saliency maps to understand how an agent learns and executes a policy. We introduce a method fo
Learning Policy Representations in Multiagent Systems  Aditya Grover 1 Maruan Al-Shedivat 2 Jayesh K. Gupta 1 Yura Burda 3 Harrison Edwards 3  Abstract  Modeling agent behavior is central to understand- ing the emergence of complex phenomena in mul- tiagent systems. Prior work in agent modeling has largely been task-speciﬁc and driven by hand- engineering domain-speciﬁc prior knowledge. We propose a general learning framework for model- ing agent behavior in any multiagent system using only a ha
Faster Derivative-Free Stochastic Algorithm  for Shared Memory Machines  Bin Gu 1 Zhouyuan Huo 1 Cheng Deng 2 Heng Huang 1 2  Abstract  Asynchronous parallel stochastic gradient opti- mization has been playing a pivotal role to solve large-scale machine learning problems in big data applications. Zeroth-order (derivative-free) meth- ods estimate the gradient only by two function evaluations, thus have been applied to solve the problems where the explicit gradient calculations are computationally
Learning to Search with MCTSnets  Arthur Guez * 1 Th´eophane Weber * 1 Ioannis Antonoglou 1 Karen Simonyan 1  Oriol Vinyals 1 Daan Wierstra 1 R´emi Munos 1 David Silver 1  Abstract  Planning problems are among the most impor- tant and well-studied problems in artiﬁcial intel- ligence. They are most typically solved by tree search algorithms that simulate ahead into the fu- ture, evaluate future states, and back-up those eval- uations to the root of a search tree. Among these algorithms, Monte-Ca
Characterizing Implicit Bias in Terms of Optimization Geometry  Suriya Gunasekar 1 Jason Lee 2 Daniel Soudry 3 Nathan Srebro 1  Abstract  We study the implicit bias of generic optimiza- tion methods—mirror descent, natural gradient descent, and steepest descent with respect to different potentials and norms—when optimiz- ing underdetermined linear regression or separa- ble linear classiﬁcation problems. We explore the question of whether the speciﬁc global mini- mum (among the many possible glob
Shampoo: Preconditioned Stochastic Tensor Optimization  Vineet Gupta 1 Tomer Koren 1 Yoram Singer 2 1  Abstract  Preconditioned gradient methods are among the most general and powerful tools in optimization. However, preconditioning requires storing and manipulating prohibitively large matrices. We de- scribe and analyze a new structure-aware precondi- tioning algorithm, called Shampoo, for stochastic optimization over tensor spaces. Shampoo main- tains a set of preconditioning matrices, each of
Latent Space Policies for Hierarchical Reinforcement Learning  Tuomas Haarnoja * 1 Kristian Hartikainen * 2 Pieter Abbeel 1 Sergey Levine 1  Abstract  We address the problem of learning hierarchical deep neural network policies for reinforcement learning. In contrast to methods that explicitly re- strict or cripple lower layers of a hierarchy to force them to use higher-level modulating signals, each layer in our framework is trained to directly solve the task, but acquires a range of diverse st
Soft Actor-Critic:  Off-Policy Maximum Entropy Deep Reinforcement  Learning with a Stochastic Actor  Tuomas Haarnoja 1 Aurick Zhou 1 Pieter Abbeel 1 Sergey Levine 1  Abstract  Model-free deep reinforcement learning (RL) al- gorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessi- tate meticulous hyperparameter tuning
Comparison-Based Random Forests  Siavash Haghiri 1 Damien Garreau 2 Ulrike von Luxburg 1 2  Abstract  Assume we are given a set of items from a gen- eral metric space, but we neither have access to the representation of the data nor to the distances between data points. Instead, suppose that we can actively choose a triplet of items (A, B, C) and ask an oracle whether item A is closer to item B or to item C. In this paper, we propose a novel random forest algorithm for regression and classiﬁcati
K-Beam Minimax: Efﬁcient Optimization for Deep Adversarial Learning  Jihun Hamm 1 Yung-Kyun Noh 2  Abstract  Minimax optimization plays a key role in ad- versarial training of machine learning algo- rithms, such as learning generative models, do- main adaptation, privacy preservation, and robust learning. In this paper, we demonstrate the fail- ure of alternating gradient descent in minimax optimization problems due to the discontinuity of solutions of the inner maximization. To address this, we
Candidates vs. Noises Estimation for Large Multi-Class Classiﬁcation Problem  Lei Han 1 Yiheng Huang 1 Tong Zhang 1  Abstract  This paper proposes a method for multi-class clas- siﬁcation problems, where the number of classes K is large. The method, referred to as Candidates vs. Noises Estimation (CANE), selects a small subset of candidate classes and samples the re- maining classes. We show that CANE is always consistent and computationally efﬁcient. More- over, the resulting estimator has low 
Stein Variational Gradient Descent Without Gradient  Jun Han 1 Qiang Liu 2  Abstract  Stein variational gradient decent (SVGD) has been shown to be a powerful approximate infer- ence algorithm for complex distributions. How- ever, the standard SVGD requires calculating the gradient of the target density and cannot be ap- plied when the gradient is unavailable. In this work, we develop a gradient-free variant of SVGD (GF-SVGD), which replaces the true gradient with a surrogate gradient, and corre
Deep Models of Interactions Across Sets  Jason Hartford * 1 Devon R Graham * 1 Kevin Leyton-Brown 1 Siamak Ravanbakhsh 1  Abstract  We use deep learning to model interactions across two or more sets of objects, such as user–movie ratings, protein–drug bindings, or ternary user- item-tag interactions. The canonical representa- tion of such interactions is a matrix (or a higher- dimensional tensor) with an exchangeability prop- erty: the encoding’s meaning is not changed by permuting rows or colum
Learning Memory Access Patterns  Milad Hashemi 1 Kevin Swersky 1 Jamie A. Smith 1 Grant Ayers 2 * Heiner Litz 3 * Jichuan Chang 1  Christos Kozyrakis 2 Parthasarathy Ranganathan 1  Abstract  The explosion in workload complexity and the recent slow-down in Moore’s law scaling call for new approaches towards efﬁcient computing. Researchers are now beginning to use recent ad- vances in machine learning in software optimiza- tions, augmenting or replacing traditional heuris- tics and data structures
Fairness Without Demographics in Repeated Loss Minimization  Tatsunori B. Hashimoto 1 2 Megha Srivastava 1 Hongseok Namkoong 3 Percy Liang 1  Abstract  Machine learning models (e.g., speech recog- nizers) are usually trained to minimize average loss, which results in representation disparity— minority groups (e.g., non-native speakers) con- tribute less to the training objective and thus tend to suffer higher loss. Worse, as model accuracy affects user retention, a minority group can shrink over
Multicalibration: Calibration for the (Computationally-Identiﬁable) Masses  ´Ursula H´ebert-Johnson 1 Michael P. Kim 1 Omer Reingold 1 Guy N. Rothblum 2  Abstract  We develop and study multicalibration as a new measure of fairness in machine learning that aims to mitigate inadvertent or malicious discrimina- tion that is introduced at training time (even from ground truth data). Multicalibration guarantees meaningful (calibrated) predictions for every sub- population that can be identiﬁed within
Recurrent Predictive State Policy Networks  Ahmed Hefny * 1 Zita Marinho * 2 3 Wen Sun 2 Siddhartha S. Srinivasa 4 Geoffrey Gordon 1  Abstract  We introduce Recurrent Predictive State Policy (RPSP) networks, a recurrent architecture that brings insights from predictive state representa- tions to reinforcement learning in partially ob- servable environments. Predictive state policy networks consist of a recursive ﬁlter, which keeps track of a belief about the state of the environment, and a react
Learning unknown ODE models with Gaussian processes  Markus Heinonen * 1 2 C¸ agatay Yıldız * 1 Henrik Mannerstr¨om 1 Jukka Intosalmi 1 Harri L¨ahdesm¨aki 1  Abstract  variate ordinary differential functions:  In conventional ODE modelling coefﬁcients of an equation driving the system state forward in time are estimated. However, for many complex sys- tems it is practically impossible to determine the equations or interactions governing the underly- ing dynamics. In these settings, parametric OD
Orthogonal Recurrent Neural Networks with Scaled Cayley Transform  Kyle E. Helfrich * 1 Devin Willmott * 1 Qiang Ye 1  Abstract  Recurrent Neural Networks (RNNs) are designed to handle sequential data but suffer from vanish- ing or exploding gradients. Recent work on Uni- tary Recurrent Neural Networks (uRNNs) have been used to address this issue and in some cases, exceed the capabilities of Long Short-Term Mem- ory networks (LSTMs). We propose a simpler and novel update scheme to maintain ortho
Fast Bellman Updates for Robust MDPs  Chin Pang Ho 1 * Marek Petrik 2 * Wolfram Wiesemann 1 *  Abstract  We describe two efﬁcient, and exact, algorithms for computing Bellman updates in robust Markov decision processes (MDPs). The ﬁrst algorithm uses a homotopy continuation method to compute updates for L1-constrained s, a-rectangular ambi- guity sets. It runs in quasi-linear time for plain L1 norms and also generalizes to weighted L1 norms. The second algorithm uses bisection to compute updates
CyCADA: Cycle-Consistent Adversarial Domain Adaptation  Judy Hoffman 1 Eric Tzeng 1 Taesung Park 1 Jun-Yan Zhu 1 Phillip Isola 1 2 Kate Saenko 3 Alexei A. Efros 1  Trevor Darrell 1  Abstract  Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models have shown tremendous progress towards adapting to new environments by focusing either on discovering domain invariant representations or by mapping between unpaired image domains. While feature space method
Sound Abstraction and Decomposition of Probabilistic Programs  Steven Holtzen 1 Guy Van den Broeck 1 Todd Millstein 1  Abstract  Probabilistic programming languages are a ﬂexi- ble tool for specifying statistical models, but this ﬂexibility comes at the expense of efﬁcient analy- sis. It is currently difﬁcult to compactly represent the subtle independence properties of a probabilis- tic program and to exploit independence proper- ties to decompose inference. Classical graphical model abstraction
Gradient Primal-Dual Algorithm Converges to Second-Order Stationary  Solution for Nonconvex Distributed Optimization Over Networks  Abstract  Mingyi Hong 1 Jason D. Lee 2 Meisam Razaviyayn 3  the following problem  N(cid:88)  In this work, we study two ﬁrst-order primal-dual based algorithms, the Gradient Primal-Dual Al- gorithm (GPDA) and the Gradient Alternating Direction Method of Multipliers (GADMM), for solving a class of linearly constrained non-convex optimization problems. We show that w
Variational Bayesian dropout: pitfalls and ﬁxes  Jiri Hron 1 Alexander G. de G. Matthews 1 Zoubin Ghahramani 1 2  Abstract  Dropout, a stochastic regularisation technique for training of neural networks, has recently been reinterpreted as a speciﬁc type of approximate inference algorithm for Bayesian neural networks. The main contribution of the reinterpretation is in providing a theoretical framework useful for analysing and extending the algorithm. We show that the proposed framework suffers f
Does Distributionally Robust Supervised Learning Give Robust Classiﬁers?  Weihua Hu 1 2 Gang Niu 2 Issei Sato 1 2 Masashi Sugiyama 2 1  Abstract  Distributionally Robust Supervised Learning (DRSL) is necessary for building reliable ma- chine learning systems. When machine learn- ing is deployed in the real world, its performance can be signiﬁcantly degraded because test data may follow a different distribution from training data. DRSL with f-divergences explicitly con- siders the worst-case dist
Dissipativity Theory for Accelerating Stochastic Variance Reduction:  A Uniﬁed Analysis of SVRG and Katyusha Using Semideﬁnite Programs  Bin Hu 1 Stephen Wright 1 Laurent Lessard 1  Abstract  Techniques for reducing the variance of gradient estimates used in stochastic programming algo- rithms for convex ﬁnite-sum problems have re- ceived a great deal of attention in recent years. By leveraging dissipativity theory from control, we provide a new perspective on two important variance-reduction al
Near Optimal Frequent Directions for Sketching Dense and Sparse Matrices  Zengfeng Huang 1  Abstract  Given a large matrix A ∈ Rn×d, we consider the problem of computing a sketch matrix B ∈ R(cid:96)×d which is signiﬁcantly smaller than but still well approximates A. We are interested in minimizing the covariance error (cid:107)AT A − BT B(cid:107)2. We con- sider the problems in the streaming model, where the algorithm can only make one pass over the input with limited working space. The popula
Learning Deep ResNet Blocks Sequentially using Boosting Theory  Furong Huang 1 Jordan T. Ash 2 John Langford 3 Robert E. Schapire 3  Abstract  We prove a multi-channel telescoping sum boost- ing theory for the ResNet architectures which si- multaneously creates a new technique for boost- ing over features (in contrast to labels) and pro- vides a new algorithm for ResNet-style architec- tures. Our proposed training algorithm, BoostRes- Net, is particularly suitable in non-differentiable architect
Learning Hidden Markov Models from Pairwise Co-occurrences  with Application to Topic Modeling  Kejun Huang 1 Xiao Fu 2 Nicholas D. Sidiropoulos 3  Abstract  We present a new algorithm for identifying the transition and emission probabilities of a hid- den Markov model (HMM) from the emitted data. Expectation-maximization becomes compu- tationally prohibitive for long observation records, which are often required for identiﬁcation. The new algorithm is particularly suitable for cases where the a
NeuralAutoregressiveFlowsChin-WeiHuang12*DavidKrueger12*AlexandreLacoste2AaronCourville13AbstractNormalizingﬂowsandautoregressivemodelshavebeensuccessfullycombinedtoproducestate-of-the-artresultsindensityestimation,viaMaskedAutoregressiveFlows(MAF)(Papa-makariosetal.,2017),andtoacceleratestate-of-the-artWaveNet-basedspeechsynthesisto20xfasterthanreal-time(Oordetal.,2017),viaInverseAutoregressiveFlows(IAF)(Kingmaetal.,2016).Weunifyandgeneralizetheseap-proaches,replacingthe(conditionally)afﬁneuni-
Topological Mixture Estimation  Steve Huntsman 1  Abstract  We introduce topological mixture estimation, a completely nonparametric and computationally efﬁcient solution to the problem of estimating a one-dimensional mixture with generic unimodal components. We repeatedly perturb the unimodal decomposition of Baryshnikov and Ghrist to pro- duce a topologically and information-theoretically optimal unimodal mixture. We also detail a smoothing process that optimally exploits topo- logical persiste
Decoupled Parallel Backpropagation with Convergence Guarantee  Zhouyuan Huo 1 Bin Gu 1 Qian Yang 1 Heng Huang 1  Abstract  Backpropagation algorithm is indispensable for the training of feedforward neural networks. It requires propagating error gradients sequentially from the output layer all the way back to the input layer. The backward locking in backpropagation algorithm constrains us from updating network layers in parallel and fully leveraging the comput- ing resources. Recently, several al
Using Reward Machines for High-Level Task Speciﬁcation  and Decomposition in Reinforcement Learning  Rodrigo Toro Icarte 1 2 Toryn Q. Klassen 1 Richard Valenzano 3 Sheila A. McIlraith 1 2  Abstract  In this paper we propose Reward Machines – a type of ﬁnite state machine that supports the spec- iﬁcation of reward functions while exposing re- ward function structure to the learner and support- ing decomposition. We then present Q-Learning for Reward Machines (QRM), an algorithm which appropriatel
Deep Variational Reinforcement Learning for POMDPs  Maximilian Igl 1 Luisa Zintgraf 1 Tuan Anh Le 1 Frank Wood 2 Shimon Whiteson 1  Abstract  Many real-world sequential decision making prob- lems are partially observable by nature, and the environment model is typically unknown. Con- sequently, there is great need for reinforcement learning methods that can tackle such problems given only a stream of rewards and incomplete and noisy observations. In this paper, we propose deep variational reinfo
Attention-based Deep Multiple Instance Learning  Maximilian Ilse * 1 Jakub M. Tomczak * 1 Max Welling 1  Abstract  Multiple instance learning (MIL) is a variation of supervised learning where a single class la- bel is assigned to a bag of instances. In this pa- per, we state the MIL problem as learning the Bernoulli distribution of the bag label where the bag label probability is fully parameterized by neural networks. Furthermore, we propose a neu- ral network-based permutation-invariant aggre-
Black-box Adversarial Attacks with Limited Queries and Information  Andrew Ilyas * 1 2 Logan Engstrom * 1 2 Anish Athalye * 1 2 Jessy Lin * 1 2  Abstract  the query-limited setting,  Current neural network-based classiﬁers are sus- ceptible to adversarial examples even in the black-box setting, where the attacker only has query access to the model. In practice, the threat model for real-world systems is often more re- strictive than the typical black-box model where the adversary can observe the
Analysis of Minimax Error Rate for Crowdsourcing  and Its Application to Worker Clustering Model  Hideaki Imamura 1 2 Issei Sato 1 2 Masashi Sugiyama 2 1  Abstract  While crowdsourcing has become an important means to label data, there is great interest in estimating the ground truth from unreliable la- bels produced by crowdworkers. The Dawid and Skene (DS) model is one of the most well-known models in the study of crowdsourcing. Despite its practical popularity, theoretical error analysis for 
Improving Regression Performance with Distributional Losses  Ehsan Imani 1 Martha White 1  Abstract  There is growing evidence that converting tar- gets to soft targets in supervised learning can pro- vide considerable gains in performance. Much of this work has considered classiﬁcation, con- verting hard zero-one values to soft labels—such as by adding label noise, incorporating label am- biguity or using distillation. In parallel, there is some evidence from a regression setting in rein- force
Deep Density Destructors  David I. Inouye 1 Pradeep Ravikumar 1  Abstract  We propose a uniﬁed framework for deep density models by formally deﬁning density destructors. A density destructor is an invertible function that transforms a given density to the uniform density—essentially destroying any structure in the original density. This destructive transfor- mation generalizes Gaussianization via ICA and more recent autoregressive models such as MAF and Real NVP. Informally, this transformation 
Unbiased Objective Estimation in Predictive Optimization  Shinji Ito 1 Akihiro Yabe 1 Ryohei Fujimaki 1  Abstract  For data-driven decision-making, one promising approach, called predictive optimization, is to solve maximization problems i n which the objec- tive function to be maximized is estimated from data. Predictive optimization, however, suffers from the problem of a calculated optimal solu- tion’s being evaluated too optimistically, i.e., the value of the objective function is overestima
Anonymous Walk Embeddings  Sergey Ivanov 1 2 Evgeny Burnaev 1  Abstract  The task of representing entire graphs has seen a surge of prominent results, mainly due to learning convolutional neural networks (CNNs) on graph- structured data. While CNNs demonstrate state- of-the-art performance in graph classiﬁcation task, such methods are supervised and therefore steer away from the original problem of network rep- resentation in task-agnostic manner. Here, we coherently propose an approach for embe
Learning Binary Latent Variable Models: A Tensor Eigenpair Approach  Ariel Jaffe 1 Roi Weiss 1 Shai Carmi 2 Yuval Kluger 3 Boaz Nadler 1  Abstract  Latent variable models with hidden binary units appear in various applications. Learning such models, in particular in the presence of noise, is a challenging computational problem. In this paper we propose a novel spectral approach to this prob- lem, based on the eigenvectors of both the second order moment matrix and third order moment ten- sor of 
Firing Bandits: Optimizing Crowdfunding  Lalit Jain 1 Kevin Jamieson 1  Abstract  In this paper, we model the problem of optimizing crowdfunding platforms, such as the non-proﬁt Kiva or for-proﬁt KickStarter, as a variant of the multi-armed bandit problem. In our setting, Bernoulli arms emit no rewards until their cumu- lative number of successes over any number of trials exceeds a ﬁxed threshold and then provides no additional reward for any additional trials - a process reminiscent to that of 
Differentially Private Matrix Completion Revisited  Prateek Jain 1 Om Thakkar 2 Abhradeep Thakurta 3  Abstract  We provide the ﬁrst provably joint differentially private algorithm with formal utility guarantees for the problem of user-level privacy-preserving collaborative ﬁltering. Our algorithm is based on the Frank-Wolfe method, and it consistently es- timates the underlying preference matrix as long as the number of users m is ω(n5/4), where n is the number of items, and each user provides n
Video Prediction with Appearance and Motion Conditions  Yunseok Jang 1 2 Gunhee Kim 2 Yale Song 3  Abstract  Video prediction aims to generate realistic future frames by learning dynamic visual patterns. One fundamental challenge is to deal with future un- certainty: How should a model behave when there are multiple correct, equally probable future? We propose an Appearance-Motion Conditional GAN to address this challenge. We provide appearance and motion information as conditions that specify h
Pathwise Derivatives Beyond the Reparameterization Trick  Martin Jankowiak * 1 Fritz Obermeyer * 1  Abstract  We observe that gradients computed via the repa- rameterization trick are in direct correspondence with solutions of the transport equation in the for- malism of optimal transport. We use this perspec- tive to compute (approximate) pathwise gradients for probability distributions not directly amenable to the reparameterization trick: Gamma, Beta, and Dirichlet. We further observe that wh
Detecting non-causal artifacts in multivariate linear regression models  Dominik Janzing 1 Bernhard Sch¨olkopf 2  Abstract  We consider linear models where d potential causes X1, . . . , Xd are correlated with one target quantity Y and propose a method to infer whether the association is causal or whether it is an artifact caused by overﬁtting or hidden common causes. We employ the idea that in the former case the vector of regression coefﬁcients has ‘generic’ ori- entation relative to the covar
A Uniﬁed Framework for Structured Low-rank Matrix Learning  Pratik Jawanpuria 1 Bamdev Mishra 1  Abstract  We consider the problem of learning a low-rank matrix, constrained to lie in a linear subspace, and introduce a novel factorization for modeling such matrices. A salient feature of the proposed factor- ization scheme is it decouples the low-rank and the structural constraints onto separate factors. We formulate the optimization problem on the Riemannian spectrahedron manifold, where the Rie
Efﬁcient end-to-end learning for quantizable representations  Yeonwoo Jeong 1 Hyun Oh Song 1  Abstract  Embedding representation learning via neural net- works is at the core foundation of modern similar- ity based search. While much effort has been put in developing algorithms for learning binary ham- ming code representations for search efﬁciency, this still requires a linear scan of the entire dataset per each query and trades off the search accuracy through binarization. To this end, we cons
Feedback-Based Tree Search for Reinforcement Learning  Daniel R. Jiang 1 Emmanuel Ekwedike 2 3 Han Liu 2 4  Abstract  Inspired by recent successes of Monte-Carlo tree search (MCTS) in a number of artiﬁcial intelli- gence (AI) application domains, we propose a reinforcement learning (RL) technique that itera- tively applies MCTS on batches of small, ﬁnite- horizon versions of the original inﬁnite-horizon Markov decision process. The terminal condition of the ﬁnite-horizon problems, or the leaf-no
Quickshift++: Provably Good Initializations for Sample-Based Mean Shift  Heinrich Jiang 1 Jennifer Jang 2 Samory Kpotufe 3  Abstract  We provide initial seedings to the Quick Shift clus- tering algorithm, which approximate the locally high-density regions of the data. Such seedings act as more stable and expressive cluster-cores than the singleton modes found by Quick Shift. We establish statistical consistency guarantees for this modiﬁcation. We then show strong clustering performance on real d
MentorNet: Learning Data-Driven Curriculum  for Very Deep Neural Networks on Corrupted Labels  Lu Jiang 1 Zhengyuan Zhou 2 Thomas Leung 1 Li-Jia Li 1 Li Fei-Fei 1 2  Abstract  Recent deep networks are capable of memoriz- ing the entire data even when the labels are com- pletely random. To overcome the overﬁtting on corrupted labels, we propose a novel technique of learning another neural network, called Men- torNet, to supervise the training of the base deep networks, namely, StudentNet. During 
The Weighted Kendall and High-order Kernels for Permutations  Yunlong Jiao 1 Jean-Philippe Vert 2  Abstract  We propose new positive deﬁnite kernels for per- mutations. First we introduce a weighted version of the Kendall kernel, which allows to weight un- equally the contributions of different item pairs in the permutations depending on their ranks. Like the Kendall kernel, we show that the weighted version is invariant to relabeling of items and can be computed efﬁciently in O(n ln(n)) operati
Junction Tree Variational Autoencoder for Molecular Graph Generation  Wengong Jin 1 Regina Barzilay 1 Tommi Jaakkola 1  Abstract  We seek to automate the design of molecules based on speciﬁc chemical properties. In com- putational terms, this task involves continuous embedding and generation of molecular graphs. Our primary contribution is the direct realization of molecular graphs, a task previously approached by generating linear SMILES strings instead of graphs. Our junction tree variational 
Network Global Testing by Counting Graphlets  Jiashun Jin 1 Zheng Tracy Ke 2 Shengming Luo 1  Abstract  Consider a large social network with possi- bly severe degree heterogeneity and mixed- memberships. We are interested in testing whether the network has only one community or there are more than one communities. The prob- lem is known to be non-trivial, partially due to the presence of severe degree heterogeneity. We con- struct a class of test statistics using the numbers of short paths and s
Regret Minimization for Partially Observable Deep Reinforcement Learning  Peter Jin 1 Kurt Keutzer 1 Sergey Levine 1  Abstract  Deep reinforcement learning algorithms that esti- mate state and state-action value functions have been shown to be effective in a variety of chal- lenging domains, including learning control strate- gies from raw image pixels. However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial
WSNet: Compact and Efﬁcient Networks Through Weight Sampling  Xiaojie Jin 1 2 Yingzhen Yang 2 Ning Xu 2 Jianchao Yang 3 Nebojsa Jojic 4 Jiashi Feng 1 Shuicheng Yan 5 1  Abstract  We present a new approach and a novel architec- ture, termed WSNet, for learning compact and ef- ﬁcient deep neural networks. Existing approaches conventionally learn full model parameters inde- pendently and then compress them via ad hoc processing such as model pruning or ﬁlter factor- ization. Alternatively, WSNet pr
Large-Scale Cox Process Inference using Variational Fourier Features  ST John 1 James Hensman 1  Abstract  Gaussian process modulated Poisson processes provide a ﬂexible framework for modeling spa- tiotemporal point patterns. So far this was re- stricted to one dimension, binning to a pre- determined grid, or small data sets of up to a few thousand data points. Here we introduce Cox process inference based on Fourier features. This sparse representation induces global rather than local constrain
Composite Functional Gradient Learning of Generative Adversarial Models  Rie Johnson * 1 Tong Zhang * 2  Abstract  This paper ﬁrst presents a theory for generative adversarial methods that does not rely on the tra- ditional minimax formulation. It shows that with a strong discriminator, a good generator can be learned so that the KL divergence between the distributions of real data and generated data im- proves after each functional gradient step until it converges to zero. Based on the theory, 
Kronecker Recurrent Units  Cijo Jose 1 2 Moustapha Ciss´e 3 Franc¸ois Fleuret 1 2  Abstract  Our work addresses two important issues with recurrent neural networks: (1) they are over- parametrized, and (2) the recurrent weight matrix is ill-conditioned. The former increases the sam- ple complexity of learning and the training time. The latter causes the vanishing and exploding gra- dient problem. We present a ﬂexible recurrent neural network model called Kronecker Recurrent Units (KRU). KRU achi
Fast Decoding in Sequence Models Using Discrete Latent Variables  Łukasz Kaiser 1 Aurko Roy 1 Ashish Vaswani 1 Niki Parmar 1 Samy Bengio 1 Jakob Uszkoreit 1  Noam Shazeer 1  Abstract  Autoregressive sequence models based on deep neural networks, such as RNNs, Wavenet and the Transformer attain state-of-the-art results on many tasks. However, they are difﬁcult to parallelize and are thus slow at processing long sequences. RNNs lack parallelism both during training and decoding, while architecture
Kernel Recursive ABC: Point Estimation with Intractable Likelihood  Takafumi Kajihara 1 2 Motonobu Kanagawa 3 Keisuke Yamazaki 2 Kenji Fukumizu 4  Abstract  We propose a novel approach to parameter esti- mation for simulator-based statistical models with intractable likelihood. Our proposed method in- volves recursive application of kernel ABC and kernel herding to the same observed data. We provide a theoretical explanation regarding why the approach works, showing (for the population setting) 
Efﬁcient Neural Audio Synthesis  Nal Kalchbrenner * 1 Erich Elsen * 2 Karen Simonyan 1 Seb Noury 1 Norman Casagrande 1 Edward Lockhart 1  Florian Stimberg 1 A¨aron van den Oord 1 Sander Dieleman 1 Koray Kavukcuoglu 1  Abstract  Sequential models achieve state-of-the-art results in audio, visual and textual domains with respect to both estimating the data distribution and gener- ating high-quality samples. Efﬁcient sampling for this class of models has however remained an elu- sive problem. With 
Learning Diffusion using Hyperparameters  Dimitris Kalimeris 1 Yaron Singer 1 Karthik Subbian 2 Udi Weinsberg 2  Abstract  In this paper we advocate for a hyperparametric approach to learn diffusion in the independent cascade (IC) model. The sample complexity of this model is a function of the number of edges in the network and consequently learning becomes infeasible when the network is large. We study a natural restriction of the hypothesis class using additional information available in order
Signal and Noise Statistics Oblivious Orthogonal Matching Pursuit  Sreejith Kallummil * 1 Sheetal Kalyani * 2  Abstract  Orthogonal matching pursuit (OMP) is a widely used algorithm for recovering sparse high dimen- sional vectors in linear regression models. The optimal performance of OMP requires a priori knowledge of either the sparsity of regression vec- tor or noise statistics. Both these statistics are rarely known a priori and are very difﬁcult to esti- mate. In this paper, we present a n
Residual Unfairness in Fair Machine Learning from Prejudiced Data  Nathan Kallus 1 Angela Zhou 2  Abstract  Recent work in fairness in machine learning has proposed adjusting for fairness by equalizing ac- curacy metrics across groups and has also studied how datasets affected by historical prejudices may lead to unfair decision policies. We connect these lines of work and study the residual unfairness that arises when a fairness-adjusted predictor is not actually fair on the target population d
Learn from Your Neighbor:  Learning Multi-modal Mappings from Sparse Annotations  Ashwin Kalyan 1 Stefan Lee 1 Anitha Kannan 2 Dhruv Batra 1 3  Abstract  Many structured prediction problems (particularly in vision and language domains) are ambiguous, with multiple outputs being ‘correct’ for an in- put – e.g. there are many ways of describing an image, multiple ways of translating a sentence; however, exhaustively annotating the applicabil- ity of all possible outputs is intractable due to expon
Semi-Supervised Learning via Compact Latent Space Clustering  Konstantinos Kamnitsas 1 2 Daniel C. Castro 1 2 Loic Le Folgoc 2 Ian Walker 2 Ryutaro Tanno 1 3  Daniel Rueckert 2 Ben Glocker 2 Antonio Criminisi 1 Aditya Nori 1  Abstract  We present a novel cost function for semi- supervised learning of neural networks that en- courages compact clustering of the latent space to facilitate separation. The key idea is to dynami- cally create a graph over embeddings of labeled and unlabeled samples of
Policy Optimization with Demonstrations  Bingyi Kang 1 Zequn Jie 2 Jiashi Feng 1  Abstract  Exploration remains a signiﬁcant challenge to re- inforcement learning methods, especially in envi- ronments where reward signals are sparse. Recent methods of learning from demonstrations have shown to be promising in overcoming exploration difﬁculties but typically require considerable high- quality demonstrations that are difﬁcult to col- lect. We propose to effectively leverage available demonstration
Improving Sign Random Projections With Additional Information  Keegan Kang * 1 Wong Wei Pin * 1  Abstract  Sign random projections (SRP) is a technique which allows the user to quickly estimate the an- gular similarity and inner products between data. We propose using additional information to im- prove these estimates which is easy to implement and cost efﬁcient. We prove that the variance of our estimator is lower than the variance of SRP. Our proposed method can also be used together with oth
Let’s be Honest: An Optimal No-Regret Framework for Zero-Sum Games  Ehsan Asadi Kangarshahi * 1 Ya-Ping Hsieh * 1 Mehmet Fatih Sahin 1 Volkan Cevher 1  Abstract  We revisit the problem of solving two-player zero- sum games in the decentralized setting. We pro- pose a simple algorithmic framework that simulta- neously achieves the best rates for honest regret as well as adversarial regret, and in addition resolves the open problem of removing the logarithmic terms in convergence to the value of t
Continual Reinforcement Learning with Complex Synapses  Christos Kaplanis 1 2 Murray Shanahan 1 3 Claudia Clopath 2  Abstract  Unlike humans, who are capable of continual learning over their lifetimes, artiﬁcial neural net- works have long been known to suffer from a phenomenon known as catastrophic forgetting, whereby new learning can lead to abrupt erasure of previously acquired knowledge. Whereas in a neural network the parameters are typically mod- elled as scalar values, an individual synap
Riemannian Stochastic Recursive Gradient Algorithm  Hiroyuki Kasai 1 Hiroyuki Sato 2 Bamdev Mishra 3  Abstract  Stochastic variance reduction algorithms have re- cently become popular for minimizing the aver- age of a large, but ﬁnite number of loss func- tions on a Riemannian manifold. The present pa- per proposes a Riemannian stochastic recursive gradient algorithm (R-SRG), which does not re- quire the inverse of retraction between two dis- tant iterates on the manifold. Convergence anal- yses
Not All Samples Are Created Equal:  Deep Learning with Importance Sampling  Angelos Katharopoulos 1 2 Franc¸ois Fleuret 1 2  Abstract  Deep neural network training spends most of the computation on examples that are properly han- dled, and could be ignored. We propose to mit- igate this phenomenon with a principled impor- tance sampling scheme that focuses computation on “informative” examples, and reduces the vari- ance of the stochastic gradients during training. Our contribution is twofold: ﬁ
Feasible Arm Identiﬁcation  Julian Katz-Samuels 1 Clayton Scott 1  Abstract  We introduce the feasible arm identiﬁcation problem, a pure exploration multi-armed bandit problem where the agent is given a set of D- dimensional arms and a polyhedron P “ tx : Ax ď bu Ă RD. Pulling an arm gives a random vector and the goal is to determine, using a ﬁxed budget of T pulls, which of the arms have means belonging to P . We propose three algorithms MD-UCBE, MD-SAR, and MD-APT and provide a uniﬁed analysis
Focused Hierarchical RNNs for Conditional Sequence Processing  Nan Rosemary Ke 1 2 3 Konrad ˙Zołna 4 1 Alessandro Sordoni 3 Zhouhan Lin 1 3 5 Adam Trischler 3  Yoshua Bengio 1 6 7 Joelle Pineau 8 9 7 Laurent Charlin 1 10 Chris Pal 1 2  Abstract  Recurrent Neural Networks (RNNs) with atten- tion mechanisms have obtained state-of-the-art results for many sequence processing tasks. Most of these models use a simple form of encoder with attention that looks over the entire sequence and assigns a wei
Preventing Fairness Gerrymandering:  Auditing and Learning for Subgroup Fairness  Michael Kearns 1 Seth Neel 1 Aaron Roth 1 Zhiwei Steven Wu 2  Abstract  We introduce a new family of fairness deﬁnitions that interpolate between statistical and individ- ual notions of fairness, obtaining some of the best properties of each. We show that checking whether these notions are satisﬁed is computa- tionally hard in the worst case, but give practical oracle-efﬁcient algorithms for learning subject to the
Improved nearest neighbor search using auxiliary information and priority  functions  Omid Keivani 1 Kaushik Sinha 1  Abstract  Nearest neighbor search using random projec- tion trees has recently been shown to achieve su- perior performance, in terms of better accuracy while retrieving less number of data points, com- pared to locality sensitive hashing based methods. However, to achieve acceptable nearest neigh- bor search accuracy for large scale applications, where number of data points and/
ContextNet: Deep learning for Star Galaxy Classiﬁcation  Noble Kennamer 1 David Kirkby 2 Alex Ihler 1 Javier S´anchez 2  Abstract  We present a framework to compose artiﬁcial neu- ral networks in cases where the data cannot be treated as independent events, our particular mo- tivation is star galaxy classiﬁcation for ground based optical surveys. Due to a turbulent atmo- sphere and imperfect instruments, a single image of an astronomical object is not enough to deﬁni- tively classify it as a sta
Frank-Wolfe with Subsampling Oracle  Thomas Kerdreux * 1 Fabian Pedregosa * 2 3 Alexandre d’Aspremont 4 1  Abstract  We analyze two novel randomized variants of the Frank-Wolfe (FW) or conditional gradient algo- rithm. While classical FW algorithms require solving a linear minimization problem over the domain at each iteration, the proposed method only requires to solve a linear minimization prob- lem over a small subset of the original domain. The ﬁrst algorithm that we propose is a random- ize
Convergence guarantees for a class of  non-convex and non-smooth optimization problems  Koulik Khamaru 1 Martin J. Wainwright 1 2  Abstract  Non-convex optimization problems arise fre- quently in machine learning, including feature se- lection, structured matrix learning, mixture model- ing, and neural network training. We consider the problem of ﬁnding critical points of a broad class of non-convex problems with non-smooth compo- nents. We analyze the behavior of two gradient- based methods—nam
Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam  Mohammad Emtiyaz Khan * 1 Didrik Nielsen * 1 Voot Tangkaratt * 1 Wu Lin 2 Yarin Gal 3 Akash Srivastava 4  Abstract  Uncertainty computation in deep learning is es- sential to design robust and reliable systems. Vari- ational inference (VI) is a promising approach for such computation, but requires more effort to implement and execute compared to maximum- likelihood methods. In this paper, we propose new natural-gradient alg
Geometry Score: A Method For Comparing Generative Adversarial Networks  Valentin Khrulkov 1 Ivan Oseledets 1 2  Abstract  One of the biggest challenges in the research of generative adversarial networks (GANs) is assess- ing the quality of generated samples and detect- ing various levels of mode collapse. In this work, we construct a novel measure of performance of a GAN by comparing geometrical properties of the underlying data manifold and the generated one, which provides both qualitative and
Blind Justice: Fairness with Encrypted Sensitive Attributes  Niki Kilbertus 1 2 Adri`a Gasc´on 3 4 Matt Kusner 3 4 Michael Veale 5 Krishna P. Gummadi 6 Adrian Weller 2 3  Abstract  Recent work has explored how to train machine learning models which do not discriminate against any subgroup of the population as determined by sensitive attributes such as gender or race. To avoid disparate treatment, sensitive attributes should not be considered. On the other hand, in or- der to avoid disparate impa
Markov Modulated Gaussian Cox Processes for  Semi-Stationary Intensity Modeling of Events Data  Minyoung Kim 1 2  Abstract  The Cox process is a ﬂexible event model that can account for uncertainty of the intensity func- tion in the Poisson process. However, previous approaches make strong assumptions in terms of time stationarity, potentially failing to generalize when the data do not conform to the assumed sta- tionarity conditions. In this paper we bring up two most popular Cox models represe
Disentangling by Factorising  Hyunjik Kim 1 2 Andriy Mnih 1  Abstract  We deﬁne and address the problem of unsuper- vised learning of disentangled representations on data generated from independent factors of varia- tion. We propose FactorVAE, a method that dis- entangles by encouraging the distribution of rep- resentations to be factorial and hence independent across the dimensions. We show that it improves upon β-VAE by providing a better trade-off be- tween disentanglement and reconstruction 
Self-Bounded Prediction Sufﬁx Tree via Approximate String Matching  Dongwoo Kim 1 2 Christian Walder 3 1  Abstract  Prediction sufﬁx trees (PST) provide an effec- tive tool for sequence modelling and prediction. Current prediction techniques for PSTs rely on exact matching between the sufﬁx of the current sequence and the previously observed sequence. We present a provably correct algorithm for learn- ing a PST with approximate sufﬁx matching by relaxing the exact matching condition. We then pre
Interpretability Beyond Feature Attribution:  Quantitative Testing with Concept Activation Vectors (TCAV)  Been Kim Martin Wattenberg Justin Gilmer Carrie Cai James Wexler  Fernanda Viegas Rory Sayres  Abstract  The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classiﬁers, operate on low-level fea- tures rather than high-level concepts. To address these challenges, we introduce Concep
Semi-Amortized Variational Autoencoders  Yoon Kim 1 Sam Wiseman 1 Andrew C. Miller 1 David Sontag 2 Alexander M. Rush 1  Abstract  Amortized variational inference (AVI) replaces instance-speciﬁc local inference with a global in- ference network. While AVI has enabled efﬁcient training of deep generative models such as varia- tional autoencoders (VAE), recent empirical work suggests that inference networks can produce sub- optimal variational parameters. We propose a hybrid approach, to use AVI t
Neural Relational Inference for Interacting Systems  Thomas Kipf * 1 Ethan Fetaya * 2 3 Kuan-Chieh Wang 2 3 Max Welling 1 4 Richard Zemel 2 3 4  Abstract  Interacting systems are prevalent in nature, from dynamical systems in physics to complex societal dynamics. The interplay of components can give rise to complex behavior, which can often be ex- plained using a simple model of the system’s con- stituent parts. In this work, we introduce the neu- ral relational inference (NRI) model: an unsuper
An Alternative View: When Does SGD Escape Local Minima?  Robert Kleinberg 1 Yuanzhi Li 2 Yang Yuan 1  Abstract  Stochastic gradient descent (SGD) is widely used in machine learning. Although being commonly viewed as a fast but not accurate version of gradi- ent descent (GD), it always ﬁnds better solutions than GD for modern neural networks. In order to understand this phenomenon, we take an alterna- tive view that SGD is working on the convolved (thus smoothed) version of the loss function. We 
Crowdsourcing with Arbitrary Adversaries  Matth¨aus Kleindessner 1 Pranjal Awasthi 1  Abstract  Most existing works on crowdsourcing assume that the workers follow the Dawid-Skene model, or the one-coin model as its special case, where every worker makes mistakes independently of other workers and with the same error probability for every task. We study a signiﬁcant extension of this restricted model. We allow almost half of the workers to deviate from the one-coin model and for those workers, t
Spatio-temporal Bayesian On-line Changepoint Detection with Model Selection  Jeremias Knoblauch 1 Theodoros Damoulas 1 2 3  Abstract  Bayesian On-line Changepoint Detection is ex- tended to on-line model selection and non- stationary spatio-temporal processes. We pro- pose spatially structured Vector Autoregressions (VARS) for modelling the process between changepoints (CPS) and give an upper bound on the approximation error of such models. The resulting algorithm performs prediction, model sele
Fast Gradient-Based Methods with Exponential Rate:  A Hybrid Control Framework  Arman Shariﬁ Kolarijani 1 Peyman Mohajerin Esfahani 1 Tam´as Keciczky 1  Abstract  Ordinary differential equations, and in general a dynamical system viewpoint, have seen a resur- gence of interest in developing fast optimization methods, mainly thanks to the availability of well- established analysis tools. In this study, we pursue a similar objective and propose a class of hybrid control systems that adopts a 2nd-o
Nonconvex Optimization for Regression with Fairness Constraints  Junpei Komiyama 1 Akiko Takeda 1 2 Junya Honda 1 2 Hajime Shimao 3  Abstract  The unfairness of a regressor is evaluated by mea- suring the correlation between the estimator and the sensitive attribute (e.g., race, gender, age), and the coefﬁcient of determination (CoD) is a natural extension of the correlation coefﬁcient when more than one sensitive attribute exists. As is well known, there is a trade-off between fair- ness and ac
On the Generalization of Equivariance and Convolution in Neural Networks  to the Action of Compact Groups  Risi Kondor 1 Shubhendu Trivedi 2  Abstract  Convolutional neural networks have been ex- tremely successful in the image recognition do- main because they ensure equivariance to trans- lations. There have been many recent attempts to generalize this framework to other domains, including graphs and data lying on manifolds. In this paper we give a rigorous, theoretical treat- ment of convolut
Compiling Combinatorial Prediction Games  Frederic Koriche 1  Abstract  In online optimization, the goal is to iteratively choose solutions from a decision space, so as to minimize the average cost over time. As long as this decision space is described by combinatorial constraints, the problem is generally intractable. In this paper, we consider the paradigm of com- piling the set of combinatorial constraints into a deterministic and Decomposable Negation Nor- mal Form (dDNNF) circuit, for which
Dynamic Evaluation of Neural Sequence Models  Ben Krause 1 Emmanuel Kahembwe 1 Iain Murray 1 Steve Renals 1  Abstract  We explore dynamic evaluation, where sequence models are adapted to the recent sequence history using gradient descent, assigning higher proba- bilities to re-occurring sequential patterns. We develop a dynamic evaluation approach that out- performs existing adaptation approaches in our comparisons. We apply dynamic evaluation to outperform all previous word-level perplexities o
Semiparametric Contextual Bandits  Akshay Krishnamurthy 1 Zhiwei Steven Wu 1 Vasilis Syrgkanis 2  Abstract  This paper studies semiparametric contextual ban- dits, a generalization of the linear stochastic ban- dit problem where the reward for an action is mod- eled as a linear function of known action features confounded by a non-linear action-independent term. We design new algorithms that achieve ˜O(dpT ) regret over T rounds, when the lin- ear function is d-dimensional, which matches the bes
Fast Maximization of Non-Submodular, Monotonic Functions on the Integer  Lattice  Alan Kuhnle 1 J. David Smith 1 Victoria G. Crawford 1 My T. Thai 1  Abstract  The optimization of submodular functions on the integer lattice has received much attention re- cently, but the objective functions of many ap- plications are non-submodular. We provide two approximation algorithms for maximizing a non- submodular function on the integer lattice sub- ject to a cardinality constraint; these are the ﬁrst al
Accurate Uncertainties for Deep Learning Using Calibrated Regression  Volodymyr Kuleshov 1 2 Nathan Fenner 2 Stefano Ermon 1  Abstract  Methods for reasoning under uncertainty are a key building block of accurate and reliable ma- chine learning systems. Bayesian methods pro- vide a general framework to quantify uncertainty. However, because of model misspeciﬁcation and the use of approximate inference, Bayesian un- certainty estimates are often inaccurate — for example, a 90% credible interval m
Trainable Calibration Measures For Neural Networks From  Kernel Mean Embeddings  Aviral Kumar 1 Sunita Sarawagi 1 Ujjwal Jain 1  Abstract  Modern neural networks have recently been found to be poorly calibrated, primarily in the direction of over-conﬁdence. Methods like entropy penalty and temperature smoothing improve calibration by clamping conﬁdence, but in doing so compro- mise the many legitimately conﬁdent predictions. We propose a more principled ﬁx that minimizes an explicit calibration 
Data-Dependent Stability of Stochastic Gradient Descent  Ilja Kuzborskij 1 Christoph H. Lampert 2  Abstract  We establish a data-dependent notion of algo- rithmic stability for Stochastic Gradient Descent (SGD), and employ it to develop novel general- ization bounds. This is in contrast to previous distribution-free algorithmic stability results for SGD which depend on the worst-case constants. By virtue of the data-dependent argument, our bounds provide new insights into learning with SGD on co
Explicit Inductive Bias for Transfer Learning with Convolutional Networks  Xuhong LI 1 Yves GRANDVALET 1 Franck DAVOINE 1  Abstract  In inductive transfer learning, ﬁne-tuning pre- trained convolutional networks substantially out- performs training from scratch. When using ﬁne- tuning, the underlying assumption is that the pre- trained model extracts generic features, which are at least partially relevant for solving the tar- get task, but would be difﬁcult to extract from the limited amount of 
Understanding the Loss Surface of Neural Networks for Binary Classiﬁcation  Shiyu Liang 1 Ruoyu Sun 1 Yixuan Li 2 R. Srikant 1  Abstract  It is widely conjectured that training algorithms for neural networks are successful because all local minima lead to similar performance; for example, see (LeCun et al., 2015; Choroman- ska et al., 2015; Dauphin et al., 2014). Perfor- mance is typically measured in terms of two met- rics: training performance and generalization per- formance. Here we focus on
Mixed batches and symmetric discriminators for GAN training  Thomas Lucas* 1 Corentin Tallec* 2 Jakob Verbeek 1 Yann Ollivier 3  Abstract  Generative adversarial networks (GANs) are pow- erful generative models based on providing feed- back to a generative network via a discriminator network. However, the discriminator usually as- sesses individual samples. This prevents the dis- criminator from accessing global distributional statistics of generated samples, and often leads to mode dropping: th
Binary Partitions with Approximate Minimum Impurity  Eduardo S. Laber * 1 Marco Molinaro * 1 Felipe de A. Mello Pereira 1  Abstract  The problem of splitting attributes is one of the main steps in the construction of decision trees. In order to decide the best split, impurity measures such as Entropy and Gini are widely used. In prac- tice, decision-tree inducers use heuristics for ﬁnd- ing splits with small impurity when they consider nominal attributes with a large number of distinct values. H
Canonical Tensor Decomposition for Knowledge Base Completion  Timothée Lacroix 1 2 Nicolas Usunier 1 Guillaume Obozinski 2  Abstract  The problem of Knowledge Base Completion can be framed as a 3rd-order binary tensor com- pletion problem. In this light, the Canonical Tensor Decomposition (CP) (Hitchcock, 1927) seems like a natural solution; however, current implementations of CP on standard Knowledge Base Completion benchmarks are lagging be- hind their competitors. In this work, we at- tempt t
On the Compositional Skills of Sequence-to-Sequence Recurrent Networks  Generalization without Systematicity:  Brenden Lake 1 2 Marco Baroni 2  Abstract  Humans can understand and produce new utter- ances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb “dax,” he or she can immediately un- derstand the meaning of “dax twice” or “sing and dax.” In this paper, we introduce the SCAN do- main, consisting of a set of simple compositional navigation co
An Estimation and Analysis Framework for the Rasch Model  Andrew S. Lan 1 Mung Chiang 2 Christoph Studer 3  Abstract  response theory (IRT) model (Lord, 1980), is given by  The Rasch model is widely used for item re- sponse analysis in applications ranging from recommender systems to psychology, education, and ﬁnance. While a number of estimators have been proposed for the Rasch model over the last decades, the available analytical performance guarantees are mostly asymptotic. This paper pro- vi
Partial Optimality and Fast Lower Bounds for Weighted Correlation Clustering  Jan-Hendrik Lange 1 2 Andreas Karrenbauer 1 Bjoern Andres 1 3 4  Abstract  Weighted correlation clustering is hard to solve and hard to approximate for general graphs. Its applications in network analysis and computer vi- sion call for efﬁcient algorithms. To this end, we make three contributions: We establish partial op- timality conditions that can be checked efﬁciently, and doing so recursively solves the problem fo
Deep Linear Networks with Arbitrary Loss: All Local Minima Are Global  Thomas Laurent * 1 James H. von Brecht * 2  Abstract  We consider deep linear networks with arbitrary convex differentiable loss. We provide a short and elementary proof of the fact that all local minima are global minima if the hidden layers are either 1) at least as wide as the input layer, or 2) at least as wide as the output layer. This result is the strongest possible in the following sense: If the loss is convex and Lip
The Multilinear Structure of ReLU Networks  Thomas Laurent * 1 James H. von Brecht * 2  Abstract  We study the loss surface of neural networks equipped with a hinge loss criterion and ReLU or leaky ReLU nonlinearities. Any such network deﬁnes a piecewise multilinear form in parame- ter space. By appealing to harmonic analysis we show that all local minima of such network are non-differentiable, except for those minima that occur in a region of parameter space where the loss surface is perfectly 
Hierarchical Imitation and Reinforcement Learning  Hoang M. Le 1 Nan Jiang 2 Alekh Agarwal 2 Miroslav Dud´ık 2 Yisong Yue 1 Hal Daum´e III 3 2  Abstract  We study how to effectively leverage expert feed- back to learn sequential decision-making poli- cies. We focus on problems with sparse rewards and long time horizons, which typically pose signiﬁcant challenges in reinforcement learning. We propose an algorithmic framework, called hi- erarchical guidance, that leverages the hierarchi- cal struc
Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace  Yoonho Lee 1 Seungjin Choi 1  Abstract  Gradient-based meta-learning methods leverage gradient descent to learn the commonalities among various tasks. While previous such meth- ods have been successful in meta-learning tasks, they resort to simple gradient descent during meta- testing. Our primary contribution is the MT-net, which enables the meta-learner to learn on each layer’s activation space a subspace that the task- s
Deep Reinforcement Learning in Continuous Action Spaces:  a Case Study in the Game of Simulated Curling  Kyowoon Lee * 1 Sol-A Kim * 1 Jaesik Choi 1 Seong-Whan Lee 2  Abstract  Many real-world applications of reinforcement learning require an agent to select optimal actions from continuous spaces. Recently, deep neural networks have successfully been applied to games with discrete actions spaces. However, deep neu- ral networks for discrete actions are not suitable for devising strategies for ga
Gated Path Planning Networks  Lisa Lee * 1 Emilio Parisotto * 1 Devendra Singh Chaplot 1 Eric Xing 1 Ruslan Salakhutdinov 1  Abstract  Value Iteration Networks (VINs) are effective dif- ferentiable path planning modules that can be used by agents to perform navigation while still maintaining end-to-end differentiability of the en- tire architecture. Despite their effectiveness, they suffer from several disadvantages including train- ing instability, random seed sensitivity, and other optimizatio
Deep Asymmetric Multi-task Feature Learning  Hae Beom Lee 1 2 Eunho Yang 3 2 Sung Ju Hwang 3 2  Abstract  We propose Deep Asymmetric Multitask Feature Learning (Deep-AMTFL) which can learn deep representations shared across multiple tasks while effectively preventing negative transfer that may happen in the feature sharing process. Speciﬁ- cally, we introduce an asymmetric autoencoder term that allows reliable predictors for the easy tasks to have high contribution to the feature learn- ing whil
Noise2Noise: Learning Image Restoration without Clean Data  Jaakko Lehtinen1 2 Jacob Munkberg1 Jon Hasselgren1 Samuli Laine1 Tero Karras1 Miika Aittala3 Timo Aila1  Abstract  We apply basic statistical reasoning to signal re- construction by machine learning – learning to map corrupted observations to clean signals – with a simple and powerful conclusion: it is possi- ble to learn to restore images by only looking at corrupted examples, at performance at and some- times exceeding training using 
Out-of-sample extension of graph adjacency spectral embedding  Keith Levin 1 Farbod Roosta-Khorasani 2 3 Michael W. Mahoney 3 4 Carey E. Priebe 5  Abstract  Many popular dimensionality reduction proce- dures have out-of-sample extensions, which allow a practitioner to apply a learned embedding to ob- servations not seen in the initial training sample. In this work, we consider the problem of obtain- ing an out-of-sample extension for the adjacency spectral embedding, a procedure for embedding th
An Optimal Control Approach to Deep Learning and  Applications to Discrete-Weight Neural Networks  Qianxiao Li 1 Shuji Hao 1  Abstract  Deep learning is formulated as a discrete-time optimal control problem. This allows one to char- acterize necessary conditions for optimality and develop training algorithms that do not rely on gra- dients with respect to the trainable parameters. In particular, we introduce the discrete-time method of successive approximations (MSA), which is based on the Pontr
Towards Binary-Valued Gates for Robust LSTM Training  Zhuohan Li 1 Di He 1 Fei Tian 2 Wei Chen 2 Tao Qin 2 Liwei Wang 1 3 Tie-Yan Liu 2  Abstract  Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. It aims to use gates to control informa- tion ﬂow (e.g., whether to skip some information or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal. In this paper, we pro- pose
On the Limitations of First-Order Approximation in GAN Dynamics  Jerry Li 1 Aleksander M ˛adry 1 John Peebles 1 Ludwig Schmidt 1  Abstract  While Generative Adversarial Networks (GANs) have demonstrated promising performance on multiple vision tasks, their learning dynamics are not yet well understood, both in theory and in practice. To address this issue, we study GAN dynamics in a simple yet rich parametric model that exhibits several of the common problematic convergence behaviors such as van
Submodular Hypergraphs: p-Laplacians, Cheeger Inequalities  and Spectral Clustering  Pan Li 1 Olgica Milenkovic 1  Abstract  We introduce submodular hypergraphs, a family of hypergraphs that have different submodular weights associated with different cuts of hyper- edges. Submodular hypergraphs arise in cluster- ing applications in which higher-order structures carry relevant information. For such hypergraphs, we deﬁne the notion of p-Laplacians and derive corresponding nodal domain theorems and
The Well-Tempered Lasso  Yuanzhi Li 1 Yoram Singer 1 2  Abstract  We study the complexity of the entire regulariza- tion path for least squares regression with 1-norm penalty, known as the Lasso. Every regression parameter in the Lasso changes linearly as a func- tion of the regularization value. The number of changes is regarded as the Lasso’s complexity. Experimental results using exact path following exhibit polynomial complexity of the Lasso in the problem size. Alas, the path complexity of 
Estimation of Markov Chain via Rank-Constrained Likelihood  Xudong Li 1 Mengdi Wang 1 Anru Zhang 2  Abstract  This paper studies the estimation of low-rank Markov chains from empirical trajectories. We propose a non-convex estimator based on rank- constrained likelihood maximization. Statisti- cal upper bounds are provided for the Kullback- Leiber divergence and the (cid:96)2 risk between the estimator and the true transition matrix. The es- timator reveals a compressed state space of the Markov
Asynchronous Decentralized Parallel Stochastic Gradient Descent  Xiangru Lian 1 * Wei Zhang 2 * Ce Zhang 3 Ji Liu 4 1  Abstract  Most commonly used distributed machine learning systems are either synchronous or centralized asyn- chronous. Synchronous algorithms like AllReduce- SGD perform poorly in a heterogeneous environ- ment, while asynchronous algorithms using a pa- rameter server suffer from 1) communication bottle- neck at parameter servers when workers are many, and 2) signiﬁcantly worse 
RLlib: Abstractions for Distributed Reinforcement Learning  Eric Liang * 1 Richard Liaw * 1 Philipp Moritz 1 Robert Nishihara 1 Roy Fox 1 Ken Goldberg 1  Joseph E. Gonzalez 1 Michael I. Jordan 1 Ion Stoica 1  Abstract  Reinforcement learning (RL) algorithms involve the deep nesting of highly irregular computation patterns, each of which typically exhibits oppor- tunities for distributed computation. We argue for distributing RL components in a composable way by adapting algorithms for top-down h
On the Spectrum of Random Features Maps of High Dimensional Data  Zhenyu Liao 1 Romain Couillet 1 2  Abstract  Random feature maps are ubiquitous in modern statistical machine learning, where they general- ize random projections by means of powerful, yet often difﬁcult to analyze nonlinear operators. In this paper, we leverage the “concentration” phe- nomenon induced by random matrix theory to perform a spectral analysis on the Gram matrix of these random feature maps, here for Gaussian mixture 
The Dynamics of Learning: A Random Matrix Approach  Zhenyu Liao 1 Romain Couillet 1 2  Abstract  Understanding the learning dynamics of neural networks is one of the key issues for the improve- ment of optimization algorithms as well as for the theoretical comprehension of why deep neu- ral nets work so well today. In this paper, we introduce a random matrix-based framework to analyze the learning dynamics of a single-layer linear network on a binary classiﬁcation problem, for data of simultaneo
Reviving and Improving Recurrent Back-Propagation  Renjie Liao * 1 2 3 Yuwen Xiong * 1 2 Ethan Fetaya 1 3 Lisa Zhang 1 3 KiJung Yoon 4 5 Xaq Pitkow 4 5  Raquel Urtasun 1 2 3 Richard Zemel 1 3 6  Abstract  In this paper, we revisit the recurrent back- propagation (RBP) algorithm (Almeida, 1987; Pineda, 1987), discuss the conditions under which it applies as well as how to satisfy them in deep neural networks. We show that RBP can be unsta- ble and propose two variants based on conjugate gradient 
Optimal Distributed Learning with Multi-pass Stochastic Gradient Methods  Junhong Lin 1 Volkan Cevher 1  Abstract  We study generalization properties of distributed algorithms in the setting of nonparametric re- gression over a reproducing kernel Hilbert space (RKHS). We investigate distributed stochastic gradient methods (SGM), with mini-batches and multi-passes over the data. We show that opti- mal generalization error bounds can be retained for distributed SGM provided that the partition leve
Optimal Rates of Sketched-regularized Algorithms for Least-Squares  Regression over Hilbert Spaces  Junhong Lin 1 Volkan Cevher 1  Abstract  We investigate regularized algorithms combining with projection for least-squares regression prob- lem over a Hilbert space, covering nonparamet- ric regression over a reproducing kernel Hilbert space. We prove convergence results with respect to variants of norms, under a capacity assumption on the hypothesis space and a regularity condi- tion on the targe
Level-Set Methods for Finite-Sum Constrained Convex Optimization  Qihang Lin 1 Runchao Ma 1 Tianbao Yang 2  Abstract  We consider the constrained optimization where the objective function and the constraints are de- ﬁned as summation of ﬁnitely many loss functions. This model has applications in machine learning such as Neyman-Pearson classiﬁcation. We con- sider two level-set methods to solve this class of problems, an existing inexact Newton method and a feasible level-set method. To update th
Detecting and Correcting for Label Shift with Black Box Predictors  Zachary C. Lipton * 1 2 Yu-Xiang Wang * 2 3 Alexander J. Smola 2  Abstract  Faced with distribution shift between training and test set, we wish to detect and quantify the shift, and to correct our classiﬁers without test set labels. Motivated by medical diagnosis, where diseases (targets), cause symptoms (observations), we fo- cus on label shift, where the label marginal p(y) changes but the conditional p(x|y) does not. We prop
Generalized Robust Bayesian Committee Machine for Large-scale Gaussian  Process Regression  Haitao Liu 1 Jianfei Cai 2 Yi Wang 3 Yew-Soon Ong 2 4  Abstract  In order to scale standard Gaussian process (GP) regression to large-scale datasets, aggregation models employ factorized training process and then combine predictions from distributed ex- perts. The state-of-the-art aggregation models, however, either provide inconsistent predictions or require time-consuming aggregation process. We ﬁrst pr
Towards Black-box Iterative Machine Teaching  Weiyang Liu * 1 Bo Dai * 1 Xingguo Li 2 Zhen Liu 1 James M. Rehg 1 Le Song 1 3  Abstract  In this paper, we make an important step to- wards the black-box machine teaching by con- sidering the cross-space machine teaching, where the teacher and the learner use different feature representations and the teacher can not fully ob- serve the learner’s model. In such scenario, we study how the teacher is still able to teach the learner to achieve faster co
Delayed Impact of Fair Machine Learning  Lydia T. Liu 1 Sarah Dean 1 Esther Rolf 1 Max Simchowitz 1 Moritz Hardt 1  Abstract  Fairness in machine learning has predominantly been studied in static classiﬁcation settings with- out concern for how decisions change the under- lying population over time. Conventional wisdom suggests that fairness criteria promote the long- term well-being of those groups they aim to pro- tect. We study how static fairness criteria interact with temporal indicators of
A Two-Step Computation of the Exact GAN Wasserstein Distance  Huidong Liu 1 Xianfeng Gu 1 Dimitris Samaras 1  Abstract  In this paper, we propose a two-step method to compute the Wasserstein distance in Wasserstein Generative Adversarial Networks (WGANs): 1) The convex part of our objective can be solved by linear programming; 2) The non-convex resid- ual can be approximated by a deep neural net- work. We theoretically prove that the proposed formulation is equivalent to the discrete Monge- Kant
Open Category Detection with PAC Guarantees  Si Liu * 1 Risheek Garrepalli * 2 Thomas G. Dietterich 2 Alan Fern 2 Dan Hendrycks 3  Abstract  Open category detection is the problem of detect- ing “alien” test instances that belong to categories or classes that were not present in the training data. In many applications, reliably detecting such aliens is central to ensuring the safety and accuracy of test set predictions. Unfortunately, there are no algorithms that provide theoretical guarantees o
Fast Variance Reduction Method with Stochastic Batch Size  Xuanqing Liu 1 Cho-Jui Hsieh 1 2  Abstract  In this paper we study a family of variance re- duction methods with randomized batch size—at each step, the algorithm ﬁrst randomly chooses the batch size and then selects a batch of sam- ples to conduct a variance-reduced stochastic up- date. We give the linear convergence rate for this framework for composite functions, and show that the optimal strategy to achieve the optimal convergence ra
Fast Stochastic AUC Maximization with O(1/n)-Convergence Rate  Mingrui Liu 1 Xiaoxuan Zhang 1 Zaiyi Chen 2 Xiaoyu Wang 3 Tianbao Yang 1  Abstract  In this paper, we consider statistical learning with AUC (area under ROC curve) maximization in the classical stochastic setting where one random data drawn from an unknown distribution is re- vealed at each iteration for updating the model. Although consistent convex surrogate losses for AUC maximization have been proposed to make the problem tractab
On Matching Pursuit and Coordinate Descent  Francesco Locatello * 1 2 Anant Raj * 1 Sai Praneeth Karimireddy 3  Gunnar Rätsch 2 Bernhard Schölkopf 1 Sebastian U. Stich 3 Martin Jaggi 3  Abstract  Two popular examples of ﬁrst-order optimization methods over linear spaces are coordinate descent and matching pursuit algorithms, with their ran- domized variants. While the former targets the optimization by moving along coordinates, the latter considers a generalized notion of directions. Exploiting 
PDE-Net: Learning PDEs from Data  Zichao Long∗ 1 Yiping Lu∗ 1 Xianzhong Ma∗ 1 2 Bin Dong 3 4 5  Abstract  Partial differential equations (PDEs) play a promi- nent role in many disciplines of science and en- gineering. PDEs are commonly derived based on empirical observations. However, with the rapid development of sensors, computational pow- er, and data storage in the past decade, huge quan- tities of data can be easily collected and efﬁciently stored. Such vast quantity of data offers new op- 
Error Estimation for Randomized Least-Squares Algorithms via the Bootstrap  Miles E. Lopes 1 Shusen Wang 2 Michael W. Mahoney 2  Abstract  Over the course of the past decade, a variety of randomized algorithms have been proposed for computing approximate least-squares (LS) solu- tions in large-scale settings. A longstanding prac- tical issue is that, for any given input, the user rarely knows the actual error of an approximate solution (relative to the exact solution). Likewise, it is difﬁcult f
Constraining the Dynamics of Deep Probabilistic Models  Marco Lorenzi 1 Maurizio Filippone 2  Abstract  We introduce a novel generative formulation of deep probabilistic models implementing “soft” constraints on their function dynamics. In partic- ular, we develop a ﬂexible methodological frame- work where the modeled functions and derivatives of a given order are subject to inequality or equal- ity constraints. We then characterize the posterior distribution over model and constraint parame- te
Spectrally Approximating Large Graphs with Smaller Graphs  Andreas Loukas 1 Pierre Vandergheynst 1  Abstract  How does coarsening affect the spectrum of a general graph? We provide conditions such that the principal eigenvalues and eigenspaces of a coarsened and original graph Laplacian matrices are close. The achieved approximation is shown to depend on standard graph-theoretic properties, such as the degree and eigenvalue distributions, as well as on the ratio between the coarsened and actual 
The Edge Density Barrier: Computational-Statistical Tradeoffs in  Combinatorial Inference  Hao Lu 1 Yuan Cao 1 Zhuoran Yang 1 Junwei Lu 2 Han Liu 3 Zhaoran Wang 4  Abstract  We study the hypothesis testing problem of infer- ring the existence of combinatorial structures in undirected graphical models. Although there ex- ist extensive studies on the information-theoretic limits of this problem, it remains largely unex- plored whether such limits can be attained by efﬁcient algorithms. In this pap
Accelerating Greedy Coordinate Descent Methods  Haihao Lu 1 Robert M. Freund 2 Vahab Mirrokni 3  Abstract  We introduce and study two algorithms to accel- erate greedy coordinate descent in theory and in practice: Accelerated Semi-Greedy Coordinate Descent (ASCD) and Accelerated Greedy Co- ordinate Descent (AGCD). On the theory side, our main results are for ASCD: we show that ASCD achieves O(1/k2) convergence, and it also achieves accelerated linear convergence for strongly convex functions. On
Structured Variationally Auto-encoded Optimization  Xiaoyu Lu 1 Javier González 2 Zhenwen Dai 2 Neil D. Lawrence 2 3  Abstract  We tackle the problem of optimizing a black- box objective function deﬁned over a highly- structured input space. This problem is ubiquitous in machine learning. Inferring the structure of a neural network or the Automatic Statistician (AS), where the kernel combination for a Gaussian pro- cess is optimized, are two of many possible exam- ples. We use the AS as a case s
Beyond Finite Layer Neural Networks:  Bridging Deep Architectures and Numerical Differential Equations  Yiping Lu 1 Aoxiao Zhong 2 Quanzheng Li 2 3 4 Bin Dong 5 6 4  Abstract  Deep neural networks have become the state- of-the-art models in numerous machine learning tasks. However, general guidance to network ar- chitecture design is still missing. In our work, we bridge deep neural network design with numeri- cal differential equations. We show that many ef- fective networks, such as ResNet, Po
End-to-end Active Object Tracking via Reinforcement Learning  Wenhan Luo * 1 Peng Sun * 1 Fangwei Zhong 2 Wei Liu 1 Tong Zhang 1 Yizhou Wang 2  Abstract  We study active object tracking, where a tracker takes as input the visual observation (i.e., frame sequence) and produces the camera control sig- nal (e.g., move forward, turn left, etc.). Conven- tional methods tackle the tracking and the camera control separately, which is challenging to tune jointly. It also incurs many human efforts for la
Competitive Caching with Machine Learned Advice  Thodoris Lykouris 1 Sergei Vassilvitskii 2  Abstract  We develop a framework for augmenting online al- gorithms with a machine learned oracle to achieve competitive ratios that provably improve upon un- conditional worst case lower bounds when the or- acle has low error. Our approach treats the oracle as a complete black box, and is not dependent on its inner workings, or the exact distribution of its errors. We apply this framework to the traditi
Batch Bayesian Optimization via Multi-objective Acquisition Ensemble for  Automated Analog Circuit Design  Wenlong Lyu 1 Fan Yang 1 Changhao Yan 1 Dian Zhou 1 2 Xuan Zeng 1  Abstract  Bayesian optimization methods are promising for the optimization of black-box functions that are expensive to evaluate. In this paper, a novel batch Bayesian optimization approach is proposed. The parallelization is realized via a multi-objective ensemble of multiple acquisition functions. In each iteration, the mu
Celer: a Fast Solver for the Lasso with Dual Extrapolation  Mathurin Massias 1 Alexandre Gramfort 1 Joseph Salmon 2  Abstract  Convex sparsity-inducing regularizations are ubiq- uitous in high-dimensional machine learning, but solving the resulting optimization problems can be slow. To accelerate solvers, state-of-the-art approaches consist in reducing the size of the op- timization problem at hand. In the context of regression, this can be achieved either by discard- ing irrelevant features (sc
The Power of Interpolation: Understanding the Effectiveness of SGD in  Modern Over-parametrized Learning†  Siyuan Ma 1 Raef Bassily 1 Mikhail Belkin 1  Abstract  1  Introduction  In this paper we aim to formally explain the phe- nomenon of fast convergence of Stochastic Gradi- ent Descent (SGD) observed in modern machine learning. The key observation is that most mod- ern learning architectures are over-parametrized and are trained to interpolate the data by driving the empirical loss (classiﬁca
Gradient Descent for Sparse Rank-One Matrix Completion for Crowd-Sourced Aggregation of Sparsely Interacting Workers  Yao Ma 1 Alex Olshevsky 2 Venkatesh Saligrama 2 Csaba Szepesvari 3  Abstract  We consider worker skill estimation for the single- coin Dawid-Skene crowdsourcing model. In practice skill-estimation is challenging because worker assignments are sparse and irregular due to the arbitrary, and uncontrolled availability of workers. We formulate skill estimation as a rank-one correlatio
Implicit Regularization in Nonconvex Statistical Estimation: Gradient Descent  Converges Linearly for Phase Retrieval and Matrix Completion  Cong Ma 1 Kaizheng Wang 1 Yuejie Chi 2 Yuxin Chen 3  Abstract  Recent years have seen a ﬂurry of activities in de- signing provably efﬁcient nonconvex optimization procedures for solving statistical estimation prob- lems. For various problems like phase retrieval or low-rank matrix completion, state-of-the-art non- convex procedures require proper regulariz
Dimensionality-Driven Learning with Noisy Labels  Xingjun Ma * 1 Yisen Wang * 2 Michael E. Houle 3 Shuo Zhou 1 Sarah M. Erfani 1 Shu-Tao Xia 2  Sudanthi Wijewickrema 1 James Bailey 1  Abstract  Datasets with signiﬁcant proportions of noisy (incorrect) class labels present challenges for training accurate Deep Neural Networks (DNNs). We propose a new perspective for understanding DNN generalization for such datasets, by inves- tigating the dimensionality of the deep represen- tation subspace of t
Approximate message passing for amplitude based optimization  Junjie Ma * 1 Ji Xu * 2 Arian Maleki 3  Abstract  We consider an (cid:96)2-regularized non-convex op- timization problem for recovering signals from their noisy phaseless observations. We design and study the performance of a message pass- ing algorithm that aims to solve this optimiza- tion problem. We consider the asymptotic set- ting m, n → ∞, m/n → δ and obtain sharp performance bounds, where m is the number of measurements and n 
Orthogonal Machine Learning: Power and Limitations  Lester Mackey 1 Vasilis Syrgkanis 1 Ilias Zadik 1 2  Abstract  √  Double machine learning provides n-consistent estimates of parameters of interest even when high-dimensional or nonparametric nuisance pa- rameters are estimated at an n−1/4 rate. The key is to employ Neyman-orthogonal moment equa- tions which are ﬁrst-order insensitive to pertur- bations in the nuisance parameters. We show that the n−1/4 requirement can be improved to n−1/(2k+2)
Learning Adversarially Fair and Transferable Representations  David Madras 1 2 Elliot Creager 1 2 Toniann Pitassi 1 2 Richard Zemel 1 2  Abstract  In this paper, we advocate for representation learn- ing as the key to mitigating unfair prediction outcomes downstream. Motivated by a scenario where learned representations are used by third parties with unknown objectives, we propose and explore adversarial representation learning as a natural method of ensuring those parties act fairly. We connect
An Efﬁcient, Generalized Bellman Update For Cooperative Inverse Reinforcement Learning  Dhruv Malik * 1 Malayandi Palaniappan * 1 Jaime F. Fisac 1 Dylan Hadﬁeld-Menell 1 Stuart Russell 1  Anca D. Dragan 1  Abstract  Our goal is for AI systems to correctly identify and act according to their human user’s objec- tives. Cooperative Inverse Reinforcement Learn- ing (CIRL) formalizes this value alignment prob- lem as a two-player game between a human and robot, in which only the human knows the pa- r
Iterative Amortized Inference  Joseph Marino 1 Yisong Yue 1 Stephan Mandt 2  Abstract  Inference models are a key component in scal- ing variational inference to deep latent variable models, most notably as encoder networks in variational auto-encoders (VAEs). By replacing conventional optimization-based inference with a learned model, inference is amortized over data examples and therefore more computationally ef- ﬁcient. However, standard inference models are restricted to direct mappings from
Streaming Principal Component Analysis in Noisy Settings  Teodor V. Marinov * 1 Poorya Mianjy * 1 Raman Arora 1  Abstract  We study streaming algorithms for principal com- ponent analysis (PCA) in noisy settings. We present computationally efﬁcient algorithms with sub-linear regret bounds for PCA in the presence of noise, missing data, and gross outliers.  1. Introduction Principal component analysis (PCA) is a ubiquitous tech- nique in statistics, machine learning and data science. Given a data
Fast Approximate Spectral Clustering for Dynamic Networks  Lionel Martin 1 Andreas Loukas 1 Pierre Vandergheynst 1  Abstract  Spectral clustering is a widely studied problem, yet its complexity is prohibitive for dynamic graphs of even modest size. We claim that it is possible to reuse information of past cluster assignments to expedite computation. Our ap- proach builds on a recent idea of sidestepping the main bottleneck of spectral clustering, i.e., com- puting the graph eigenvectors, by a po
Bayesian Model Selection for Change Point Detection and Clustering  Othmane Mazhar 1 Cristian R. Rojas 1 Carlo Fischione 1 Mohammad Reza Hesamzadeh 1  Abstract  We address a generalization of change point de- tection with the purpose of detecting the change locations and the levels of clusters of a piece- wise constant signal. Our approach is to model it as a nonparametric penalized least square model selection on a family of models indexed over the collection of partitions of the design points 
Optimization, Fast and Slow: Optimally Switching between Local and Bayesian  Optimization  Mark McLeod 1 Michael A. Osborne 1 2 Stephen J. Roberts 1 2  Abstract  We develop the ﬁrst Bayesian Optimization algo- rithm, BLOSSOM, which selects between mul- tiple alternative acquisition functions and tradi- tional local optimization at each step. This is com- bined with a novel stopping condition based on ex- pected regret. This pairing allows us to obtain the best characteristics of both local and B
Bounds on the Approximation Power of Feedforward Neural Networks  Mohammad Mehrabi 1 Aslan Tchamkerten 2 Mansoor I. Youseﬁ 2  Abstract  The approximation power of general feedforward neural networks with piecewise linear activation functions is investigated. First, lower bounds on the size of a network are established in terms of the approximation error and network depth and width. These bounds improve upon state- of-the-art bounds for certain classes of functions, such as strongly convex functi
Differentiable Dynamic Programming for Structured Prediction and Attention  Arthur Mensch 1 Mathieu Blondel 2  Abstract  Dynamic programming (DP) solves a variety of structured combinatorial problems by iteratively breaking them down into smaller subproblems. In spite of their versatility, many DP algorithms are non-differentiable, which hampers their use as a layer in neural networks trained by back- propagation. To address this issue, we propose to smooth the max operator in the dynamic pro- g
Ranking Distributions based on Noisy Sorting  Adil El Mesaoudi-Paul 1 Eyke H¨ullermeier 1 R´obert Busa-Fekete 2  Abstract  We propose a new statistical model for ranking data, i.e., a new family of probability distributions on permutations. Our model is inspired by the idea of a data-generating process in the form of a noisy sorting procedure, in which deterministic comparisons between pairs of items are replaced by Bernoulli trials. The probability of produc- ing a certain ranking as a result t
Which Training Methods for GANs do actually Converge?  Lars Mescheder 1 Andreas Geiger 1 2 Sebastian Nowozin 3  Abstract  Method  Recent work has shown local convergence of GAN training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is nec- essary: we describe a simple yet prototypical counterexample showing that in the more real- istic case of distributions that are not absolutely continuous, unregularized GAN train
Conﬁgurable Markov Decision Processes  Alberto Maria Metelli 1 * Mirco Mutti 1 * Marcello Restelli 1  Abstract  In many real-world problems, there is the possibil- ity to conﬁgure, to a limited extent, some environ- mental parameters to improve the performance of a learning agent. In this paper, we propose a novel framework, Conﬁgurable Markov Deci- sion Processes (Conf-MDPs), to model this new type of interaction with the environment. Fur- thermore, we provide a new learning algorithm, Safe Pol
prDeep: Robust Phase Retrieval with a Flexible Deep Network  Christopher A. Metzler 1 Philip Schniter 2 Ashok Veeraraghavan 1 Richard G. Baraniuk 1  Abstract  Phase retrieval algorithms have become an impor- tant component in many modern computational imaging systems. For instance, in the context of ptychography and speckle correlation imag- ing, they enable imaging past the diffraction limit and through scattering media, respectively. Un- fortunately, traditional phase retrieval algorithms stru
Pseudo-task Augmentation: From Deep Multitask  Learning to Intratask Sharing—and Back  Elliot Meyerson 1 2 Risto Miikkulainen 1 2  Abstract  Deep multitask learning boosts performance by sharing learned structure across related tasks. This paper adapts ideas from deep multitask learn- ing to the setting where only a single task is avail- able. The method is formalized as pseudo-task augmentation, in which models are trained with multiple decoders for each task. Pseudo-tasks simulate the effect o
The Hidden Vulnerability of Distributed Learning in Byzantium  El Mahdi El Mhamdi 1 Rachid Guerraoui 1 S´ebastien Rouault 1  Abstract  While machine learning is going through an era of celebrated success, concerns have been raised about the vulnerability of its backbone: stochas- tic gradient descent (SGD). Recent approaches have been proposed to ensure the robustness of distributed SGD against adversarial (Byzantine) workers sending poisoned gradients during the training phase. Some of these ap
Stochastic PCA with (cid:96)2 and (cid:96)1 Regularization  Poorya Mianjy 1 Raman Arora 1  Abstract  We revisit convex relaxation based methods for stochastic optimization of principal component analysis (PCA). While methods that directly solve the nonconvex problem have been shown to be optimal in terms of statistical and computational efﬁciency, the methods based on convex relax- ation have been shown to enjoy comparable, or even superior, empirical performance – this moti- vates the need for 
On the Implicit Bias of Dropout  Poorya Mianjy 1 Raman Arora 1 Rene Vidal 2  Abstract  Algorithmic approaches endow deep learning sys- tems with implicit bias that helps them general- ize even in over-parametrized settings. In this paper, we focus on understanding such a bias in- duced in learning through dropout, a popular tech- nique to avoid overﬁtting in deep learning. For single hidden-layer linear neural networks, we show that dropout tends to make the norm of in- coming/outgoing weight ve
One-Shot Segmentation in Clutter  Claudio Michaelis 1 2 Matthias Bethge 1 2 3 4 Alexander S. Ecker 1 2 4  Abstract  We tackle the problem of one-shot segmentation: ﬁnding and segmenting a previously unseen ob- ject in a cluttered scene based on a single instruc- tion example. We propose a novel dataset, which we call cluttered Omniglot. Using a baseline ar- chitecture combining a Siamese embedding for detection with a U-net for segmentation we show that increasing levels of clutter make the task
Differentiable plasticity: training plastic neural networks with  backpropagation  Thomas Miconi 1 Jeff Clune 1 Kenneth O. Stanley 1  Abstract  How can we build agents that keep learning from experience, quickly and efﬁciently, after their ini- tial training? Here we take inspiration from the main mechanism of learning in biological brains: synaptic plasticity, carefully tuned by evolution to produce efﬁcient lifelong learning. We show that plasticity, just like connection weights, can be optimi
Training Neural Machines with Trace-Based Supervision  Matthew B. Mirman 1 Dimitar Dimitrov 1 Pavle Djordjevic 1 Timon Gehr 1 Martin Vechev 1  Abstract  We investigate the effectiveness of trace-based supervision methods for training existing neural abstract machines. To deﬁne the class of neural machines amenable to trace-based supervision, we introduce the concept of a differential neural computational machine (∂NCM) and show that several existing architectures (NTMs, NRAMs) can be described a
Differentiable Abstract Interpretation for Provably Robust Neural Networks  Matthew Mirman 1 Timon Gehr 1 Martin Vechev 1  Abstract  We introduce a scalable method for training robust neural networks based on abstract interpretation. We present several abstract transformers which balance efﬁciency with precision and show these can be used to train large neural networks that are certiﬁably robust to adversarial perturbations.  1. Introduction Neural networks are increasingly gaining importance in
A Delay-tolerant Proximal-Gradient Algorithm for Distributed Learning  Konstantin Mishchenko 1 Franck Iutzeler 2 J´erˆome Malick 3 Massih-Reza Amini 2  Abstract  Distributed learning aims at computing high- quality models by training over scattered data. This covers a diversity of scenarios, including computer clusters or mobile agents. One of the main challenges is then to deal with heteroge- neous machines and unreliable communications. In this setting, we propose and analyze a ﬂexible asynchr
Data Summarization at Scale:  A Two-Stage Submodular Approach  Marko Mitrovic * 1 Ehsan Kazemi * 1 Morteza Zadimoghaddam 2 Amin Karbasi 1  Abstract  The sheer scale of modern datasets has resulted in a dire need for summarization techniques that can identify representative elements in a dataset. Fortunately, the vast majority of data summariza- tion tasks satisfy an intuitive diminishing returns condition known as submodularity, which allows us to ﬁnd nearly-optimal solutions in linear time. We 
The Hierarchical Adaptive Forgetting Variational Filter  Vincent Moens 1  Abstract  A common problem in Machine Learning and statistics consists in detecting whether the cur- rent sample in a stream of data belongs to the same distribution as previous ones, is an isolated outlier or inaugurates a new distribution of data. We present a hierarchical Bayesian algorithm that aims at learning a time-speciﬁc approximate pos- terior distribution of the parameters describing the distribution of the data
Decentralized Submodular Maximization: Bridging Discrete and Continuous Settings  Aryan Mokhtari 1 Hamed Hassani 2 Amin Karbasi 3  Abstract  In this paper, we showcase the interplay be- tween discrete and continuous optimization in network-structured settings. We propose the ﬁrst fully decentralized optimization method for a wide class of non-convex objective func- tions that possess a diminishing returns prop- erty. More speciﬁcally, given an arbitrary con- nected network and a global continuou
DICOD: Distributed Convolutional Coordinate Descent for  Convolutional Sparse Coding  Moreau Thomas 1 Oudre Laurent 2 Vayatis Nicolas 1  Abstract  In this paper, we introduce DICOD, a convolu- tional sparse coding algorithm which builds shift invariant representations for long signals. This algorithm is designed to run in a distributed set- ting, with local message passing, making it com- munication efﬁcient. It is based on coordinate descent and uses locally greedy updates which accelerate the 
WHInter: A Working set algorithm for High-dimensional  sparse second order Interaction models.  Marine Le Morvan 1 2 3 Jean-Philippe Vert 1 2 3 4  Abstract  Learning sparse linear models with two-way inter- actions is desirable in many application domains such as genomics. (cid:96)1-regularised linear models are popular to estimate sparse models, yet stan- dard implementations fail to address speciﬁcally the quadratic explosion of candidate two-way in- teractions in high dimensions, and typicall
Dropout Training, Data-dependent Regularization, and Generalization Bounds  Wenlong Mou 1 Yuchen Zhou 2 Jun Gao 3 Liwei Wang 3 4  Abstract  We study the problem of generalization guaran- tees for dropout training. A general framework is ﬁrst proposed for learning procedures with ran- dom perturbation on model parameters. The gen- eralization error is bounded by sum of two off- set Rademacher complexities: the main term is Rademacher complexity of the hypothesis class with minus offset induced by
Kernelized Synaptic Weight Matrices  Lorenz K. Muller 1 Julien N.P. Martel 1 Giacomo Indiveri 1  Abstract  1.1. Related Work  In this paper we introduce a novel neural net- work architecture, in which weight matrices are reparametrized in terms of low-dimensional vec- tors, interacting through kernel functions. A layer of our network can be interpreted as introduc- ing a (potentially inﬁnitely wide) linear layer be- tween input and output. We describe the theory underpinning this model and valid
Rapid Adaptation with Conditionally Shifted Neurons  Tsendsuren Munkhdalai 1 Xingdi Yuan 1 Soroush Mehri 1 Adam Trischler 1  Abstract  We describe a mechanism by which artiﬁcial neu- ral networks can learn rapid adaptation – the abil- ity to adapt on the ﬂy, with little data, to new tasks – that we call conditionally shifted neurons. We apply this mechanism in the framework of metalearning, where the aim is to replicate some of the ﬂexibility of human learning in machines. Conditionally shifted 
On the Relationship between Data Efﬁciency and Error  for Uncertainty Sampling  Stephen Mussmann 1 Percy Liang 1  Abstract  While active learning offers potential cost sav- ings, the actual data efﬁciency—the reduction in amount of labeled data needed to obtain the same error rate—observed in practice is mixed. This paper poses a basic question: when is ac- tive learning actually helpful? We provide an answer for logistic regression with the popular active learning algorithm, uncertainty samplin
Fitting New Speakers Based on a Short Untranscribed Sample  Eliya Nachmani 1 2 Adam Polyak 1 Yaniv Taigman 1 Lior Wolf 1 2  Abstract  Learning-based Text To Speech systems have the potential to generalize from one speaker to the next and thus require a relatively short sample of any new voice. However, this promise is cur- rently largely unrealized. We present a method that is designed to capture a new speaker from a short untranscribed audio sample. This is done by employing an additional netwo
Smoothed Action Value Functions for Learning Gaussian Policies  Oﬁr Nachum 1 Mohammad Norouzi 1 George Tucker 1 Dale Schuurmans 1 2  Abstract  State-action value functions (i.e., Q-values) are ubiquitous in reinforcement learning (RL), giving rise to popular algorithms such as SARSA and Q- learning. We propose a new notion of action value deﬁned by a Gaussian smoothed version of the expected Q-value. We show that such smoothed Q-values still satisfy a Bellman equation, making them learnable from
Nearly Optimal Robust Subspace Tracking  Praneeth Narayanamurthy 1 Namrata Vaswani 1  Abstract  Robust subspace tracking (RST) can be simply understood as a dynamic (time-varying) exten- sion of robust PCA. More precisely, it is the prob- lem of tracking data lying in a ﬁxed or slowly- changing low-dimensional subspace while being robust to sparse outliers. This work develops a recursive projected compressive sensing algo- rithm called “Nearly Optimal RST (NORST)”, and obtains one of the ﬁrst gu
Stochastic Proximal Algorithms for AUC Maximization  Michael Natole Jr. 1 Yiming Ying 1 Siwei Lyu 2  Abstract  Stochastic optimization algorithms such as stochastic gradient descent (SGD) update the model sequentially with cheap per-iteration costs, making them amenable for large-scale data analysis. Most of the existing studies focus on the classiﬁcation accuracy. However, these can not be directly applied to the important problems of maximizing the Area under the ROC curve (AUC) in imbalanced 
Mitigating Bias in Adaptive Data Gathering via Differential Privacy  Seth Neel 1 Aaron Roth 2  Abstract  Data that is gathered adaptively — via bandit al- gorithms, for example — exhibits bias. This is true both when gathering simple numeric val- ued data — the empirical means kept track of by stochastic bandit algorithms are biased down- wards — and when gathering more complicated data — running hypothesis tests on complex data gathered via contextual bandit algorithms leads to false discovery.
Optimization Landscape and Expressivity of Deep CNNs  Quynh Nguyen 1 Matthias Hein 2  Abstract  We analyze the loss landscape and expressiveness of practical deep convolutional neural networks (CNNs) with shared weights and max pooling lay- ers. We show that such CNNs produce linearly independent features at a “wide” layer which has more neurons than the number of training samples. This condition holds e.g. for the VGG network. Furthermore, we provide for such wide CNNs necessary and sufﬁcient c
Neural Networks Should Be Wide Enough to Learn Disconnected Decision  Regions  Quynh Nguyen 1 Mahesh Chandra Mukkamala 1 Matthias Hein 2  Abstract  In the recent literature the important role of depth in deep learning has been emphasized. In this paper we argue that sufﬁcient width of a feed- forward network is equally important by answer- ing the simple question under which conditions the decision regions of a neural network are con- nected. It turns out that for a class of activation functions
SGD and Hogwild! Convergence Without the Bounded Gradients Assumption  Lam M. Nguyen 1 2 Phuong Ha Nguyen 3 Marten van Dijk 3 Peter Richt´arik 4 Katya Scheinberg 1  Martin Tak´aˇc 1  Abstract  Stochastic gradient descent (SGD) is the optimiza- tion algorithm of choice in many machine learn- ing applications such as regularized empirical risk minimization and training deep neural networks. The classical convergence analysis of SGD is car- ried out under the assumption that the norm of the stochas
Active Testing: An Efﬁcient and Robust Framework for Estimating Accuracy.  Phuc Nguyen 1 Deva Ramanan 2 Charless Fowlkes 1  Abstract  Much recent work on visual recognition aims to scale up learning to massive, noisily-annotated datasets. We address the problem of scaling- up the evaluation of such models to large-scale datasets with noisy labels. Current protocols for doing so require a human user to either vet (re- annotate) a small fraction of the test set and ig- nore the rest, or else corre
On Learning Sparsely Used Dictionaries from Incomplete Samples  Thanh V. Nguyen 1 Akshay Soni 2 Chinmay Hegde 1  Abstract  Existing algorithms for dictionary learning as- sume that the entries of the (high-dimensional) input data are fully observed. However, in several practical applications, only an incomplete fraction of the data entries may be available. For incom- plete settings, no provably correct and polynomial- time algorithm has been reported in the dictio- nary learning literature. In 
Learning Continuous Hierarchies  in the Lorentz Model of Hyperbolic Geometry  Maximilian Nickel 1 Douwe Kiela 1  Abstract  We are concerned with the discovery of hierar- chical relationships from large-scale unstructured similarity scores. For this purpose, we study dif- ferent models of hyperbolic space and ﬁnd that learning embeddings in the Lorentz model is sub- stantially more efﬁcient than in the Poincaré-ball model. We show that the proposed approach allows us to learn high-quality embeddi
State Space Gaussian Processes with Non-Gaussian Likelihood  Hannes Nickisch 1 Arno Solin 2 Alexander Grigorievskiy 2 3  Abstract  We provide a comprehensive overview and tool- ing for GP modeling with non-Gaussian likeli- hoods using state space methods. The state space formulation allows to solve one-dimensional GP models in O(n) time and memory complexity. While existing literature has focused on the con- nection between GP regression and state space methods, the computational primitives allo
SparseMAP: Differentiable Sparse Structured Inference  Vlad Niculae 1 André F. T. Martins 2 Mathieu Blondel 3 Claire Cardie 1  Abstract  Structured prediction requires searching over a combinatorial number of structures. To tackle it, we introduce SparseMAP: a new method for sparse structured inference, and its natural loss function. SparseMAP automatically selects only a few global structures: it is situated between MAP inference, which picks a single structure, and marginal inference, which as
A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based  Visualizations  Weili Nie 1 Yang Zhang 2 Ankit B. Patel 1 3  Abstract  Backpropagation-based visualizations have been proposed to interpret convolutional neural net- works (CNNs), however a theory is missing to justify their behaviors: Guided backpropagation (GBP) and deconvolutional network (DeconvNet) generate more human-interpretable but less class- sensitive visualizations than saliency map. Moti- vated by this, we 
Functional Gradient Boosting based on Residual Network Perception  Atsushi Nitanda 1 2 Taiji Suzuki 1 2  Abstract  Residual Networks (ResNets) have become state- of-the-art models in deep learning and several theoretical studies have been devoted to under- standing why ResNet works so well. One attrac- tive viewpoint on ResNet is that it is optimizing the risk in a functional space by combining an ensemble of effective features. In this paper, we adopt this viewpoint to construct a new gradient 
Beyond 1/2-Approximation for Submodular Maximization  on Massive Data Streams  Ashkan Norouzi-Fard * 1 Jakub Tarnawski * 1 Slobodan Mitrovi´c * 1 Amir Zandieh * 1 Aidasadat Mousavifar 1  Ola Svensson 1  Abstract  Many tasks in machine learning and data min- ing, such as data diversiﬁcation, non-parametric learning, kernel machines, clustering etc., require extracting a small but representative summary from a massive dataset. Often, such problems can be posed as maximizing a submodular set functi
The Uncertainty Bellman Equation and Exploration  Brendan O’Donoghue 1 Ian Osband 1 Remi Munos 1 Volodymyr Mnih 1  Abstract  We consider the exploration/exploitation prob- lem in reinforcement learning. For exploitation, it is well known that the Bellman equation con- nects the value at any time-step to the expected value at subsequent time-steps. In this paper we consider a similar uncertainty Bellman equation (UBE), which connects the uncertainty at any time-step to the expected uncertainties 
Is Generator Conditioning Causally Related to GAN Performance?  Augustus Odena 1 Jacob Buckman 1 Catherine Olsson 1 Tom B. Brown 1 Christopher Olah 1 Colin Raffel 1  Ian Goodfellow 1  Abstract  Recent work (Pennington et al., 2017) suggests that controlling the entire distribution of Jacobian singular values is an important design considera- tion in deep learning. Motivated by this, we study the distribution of singular values of the Jacobian of the generator in Generative Adversarial Net- works
Learning in Reproducing Kernel Kre˘ın Spaces  Dino Oglic 1 2 Thomas G¨artner 1  Abstract  We formulate a novel regularized risk minimiza- tion problem for learning in reproducing kernel Kre˘ın spaces and show that the strong representer theorem applies to it. As a result of the latter, the learning problem can be expressed as the mini- mization of a quadratic form over a hypersphere of constant radius. We present an algorithm that can ﬁnd a globally optimal solution to this non- convex optimizat
BOCK : Bayesian Optimization with Cylindrical Kernels  ChangYong Oh 1 Efstratios Gavves 1 Max Welling 1 2  Abstract  A major challenge in Bayesian Optimization is the boundary issue (Swersky, 2017) where an algo- rithm spends too many evaluations near the bound- ary of its search space. In this paper we propose BOCK, Bayesian Optimization with Cylindrical Kernels, whose basic idea is to transform the ball geometry of the search space using a cylindrical transformation. Because of the transformed
Self-Imitation Learning  Junhyuk Oh * 1 Yijie Guo * 1 Satinder Singh 1 Honglak Lee 2 1  Abstract  This paper proposes Self-Imitation Learning (SIL), a simple off-policy actor-critic algorithm that learns to reproduce the agent’s past good de- cisions. This algorithm is designed to verify our hypothesis that exploiting past good experiences can indirectly drive deep exploration. Our empiri- cal results show that SIL signiﬁcantly improves advantage actor-critic (A2C) on several hard ex- ploration 
A probabilistic framework for multi-view feature learning  with many-to-many associations via neural networks  Akifumi Okuno 1 2 Tetsuya Hada 3 Hidetoshi Shimodaira 1 2  Abstract  A simple framework Probabilistic Multi-view Graph Embedding (PMvGE) is proposed for multi-view feature learning with many-to-many associations so that it generalizes various existing multi-view methods. PMvGE is a probabilistic model for predicting new associations via graph embedding of the nodes of data vectors with 
Transformation Autoregressive Networks  Junier B Oliva 1 2 Avinava Dubey 2 Manzil Zaheer 2  Barnab´as P´oczos 2 Ruslan Salakhutdinov 2 Eric P Xing 2 Jeff Schneider 2  Abstract  tion p(x) has been of keen interest to machine  The fundamental task of general density estima-  learning. In this work, we attempt to systemati- cally characterize methods for density estimation. Broadly speaking, most of the existing methods can be categorized into either using: a) autoregres- sive models to estimate th
Design of Experiments for Model Discrimination  Hybridising Analytical and Data-Driven Approaches  Simon Olofsson 1 Marc Peter Deisenroth 1 2 Ruth Misener 1  Abstract  Healthcare companies must submit pharmaceuti- cal drugs or medical devices to regulatory bodies before marketing new technology. Regulatory bodies frequently require transparent and inter- pretable computational modelling to justify a new healthcare technology, but researchers may have several competing models for a biological sys
Parallel WaveNet: Fast High-Fidelity Speech Synthesis  Aaron van den Oord 1 Yazhe Li 1 Igor Babuschkin 1 Karen Simonyan 1 Oriol Vinyals 1 Koray Kavukcuoglu 1  George van den Driessche 1 Edward Lockhart 1 Luis C. Cobo 1 Florian Stimberg 1 Norman Casagrande 1  Dominik Grewe 1 Seb Noury 1 Sander Dieleman 1 Erich Elsen 1 Nal Kalchbrenner 1 Heiga Zen 1 Alex Graves 1  Helen King 1 Tom Walters 1 Dan Belov 1 Demis Hassabis 1  Abstract  recently-developed WaveNet  The architec- ture (van den Oord et al.,
Learning Localized Spatio-Temporal Models From Streaming Data  Muhammad Osama 1 Dave Zachariah 1 Thomas Sch¨on 1  Abstract  We address the problem of predicting spatio- temporal processes with temporal patterns that vary across spatial regions, when data is ob- tained as a stream. That is, when the training dataset is augmented sequentially. Speciﬁcally, we develop a localized spatio-temporal covari- ance model of the process that can capture spa- tially varying temporal periodicities in the dat
Autoregressive Quantile Networks for Generative Modeling  Georg Ostrovski * 1 Will Dabney * 1 R´emi Munos 1  Abstract  We introduce autoregressive implicit quantile networks (AIQN), a fundamentally different ap- proach to generative modeling than those com- monly used, that implicitly captures the distribu- tion using quantile regression. AIQN is able to achieve superior perceptual quality and improve- ments in evaluation metrics, without incurring a loss of sample diversity. The method can be a
Efﬁcient First-Order Algorithms for Adaptive Signal Denoising  Dmitrii Ostrovskii 1 Zaid Harchaoui 2  Abstract  We consider the problem of discrete-time signal denoising, focusing on a speciﬁc family of non- linear convolution-type estimators. Each such estimator is associated with a time-invariant ﬁl- ter which is obtained adaptively, by solving a certain convex optimization problem. Adaptive convolution-type estimators were demonstrated to have favorable statistical properties, see (Judit- sky
Analyzing Uncertainty in Neural Machine Translation  Myle Ott 1 Michael Auli 1 David Grangier 1 Marc’Aurelio Ranzato 1  Abstract  Machine translation is a popular test bed for re- search in neural sequence-to-sequence models but despite much recent research, there is still a lack of understanding of these models. Practition- ers report performance degradation with large beams, the under-estimation of rare words and a lack of diversity in the ﬁnal translations. Our study relates some of these iss
Learning Compact Neural Networks with Regularization  Samet Oymak 1  Abstract  Proper regularization is critical for speeding up training, improving generalization performance, and learning compact models that are cost efﬁ- cient. We propose and analyze regularized gradi- ent descent algorithms for learning shallow neural networks. Our framework is general and cov- ers weight-sharing (convolutional networks), spar- sity (network pruning), and low-rank constraints among others. We ﬁrst introduce 
Tree Edit Distance Learning via Adaptive Symbol Embeddings  Benjamin Paaßen 1 Claudio Gallicchio 2 Alessio Micheli 2 Barbara Hammer 1  Abstract  Metric learning has the aim to improve classiﬁ- cation accuracy by learning a distance measure which brings data points from the same class closer together and pushes data points from dif- ferent classes further apart. Recent research has demonstrated that metric learning approaches can also be applied to trees, such as molecular struc- tures, abstract 
Reinforcement Learning with Function-Valued Action Spaces  for Partial Differential Equation Control  Yangchen Pan 1 2 Amir-massoud Farahmand 3 2 Martha White 1 Saleh Nabi 2 Piyush Grover 2  Daniel Nikovski 2  Abstract  Recent work has shown that reinforcement learn- ing (RL) is a promising approach to control dy- namical systems described by partial differential equations (PDE). This paper shows how to use RL to tackle more general PDE control problems that have continuous high-dimensional acti
Learning to Speed Up Structured Output Prediction  Xingyuan Pan 1 Vivek Srikumar 1  Abstract  Predicting structured outputs can be computation- ally onerous due to the combinatorially large out- put spaces. In this paper, we focus on reducing the prediction time of a trained black-box structured classiﬁer without losing accuracy. To do so, we train a speedup classiﬁer that learns to mimic a black-box classiﬁer under the learning-to-search approach. As the structured classiﬁer predicts more examp
Theoretical Analysis of Image-to-Image Translation with Adversarial Learning  Xudong Pan 1 Mi Zhang 1 Daizong Ding 1  Abstract  Recently, a uniﬁed model for image-to-image translation tasks within adversarial learning framework (Isola et al., 2017) has aroused widespread research interests in computer vision practitioners. Their reported empirical success however lacks solid theoretical interpretations for its inherent mechanism. In this paper, we refor- mulate their model from a brand-new geome
Max-Mahalanobis Linear Discriminant Analysis Networks  Tianyu Pang 1 Chao Du 1 Jun Zhu 1  Abstract  A deep neural network (DNN) consists of a non- linear transformation from an input to a feature representation, followed by a common softmax linear classiﬁer. Though many efforts have been devoted to designing a proper architecture for nonlinear transformation, little investigation has been done on the classiﬁer part. In this paper, we show that a properly designed classiﬁer can im- prove robustne
Stochastic Variance-Reduced Policy Gradient  Matteo Papini * 1 Damiano Binaghi * 1 Giuseppe Canonaco * 1 Matteo Pirotta 2 Marcello Restelli 1  Abstract  In this paper, we propose a novel reinforcement- learning algorithm consisting in a stochastic variance-reduced version of policy gradient for solving Markov Decision Processes (MDPs). Stochastic variance-reduced gradient (SVRG) methods have proven to be very successful in supervised learning. However, their adaptation to policy gradient is not 
Learning Independent Causal Mechanisms  Giambattista Parascandolo 1 2 Niki Kilbertus 1 3 Mateo Rojas-Carulla 1 3 Bernhard Sch¨olkopf 1  Abstract  Statistical learning relies upon data sampled from a distribution, and we usually do not care what actually generated it in the ﬁrst place. From the point of view of causal modeling, the structure of each distribution is induced by physical mecha- nisms that give rise to dependences between ob- servables. Mechanisms, however, can be mean- ingful autono
Time Limits in Reinforcement Learning  Fabio Pardo 1 Arash Tavakoli 1 Vitaly Levdik 1 Petar Kormushev 1  Abstract  In reinforcement learning, it is common to let an agent interact for a ﬁxed amount of time with its environment before resetting it and repeating the process in a series of episodes. The task that the agent has to learn can either be to maximize its performance over (i) that ﬁxed period, or (ii) an indeﬁnite period where time limits are only used during training to diversify experie
Image Transformer  Niki Parmar * 1 Ashish Vaswani * 1 Jakob Uszkoreit 1  Łukasz Kaiser 1 Noam Shazeer 1 Alexander Ku 2 3 Dustin Tran 4  Abstract  Image generation has been successfully cast as an autoregressive sequence generation or trans- formation problem. Recent work has shown that self-attention is an effective way of modeling tex- tual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation o
PIPPS: Flexible Model-Based Policy Search  Robust to the Curse of Chaos  Paavo Parmas 1 Carl Edward Rasmussen 2 Jan Peters 3 4 Kenji Doya 1  Abstract  Previously, the exploding gradient problem has been explained to be central in deep learning and model-based reinforcement learning, because it causes numerical issues and instability in opti- mization. Our experiments in model-based rein- forcement learning imply that the problem is not just a numerical issue, but it may be caused by a fundamenta
High-Quality Prediction Intervals for Deep Learning:  A Distribution-Free, Ensembled Approach  Tim Pearce 1 2 Mohamed Zaki 1 Alexandra Brintrup 1 Andy Neely 1  Abstract  This paper considers the generation of prediction intervals (PIs) by neural networks for quantifying uncertainty in regression tasks. It is axiomatic that high-quality PIs should be as narrow as possible, whilst capturing a speciﬁed portion of data. We derive a loss function directly from this axiom that requires no distribution
Adaptive Three Operator Splitting  Fabian Pedregosa 1 2 Gauthier Gidel 3  Abstract  We propose and analyze an adaptive step-size variant of the Davis-Yin three operator split- ting. This method can solve optimization prob- lems composed of a sum of a smooth term for which we have access to its gradient and an ar- bitrary number of potentially non-smooth terms for which we have access to their proximal opera- tor. The proposed method sets the step-size based on local information of the objective 
Efﬁcient Neural Architecture Search via Parameter Sharing  Hieu Pham * 1 2 Melody Y. Guan * 3 Barret Zoph 1 Quoc V. Le 1 Jeff Dean 1  Abstract  We propose Efﬁcient Neural Architecture Search (ENAS), a fast and inexpensive approach for au- tomatic model design. ENAS constructs a large computational graph, where each subgraph repre- sents a neural network architecture, hence forcing all architectures to share their parameters. A con- troller is trained with policy gradient to search for a subgraph
Bandits with Delayed, Aggregated Anonymous Feedback  Ciara Pike-Burke 1 Shipra Agrawal 2 Csaba Szepesvári 3 4 Steffen Grünewälder 1  Abstract  We study a variant of the stochastic K-armed bandit problem, which we call “bandits with de- layed, aggregated anonymous feedback”. In this problem, when the player pulls an arm, a reward is generated, however it is not immediately ob- served. Instead, at the end of each round the player observes only the sum of a number of pre- viously generated rewards 
Constant-Time Predictive Distributions for Gaussian Processes  Geoff Pleiss 1 Jacob R. Gardner 1 Kilian Q. Weinberger 1 Andrew Gordon Wilson 1  Abstract  One of the most compelling features of Gaussian process (GP) regression is its ability to provide well-calibrated posterior distributions. Recent ad- vances in inducing point methods have sped up GP marginal likelihood and posterior mean com- putations, leaving posterior covariance estimation and sampling as the remaining computational bot- tle
Local Convergence Properties of SAGA/Prox-SVRG and Acceleration  Clarice Poon * 1 Jingwei Liang * 1 Carola-Bibiane Sch¨onlieb 1  Abstract  In this paper, we present a local convergence anal- ysis for a class of stochastic optimisation meth- ods: the proximal variance reduced stochastic gradient methods, and mainly focus on SAGA (Defazio et al., 2014) and Prox-SVRG (Xiao & Zhang, 2014). Under the assumption that the non-smooth component of the optimisation prob- lem is partly smooth relative to a
Equivalence of Multicategory SVM and Simplex Cone SVM:  Fast Computations and Statistical Theory  Guillaume A. Pouliot 1  Abstract  The multicategory SVM (MSVM) of Lee et al. (2004) is a natural generalization of the classical, binary support vector machines (SVM). However, its use has been limited by computational difﬁ- culties. The simplex-cone SVM (SCSVM) of Mroueh et al. (2012) is a computationally ef- ﬁcient multicategory classiﬁer, but its use has been limited by a seemingly opaque interpr
Learning Dynamics of Linear Denoising Autoencoders  Arnu Pretorius 1 2 Steve Kroon 1 2 Herman Kamper 3  Abstract  Denoising autoencoders (DAEs) have proven use- ful for unsupervised representation learning, but a thorough theoretical understanding is still lack- ing of how the input noise inﬂuences learning. Here we develop theory for how noise inﬂuences learning in DAEs. By focusing on linear DAEs, we are able to derive analytic expressions that exactly describe their learning dynamics. We ver-
JointGAN: Multi-Domain Joint Distribution Learning with  Generative Adversarial Nets  Yunchen Pu 1 Shuyang Dai 2 Zhe Gan 3 Weiyao Wang 2 Guoyin Wang 2 Yizhe Zhang 3 Ricardo Henao 2  Lawrence Carin 2  Abstract  A new generative adversarial network is devel- oped for joint distribution matching. Distinct from most existing approaches, that only learn condi- tional distributions, the proposed model aims to learn a joint distribution of multiple random vari- ables (domains). This is achieved by lear
Selecting Representative Examples for Program Synthesis  Yewen Pu 1 Zachery Miranda 1 Armando Solar-Lezama 1 Leslie Pack Kaelbling 1  Abstract  Program synthesis is a class of regression prob- lems where one seeks a solution, in the form of a source-code program, mapping the inputs to their corresponding outputs exactly. Due to its precise and combinatorial nature, program syn- thesis is commonly formulated as a constraint sat- isfaction problem, where input-output examples are encoded as constr
Bridging Symbolic Grammars and Sequence Data for Future Prediction  Generalized Earley Parser:  Siyuan Qi 1 Baoxiong Jia 1 2 Song-Chun Zhu 1  Abstract  Future predictions on sequence data (e.g., videos or audios) require the algorithms to capture non- Markovian and compositional properties of high- level semantics. Context-free grammars are nat- ural choices to capture such properties, but tradi- tional grammar parsers (e.g., Earley parser) only take symbolic sentences as inputs. In this paper, 
Do Outliers Ruin Collaboration?  Mingda Qiao 1  Abstract  We consider the problem of learning a binary clas- siﬁer from n different data sources, among which at most an η fraction are adversarial. The over- head is deﬁned as the ratio between the sample complexity of learning in this setting and that of learning the same hypothesis class on a sin- gle data distribution. We present an algorithm that achieves an O(ηn + ln n) overhead, which is proved to be worst-case optimal. We also dis- cuss the
Gradually Updated Neural Networks for Large-Scale Image Recognition  Siyuan Qiao 1 Zhishuai Zhang 1 Wei Shen 1 2 Bo Wang 3 Alan Yuille 1  Abstract  Depth is one of the keys that make neural net- works succeed in the task of large-scale image recognition. The state-of-the-art network archi- tectures usually increase the depths by cascading convolutional layers or building blocks. In this paper, we present an alternative method to in- crease the depth. Our method is by introducing computation orde
DCFNet: Deep Neural Network with Decomposed Convolutional Filters  Qiang Qiu 1 Xiuyuan Cheng 1 Robert Calderbank 1 Guillermo Sapiro 1  Abstract  Filters in a Convolutional Neural Network (CNN) contain model parameters learned from enormous amounts of data. In this paper, we suggest to decompose convolutional ﬁlters in CNN as a trun- cated expansion with pre-ﬁxed bases, namely the Decomposed Convolutional Filters network (DCFNet), where the expansion coefﬁcients re- main learned from data. Such a
Non-convex Conditional Gradient Sliding  Chao Qu 1 Yan Li 2 Huan Xu 2  Abstract  We investigate a projection free optimization method, namely non-convex conditional gradi- ent sliding (NCGS) for non-convex optimiza- tion problems on the batch, stochastic and ﬁnite- sum settings. Conditional gradient sliding (CGS) method, by integrating Nesterov’s accelerated gra- dient method with Frank-Wolfe (FW) method in a smart way, outperforms FW for convex opti- mization, by reducing the amount of gradient
Machine Theory of Mind  Neil C. Rabinowitz 1 Frank Perbet 1 H. Francis Song 1 Chiyuan Zhang 2 Matthew Botvinick 1  Abstract  Theory of mind (ToM) broadly refers to humans’ ability to represent the mental states of others, including their desires, beliefs, and intentions. We design a Theory of Mind neural network – a ToMnet – which uses meta-learning to build such models of the agents it encounters. The ToMnet learns a strong prior model for agents’ future behaviour, and, using only a small num- 
Fast Parametric Learning with Activation Memorization  Jack W Rae 1 2 Chris Dyer 1 Peter Dayan 3 Timothy P Lillicrap 1 2  Abstract  Neural networks trained with backpropagation often struggle to identify classes that have been observed a small number of times. In applications where most class labels are rare, such as language modelling, this can become a performance bottle- neck. One potential remedy is to augment the net- work with a fast-learning non-parametric model which stores recent activa
Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?  Maithra Raghu 1 2 Alex Irpan 1 Jacob Andreas 3 Robert Kleinberg 2 Quoc Le 1 Jon Kleinberg 2  Abstract  Deep reinforcement learning has achieved many recent successes, but our understanding of its strengths and limitations is hampered by the lack of rich environments in which we can fully char- acterize optimal behavior, and correspondingly diagnose individual actions against such a charac- terization. Here we consider a family
Cut-Pursuit Algorithm for Regularizing Nonsmooth Functionals  with Graph Total Variation  Hugo Raguet 1 Loïc Landrieu 2  Abstract  We present an extension of the cut-pursuit al- gorithm, introduced by Landrieu & Obozinski (2017), to the graph total-variation regularization of functions with a separable nondifferentiable part. We propose a modiﬁed algorithmic scheme as well as adapted proofs of convergence. We also present a heuristic approach for handling the cases in which the values associated
Modeling Others using Oneself in Multi-Agent Reinforcement Learning  Roberta Raileanu 1 Emily Denton 1 Arthur Szlam 2 Rob Fergus 1 2  Abstract  We consider the multi-agent reinforcement learn- ing setting with imperfect information. The re- ward function depends on the hidden goals of both agents, so the agents must infer the other players’ goals from their observed behavior in order to maximize their returns. We propose a new approach for learning in these domains: Self Other-Modeling (SOM), in
On Nesting Monte Carlo Estimators  Tom Rainforth 1 Robert Cornish 1 2 Hongseok Yang 3 Andrew Warrington 2 Frank Wood 4  Abstract  Many problems in machine learning and statistics involve nested expectations and thus do not permit conventional Monte Carlo (MC) estimation. For such problems, one must nest estimators, such that terms in an outer estimator themselves in- volve calculation of a separate, nested, estimation. We investigate the statistical implications of nest- ing MC estimators, inclu
Tighter Variational Bounds are Not Necessarily Better  Tom Rainforth 1 Adam R. Kosiorek 1 2 Tuan Anh Le 2 Chris J. Maddison 1 Maximilian Igl 2 Frank Wood 3  Yee Whye Teh 1  Abstract  We provide theoretical and empirical evidence that using tighter evidence lower bounds (ELBOs) can be detrimental to the process of learning an inference network by reducing the signal-to-noise ratio of the gradient estimator. Our results call into question common implicit assumptions that tighter ELBOs are better v
SAFFRON: an Adaptive Algorithm for  Online Control of the False Discovery Rate  Aaditya Ramdas 1 Tijana Zrnic 2 Martin J. Wainwright 1 Michael I. Jordan 1  Abstract  In the online false discovery rate (FDR) problem, one observes a possibly inﬁnite sequence of p- values P1, P2, . . . , each testing a different null hy- pothesis, and an algorithm must pick a sequence of rejection thresholds α1, α2, . . . in an online fashion, effectively rejecting the k-th null hypoth- esis whenever Pk ≤ αk. Impor
QMIX: Monotonic Value Function Factorisation for  Deep Multi-Agent Reinforcement Learning  Tabish Rashid * 1 Mikayel Samvelyan * 2 Christian Schroeder de Witt 1  Gregory Farquhar 1 Jakob Foerster 1 Shimon Whiteson 1  Abstract  In many real-world settings, a team of agents must coordinate their behaviour while acting in a de- centralised way. At the same time, it is often possible to train the agents in a centralised fash- ion in a simulated or laboratory setting, where global state information i
Gradient Coding from Cyclic MDS Codes and Expander Graphs  Netanel Raviv 1 Itzhak Tamo 2 Rashish Tandon 3 Alexandros G. Dimakis 4  Abstract  Gradient coding is a technique for straggler mit- igation in distributed learning. In this paper we design novel gradient codes using tools from clas- sical coding theory, namely, cyclic MDS codes, which compare favourably with existing solutions, both in the applicable range of parameters and in the complexity of the involved algorithms. Sec- ond, we intro
LearningImplicitGenerativeModelswiththeMethodofLearnedMomentsSumanRavuri1ShakirMohamed1MihaelaRosca1OriolVinyals1AbstractWeproposeamethodofmoments(MoM)algo-rithmfortraininglarge-scaleimplicitgenerativemodels.Momentestimationinthissettingen-counterstwoproblems:itisoftendifﬁculttode-ﬁnethemillionsofmomentsneededtolearnthemodelparameters,anditishardtodeterminewhichpropertiesareusefulwhenspecifyingmo-ments.Toaddresstheﬁrstissue,weintroduceamomentnetwork,anddeﬁnethemomentsasthenetwork’shiddenunitsand
Weightless: Lossy weight encoding for deep neural network compression  Brandon Reagen 1 Udit Gupta 1 Robert Adolf 1 Michael M. Mitzenmacher 1 Alexander M. Rush 1  Gu-Yeon Wei 1 David Brooks 1 2  Abstract  The large memory requirements of deep neural networks limit their deployment and adoption on many devices. Model compression methods effectively reduce the memory requirements of these models, usually through applying transfor- mations such as weight pruning or quantization. In this paper, we p
Learning to Reweight Examples for Robust Deep Learning  Mengye Ren 1 2 Wenyuan Zeng 1 2 Bin Yang 1 2 Raquel Urtasun 1 2  Abstract  Deep neural networks have been shown to be very powerful modeling tools for many supervised learning tasks involving complex input patterns. However, they can also easily overﬁt to training set biases and label noises. In addition to various regularizers, example reweighting algorithms are popular solutions to these problems, but they require careful tuning of additi
Learning by Playing – Solving Sparse Reward Tasks from Scratch  Martin Riedmiller * 1 Roland Hafner * 1 Thomas Lampe 1 Michael Neunert 1 Jonas Degrave 1  Tom Van de Wiele 1 Volodymyr Mnih 1 Nicolas Heess 1 Tobias Springenberg 1  Abstract  We propose Scheduled Auxiliary Control (SAC- X), a new learning paradigm in the context of Reinforcement Learning (RL). SAC-X enables learning of complex behaviors – from scratch – in the presence of multiple sparse reward signals. To this end, the agent is equ
Been There, Done That: Meta-Learning with Episodic Recall  Samuel Ritter 1 2 Jane X. Wang 1 Zeb Kurth-Nelson 1 3 Siddhant M. Jayakumar 1  Charles Blundell 1 Razvan Pascanu 1 Matthew Botvinick 1 4  Abstract  Meta-learning agents excel at rapidly learning new tasks from open-ended task distributions; yet, they forget what they learn about each task as soon as the next begins. When tasks reoc- cur – as they do in natural environments – meta- learning agents must explore again instead of im- mediate
A Hierarchical Latent Vector Model  for Learning Long-Term Structure in Music  Adam Roberts 1 Jesse Engel 1 Colin Raffel 1 Curtis Hawthorne 1 Douglas Eck 1  Abstract  The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However, it has thus far seen limited application to sequential data, and, as we demonstrate, existing recurrent VAE models have difﬁculty modeling sequences with long-term structure. 
Learning to Optimize Combinatorial Functions  Nir Rosenfeld 1 Eric Balkanski 1 Amir Globerson 2 Yaron Singer 1  Abstract  Submodular functions have become a ubiquitous tool in machine learning. They are learnable from data, and can be optimized efﬁciently and with guarantees. Nonetheless, recent negative results show that optimizing learned surrogates of submodular functions can result in arbitrarily bad approximations of the true optimum. Our goal in this paper is to highlight the source of thi
Fast Information-theoretic Bayesian Optimisation  Binxin Ru 1 Mark McLeod 1 Diego Granziol 1 Michael A. Osborne 1 2  Abstract  Information-theoretic Bayesian optimisation tech- niques have demonstrated state-of-the-art perfor- mance in tackling important global optimisation problems. However, current information-theoretic approaches require many approximations in im- plementation, introduce often-prohibitive compu- tational overhead and limit the choice of kernels available to model the objectiv
Deep One-Class Classiﬁcation  Lukas Ruff * 1 Robert A. Vandermeulen * 2 Nico G¨ornitz 3 Lucas Deecke 4 Shoaib A. Siddiqui 2 5  Alexander Binder 6 Emmanuel M¨uller 1 Marius Kloft 2  Abstract  Despite the great advances made by deep learn- ing in many machine learning problems, there is a relative dearth of deep learning approaches for anomaly detection. Those approaches which do exist involve networks trained to perform a task other than anomaly detection, namely gener- ative models or compressio
Augment and Reduce: Stochastic Inference for Large Categorical Distributions  Francisco J. R. Ruiz 1 2 Michalis K. Titsias 3 Adji B. Dieng 2 David M. Blei 2  Abstract  Categorical distributions are ubiquitous in ma- chine learning, e.g., in classiﬁcation, language models, and recommendation systems. However, when the number of possible outcomes is very large, using categorical distributions becomes computationally expensive, as the complexity scales linearly with the number of outcomes. To addre
Probabilistic Boolean Tensor Decomposition  Tammo Rukat 1 2 Chris C. Holmes 1 2 Christopher Yau 2 3  Abstract  Boolean tensor decomposition approximates data of multi-way binary relationships as product of interpretable low-rank binary factors, following the rules of Boolean algebra. Here, we present its ﬁrst probabilistic treatment. We facilitate scal- able sampling-based posterior inference by ex- ploitation of the combinatorial structure of the factor conditionals. Maximum a posteriori decom-
Black-Box Variational Inference for Stochastic Differential Equations  Thomas Ryder * 1 2 Andrew Golightly 1 A. Stephen McGough 2 Dennis Prangle * 1  Abstract  Parameter inference for stochastic differential equations is challenging due to the presence of a latent diffusion process. Working with an Euler- Maruyama discretisation for the diffusion, we use variational inference to jointly learn the parame- ters and the diffusion paths. We use a standard mean-ﬁeld variational approximation of the p
Spurious Local Minima are Common in Two-Layer ReLU Neural Networks  Itay Safran 1 Ohad Shamir 1  Abstract  i=1 max{0, w(cid:62)  the form x (cid:55)→(cid:80)k  We consider the optimization problem associated with training simple ReLU neural networks of i x} with respect to the squared loss. We provide a computer- assisted proof that even if the input distribution is standard Gaussian, even if the dimension is ar- bitrarily large, and even if the target values are generated by such a network, wit
Learning Equations for Extrapolation and Control  Subham S. Sahoo 1 Christoph H. Lampert 2 Georg Martius 3  Abstract  We present an approach to identify concise equa- tions from data using a shallow neural network approach. In contrast to ordinary black-box re- gression, this approach allows understanding func- tional relations and generalizing them from ob- served data to unseen parts of the parameter space. We show how to extend the class of learnable equations for a recently proposed equation
Tempered Adversarial Networks  Mehdi S. M. Sajjadi 1 2 Giambattista Parascandolo 1 2 Arash Mehrjou 1 Bernhard Sch¨olkopf 1  Abstract  Generative adversarial networks (GANs) have been shown to produce realistic samples from high-dimensional distributions, but training them is considered hard. A possible explanation for training instabilities is the inherent imbalance be- tween the networks: While the discriminator is trained directly on both real and fake samples, the generator only has control o
Representation Tradeoffs for Hyperbolic Embeddings  Frederic Sala 1 Christopher De Sa 2 Albert Gu 1 Christopher R´e 1  Abstract  Hyperbolic embeddings offer excellent quality with few dimensions when embedding hierarchi- cal data structures. We give a combinatorial con- struction that embeds trees into hyperbolic space with arbitrarily low distortion without optimiza- tion. On WordNet, this algorithm obtains a mean- average-precision of 0.989 with only two dimen- sions, outperforming existing wo
Graph Networks as Learnable Physics Engines for Inference and Control  Alvaro Sanchez-Gonzalez 1 Nicolas Heess 1 Jost Tobias Springenberg 1 Josh Merel 1 Martin Riedmiller 1  Raia Hadsell 1 Peter Battaglia 1  Abstract  Understanding and interacting with everyday physical scenes requires rich knowledge about the structure of the world, represented either im- plicitly in a value or policy function, or explic- itly in a transition model. Here we introduce a new class of learnable models—based on gra
A Classiﬁcation–Based Study of Covariate Shift in GAN Distributions  Shibani Santurkar 1 Ludwig Schmidt 1 Aleksander M ˛adry 1  Abstract  A basic, and still largely unanswered, question in the context of Generative Adversarial Networks (GANs) is whether they are truly able to capture all the fundamental characteristics of the distribu- tions they are trained on. In particular, evaluating the diversity of GAN distributions is challenging and existing methods provide only a partial under- standing
TAPAS: Tricks to Accelerate (encrypted) Prediction As a Service  Amartya Sanyal 1 2 Matt J. Kusner 2 3 Adri`a Gasc´on 2 3 Varun Kanade 1 2  Abstract  Machine learning methods are widely used for a variety of prediction problems. Prediction as a service is a paradigm in which service providers with technological expertise and computational resources may perform predictions for clients. However, data privacy severely restricts the ap- plicability of such services, unless measures to keep client da
Tight Regret Bounds for Bayesian Optimization in One Dimension  Jonathan Scarlett 1  Abstract  We consider the problem of Bayesian optimiza- tion (BO) in one dimension, under a Gaussian process prior and Gaussian sampling noise. We provide a theoretical analysis showing that, under fairly mild technical assumptions on the kernel, the best possible cumulative regret up to time T behaves as Ω(√T ) and O(√T log T ). This gives a tight characterization up to a √log T factor, and includes the ﬁrst no
Learning with Abandonment  Sven Schmit 1 Ramesh Johari 2  Abstract  Consider a platform that wants to learn a per- sonalized policy for each user, but the platform faces the risk of a user abandoning the platform if they are dissatisﬁed with the actions of the plat- form. For example, a platform is interested in personalizing the number of newsletters it sends, but faces the risk that the user unsubscribes for- ever. We propose a general thresholded learn- ing model for scenarios like this, and 
Distantly Supervised Multitask Learning in Critical Care  Not to Cry Wolf:  Patrick Schwab 1 Emanuela Keller 2 Carl Muroi 2 David J. Mack 2 Christian Str¨assle 2 Walter Karlen 1  Abstract  Patients in the intensive care unit (ICU) require constant and close supervision. To assist clinical staff in this task, hospitals use monitoring sys- tems that trigger audiovisual alarms if their al- gorithms indicate that a patient’s condition may be worsening. However, current monitoring sys- tems are extre
Progress & Compress: A scalable framework for continual learning  Jonathan Schwarz 1 Jelena Luketina 2 Wojciech M. Czarnecki 1 Agnieszka Grabska-Barwinska 1  Yee Whye Teh 1 Razvan Pascanu * 1 Raia Hadsell * 1  Abstract  We introduce a conceptually simple and scalable framework for continual learning domains where tasks are learned sequentially. Our method is con- stant in the number of parameters and is designed to preserve performance on previously encoun- tered tasks while accelerating learnin
Multi-Fidelity Black-Box Optimization with Hierarchical Partitions  Rajat Sen 1 Kirthevasan Kandasamy 2 Sanjay Shakkottai 1  Abstract  simple regret Rn,  Motivated by settings such as hyper-parameter tuning and physical simulations, we consider the problem of black-box optimization of a function. Multi-ﬁdelity techniques have become popular for applications where exact function evaluations are expensive, but coarse (biased) approximations are available at much lower cost. A canonical example is 
Overcoming Catastrophic Forgetting with Hard Attention to the Task  Joan Serr`a 1 D´ıdac Sur´ıs 1 2 Marius Miron 1 3 Alexandros Karatzoglou 1  Abstract  Catastrophic forgetting occurs when a neural net- work loses the information learned in a previous task after training on subsequent tasks. This prob- lem remains a hurdle for artiﬁcial intelligence systems with sequential learning capabilities. In this paper, we propose a task-based hard attention mechanism that preserves previous tasks’ inform
Bounding and Counting Linear Regions of Deep Neural Networks  Thiago Serra * 1 Christian Tjandraatmadja * 1 Srikumar Ramalingam 2  Abstract  We investigate the complexity of deep neural networks (DNN) that represent piecewise linear (PWL) functions. In particular, we study the num- ber of linear regions, i.e. pieces, that a PWL func- tion represented by a DNN can attain, both the- oretically and empirically. We present (i) tighter upper and lower bounds for the maximum number of linear regions o
First Order Generative Adversarial Networks  Calvin Seward 1 2 Thomas Unterthiner 2 Urs Bergmann 1 Nikolay Jetchev 1 Sepp Hochreiter 2  Abstract  GANs excel at learning high dimensional distribu- tions, but they can update generator parameters in directions that do not correspond to the steepest descent direction of the objective. Prominent ex- amples of problematic update directions include those used in both Goodfellow’s original GAN and the WGAN-GP. To formally describe an opti- mal update di
Finding Inﬂuential Training Samples for Gradient Boosted Decision Trees  Boris Sharchilev 1 2 Yury Ustinovsky 3 Pavel Serdyukov 2 Maarten de Rijke 1  Abstract  We address the problem of ﬁnding inﬂuential training samples for a particular case of tree ensemble-based models, e.g., Random Forest (RF) or Gradient Boosted Decision Trees (GBDT). A natural way of formalizing this problem is studying how the model’s predictions change upon leave-one-out retraining, leaving out each in- dividual training
Solving Partial Assignment Problems using Random Clique Complexes  Charu Sharma 1 Deepak Nathani 1 Manohar Kaul 1  Abstract  We present an alternate formulation of the partial assignment problem as matching random clique complexes, that are higher-order analogues of ran- dom graphs, designed to provide a set of invari- ants that better detect higher-order structure. The proposed method creates random clique adjacency matrices for each k-skeleton of the random clique complexes and matches them, t
Adafactor: Adaptive Learning Rates with Sublinear Memory Cost  Noam Shazeer 1 Mitchell Stern 1 2  Abstract  In several recently proposed stochastic optimiza- tion methods (e.g. RMSProp, Adam, Adadelta), parameter updates are scaled by the inverse square roots of exponential moving averages of squared past gradients. Maintaining these per- parameter second-moment estimators requires memory equal to the number of parameters. For the case of neural network weight matrices, we propose maintaining on
Locally Private Hypothesis Testing  Or Sheffet 1  Abstract  We initiate the study of differentially private hy- pothesis testing in the local-model, under both the standard (symmetric) randomized-response mechanism (Warner, 1965; Kasiviswanathan et al., 2008) and the newer (non-symmetric) mechanisms (Bassily & Smith, 2015; Bassily et al., 2017). First, we study the general frame- work of mapping each user’s type into a sig- nal and show that the problem of ﬁnding the maximum-likelihood distribut
Learning in Integer Latent Variable Models  with Nested Automatic Differentiation  Daniel Sheldon 1 2 Kevin Winner 1 Debora Sujono 1  Abstract  We develop nested automatic differentiation (AD) algorithms for exact inference and learning in integer latent variable models. Recently, Win- ner, Sujono, and Sheldon showed how to reduce marginalization in a class of integer latent vari- able models to evaluating a probability generating function which contains many levels of nested high-order derivati
Towards More Efﬁcient Stochastic Decentralized Learning:  Faster Convergence and Sparse Communication  Zebang Shen 1 2 Aryan Mokhtari 3 Tengfei Zhou 1 Peilin Zhao 4 Hui Qian 1  Abstract  Recently, the decentralized optimization problem is attracting growing attention. Most existing methods are deterministic with high per-iteration cost and have a convergence rate quadratically depending on the problem condition number. Be- sides, the dense communication is necessary to en- sure the convergence e
An Algorithmic Framework of Variable Metric Over-Relaxed  Hybrid Proximal Extra-Gradient Method  Li Shen 1 Peng Sun 1 Yitong Wang 1 Wei Liu 1 Tong Zhang 1  Abstract  We propose a novel algorithmic framework of Variable Metric Over-Relaxed Hybrid Proximal Extra-gradient (VMOR-HPE) method with a global convergence guarantee for the maximal monotone operator inclusion problem. Its itera- tion complexities and local linear convergence rate are provided, which theoretically demon- strate that a large
A Spectral Approach to Gradient Estimation for Implicit Distributions  Jiaxin Shi 1 Shengyang Sun 2 Jun Zhu 1  Abstract  Recently there have been increasing interests in learning and inference with implicit distributions (i.e., distributions without tractable densities). To this end, we develop a gradient estimator for implicit distributions based on Stein’s identity and a spectral decomposition of kernel operators, where the eigenfunctions are approximated by the Nystr¨om method. Unlike the pre
TACO: Learning Task Decomposition via Temporal Alignment for Control  Kyriacos Shiarlis 1 Markus Wulfmeier 2 Sasha Salter 2 Shimon Whiteson 3 Ingmar Posner 2  Abstract  Many advanced Learning from Demonstration (LfD) methods consider the decomposition of complex, real-world tasks into simpler sub-tasks. By reusing the corresponding sub-policies within and between tasks, they provide training data for each policy from different high-level tasks and compose them to perform novel ones. However, mos
CRAFTML, an Efﬁcient Clustering-based Random  Forest for Extreme Multi-label Learning  Wissam Siblini 1 2 Pascale Kuntz 1 Frank Meyer 2  Abstract  Extreme Multi-label Learning (XML) considers large sets of items described by a number of la- bels that can exceed one million. Tree-based methods, which hierarchically partition the prob- lem into small scale sub-problems, are particu- larly promising in this context to reduce the learn- ing/prediction complexity and to open the way to parallelizatio
Asynchronous Stochastic Quasi-Newton MCMC for Non-Convex Optimization  Umut S¸ims¸ekli 1 C¸ a˘gatay Yıldız 2 Thanh Huy Nguyen 1 Ga¨el Richard 1 A. Taylan Cemgil 3  Abstract  Recent studies have illustrated that stochastic gradient Markov Chain Monte Carlo techniques have a strong potential in non-convex optimiza- tion, where local and global convergence guaran- tees can be shown under certain conditions. By building up on this recent theory, in this study, we develop an asynchronous-parallel sto
K-means clustering using random matrix sparsiﬁcation  Kaushik Sinha 1  Abstract  K-means clustering algorithm using Lloyd’s heuristic is one of the most commonly used tools in data mining and machine learning that shows promising performance. However, it suffers from a high computational cost resulting from pair- wise Euclidean distance computations between data points and cluster centers in each iteration of Lloyd’s heuristic. Main contributing factor of this computational bottle neck is a matr
Towards End-to-End Prosody Transfer  for Expressive Speech Synthesis with Tacotron  RJ Skerry-Ryan 1 Eric Battenberg 1 Ying Xiao 1 Yuxuan Wang 1 Daisy Stanton 1 Joel Shor 1 Ron J. Weiss 1  Rob Clark 1 Rif A. Saurous 1  Abstract  We present an extension to the Tacotron speech synthesis architecture that learns a latent embed- ding space of prosody, derived from a reference acoustic representation containing the desired prosody. We show that conditioning Tacotron on this learned embedding space re
An Inference-Based Policy Gradient Method for Learning Options  Matthew J. A. Smith 1 Herke Van Hoof 2 Joelle Pineau 1  Abstract  In the pursuit of increasingly intelligent learning systems, abstraction plays a vital role in enabling sophisticated decisions to be made in complex environments. The options framework provides formalism for such abstraction over sequences of decisions. However most models require that op- tions be given a priori, presumably speciﬁed by hand, which is neither efﬁcien
Accelerating Natural Gradient with  Higher-Order Invariance  Yang Song 1 Jiaming Song 1 Stefano Ermon 1  Abstract  An appealing property of the natural gradient is that it is invariant to arbitrary differentiable repa- rameterizations of the model. However, this in- variance property requires inﬁnitesimal steps and is lost in practical implementations with small but ﬁnite step sizes. In this paper, we study invari- ance properties from a combined perspective of Riemannian geometry and numerical 
Knowledge Transfer with Jacobian Matching  Suraj Srinivas 1 Franc¸ois Fleuret 1  Abstract  Classical distillation methods transfer representa- tions from a “teacher” neural network to a “stu- dent” network by matching their output activa- tions. Recent methods also match their Jacobians, or the gradient of output activations with the input. However, this involves making some ad hoc deci- sions, in particular, the choice of the loss function. In this paper, we ﬁrst establish an equivalence betwee
Universal Planning Networks  Aravind Srinivas 1 Allan Jabri 1 Pieter Abbeel 1 Sergey Levine 1 Chelsea Finn 1  Abstract  A key challenge in complex visuomotor control is learning abstract representations that are ef- fective for specifying goals, planning, and gen- eralization. To this end, we introduce univer- sal planning networks (UPN). UPNs embed dif- ferentiable planning within a goal-directed pol- icy. This planning computation unrolls a forward model in a latent space and infers an optimal
Approximation Algorithms for Cascading Prediction Models  Matthew Streeter 1  Abstract  We present an approximation algorithm that takes a pool of pre-trained models as input and produces from it a cascaded model with similar accuracy but lower average-case cost. Applied to state-of-the-art ImageNet classiﬁcation mod- els, this yields up to a 2x reduction in ﬂoating point multiplications, and up to a 6x reduction in average-case memory I/O. The auto-generated cascades exhibit intuitive propertie
Learning Low-Dimensional Temporal Representations  Bing Su 1 Ying Wu 2  Abstract  Low-dimensional discriminative representations enhance machine learning methods in both per- formance and complexity, motivating supervised dimensionality reduction (DR) that transforms high-dimensional data to a discriminative sub- space. Most DR methods require data to be i.i.d., however, in some domains, data naturally come in sequences, where the observations are tem- porally correlated. We propose a DR method 
Exploiting the Potential of Standard Convolutional Autoencoders  for Image Restoration by Evolutionary Search  Masanori Suganuma 1 2 Mete Ozay 2 Takayuki Okatani 2 1  Abstract  Researchers have applied deep neural networks to image restoration tasks, in which they proposed various network architectures, loss functions, and training methods. In particular, adversarial train- ing, which is employed in recent studies, seems to be a key ingredient to success. In this paper, we show that simple convo
Stagewise Safe Bayesian Optimization with Gaussian Processes  Yanan Sui 1 Vincent Zhuang 1 Joel W. Burdick 1 Yisong Yue 1  Abstract  Enforcing safety is a key aspect of many prob- lems pertaining to sequential decision making un- der uncertainty, which require the decisions made at every step to be both informative of the op- timal decision and also safe. For example, we value both efﬁcacy and comfort in medical ther- apy, and efﬁciency and safety in robotic control. We consider this problem of 
Neural Program Synthesis from Diverse Demonstration Videos  Shao-Hua Sun * 1 Hyeonwoo Noh * 2 Sriram Somasundaram 1 Joseph J. Lim 1  Abstract  Interpreting decision making logic in demonstra- tion videos is key to collaborating with and mim- icking humans. To empower machines with this ability, we propose a neural program synthesizer that is able to explicitly synthesize underlying programs from behaviorally diverse and visually complicated demonstration videos. We introduce a summarizer module 
Scalable Approximate Bayesian Inference for Particle Tracking Data  Ruoxi Sun 1 Liam Paninski 2  Abstract  Many important datasets in physics, chemistry, and biology consist of noisy sequences of im- ages of multiple moving overlapping particles. In many cases, the observed particles are indis- tinguishable, leading to unavoidable uncertainty about nearby particles’ identities. Exact Bayesian inference is intractable in this setting, and previ- ous approximate Bayesian methods scale poorly. Non-
Graphical Nonconvex Optimization via an Adaptive Convex Relaxation  Qiang Sun 1 Kean Ming Tan 2 Han Liu 3 Tong Zhang 3  Abstract  We consider the problem of learning high- dimensional Gaussian graphical models. The graphical lasso is one of the most popular methods for estimating Gaussian graphical models. How- ever, it does not achieve the oracle rate of con- vergence. In this paper, we propose the graphical nonconvex optimization for optimal estimation in Gaussian graphical models, which is th
Convolutional Imputation of Matrix Networks  Qingyun Sun * 1 Mengyuan Yan * 2 David Donoho 3 Stephen Boyd 2  Abstract  A matrix network is a family of matrices, with re- latedness modeled by a weighted graph. We con- sider the task of completing a partially observed matrix network. We assume a novel sampling scheme where a fraction of matrices might be completely unobserved. How can we recover the entire matrix network from incomplete observa- tions? This mathematical problem arises in many appl
Differentiable Compositional Kernel Learning for Gaussian Processes  Shengyang Sun 1 2 Guodong Zhang 1 2 Chaoqi Wang 1 2 Wenyuan Zeng 1 2 3 Jiaman Li 1 2 Roger Grosse 1 2  Abstract  The generalization properties of Gaussian pro- cesses depend heavily on the choice of kernel, and this choice remains a dark art. We present the Neural Kernel Network (NKN), a ﬂexible family of kernels represented by a neural network. The NKN’s architecture is based on the composition rules for kernels, so that each 
Learning the Reward Function for a Misspeciﬁed Model  Erik Talvitie 1  Abstract  In model-based reinforcement learning it is typi- cal to decouple the problems of learning the dy- namics model and learning the reward function. However, when the dynamics model is ﬂawed, it may generate erroneous states that would never occur in the true environment. It is not clear a priori what value the reward function should as- sign to such states. This paper presents a novel error bound that accounts for the
D2: Decentralized Training over Decentralized Data  Hanlin Tang 1 Xiangru Lian 1 Ming Yan 2 3 Ce Zhang 4 Ji Liu 5 1  Abstract  While training a machine learning model using multiple workers, each of which collects data from its own data source, it would be useful when the data collected from different workers are unique and different. Ironically, recent anal- ysis of decentralized parallel stochastic gradient descent (D-PSGD) relies on the assumption that the data hosted on different workers are
Neural Inverse Rendering for General Reﬂectance Photometric Stereo  Tatsunori Taniai 1 Takanori Maehara 1  Abstract  We present a novel convolutional neural network architecture for photometric stereo (Woodham, 1980), a problem of recovering 3D object surface normals from multiple images observed under varying illuminations. Despite its long history in computer vision, the problem still shows fun- damental challenges for surfaces with unknown general reﬂectance properties (BRDFs). Lever- aging d
Black Box FDR  Wesley Tansey 1 2 Yixin Wang 1 3 David M. Blei 1 3 4 Raul Rabadan 2  Abstract  Analyzing large-scale, multi-experiment studies requires scientists to test each experimental out- come for statistical signiﬁcance and then assess the results as a whole. We present Black Box FDR (BB-FDR), an empirical-Bayes method for analyzing multi-experiment studies when many covariates are gathered per experiment. BB-FDR learns a series of black box predictive models to boost power and control the
Best Arm Identiﬁcation in Linear Bandits  with Linear Dimension Dependency  Chao Tao 1 Sa´ul A. Blanco 1 Yuan Zhou 1 2 3  Abstract  We study the best arm identiﬁcation problem in linear bandits, where the mean reward of each arm depends linearly on an unknown d-dimensional parameter vector θ, and the goal is to identify the arm with the largest expected reward. We ﬁrst de- sign and analyze a novel randomized θ estimator based on the solution to the convex relaxation of an optimal G-allocation ex
χ2 Generative Adversarial Network  Chenyang Tao 1 Liqun Chen 1 Ricardo Henao 1 Jianfeng Feng 2 Lawrence Carin 1  Abstract  To assess the difference between real and syn- thetic data, Generative Adversarial Networks (GANs) are trained using a distribution discrep- ancy measure. Three widely employed measures are information-theoretic divergences, integral probability metrics, and Hilbert space discrep- ancy metrics. We elucidate the theoretical connec- tions between these three popular GAN traini
Lyapunov Functions for First-Order Methods:  Tight Automated Convergence Guarantees  Adrien Taylor * 1 Bryan Van Scoy * 2 Laurent Lessard * 2 3  Abstract  We present a novel way of generating Lyapunov functions for proving linear convergence rates of ﬁrst-order optimization methods. Our ap- proach provably obtains the fastest linear con- vergence rate that can be veriﬁed by a quadratic Lyapunov function (with given states), and only relies on solving a small-sized semideﬁnite pro- gram. Our appr
Bayesian Uncertainty Estimation for Batch Normalized Deep Networks  Mattias Teye 1 2 * Hossein Azizpour 1 * Kevin Smith 1 3  Abstract  We show that training a deep network using batch normalization is equivalent to approximate infer- ence in Bayesian models. We further demon- strate that this ﬁnding allows us to make mean- ingful estimates of the model uncertainty us- ing conventional architectures, without modiﬁ- cations to the network or the training proce- dure. Our approach is thoroughly val
Decoupling Gradient-Like Learning Rules from Representations  Philip S. Thomas 1 Christoph Dann 2 Emma Brunskill 3  Abstract  In machine learning, learning often corresponds to changing the parameters of a parameterized function. A learning rule is an algorithm or math- ematical expression that speciﬁes precisely how the parameters should be changed. When creating a machine learning system, we must make two decisions: what representation should be used (i.e., what parameterized function should b
CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor  Decompositions  Kevin Tian * 1 Teng Zhang * 2 James Zou 3  Abstract  Word embedding is a useful approach to cap- ture co-occurrence structures in large text corpora. However, in addition to the text data itself, we of- ten have additional covariates associated with indi- vidual corpus documents—e.g. the demographic of the author, time and venue of publication—and we would like the embedding to naturally capture this informatio
Importance Weighted Transfer of Samples in Reinforcement Learning  Andrea Tirinzoni 1 Andrea Sessa 1 Matteo Pirotta 2 Marcello Restelli 1  Abstract  We consider the transfer of experience samples (i.e., tuples (cid:104)s, a, s(cid:48), r(cid:105)) in reinforcement learn- ing (RL), collected from a set of source tasks to improve the learning process in a given tar- get task. Most of the related approaches focus on selecting the most relevant source samples for solving the target task, but then al
Adversarial Regression with Multiple Learners  Liang Tong * 1 Sixie Yu * 1 Scott Alfeld 2 Yevgeniy Vorobeychik 1  Abstract  Despite the considerable success enjoyed by ma- chine learning techniques in practice, numerous studies demonstrated that many approaches are vulnerable to attacks. An important class of such attacks involves adversaries changing features at test time to cause incorrect predictions. Previous investigations of this problem pit a single learner against an adversary. However, 
Convergent TREE BACKUP and RETRACE with Function Approximation  Ahmed Touati 1 2 Pierre-Luc Bacon 3 Doina Precup 3 4 Pascal Vincent 1 2 4  Abstract  Off-policy learning is key to scaling up reinforce- ment learning as it allows to learn about a target policy from the experience generated by a differ- ent behavior policy. Unfortunately, it has been challenging to combine off-policy learning with function approximation and multi-step bootstrap- ping in a way that leads to both stable and efﬁcient 
Theoretical Analysis of Sparse Subspace Clustering with Missing Entries  Manolis C. Tsakiris * 1 Ren´e Vidal * 2  Abstract  Sparse Subspace Clustering (SSC) is a popu- lar unsupervised machine learning method for clustering data lying close to an unknown union of low-dimensional linear subspaces; a problem with numerous applications in pattern recogni- tion and computer vision. Even though the be- havior of SSC for complete data is by now well- understood, little is known about its theoreti- cal
StrassenNets: Deep Learning with a Multiplication Budget  Michael Tschannen 1 Aran Khanna 2 Anima Anandkumar 2 3  Abstract  A large fraction of the arithmetic operations re- quired to evaluate deep neural networks (DNNs) consists of matrix multiplications, in both con- volution and fully connected layers. We perform end-to-end learning of low-cost approximations of matrix multiplications in DNN layers by casting matrix multiplications as 2-layer sum-product net- works (SPNs) (arithmetic circuits
Invariance of Weight Distributions in Rectiﬁed MLPs  Russell Tsuchida 1 Farbod Roosta-Khorasani 2 3 Marcus Gallagher 1  Abstract  An interesting approach to analyzing neural net- works that has received renewed attention is to examine the equivalent kernel of the neural net- work. This is based on the fact that a fully connected feedforward network with one hid- den layer, a certain weight distribution, an acti- vation function, and an inﬁnite number of neu- rons can be viewed as a mapping into 
Least-Squares Temporal Difference Learning for the Linear Quadratic  Regulator  Stephen Tu 1 Benjamin Recht 1  Abstract  Reinforcement learning (RL) has been success- fully used to solve many continuous control tasks. Despite its impressive results however, fundamen- tal questions regarding the sample complexity of RL on continuous problems remain open. We study the performance of RL in this setting by con- sidering the behavior of the Least-Squares Tem- poral Difference (LSTD) estimator on the 
The Mirage of Action-Dependent Baselines in Reinforcement Learning  George Tucker 1 Surya Bhupatiraju 1 2 Shixiang Gu 1 3 4 Richard E. Turner 3 Zoubin Ghahramani 3 5  Sergey Levine 1 6  Abstract  Policy gradient methods are a widely used class of model-free reinforcement learning algorithms where a state-dependent baseline is used to reduce gradient estimator variance. Several recent papers extend the baseline to depend on both the state and action and suggest that this signiﬁcantly reduces vari
Adversarial Risk and the Dangers of Evaluating Against Weak Attacks  Jonathan Uesato 1 Brendan O’Donoghue 1 Aaron van den Oord 1 Pushmeet Kohli 1  Abstract  This paper investigates recently proposed ap- proaches for defending against adversarial exam- ples and evaluating adversarial robustness. We motivate adversarial risk as an objective for achieving models robust to worst-case inputs. We then frame commonly used attacks and evaluation metrics as deﬁning a tractable surrogate objective to the 
DVAE++: Discrete Variational Autoencoders with  Overlapping Transformations  Arash Vahdat 1 William G. Macready 1 Zhengbing Bian 1 Amir Khoshaman 1 Evgeny Andriyash 1  Abstract  Training of discrete latent variable models re- mains challenging because passing gradient in- formation through discrete units is difﬁcult. We propose a new class of smoothing transformations based on a mixture of two overlapping distribu- tions, and show that the proposed transformation can be used for training binary 
Programmatically Interpretable Reinforcement Learning  Abhinav Verma 1 Vijayaraghavan Murali 1 Rishabh Singh 2 Pushmeet Kohli 3 Swarat Chaudhuri 1  Abstract  We present a reinforcement learning framework, called Programmatically Interpretable Reinforce- ment Learning (PIRL), that is designed to gen- erate interpretable and veriﬁable agent policies. Unlike the popular Deep Reinforcement Learn- ing (DRL) paradigm, which represents policies by neural networks, PIRL represents policies us- ing a hig
Clustering Semi-Random Mixtures of Gaussians  Pranjal Awasthi 1 Aravindan Vijayaraghavan 2  Abstract  Gaussian mixture models (GMM) are the most widely used statistical model for the k-means clus- tering problem and form a popular framework for clustering in machine learning and data analysis. In this paper, we propose a natural robust model for k-means clustering that generalizes the Gaus- sian mixture model, and that we believe will be useful in identifying robust algorithms. Our ﬁrst contribu
A Probabilistic Theory of Supervised Similarity Learning  for Pointwise ROC Curve Optimization  Robin Vogel 1 2 Aur´elien Bellet 3 St´ephan Cl´emenc¸on 1  Abstract  The performance of many machine learning tech- niques depends on the choice of an appropriate similarity or distance measure on the input space. Similarity learning (or metric learning) aims at building such a measure from training data so that observations with the same (resp. different) la- bel are as close (resp. far) as possible.
Hierarchical Multi-Label Classiﬁcation Networks  Jônatas Wehrmann 1 Ricardo Cerri 2 Rodrigo C. Barros 1  Abstract  One of the most challenging machine learning problems is a particular case of data classiﬁca- tion in which classes are hierarchically structured and objects can be assigned to multiple paths of the class hierarchy at the same time. This task is known as hierarchical multi-label classiﬁcation (HMC), with applications in text classiﬁcation, image annotation, and in bioinformatics pro
Transfer Learning via Learning to Transfer  Ying Wei 1 2 Yu Zhang 1 Junzhou Huang 2 Qiang Yang 1  Abstract  In transfer learning, what and how to transfer are two primary issues to be addressed, as different transfer learning algorithms applied between a source and a target domain result in different knowledge transferred and thereby the perfor- mance improvement in the target domain. Deter- mining the optimal one that maximizes the perfor- mance improvement requires either exhaustive ex- plorat
Semi-Supervised Learning on Data Streams via Temporal Label Propagation  Tal Wagner 1 Sudipto Guha 2 Shiva Prasad Kasiviswanathan 2 Nina Mishra 2  Abstract  We consider the problem of labeling points on a fast-moving data stream when only a small num- ber of labeled examples are available. In our setting, incoming points must be processed ef- ﬁciently and the stream is too large to store in its entirety. We present a semi-supervised learning algorithm for this task. The algorithm maintains a sma
Neural Dynamic Programming for Musical Self Similarity  Christian J. Walder 1 2 Dongwoo Kim 2 3  Abstract  We present a neural sequence model designed speciﬁcally for symbolic music. The model is based on a learned edit distance mechanism which generalises a classic recursion from computer sci- ence, leading to a neural dynamic program. Re- peated motifs are detected by learning the transfor- mations between them. We represent the arising computational dependencies using a novel data structure, 
Thompson Sampling for Combinatorial Semi-Bandits  Siwei Wang 1 Wei Chen 2  Abstract  We study the application of the Thompson sam- pling (TS) methodology to the stochastic combi- natorial multi-armed bandit (CMAB) framework. We analyze the standard TS algorithm for the general CMAB, and obtain the ﬁrst distribution- dependent regret bound of O(m log T /∆min) for TS under general CMAB, where m is the number of arms, T is the time horizon, and ∆min is the minimum gap between the expected reward of
PredRNN++: Towards A Resolution of the Deep-in-Time Dilemma in  Spatiotemporal Predictive Learning  Yunbo Wang 1 Zhifeng Gao 1 Mingsheng Long 1 Jianmin Wang 1 Philip S. Yu 1  Abstract  We present PredRNN++, a recurrent network for spatiotemporal predictive learning. In pursuit of a great modeling capability for short-term video dynamics, we make our network deeper in time by leveraging a new recurrent structure named Causal LSTM with cascaded dual memories. To alleviate the gradient propagation 
Analyzing the Robustness of Nearest Neighbors to Adversarial Examples  Yizhen Wang 1 Somesh Jha 2 Kamalika Chaudhuri 1  Abstract  Motivated by safety-critical applications, test- time attacks on classiﬁers via adversarial exam- ples has recently received a great deal of attention. However, there is a general lack of understanding on why adversarial examples arise; whether they originate due to inherent properties of data or due to lack of training samples remains ill-understood. In this work, we
Competitive Multi-agent Inverse Reinforcement Learning  with Sub-optimal Demonstrations  Xingyu Wang 1 Diego Klabjan 1  Abstract  This paper considers the problem of inverse rein- forcement learning in zero-sum stochastic games when expert demonstrations are known to be sub- optimal. Compared to previous works that decou- ple agents in the game by assuming optimality in expert policies, we introduce a new objective function that directly pits experts against Nash Equilibrium policies, and we des
Coded Sparse Matrix Multiplication  Sinong Wang 1 Jiashang Liu 1 Ness Shroff 2  Abstract  (cid:124)  In a large-scale and distributed matrix multiplica- B, where C ∈ Rr×t, the tion problem C = A coded computation plays an important role to ef- fectively deal with “stragglers” (distributed com- putations that may get delayed due to few slow or faulty processors). However, existing coded schemes could destroy the signiﬁcant sparsity that exists in large-scale machine learning problems, and could r
A Fast and Scalable Joint Estimator for Integrating Additional Knowledge in  Learning Multiple Related Sparse Gaussian Graphical Models  Beilun Wang 1 Arshdeep Sekhon 1 Yanjun Qi 1  Abstract  We consider the problem of including additional knowledge in estimating sparse Gaussian graph- ical models (sGGMs) from aggregated samples, arising often in bioinformatics and neuroimag- ing applications. Previous joint sGGM estimators either fail to use existing knowledge or cannot scale-up to many tasks (
Provable Variable Selection for Streaming Features  Jing Wang 1 Jie Shen 2 Ping Li 3  Abstract  In large-scale machine learning applications and high-dimensional statistics, it is ubiquitous to ad- dress a considerable number of features among which many are redundant. As a remedy, on- line feature selection has attracted increasing at- tention in recent years. It sequentially reveals features and evaluates the importance of them. Though online feature selection has proven an el- egant methodolo
Style Tokens: Unsupervised Style Modeling, Control and Transfer in  End-to-End Speech Synthesis  Yuxuan Wang 1 Daisy Stanton 1 Yu Zhang 1 RJ Skerry-Ryan 1 Eric Battenberg 1 Joel Shor 1 Ying Xiao 1  Fei Ren 1 Ye Jia 1 Rif A. Saurous 1  Abstract  In this work, we propose “global style tokens” (GSTs), a bank of embeddings that are jointly trained within Tacotron, a state-of-the-art end-to- end speech synthesis system. The embeddings are trained with no explicit labels, yet learn to model a large ra
Adversarial Distillation of Bayesian Neural Network Posteriors  Kuan-Chieh Wang 1 2 Paul Vicol 1 2 James Lucas 1 2 Li Gu 1 Roger Grosse 1 2 Richard Zemel 1 2  Abstract  Bayesian neural networks (BNNs) allow us to reason about uncertainty in a principled way. Stochastic Gradient Langevin Dynamics (SGLD) enables efﬁcient BNN learning by drawing sam- ples from the BNN posterior using mini-batches. However, SGLD and its extensions require stor- age of many copies of the model parameters, a potential
Minimax Concave Penalized Multi-Armed Bandit Model with  High-Dimensional Convariates  Xue Wang * 1 Mike Mingcheng Wei * 2 Tao Yao * 1  Abstract  In this paper, we propose a Minimax Concave Penalized Multi-Armed Bandit (MCP-Bandit) algorithm for a decision-maker facing high- dimensional data with latent sparse structure in an online learning and decision-making process. We demonstrate that the MCP-Bandit algorithm asymptotically achieves the optimal cumulative regret in the sample size T , O(log
Online Convolutional Sparse Coding with Sample-Dependent Dictionary  Yaqing Wang 1 Quanming Yao 1 2 James T. Kwok 1 Lionel M. Ni 3  Abstract  Convolutional sparse coding (CSC) has been popularly used for the learning of shift-invariant dictionaries in image and signal processing. How- ever, existing methods have limited scalability. In this paper, instead of convolving with a dictionary shared by all samples, we propose the use of a sample-dependent dictionary in which each ﬁlter is a linear com
Stein Variational Message Passing for Continuous Graphical Models  Dilin Wang* 1 Zhe Zeng* 2 Qiang Liu 1  Abstract  We propose a novel distributed inference al- gorithm for continuous graphical models, by extending Stein variational gradient descent (SVGD) (Liu & Wang, 2016) to leverage the Markov dependency structure of the distribution of interest. Our approach combines SVGD with a set of structured local kernel functions deﬁned on the Markov blanket of each node, which al- leviates the curse 
Approximate Leave-One-Out for Fast Parameter Tuning in High Dimensions  Shuaiwen Wang * 1 Wenda Zhou * 1 Haihao Lu 2 Arian Maleki 1 Vahab Mirrokni 3  Abstract  (cid:80)n j=1(cid:96)(x(cid:62)  Consider the following class of leaning schemes:  ˆβ := arg min  β  j β; yj) + λR(β), (1)  schemes, especially in high-dimensional settings, studies the following optimization problem:  ˆβ := arg min  β  j=1  (cid:96)(x(cid:62)  j β; yj) + λR(β),  (2)  n(cid:88)  where xi ∈ Rp and yi ∈ R denote the ith fea
Curriculum Learning by Transfer Learning: Theory and Experiments with  Deep Networks  Daphna Weinshall 1 Gad Cohen 1 Dan Amir 1  Abstract  We provide theoretical investigation of curricu- lum learning in the context of stochastic gradi- ent descent when optimizing the convex linear regression loss. We prove that the rate of con- vergence of an ideal curriculum learning method is monotonically increasing with the difﬁculty of the examples. Moreover, among all equally dif- ﬁcult points, convergenc
Extracting Automata from Recurrent Neural Networks  Using Queries and Counterexamples  Gail Weiss 1 Yoav Goldberg 2 Eran Yahav 1  Abstract  We present a novel algorithm that uses exact learn- ing and abstraction to extract a deterministic ﬁ- nite automaton describing the state dynamics of a given trained RNN. We do this using Angluin’s L∗ algorithm as a learner and the trained RNN as an oracle. Our technique efﬁciently extracts accurate automata from trained RNNs, even when the state vectors are
LEAPSANDBOUNDS: A Method for Approximately Optimal Algorithm  Conﬁguration  Gell´ert Weisz 1 Andr´as Gy¨orgy 1 2 Csaba Szepesv´ari 1 3  Abstract  We consider the problem of conﬁguring general- purpose solvers to run efﬁciently on problem in- stances drawn from an unknown distribution. The goal of the conﬁgurator is to ﬁnd a conﬁguration that runs fast on average on most instances, and do so with the least amount of total work. It can run a chosen solver on a random instance until the solver ﬁnis
Deep Predictive Coding Network for Object Recognition  Haiguang Wen 1 Kuan Han 1 Junxing Shi 1 Yizhen Zhang 1 Eugenio Culurciello 1 2 Zhongming Liu 1 2  Abstract  Based on the predictive coding theory in neuro- science, we designed a bi-directional and recur- rent neural net, namely deep predictive coding networks (PCN), that has feedforward, feedback, and recurrent connections. Feedback connections from a higher layer carry the prediction of its lower-layer representation; feedforward connec- t
Towards Fast Computation of Certiﬁed Robustness for ReLU Networks  Tsui-Wei Weng * 1 Huan Zhang * 2 Hongge Chen 1 Zhao Song 3 4 Cho-Jui Hsieh 2 Duane Boning 1  Inderjit S. Dhillon 4 Luca Daniel 1  Abstract  Verifying the robustness property of a general Rectiﬁed Linear Unit (ReLU) network is an NP- complete problem. Although ﬁnding the exact minimum adversarial distortion is hard, giving a certiﬁed lower bound of the minimum distortion is possible. Current available methods of computing such a b
Provable Defenses against Adversarial Examples  via the Convex Outer Adversarial Polytope  Eric Wong 1 J. Zico Kolter 2  Abstract  We propose a method to learn deep ReLU-based classiﬁers that are provably robust against norm- bounded adversarial perturbations on the training data. For previously unseen examples, the ap- proach is guaranteed to detect all adversarial ex- amples, though it may ﬂag some non-adversarial examples as well. The basic idea is to consider a convex outer approximation of 
Local Density Estimation in High Dimensions  Xian Wu 1 Moses Charikar 1 Vishnu Natchu 2  Abstract  An important question that arises in the study of high dimensional vector representations learned from data is: given a set D of vectors and a query q, estimate the number of points within a speci- ﬁed distance threshold of q. Our algorithm uses locality sensitive hashing to preprocess the data to accurately and efﬁciently estimate the answers to such questions via an unbiased estimator that uses i
Adaptive Exploration-Exploitation Tradeoff for Opportunistic Bandits  Huasen Wu 1 Xueying Guo 2 Xin Liu 2  Abstract  In this paper, we propose and study opportunis- tic bandits - a new variant of bandits where the regret of pulling a suboptimal arm varies under different environmental conditions, such as net- work load or produce price. When the load/price is low, so is the cost/regret of pulling a sub- optimal arm (e.g., trying a suboptimal network conﬁguration). Therefore, intuitively, we coul
Error Compensated Quantized SGD and its Applications to Large-scale  Distributed Optimization  Jiaxiang Wu 1 Weidong Huang 1 Junzhou Huang 1 Tong Zhang 1  Abstract  Large-scale distributed optimization is of great importance in various applications. For data- parallel based distributed learning, the inter-node gradient communication often becomes the per- formance bottleneck. In this paper, we propose the error compensated quantized stochastic gra- dient descent algorithm to improve the training
Reinforcing Adversarial Robustness using Model Conﬁdence  Induced by Adversarial Training  Xi Wu * 1 Uyeong Jang * 2 Jiefeng Chen 2 Lingjiao Chen 2 Somesh Jha 2  Abstract  In this paper we study leveraging conﬁdence in- formation induced by adversarial training to rein- force adversarial robustness of a given adversar- ially trained model. A natural measure of conﬁ- dence is (cid:107)F (x)(cid:107)∞ (i.e. how conﬁdent F is about its prediction?). We start by analyzing an adver- sarial training f
Discrete-Continuous Mixtures in Probabilistic Programming:  Generalized Semantics and Inference Algorithms  Yi Wu 1 Siddharth Srivastava 2 Nicholas Hay 3 Simon S. Du 4 Stuart Russell 1  Abstract  Despite the recent successes of probabilistic pro- gramming languages (PPLs) in AI applications, PPLs offer only limited support for random vari- ables whose distributions combine discrete and continuous elements. We develop the notion of measure-theoretic Bayesian networks (MTBNs) and use it to provide
Variance Regularized Counterfactual Risk Minimization  via Variational Divergence Minimization  Abstract  Hang Wu 1 May D. Wang 1  Off-policy learning, the task of evaluating and improving policies using historic data collected from a logging policy, is important because on- policy evaluation is usually expensive and has adverse impacts. One of the major challenge of off-policy learning is to derive counterfactual es- timators that also has low variance and thus low generalization error. In this
Deep k-Means: Re-Training and Parameter Sharing with Harder Cluster  Assignments for Compressing Deep Convolutions  Junru Wu 1 Yue Wang 2 Zhenyu Wu 1 Zhangyang Wang 1 Ashok Veeraraghavan 2 Yingyan Lin 2  Abstract  The current trend of pushing CNNs deeper with convolutions has created a pressing demand to achieve higher compression gains on CNNs where convolutions dominate the computation and pa- rameter amount (e.g., GoogLeNet, ResNet and Wide ResNet). Further, the high energy consump- tion of c
Bayesian Quadrature for Multiple Related Integrals  Xiaoyue Xi 1 * Franc¸ois-Xavier Briol 1 2 3 * Mark Girolami 1 2 3  Abstract  Bayesian probabilistic numerical methods are a set of tools providing posterior distributions on the output of numerical methods. The use of these methods is usually motivated by the fact that they can represent our uncertainty due to in- complete/ﬁnite information about the continuous mathematical problem being approximated. In this paper, we demonstrate that this par
Model-Level Dual Learning  Yingce Xia 1 2 Xu Tan 2 Fei Tian 2 Tao Qin 2 Nenghai Yu 1 Tie-Yan Liu 2  Abstract intelligence tasks appear  Many artiﬁcial in dual forms like English↔French translation and speech↔text transformation. Existing dual learn- ing schemes, which are proposed to solve a pair of such dual tasks, explore how to leverage such dualities from data level. In this work, we pro- pose a new learning framework, model-level dual learning, which takes duality of tasks into con- siderat
Dynamical Isometry and a Mean Field Theory of CNNs:  How to Train 10,000-Layer Vanilla Convolutional Neural Networks  Lechao Xiao 1 2 Yasaman Bahri 1 2 Jascha Sohl-Dickstein 1 Samuel S. Schoenholz 1 Jeffrey Pennington 1  Abstract  In recent years, state-of-the-art methods in com- puter vision have utilized increasingly deep con- volutional neural network architectures (CNNs), with some of the most successful models employ- ing hundreds or even thousands of layers. A va- riety of pathologies such
Orthogonality-Promoting Distance Metric Learning: Convex Relaxation and  Theoretical Analysis  Pengtao Xie 1 2 Wei Wu 2 Yichen Zhu 3 Eric P. Xing 1  Abstract  Distance metric learning (DML), which learns a distance metric from labeled “similar” and “dissimilar” data pairs, is widely utilized. Re- cently, several works investigate orthogonality- promoting regularization (OPR), which encour- ages the projection vectors in DML to be close to being orthogonal, to achieve three effects: (1) high bala
Nonoverlap-Promoting Variable Selection  Pengtao Xie 1 2 Hongbao Zhang 1 Yichen Zhu 3 Eric P. Xing 1  Abstract  Variable selection is a classic problem in ma- chine learning (ML), widely used to ﬁnd impor- tant explanatory factors, and improve general- ization performance and interpretability of ML models. In this paper, we consider variable se- lection for models where multiple responses are to be predicted based on the same set of covari- ates. Since each response is relevant to a unique subse
Learning Semantic Representations for Unsupervised Domain Adaptation  Shaoan Xie 1 2 Zibin Zheng 1 2 Liang Chen 1 2 Chuan Chen 1 2  Abstract  It is important to transfer the knowledge from label-rich source domain to unlabeled target do- main due to the expensive cost of manual la- beling efforts. Prior domain adaptation methods address this problem through aligning the glob- al distribution statistics between source domain and target domain, but a drawback of prior meth- ods is that they ignore
Rates of Convergence of Spectral Methods for Graphon Estimation  Jiaming Xu 1  Abstract  approximations of the graphon function f.  This paper studies the problem of estimating the graphon function – a generative mechanism for a class of random graphs that are useful approx- imations to real networks. Speciﬁcally, a graph of n vertices is generated such that each pair of two vertices i and j are connected independently with probability ρn × f (xi, xj), where xi is the unknown d-dimensional label
Learning Registered Point Processes from Idiosyncratic Observations  Hongteng Xu 1 2 Lawrence Carin 1 Hongyuan Zha 3  Abstract  A parametric point process model is developed, with modeling based on the assumption that se- quential observations often share latent phenom- ena, while also possessing idiosyncratic effects. An alternating optimization method is proposed to learn a “registered” point process that accounts for shared structure, as well as “warping” functions that characterize idiosyncr
Representation Learning on Graphs with Jumping Knowledge Networks  Keyulu Xu 1 Chengtao Li 1 Yonglong Tian 1 Tomohiro Sonobe 2  Ken-ichi Kawarabayashi 2 Stefanie Jegelka 1  Abstract  Recent deep learning approaches for representa- tion learning on graphs follow a neighborhood ag- gregation procedure. We analyze some important properties of these models, and propose a strat- egy to overcome those. In particular, the range of “neighboring” nodes that a node’s representation draws from strongly dep
Learning to Explore via Meta-Policy Gradient  Tianbing Xu 1 Qiang Liu 2 Liang Zhao 1 Jian Peng 3  Abstract  The performance of off-policy learning, includ- ing deep Q-learning and deep deterministic policy gradient (DDPG), critically depends on the choice of the exploration strategy. Existing exploration methods are mostly based on adding noises to the on-going actor policy and therefore only explore locally close to what the actor policy dictates. In this work, we develop a simple meta-policy g
Nonparametric Regression with Comparisons:  Escaping the Curse of Dimensionality with Ordinal Information  Yichong Xu 1 Hariank Muthakana 1 Sivaraman Balakrishnan 2 Artur Dubrawski 3 Aarti Singh 1  Abstract  In supervised learning, we leverage a labeled dataset to design methods for function estimation. In many practical situations, we are able to obtain alternative feedback, possibly at a low cost. A broad goal is to understand the usefulness of, and to design algorithms to exploit, this altern
Optimal Tuning for Divide-and-conquer Kernel Ridge Regression with Massive  Data  Ganggang Xu 1 Zuofeng Shang 2 Guang Cheng 3  Abstract  Divide-and-conquer is a powerful approach for large and massive data analysis. In the nonparam- eteric regression setting, although various theoret- ical frameworks have been established to achieve optimality in estimation or hypothesis testing, how to choose the tuning parameter in a practi- cally effective way is still an open problem. In this paper, we propo
Continuous and Discrete-time Accelerated Stochastic Mirror Descent for  Strongly Convex Functions  Pan Xu * 1 Tianhao Wang * 2 Quanquan Gu 1  Abstract  We provide a second-order stochastic differen- tial equation (SDE), which characterizes the continuous-time dynamics of accelerated stochas- tic mirror descent (ASMD) for strongly convex functions. This SDE plays a central role in de- signing new discrete-time ASMD algorithms via numerical discretization and providing neat analy- ses of their con
A Semantic Loss Function for Deep Learning with Symbolic Knowledge  Jingyi Xu 1 Zilu Zhang 2 Tal Friedman 1 Yitao Liang 1 Guy Van den Broeck 1  Abstract  This paper develops a novel methodology for us- ing symbolic knowledge in deep learning. From ﬁrst principles, we derive a semantic loss func- tion that bridges between neural output vectors and logical constraints. This loss function cap- tures how close the neural network is to satis- fying the constraints on its output. An experi- mental eva
Causal Bandits with Propagating Inference  Akihiro Yabe 1 Daisuke Hatano 2 Hanna Sumita 3 Shinji Ito 1 Naonori Kakimura 4 Takuro Fukunaga 2  Ken-ichi Kawarabayashi 5  Abstract  (cid:112)  Bandit is a framework for designing sequential ex- periments, where a learner selects an arm A ∈ A and obtains an observation corresponding to A in each experiment. Theoretically, the tight regret lower-bound for the general bandit is polynomial with respect to the number of arms |A|, and thus, to overcome this
Active Learning with Logged Data  Songbai Yan 1 Kamalika Chaudhuri 1 Tara Javidi 1  Abstract  We consider active learning with logged data, where labeled examples are drawn conditioned on a predetermined logging policy, and the goal is to learn a classiﬁer on the entire population, not just conditioned on the logging policy. Prior work addresses this problem either when only logged data is available, or purely in a controlled ran- dom experimentation setting where the logged data is ignored. In 
Binary Classiﬁcation with Karmic, Threshold-Quasi-Concave Metrics  Bowei Yan 1 Oluwasanmi Koyejo 2 Kai Zhong 1 Pradeep Ravikumar 3  Abstract  Complex performance measures, beyond the pop- ular measure of accuracy, are increasingly being used in the context of binary classiﬁcation. These complex performance measures are typically not even decomposable, that is, the loss evaluated on a batch of samples cannot typically be expressed as a sum or average of losses evaluated at individual samples, whi
Characterizing and Learning Equivalence Classes of Causal DAGs  under Interventions  Karren D. Yang 1 Abigail Katcoff 1 Caroline Uhler 1  Abstract  We consider the problem of learning causal DAGs in the setting where both observational and inter- ventional data is available. This setting is com- mon in biology, where gene regulatory networks can be intervened on using chemical reagents or gene deletions. Hauser & B¨uhlmann (2012) pre- viously characterized the identiﬁability of causal DAGs under
Dependent Relational Gamma Process Models for Longitudinal Networks  Sikun Yang 1 Heinz Koeppl 1  Abstract  A probabilistic framework based on the covariate- dependent relational gamma process is developed to analyze relational data arising from longitu- dinal networks. The proposed framework char- acterizes networked nodes by nonnegative node- group memberships, which allow each node to belong to multiple latent groups simultaneously, and encodes edge probabilities between each pair of nodes us
Goodness-of-Fit Testing for Discrete Distributions via Stein Discrepancy  Jiasen Yang 1 Qiang Liu * 2 Vinayak Rao * 1 Jennifer Neville 1 3  Abstract  Recent work has combined Stein’s method with reproducing kernel Hilbert space theory to de- velop nonparametric goodness-of-ﬁt tests for un- normalized probability distributions. However, the currently available tests apply exclusively to distributions with smooth density functions. In this work, we introduce a kernelized Stein dis- crepancy measur
Mean Field Multi-Agent Reinforcement Learning  Yaodong Yang 1 * Rui Luo 1 * Minne Li 1 Ming Zhou 2 Weinan Zhang 2 Jun Wang 1  Abstract  Existing multi-agent reinforcement learning meth- ods are limited typically to a small number of agents. When the agent number increases largely, the learning becomes intractable due to the curse of the dimensionality and the exponential growth of agent interactions. In this paper, we present Mean Field Reinforcement Learning where the interactions within the po
Yes, but Did It Work?: Evaluating Variational Inference  Yuling Yao 1 Aki Vehtari 2 Daniel Simpson 3 Andrew Gelman 1  Abstract  While it’s always possible to compute a varia- tional approximation to a posterior distribution, it can be difﬁcult to discover problems with this approximation. We propose two diagnostic al- gorithms to alleviate this problem. The Pareto- smoothed importance sampling (PSIS) diagnostic gives a goodness of ﬁt measurement for joint dis- tributions, while simultaneously im
Hierarchical Text Generation and Planning for Strategic Dialogue  Denis Yarats * 1 Mike Lewis * 1  Abstract  End-to-end models for goal-orientated dialogue are challenging to train, because linguistic and strategic aspects are entangled in latent state vec- tors. We introduce an approach to learning rep- resentations of messages in dialogues by max- imizing the likelihood of subsequent sentences and actions, which decouples the semantics of the dialogue utterance from its linguistic realiza- tio
Massively Parallel Algorithms and Hardness for Single-Linkage Clustering  under (cid:96)p Distances  Grigory Yaroslavtsev 1 Adithya Vadapalli 1  Abstract  We present ﬁrst massively parallel (MPC) algo- rithms and hardness of approximation results for computing Single-Linkage Clustering of n input d-dimensional vectors under Hamming, (cid:96)1, (cid:96)2 and (cid:96)∞ distances. All our algorithms run in O(log n) rounds of MPC for any ﬁxed d and achieve (1 + (cid:15))-approximation for all distan
Variable Selection via Penalized Neural Network: a Drop-Out-One Loss  Approach  Mao Ye * 1 Yan Sun * 1  Abstract  We propose a variable selection method for high dimensional regression models, which allows for complex, nonlinear, and high-order interactions among variables. The proposed method approxi- mates this complex system using a penalized neu- ral network and selects explanatory variables by measuring their utility in explaining the variance of the response variable. This measurement is b
Loss Decomposition for Fast Learning in Large Output Spaces  Ian E.H. Yen 1 Satyen Kale 2 Felix X. Yu 2 Dan Holtmann-Rice 2 Sanjiv Kumar 2 Pradeep Ravikumar 1  Abstract  For problems with large output spaces, evalua- tion of the loss function and its gradient are ex- pensive, typically taking linear time in the size of the output space. Recently, methods have been developed to speed up learning via efﬁcient data structures for Nearest-Neighbor Search (NNS) or Maximum Inner-Product Search (MIPS).
Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates  Dong Yin 1 Yudong Chen 2 Kannan Ramchandran 1 Peter Bartlett 1 3  Abstract  In this paper, we develop distributed optimiza- tion algorithms that are provably robust against Byzantine failures—arbitrary and potentially ad- versarial behavior, in distributed computing sys- tems, with a focus on achieving optimal statistical performance. A main result of this work is a sharp analysis of two robust distributed gradient descent
Semi-Implicit Variational Inference  Mingzhang Yin 1 Mingyuan Zhou 2  Abstract  Semi-implicit variational inference (SIVI) is in- troduced to expand the commonly used analytic variational distribution family, by mixing the vari- ational parameter with a ﬂexible distribution. This mixing distribution can assume any density func- tion, explicit or not, as long as independent ran- dom samples can be generated via reparameteri- zation. Not only does SIVI expand the variational family to incorporate 
Disentangled Sequential Autoencoder  Yingzhen Li 1 Stephan Mandt 2  Abstract  We present a VAE architecture for encoding and generating high dimensional sequential data, such as video or audio. Our deep generative model learns a latent representation of the data which is split into a static and dynamic part, allowing us to approximately disentangle latent time-dependent features (dynamics) from features which are pre- served over time (content). This architecture gives us partial control over ge
Probably Approximately Metric-Fair Learning  Guy N. Rothblum * 1 Gal Yona * 1  Abstract  The seminal work of Dwork et al. [ITCS 2012] introduced a metric-based notion of individual fairness: given a task-speciﬁc similarity metric, their notion required that every pair of similar individuals should be treated similarly. In the context of machine learning, however, individual fairness does not generalize from a training set to the underlying population. We show that this can lead to computational 
GAIN: Missing Data Imputation using Generative Adversarial Nets  Jinsung Yoon 1 * James Jordon 2 * Mihaela van der Schaar 1 2 3  Abstract  We propose a novel method for imputing missing data by adapting the well-known Generative Ad- versarial Nets (GAN) framework. Accordingly, we call our method Generative Adversarial Impu- tation Nets (GAIN). The generator (G) observes some components of a real data vector, imputes the missing components conditioned on what is actually observed, and outputs a c
RadialGAN: Leveraging multiple datasets to improve target-speciﬁc predictive  models using Generative Adversarial Networks  Jinsung Yoon 1 James Jordon 2 Mihaela van der Schaar 1 2 3  Abstract  Training complex machine learning models for prediction often requires a large amount of data that is not always readily available. Leveraging these external datasets from related but different sources is therefore an important task if good pre- dictive models are to be built for deployment in settings wh
GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models  Jiaxuan You * 1 Rex Ying * 1 Xiang Ren 2 William L. Hamilton 1 Jure Leskovec 1  Abstract  Modeling and generating graphs is fundamental for studying networks in biology, engineering, and social sciences. However, modeling com- plex distributions over graphs and then efﬁciently sampling from these distributions is challenging due to the non-unique, high-dimensional nature of graphs and the complex, non-local dependencies that
An Efﬁcient Semismooth Newton Based Algorithm for Convex Clustering  Yancheng Yuan 1 Defeng Sun 2 Kim-Chuan Toh 3  Abstract  Clustering is a fundamental problem in unsuper- vised learning. Popular methods like K-means, may suffer from instability as they are prone to get stuck in its local minima. Recently, the sum- of-norms (SON) model (also known as cluster- ing path), which is a convex relaxation of hier- archical clustering model, has been proposed in (Lindsten et al., 2011) and (Hocking et 
A Conditional Gradient Framework for Composite Convex Minimization  with Applications to Semideﬁnite Programming  Alp Yurtsever 1 Olivier Fercoq 2 Francesco Locatello 3 4 Volkan Cevher 1  Abstract  We propose a conditional gradient framework for a composite convex minimization template with broad applications. Our approach combines smoothing and homotopy techniques under the √ CGM framework, and provably achieves the opti- mal O(1/ k) convergence rate. We demonstrate that the same rate holds if 
Policy Optimization as Wasserstein Gradient Flows  Ruiyi Zhang 1 Changyou Chen 2 Chunyuan Li 1 Lawrence Carin 1  Abstract  Policy optimization is a core component of rein- forcement learning (RL), and most existing RL methods directly optimize parameters of a policy based on maximizing the expected total reward, or its surrogate. Though often achieving encour- aging empirical success, its underlying mathemat- ical principle on policy-distribution optimization is unclear. We place policy optimiza
Problem Dependent Reinforcement Learning Bounds Which Can Identify  Bandit Structure in MDPs  Andrea Zanette 1 Emma Brunskill 1  Abstract  In order to make good decision under uncertainty an agent must learn from observations. To do so, two of the most common frameworks are Contextual Bandits and Markov Decision Pro- cesses (MDPs). In this paper, we study whether there exist algorithms for the more general frame- work (MDP) which automatically provide the best performance bounds for the speciﬁc 
Fast and Sample Efﬁcient Inductive Matrix Completion via Multi-Phase  Procrustes Flow  Xiao Zhang * 1 Simon S. Du * 2 Quanquan Gu 3  Abstract  We revisit the inductive matrix completion prob- lem that aims to recover a rank-r matrix with ambient dimension d given n features as the side prior information. The goal is to make use of the known n features to reduce sample and compu- tational complexities. We present and analyze a new gradient-based non-convex optimization algorithm that converges to
Large-Scale Sparse Inverse Covariance Estimation via Thresholding and  Max-Det Matrix Completion  Richard Y. Zhang 1 Salar Fattahi 1 Somayeh Sojoudi 2  Abstract  The sparse inverse covariance estimation prob- lem is commonly solved using an (cid:96)1-regularized Gaussian maximum likelihood estimator known as “graphical lasso”, but its computational cost becomes prohibitive for large data sets. A re- cent line of results showed–under mild assump- tions–that the graphical lasso estimator can be re
High Performance Zero-Memory Overhead Direct Convolutions  Jiyuan Zhang 1 Franz Franchetti 1 Tze Meng Low 1  Abstract  The computation of convolution layers in deep neural networks typically rely on high perfor- mance routines that trade space for time by using additional memory (either for packing purposes or required as part of the algorithm) to improve per- formance. The problems with such an approach are two-fold. First, these routines incur additional memory overhead which reduces the overa
Safe Element Screening for Submodular Function Minimization  Weizhong Zhang 1 Bin Hong 2 Lin Ma 1 Wei Liu 1 Tong Zhang 1  Abstract  Submodular functions are discrete analogs of con- vex functions, which have applications in various ﬁelds, including machine learning and computer vision. However, in large-scale applications, solv- ing Submodular Function Minimization (SFM) problems remains challenging. In this paper, we make the ﬁrst attempt to extend the emerging tech- nique named screening in la
Improving the Privacy and Accuracy of ADMM-Based Distributed Algorithms  Xueru Zhang 1 Mohammad Mahdi Khalili 1 Mingyan Liu 1  Abstract  Alternating direction method of multiplier (ADMM) is a popular method used to design dis- tributed versions of a machine learning algorithm, whereby local computations are performed on lo- cal data with the output exchanged among neigh- bors in an iterative fashion. During this iterative process the leakage of data privacy arises. A dif- ferentially private ADM
Stabilizing Gradients for Deep Neural Networks via Efﬁcient SVD  Parameterization  Jiong Zhang 1 Qi Lei 1 Inderjit S. Dhillon 1 2  Abstract  Vanishing and exploding gradients are two of the main obstacles in training deep neural networks, especially in capturing long range dependencies in recurrent neural networks (RNNs). In this pa- per, we present an efﬁcient parametrization of the transition matrix of an RNN that allows us to stabilize the gradients that arise in its training. Speciﬁcally, we
Learning Long Term Dependencies via Fourier Recurrent Units∗  Jiong Zhang 1 Yibo Lin 2 Zhao Song 3 Inderjit S. Dhillon 4  Abstract  It is a known fact that training recurrent neural networks for tasks that have long term depen- dencies is challenging. One of the main reasons is the vanishing or exploding gradient problem, which prevents gradient information from prop- agating to early layers. In this paper we propose a simple recurrent architecture, the Fourier Re- current Unit (FRU), that stabi
Tropical Geometry of Deep Neural Networks  Liwen Zhang 1 Gregory Naitzat 2 Lek-Heng Lim 2 3  Abstract  We establish, for the ﬁrst time, explicit connec- tions between feedforward neural networks with ReLU activation and tropical geometry — we show that the family of such neural networks is equivalent to the family of tropical rational maps. Among other things, we deduce that feedforward ReLU neural networks with one hidden layer can be characterized by zonotopes, which serve as building blocks f
Deep Bayesian Nonparametric Tracking  Aonan Zhang 1 John Paisley 1  Abstract  Time-series data often exhibit irregular behavior, making them hard to analyze and explain with a simple dynamic model. For example, information in social networks may show change-point-like bursts that then diffuse with smooth dynamics. Powerful models such as deep neural networks learn smooth functions from data, but are not as well-suited in off-the-shelf form for discov- ering and explaining sparse, discrete and bu
Composable Planning with Attributes  Amy Zhang * 1 Adam Lerer * 1 Sainbayar Sukhbaatar 2 Rob Fergus 1 2 Arthur Szlam 1  Abstract  The tasks that an agent will need to solve often are not known during training. However, if the agent knows which properties of the environment are important then, after learning how its actions affect those properties, it may be able to use this knowledge to solve complex tasks without train- ing speciﬁcally for them. Towards this end, we consider a setup in which an
Noisy Natural Gradient as Variational Inference  Guodong Zhang * 1 2 Shengyang Sun * 1 2 David Duvenaud 1 2 Roger Grosse 1 2  Abstract  Variational Bayesian neural nets combine the ﬂex- ibility of deep learning with Bayesian uncertainty estimation. Unfortunately, there is a tradeoff between cheap but simple variational families (e.g. fully factorized) or expensive and compli- cated inference procedures. We show that natural gradient ascent with adaptive weight noise implic- itly ﬁts a variationa
A Primal-Dual Analysis of Global Optimality in Nonconvex Low-Rank  Matrix Recovery  Xiao Zhang * 1 Lingxiao Wang * 2 Yaodong Yu 1 Quanquan Gu 2  Abstract  We propose a primal-dual based framework for analyzing the global optimality of nonconvex low- rank matrix recovery. Our analysis are based on the restricted strongly convex and smooth con- ditions, which can be veriﬁed for a broad fam- ily of loss functions. In addition, our analytic framework can directly handle the widely-used incoherence c
Fully Decentralized Multi-Agent  Reinforcement Learning with Networked Agents  Kaiqing Zhang 1 Zhuoran Yang 2 Han Liu 3 Tong Zhang 4 Tamer Bas¸ar 1  Abstract  We consider the fully decentralized multi-agent reinforcement learning (MARL) problem, where the agents are connected via a time-varying and possibly sparse communication network. Speciﬁ- cally, we assume that the reward functions of the agents might correspond to different tasks, and are only known to the corresponding agent. More- over, 
Dynamic Regret of Strongly Adaptive Methods  Lijun Zhang 1 Tianbao Yang 2 Rong Jin 3 Zhi-Hua Zhou 1  Abstract  To cope with changing environments, recent de- velopments in online learning have introduced the concepts of adaptive regret and dynamic re- gret independently. In this paper, we illustrate an intrinsic connection between these two con- cepts by showing that the dynamic regret can be expressed in terms of the adaptive regret and the functional variation. This observation implies that st
Inter and Intra Topic Structure Learning with Word Embeddings  He Zhao 1 Lan Du 1 Wray Buntine 1 Mingyaun Zhou 2  Abstract  One important task of topic modeling for text anal- ysis is interpretability. By discovering structured topics one is able to yield improved interpretabil- ity as well as modeling accuracy. In this paper, we propose a novel topic model with a deep struc- ture that explores both inter-topic and intra-topic structures informed by word embeddings. Speciﬁ- cally, our model disc
Adversarially Regularized Autoencoders  Jake (Junbo) Zhao * 1 2 Yoon Kim * 3 Kelly Zhang 1 Alexander M. Rush 3 Yann LeCun 1 2  Abstract  Deep latent variable models, trained using varia- tional autoencoders or generative adversarial net- works, are now a key technique for representa- tion learning of continuous structures. However, applying similar methods to discrete structures, such as text sequences or discretized images, has proven to be more challenging. In this work, we propose a ﬂexible m
MSplit LBI: Realizing Feature Selection and Dense Estimation Simultaneously  in Few-shot and Zero-shot Learning  Bo Zhao † 1 Xinwei Sun † 2 Yanwei Fu ‡ 3 4 Yuan Yao ‡ 5 Yizhou Wang 1  Abstract  It is one typical and general topic of learn- ing a good embedding model to efﬁciently learn the representation coefﬁcients between two spaces/subspaces. To solve this task, L1 regular- ization is widely used for the pursuit of feature selection and avoiding overﬁtting, and yet the sparse estimation of fe
Composite Marginal Likelihood Methods for Random Utility Models  Zhibing Zhao 1 Lirong Xia 1  Abstract  We propose a novel and ﬂexible rank-breaking- then-composite-marginal-likelihood (RBCML) framework for learning random utility models (RUMs), which include the Plackett-Luce model. We characterize conditions for the objective func- tion of RBCML to be strictly log-concave by prov- ing that strict log-concavity is preserved under convolution and marginalization. We characterize necessary and su
Lightweight Stochastic Optimization  for Minimizing Finite Sums with Inﬁnite Data  Shuai Zheng 1 James T. Kwok 1  Abstract  Variance reduction has been commonly used in stochastic optimization. It relies crucially on the assumption that the data set is ﬁnite. However, when the data are imputed with random noise as in data augmentation, the perturbed data set be- comes essentially inﬁnite. Recently, the stochas- tic MISO (S-MISO) algorithm is introduced to address this expected risk minimization 
A Robust Approach to Sequential Information Theoretic Planning  Sue Zheng 1 Jason Pacheco 1 John W. Fisher, III 1  Abstract  In many sequential planning applications a nat- ural approach to generating high quality plans is to maximize an information reward such as mu- tual information (MI). Unfortunately, MI lacks a closed form in all but trivial models, and so must be estimated. In applications where the cost of plan execution is expensive, one desires plan- ning estimates which admit theoretic
Revealing Common Statistical Behaviors in Heterogeneous Populations  Andrey Zhitnikov 1 Rotem Mulayoff 1 Tomer Michaeli 1  Abstract  In many areas of neuroscience and biological data analysis, it is desired to reveal common patterns among a group of subjects. Such analyses play im- portant roles e.g., in detecting functional brain net- works from fMRI scans and in identifying brain regions which show increased activity in response to certain stimuli. Group level techniques usu- ally assume that 
Understanding Generalization and Optimization Performance of Deep CNNs  Pan Zhou 1 Jiashi Feng 1  Abstract  This work aims to provide understandings on the remarkable success of deep convolutional neu- ral networks (CNNs) by theoretically analyzing their generalization performance and establish- ing optimization guarantees for gradient descent based training algorithms. Speciﬁcally, for a CNN model consisting of l convolutional layers and one fully connected layer, we prove that its general-  iz
Distributed Asynchronous Optimization with Unbounded Delays:  How Slow Can You Go?  Zhengyuan Zhou1 Panayotis Mertikopoulos2 Nicholas Bambos1 Peter Glynn1 Yinyu Ye1 Li-Jia Li3 Li Fei-Fei1 3  Abstract  One of the most widely used training meth- ods for large-scale machine learning problems is distributed asynchronous stochastic gradient descent (DASGD). However, a key issue in its implementation is that of delays: when a “worker” node asynchronously contributes a gradient up- date to the “master”
A Simple Stochastic Variance Reduced Algorithm with Fast Convergence Rates  Kaiwen Zhou 1 Fanhua Shang 2 James Cheng 1  Abstract  Recent years have witnessed exciting progress in the study of stochastic variance reduced gradient methods (e.g., SVRG, SAGA), their accelerated variants (e.g, Katyusha) and their extensions in many different settings (e.g., online, sparse, asyn- chronous, distributed). Among them, accelerated methods enjoy improved convergence rates but have complex coupling structur
Stochastic Variance-Reduced Cubic Regularized Newton Methods  Dongruo Zhou 1 Pan Xu 1 Quanquan Gu 1  Abstract  We propose a stochastic variance-reduced cu- bic regularized Newton method (SVRC) for non- convex optimization. At the core of our algo- rithm is a novel semi-stochastic gradient along with a semi-stochastic Hessian, which are specif- ically designed for cubic regularization method. We show that our algorithm is guaranteed to con- verge to an (✏,p✏)-approximate local minimum  within eO(
Racing Thompson: an Efﬁcient Algorithm for Thompson Sampling with  Non-conjugate Priors  Yichi Zhou 1 Jun Zhu 1 Jingwe Zhuo 1  Abstract  Thompson sampling has impressive empirical per- formance for many multi-armed bandit problems. But current algorithms for Thompson sampling only work for the case of conjugate priors since they require to perform online Bayesian posterior inference, which is a difﬁcult task when the prior is not conjugate. In this paper, we propose a novel algorithm for Thompso
DistributedNonparametricRegressionunderCommunicationConstraintsYuanchengZhu1JohnLafferty2AbstractThispaperstudiestheproblemofnonparametricestimationofasmoothfunctionwithdatadis-tributedacrossmultiplemachines.Weassumeanindependentsamplefromawhitenoisemodeliscollectedateachmachine,andanestimatoroftheunderlyingtruefunctionneedstobeconstructedatacentralmachine.Weplacelimitsonthenum-berofbitsthateachmachinecanusetotransmitinformationtothecentralmachine.Ourresultsgivebothasymptoticlowerboundsandmatchi
Message Passing Stein Variational Gradient Descent  Jingwei Zhuo 1 Chang Liu 1 Jiaxin Shi 1 Jun Zhu 1 Ning Chen 1 Bo Zhang 1  Abstract  Stein variational gradient descent (SVGD) is a re- cently proposed particle-based Bayesian inference method, which has attracted a lot of interest due to its remarkable approximation ability and particle efﬁciency compared to traditional variational in- ference and Markov Chain Monte Carlo methods. However, we observed that particles of SVGD tend to collapse to 
Stochastic Variance-Reduced Hamilton Monte Carlo Methods  Difan Zou * 1 Pan Xu * 1 Quanquan Gu 1  Abstract  We propose a fast stochastic Hamilton Monte Carlo (HMC) method, for sampling from a smooth and strongly log-concave distribution. At the core of our proposed method is a vari- ance reduction technique inspired by the recent advance in stochastic optimization. We show that, to achieve ✏ accuracy in 2-Wasserstein dis-  tance, our algorithm achieves eOn + 2d1/2/✏ + 4/3d1/3n2/3/✏2/3 gradient
Hierarchical Long-term Video Prediction without Supervision  Nevan Wichers 1 Ruben Villegas 2 Dumitru Erhan 1 Honglak Lee 1  Abstract  Much of recent research has been devoted to video prediction and generation, yet most of the previ- ous works have demonstrated only limited success in generating videos on short-term horizons. The hierarchical video prediction method by Villegas et al. (2017b) is an example of a state-of-the-art method for long-term video prediction, but their method is limited 
Learning Local Invariant Mahalanobis Distances  Ethan Fetaya Weizmann Institute of science Shimon Ullman Weizmann Institute of science  Abstract  For many tasks and data types, there are natu- ral transformations to which the data should be invariant or insensitive. For instance, in visual recognition, natural images should be insensi- tive to rotation and translation. This require- ment and its implications have been important in many machine learning applications, and tol- erance for image tra
A Relative Exponential Weighing Algorithm for Adversarial Utility-based  Dueling Bandits  Pratik Gajane Tanguy Urvoy Fabrice Cl´erot Orange-labs, Lannion, France  PRATIK.GAJANE@ORANGE.COM TANGUY.URVOY@ORANGE.COM FABRICE.CLEROT@ORANGE.COM  Abstract  We study the K-armed dueling bandit problem which is a variation of the classical Multi-Armed Bandit (MAB) problem in which the learner re- ceives only relative feedback about the selected pairs of arms. We propose an efﬁcient algorithm called Relativ
Paired-Dual Learning for Fast Training of Latent Variable Hinge-Loss MRFs  Stephen H. Bach∗ Bert Huang∗ Jordan Boyd-Graber Lise Getoor ∗ Equal contributors.  Abstract  Latent variables allow probabilistic graphical models to capture nuance and structure in im- portant domains such as network science, natural language processing, and computer vision. Naive approaches to learning such complex models can be prohibitively expensive—because they require repeated inferences to update beliefs about la-
Tracking Approximate Solutions of Parameterized Optimization Problems  over Multi-Dimensional (Hyper-)Parameter Domains  Katharina Blechschmidt Joachim Giesen S¨oren Laue Friedrich-Schiller-Universit¨at Jena, Germany  KATHI.JENA@WEB.DE JOACHIM.GIESEN@UNI-JENA.DE SOEREN.LAUE@UNI-JENA.DE  Abstract  Many machine learning methods are given as pa- rameterized optimization problems. Important examples of such parameters are regularization- and kernel hyperparameters. These parameters have to be tuned 
How Can Deep Rectiﬁer Networks Achieve Linear Separability and  Preserve Distances?  Senjian An School of Computer Science and Software Engineering, The University of Western Australia, Australia Farid Boussaid School of Electrical, Electronic and Computer Engineering, The University of Western Australia, Australia Mohammed Bennamoun School of Computer Science and Software Engineering, The University of Western Australia, Australia  MOHAMMED.BENNAMOUN@UWA.EDU.AU  FARID.BOUSSAID@UWA.EDU.AU  SENJI
Faster Cover Trees  Mike Izbicki Christian R. Shelton University of California Riverside, 900 University Ave, Riverside, CA 92521  MIZBI001@UCR.EDU CSHELTON@CS.UCR.EDU  Abstract  est neighbor of p in X is deﬁned as  The cover tree data structure speeds up exact nearest neighbor queries over arbitrary metric spaces (Beygelzimer et al., 2006). This paper makes cover trees even faster. In particular, we provide  1. A simpler deﬁnition of the cover tree that reduces the number of nodes from O(n) to 
Non-Linear Cross-Domain Collaborative Filtering via Hyper-Structure  Transfer  Yan-Fu Liu National Tsing Hua University, ROC Cheng-Yu Hsu National Tsing Hua University, ROC Shan-Hung Wu National Tsing Hua University, ROC  YFLIU@NETDB.CS.NTHU.EDU.TW  CYHSU@NETDB.CS.NTHU.EDU.TW  SHWU@CS.NTHU.EDU.TW  Abstract  The Cross Domain Collaborative Filtering (CDCF) exploits the rating matrices from mul- tiple domains to make better recommendations. Existing CDCF methods adopt the sub-structure sharing tech
Modeling Order in Neural Word Embeddings at Scale  5 1 0 2     n u J    1 1      ] L C . s c [      1 3 1 7 7 2 1 / t i  m b u s : v i X r a  Andrew Trask Digital Reasoning Systems, Inc., Nashville, TN USA David Gilmore Digital Reasoning Systems, Inc., Nashville, TN USA Matthew Russell Digital Reasoning Systems, Inc., Nashville, TN USA  ANDREW.TRASK@DIGITALREASONING.COM  DAVID.GILMORE@DIGITALREASONING.COM  MATTHEW.RUSSELL@DIGITALREASONING.COM  Abstract  Natural Language Processing (NLP) systems 
Multi-instance multi-label learning in the presence of novel class instances  Anh T. Pham Raviv Raich Xiaoli Z. Fern School of Electrical Engineering and Computer Science, Corvallis, OR 97330-5501 USA Jes´us P´erez Arriaga Departamento de Ingenier´ıa de Comunicaciones, Universidad de Cantabria, 39005 Santander, Spain  PHAMAN@EECS.OREGONSTATE.EDU RAICH@EECS.OREGONSTATE.EDU XFERN@EECS.OREGONSTATE.EDU  JPEREZ@GTAS.DICOM.UNICAN.ES  Abstract  Multi-instance multi-label learning (MIML) is a framework 
AReS and MaRS - Adversarial and MMD-Minimizing Regression for SDEs  Gabriele Abbati * 1 Philippe Wenk * 2 3 Michael A Osborne 1 Andreas Krause 2 Bernhard Sch¨olkopf 4  Stefan Bauer 4  Abstract  paper, we use exclusively the Itˆo-form  Stochastic differential equations are an important modeling class in many disciplines. Consequently, there exist many methods relying on various dis- cretization and numerical integration schemes. In this paper, we propose a novel, probabilistic model for estimatin
Dynamic Weights in Multi-Objective Deep Reinforcement Learning  Axel Abels 1 2 Diederik M. Roijers 3 Tom Lenaerts 1 2 Ann Now´e 2 Denis Steckelmacher 2  Abstract  Many real world decision problems are charac- terized by multiple conﬂicting objectives which must be balanced based on their relative impor- tance. In the dynamic weights setting the rela- tive importance changes over time and special- ized algorithms that deal with such change, such as the tabular Reinforcement Learning (RL) al- gori
MixHop: Higher-Order Graph Convolutional Architectures  via Sparsiﬁed Neighborhood Mixing  Sami Abu-El-Haija 1 Bryan Perozzi 2 Amol Kapoor 2 Nazanin Alipourfard 1 Kristina Lerman 1  Hrayr Harutyunyan 1 Greg Ver Steeg 1 Aram Galstyan 1  Abstract  Existing popular methods for semi-supervised learning with Graph Neural Networks (such as the Graph Convolutional Network) provably cannot learn a general class of neighborhood mixing rela- tionships. To address this weakness, we propose a new model, Mix
Communication-Constrained Inference and the Role of Shared Randomness  Jayadev Acharya 1 Clément Canonne 2 Himanshu Tyagi 3  Abstract  A central server needs to perform statistical in- ference based on samples that are distributed over multiple users who can each send a mes- sage of limited length to the center. We study problems of distribution learning and identity test- ing in this distributed inference setting and exam- ine the role of shared randomness as a resource. We propose a general pu
Distributed Learning with Sublinear Communication  Jayadev Acharya 1 Christopher De Sa 1 Dylan J. Foster 2 Karthik Sridharan 1  Abstract  In distributed statistical learning, N samples are split across m machines and a learner wishes to use minimal communication to learn as well as if the examples were on a single machine. This model has received substantial interest in machine learning due to its scalability and potential for parallel speedup. However, in high-dimensional settings, where the nu
Communication Complexity in Locally Private Distribution Estimation and  Heavy Hitters  Jayadev Acharya 1 Ziteng Sun 1  Abstract  We consider the problems of distribution esti- mation, and heavy hitter (frequency) estimation under privacy, and communication constraints. While the constraints have been studied separately, optimal schemes for one are sub-optimal for the other. We propose a sample-optimal ε-locally dif- ferentially private (LDP) scheme for distribution estimation, where each user c
Learning Models from Data with Measurement Error:  Tackling Underreporting  Roy Adams 1 Yuelong Ji 2 Xiaobin Wang 2 Suchi Saria 1 3 4  Abstract  Measurement error in observational datasets can lead to systematic bias in inferences based on these datasets. As studies based on observational data are increasingly used to inform decisions with real-world impact, it is critical that we de- velop a robust set of techniques for analyzing and adjusting for these biases. In this paper we present a method
TibGM: A Transferable and Information-Based Graphical Model  Approach for Reinforcement Learning  Tameem Adel 1 Adrian Weller 1 2  Abstract  One of the challenges to reinforcement learning (RL) is scalable transferability among complex tasks. Incorporating a graphical model (GM), along with the rich family of related methods, as a basis for RL frameworks provides potential to address issues such as transferability, generalisa- tion and exploration. Here we propose a ﬂexible GM-based RL framework
PAC Learnability of Node Functions in Networked Dynamical Systems  Abhijin Adiga 1 Chris J. Kuhlman 1 Madhav V. Marathe 1 2 S. S. Ravi 1 3 Anil K. Vullikanti 1 2  Abstract  We consider the PAC learnability of the functions at the nodes of a discrete networked dynamical system, assuming that the underlying network is known. We provide tight bounds on the sample complexity of learning threshold functions. We establish a computational intractability result for efﬁcient PAC learning of such function
Static Automatic Batching in TensorFlow  Ashish Agarwal 1  Abstract  Dynamic neural networks are becoming increas- ingly common, and yet it is hard to implement them efﬁciently. On-the-ﬂy operation batching for such models is sub-optimal and suffers from run time overheads, while writing manually batched versions can be hard and error-prone. To ad- dress this, we extend TensorFlow with pfor, a parallel-for loop optimized using static loop vec- torization. With pfor, users can express computa- ti
Efﬁcient Full-Matrix Adaptive Regularization  Naman Agarwal 1 Brian Bullins 1 2 Xinyi Chen 1 Elad Hazan 1 2 Karan Singh 1 2 Cyril Zhang 1 2 Yi Zhang 1 2  Abstract  Adaptive regularization methods pre-multiply a descent direction by a preconditioning matrix. Due to the large number of parameters of ma- chine learning problems, full-matrix precondi- tioning methods are prohibitively expensive. We show how to modify full-matrix adaptive reg- ularization in order to make it practical and effective. 
Online Control with Adversarial Disturbances  Naman Agarwal 1 Brian Bullins 2 1 Elad Hazan 2 1 Sham M. Kakade 3 4 1 Karan Singh 2 1  Abstract  We study the control of linear dynamical systems with adversarial disturbances, as opposed to statis- tical noise. We present an efﬁcient algorithm that achieves nearly-tight regret bounds in this setting. Our result generalizes upon previous work in two main aspects: the algorithm can accommodate adversarial noise in the dynamics, and can handle general 
Fair Regression: Quantitative Deﬁnitions and Reduction-based Algorithms  Alekh Agarwal 1 Miroslav Dud´ık 2 Zhiwei Steven Wu 3  Abstract  In this paper, we study the prediction of a real- valued target, such as a risk score or recidivism rate, while guaranteeing a quantitative notion of fairness with respect to a protected attribute such as gender or race. We call this class of problems fair regression. We propose general schemes for fair regression under two notions of fairness: (1) statistical 
Learning to Generalize from Sparse and Underspeciﬁed Rewards  Rishabh Agarwal 1 * Chen Liang 1 Dale Schuurmans 1 2 Mohammad Norouzi 1  Abstract  We consider the problem of learning from sparse and underspeciﬁed rewards, where an agent re- ceives a complex input, such as a natural language instruction, and needs to generate a complex re- sponse, such as an action sequence, while only receiving binary success-failure feedback. Such success-failure rewards are often underspeciﬁed: they do not disti
The Kernel Interaction Trick: Fast Bayesian Discovery of Pairwise  Interactions in High Dimensions  Raj Agrawal 1 Jonathan H. Huggins 2 Brian L. Trippe 1 Tamara Broderick 1  Abstract  Discovering interaction effects on a response of interest is a fundamental problem faced in biol- ogy, medicine, economics, and many other sci- entiﬁc disciplines. In theory, Bayesian meth- ods for discovering pairwise interactions enjoy many beneﬁts such as coherent uncertainty quan- tiﬁcation, the ability to inco
Understanding the Impact of Entropy on Policy Optimization  Zafarali Ahmed 1 2 Nicolas Le Roux 1 3 Mohammad Norouzi 3 Dale Schuurmans 3 4  Abstract  Entropy regularization is commonly used to im- prove policy optimization in reinforcement learn- ing. It is believed to help with exploration by encouraging the selection of more stochastic poli- cies. In this work, we analyze this claim using new visualizations of the optimization landscape based on randomly perturbing the loss function. We ﬁrst sh
Fairwashing: the risk of rationalization  Ulrich A¨ıvodji 1 Hiromi Arai 2 3 Olivier Fortineau 4 S´ebastien Gambs 1 Satoshi Hara 5 Alain Tapp 6 7  Abstract  Black-box explanation is the problem of explain- ing how a machine learning model – whose in- ternal logic is hidden to the auditor and gen- erally complex – produces its outcomes. Cur- rent approaches for solving this problem include model explanation, outcome explanation as well as model inspection. While these techniques can be beneﬁcial b
Adaptive Stochastic Natural Gradient Method for One-Shot Neural  Architecture Search  Youhei Akimoto * 1 Shinichi Shirakawa * 2 Nozomu Yoshinari 2 Kento Uchida 2 Shota Saito 2 3 Kouhei Nishida 4  Abstract  High sensitivity of neural architecture search (NAS) methods against their input such as step- size (i.e., learning rate) and search space prevents practitioners from applying them out-of-the-box to their own problems, albeit its purpose is to au- tomate a part of tuning process. Aiming at a f
Projections for Approximate Policy Iteration Algorithms  Riad Akrour 1 Joni Pajarinen 1 2 Gerhard Neumann 3 4 Jan Peters 1 5  Abstract  Approximate policy iteration is a class of rein- forcement learning (RL) algorithms where the policy is encoded using a function approximator and which has been especially prominent in RL with continuous action spaces. In this class of RL algorithms, ensuring increase of the policy return during policy update often requires to con- strain the change in action di
Validating Causal Inference Models via Inﬂuence Functions  Ahmed M. Alaa 1 Mihaela van der Schaar 1 2 3  Abstract  The problem of estimating causal effects of treat- ments from observational data falls beyond the realm of supervised learning — because counter- factual data is inaccessible, we can never observe the true causal effects. In the absence of “super- vision”, how can we evaluate the performance of causal inference methods? In this paper, we use inﬂuence functions — the functional deriv
Multi-objective training of Generative Adversarial Networks  with multiple discriminators  Isabela Albuquerque * 1 Jo˜ao Monteiro * 1 Thang Doan 2 Breandan Considine 3 Tiago Falk 1  Ioannis Mitliagkas 3  Abstract  Recent literature has demonstrated promising re- sults for training Generative Adversarial Net- works by employing a set of discriminators, in contrast to the traditional game involving one gen- erator against a single adversary. Such methods perform single-objective optimization on so
Graph Element Networks: adaptive, structured computation and memory  Ferran Alet 1 Adarsh K. Jeewajee 1 Maria Bauza 2  Alberto Rodriguez 2 Tom´as Lozano-P´erez 1 Leslie Pack Kaelbling 1  Abstract  We explore the use of graph neural networks (GNNs) to model spatial processes in which there is no a priori graphical structure. Similar to ﬁnite element analysis, we assign nodes of a GNN to spatial locations and use a computational process deﬁned on the graph to model the relationship between an init
Analogies Explained: Towards Understanding Word Embeddings  Carl Allen 1 Timothy Hospedales 1  Abstract  Word embeddings generated by neural network methods such as word2vec (W2V) are well known to exhibit seemingly linear behaviour, e.g. the embeddings of analogy “woman is to queen as man is to king” approximately describe a paral- lelogram. This property is particularly intriguing since the embeddings are not trained to achieve it. Several explanations have been proposed, but each introduces a
Inﬁnite Mixture Prototypes for Few-Shot Learning  Kelsey R. Allen 1 Evan Shelhamer * 2 Hanul Shin * 1 Joshua B. Tenenbaum 1  Abstract  We propose inﬁnite mixture prototypes to adap- tively represent both simple and complex data dis- tributions for few-shot learning. Inﬁnite mixture prototypes combine deep representation learning with Bayesian nonparametrics, representing each class by a set of clusters, unlike existing proto- typical methods that represent each class by a single cluster. By infe
A Convergence Theory for Deep Learning via Over-Parameterization  Zeyuan Allen-Zhu * 1 Yuanzhi Li * 2 3 Zhao Song * 4 5 6  Abstract  Deep neural networks (DNNs) have demon- strated dominating performance in many ﬁelds; since AlexNet, networks used in practice are go- ing wider and deeper. On the theoretical side, a long line of works have been focusing on why we can train neural networks when there is only one hidden layer. The theory of multi-layer net- works remains unsettled. In this work, we
Asynchronous Batch Bayesian Optimisation with Improved Local Penalisation  Ahsan S. Alvi * 1 2 Binxin Ru * 1 Jan Calliess 1 3 Stephen J. Roberts 1 2 3 Michael A. Osborne 1 2  Abstract  Batch Bayesian optimisation (BO) has been suc- cessfully applied to hyperparameter tuning us- ing parallel computing, but it is wasteful of re- sources: workers that complete jobs ahead of others are left idle. We address this problem by developing an approach, Penalising Locally for Asynchronous Bayesian Optimisa
Bounding User Contributions:  A Bias-Variance Trade-off in Differential Privacy  Kareem Amin 1 Alex Kulesza 1 Andres Mu˜noz Medina 1 Sergei Vassilvitskii 1  Abstract  Differentially private learning algorithms protect individual participants in the training dataset by guaranteeing that their presence does not signif- icantly change the resulting model. In order to make this promise, such algorithms need to know the maximum contribution that can be made by a single user: the more data an individu
Explaining Deep Neural Networks with a Polynomial Time Algorithm for  Shapley Values Approximation  Marco Ancona 1 Cengiz ¨Oztireli 2 Markus Gross 1 2  Abstract  The problem of explaining the behavior of deep neural networks has recently gained a lot of atten- tion. While several attribution methods have been proposed, most come without strong theoretical foundations, which raises questions about their reliability. On the other hand, the literature on cooperative game theory suggests Shapley val
Scaling Up Ordinal Embedding: A Landmark Approach  Jesse Anderton 1 Javed Aslam 1  Abstract  Ordinal Embedding is the problem of placing n objects into Rd to satisfy constraints like “object a is closer to b than to c.” It can accommodate data that embeddings from features or distances cannot, but is a more difﬁcult problem. We pro- pose a novel landmark-based method as a partial solution. At small to medium scales, we present a novel combination of existing methods with some new theoretical jus
Sorting Out Lipschitz Function Approximation  Cem Anil * 1 2 James Lucas * 1 2 Roger Grosse 1 2  Abstract  Training neural networks under a strict Lipschitz constraint is useful for provable adversarial ro- bustness, generalization bounds, interpretable gra- dients, and Wasserstein distance estimation. By the composition property of Lipschitz functions, it sufﬁces to ensure that each individual afﬁne trans- formation or nonlinear activation is 1-Lipschitz. The challenge is to do this while maint
Sparse Multi-Channel Variational Autoencoder for the Joint Analysis of Heterogeneous Data  Luigi Antelmi 1 Nicholas Ayache 1 Philippe Robert 2 3 Marco Lorenzi 1  for the Alzheimer’s Disease Neuroimaging Initiative *  Abstract  Interpretable modeling of heterogeneous data channels is essential in medical applications, for example when jointly analyzing clinical scores and medical images. Variational Autoencoders (VAE) are powerful generative models that learn representations of complex data. The 
Unsupervised Label Noise Modeling and Loss Correction  Eric Arazo * 1 Diego Ortego * 1 Paul Albert 1 Noel E. O’Connor 1 Kevin McGuinness 1  Abstract  Despite being robust to small amounts of label noise, convolutional neural networks trained with stochastic gradient methods have been shown to easily ﬁt random labels. When there are a mix- ture of correct and mislabelled targets, networks tend to ﬁt the former before the latter. This suggests using a suitable two-component mix- ture model as an u
Fine-Grained Analysis of Optimization and Generalization for  Overparameterized Two-Layer Neural Networks  Sanjeev Arora * 1 2 Simon S. Du * 3 Wei Hu * 1 Zhiyuan Li * 1 Ruosong Wang * 3  Abstract  Recent works have cast some light on the mys- tery of why deep nets ﬁt any data and generalize despite being very overparametrized. This paper analyzes training and generalization for a simple 2-layer ReLU net with random initialization, and provides the following improvements over recent works: (i) Us
Distributed Weighted Matching via Randomized Composable Coresets  Sepehr Assadi 1 MohammadHossein Bateni 2 Vahab Mirrokni 2  Abstract  Maximum weight matching is one of the most fun- damental combinatorial optimization problems with a wide range of applications in data min- ing and bioinformatics. Developing distributed weighted matching algorithms is challenging due to the sequential nature of efﬁcient algorithms for this problem. In this paper, we develop a simple distributed algorithm for the
StochasticGradientPushforDistributedDeepLearningMahmoudAssran12NicolasLoizou13NicolasBallas1MikeRabbat1AbstractDistributeddata-parallelalgorithmsaimtoaccel-eratethetrainingofdeepneuralnetworksbypar-allelizingthecomputationoflargemini-batchgra-dientupdatesacrossmultiplenodes.Approachesthatsynchronizenodesusingexactdistributedav-eraging(e.g.,viaALLREDUCE)aresensitivetostragglersandcommunicationdelays.ThePUSH-SUMgossipalgorithmisrobusttotheseissues,butonlyperformsapproximatedistributedaver-aging.Th
Linear-Complexity Data-Parallel Earth Mover’s Distance Approximations  Kubilay Atasu 1 Thomas Mittelholzer 2  Abstract  The Earth Mover’s Distance (EMD) is a state- of-the art metric for comparing discrete proba- bility distributions, but its high distinguishabil- ity comes at a high cost in computational com- plexity. Even though linear-complexity approx- imation algorithms have been proposed to im- prove its scalability, these algorithms are either limited to vector spaces with only a few dime
Beneﬁts and Pitfalls of the Exponential Mechanism  with Applications to Hilbert Spaces and Functional PCA  Jordan Awan 1 Ana Kenney 1 Matthew Reimherr 1 Aleksandra Slavkovi´c 1  Abstract  The exponential mechanism is a fundamental tool of Differential Privacy (DP) due to its strong pri- vacy guarantees and ﬂexibility. We study its ex- tension to settings with summaries based on inﬁ- nite dimensional outputs such as with functional data analysis, shape analysis, and nonparametric statistics. We s
Feature Grouping as a Stochastic Regularizer  for High-Dimensional Structured Data  Serg¨ul Ayd¨ore 1 Bertrand Thirion 2 Ga¨el Varoquaux 2  Abstract  In many applications where collecting data is ex- pensive, for example neuroscience or medical imaging, the sample size is typically small com- pared to the feature dimension. These datasets call for intelligent regularization that exploits known structure, such as correlations between the fea- tures arising from the measurement device. How- ever, 
Beyond the Chinese Restaurant and Pitman-Yor processes:  Statistical Models with double power-law behavior  Fadhel Ayed * 1 Juho Lee * 1 2 Franc¸ois Caron 1  Abstract  Bayesian nonparametric approaches, in particular the Pitman-Yor process and the associated two- parameter Chinese Restaurant process, have been successfully used in applications where the data exhibit a power-law behavior. Examples include natural language processing, natural images or networks. There is also growing empirical evi
Scalable Fair Clustering  Arturs Backurs 1 2 Piotr Indyk 1 3 Krzysztof Onak 1 4 Baruch Schieber 1 5 Ali Vakilian 1 3 Tal Wagner 1 3  Abstract  We study the fair variant of the classic k- median problem introduced by Chierichetti et al. (Chierichetti et al., 2017) in which the points are colored, and the goal is to minimize the same average distance objective as in the standard k- median problem while ensuring that all clusters have an “approximately equal” number of points of each color. Chieric
Entropic GANs meet VAEs: A Statistical Approach to Compute Sample  Likelihoods in GANs  Yogesh Balaji 1 Hamed Hassani 2 Rama Chellappa 3 Soheil Feizi 1  Abstract  Building on the success of deep learning, two modern approaches to learn a probability model from the data are Generative Adversarial Net- works (GANs) and Variational AutoEncoders (VAEs). VAEs consider an explicit probability model for the data and compute a generative distri- bution by maximizing a variational lower-bound on the log-
Provable Guarantees for Gradient-Based Meta-Learning  Mikhail Khodak 1  Maria-Florina Balcan 1  Ameet Talwalkar 1 2  Abstract  We study the problem of meta-learning through the lens of online convex optimization, developing a meta-algorithm bridging the gap between pop- ular gradient-based meta-learning and classical regularization-based multi-task transfer methods. Our method is the ﬁrst to simultaneously satisfy good sample efﬁciency guarantees in the convex setting, with generalization bounds
Open-ended Learning in Symmetric Zero-sum Games  David Balduzzi 1 Marta Garnelo 1 Yoram Bachrach 1 Wojciech M. Czarnecki 1 Julien Perolat 1  Max Jaderberg 1 Thore Graepel 1  Abstract  Zero-sum games such as chess and poker are, ab- stractly, functions that evaluate pairs of agents, for example labeling them ‘winner’ and ‘loser’. If the game is approximately transitive, then self- play generates sequences of agents of increas- ing strength. However, nontransitive games, such as rock-paper-scissor
Concrete Autoencoders: Differentiable Feature Selection and Reconstruction  Abubakar Abid * 1 Muhammed Fatih Balin * 2 James Zou 1 3 4  Abstract  We introduce the concrete autoencoder, an end- to-end differentiable method for global feature selection, which efﬁciently identiﬁes a subset of the most informative features and simultaneously learns a neural network to reconstruct the input data from the selected features. Our method is unsupervised, and is based on using a concrete selector layer as
HOList: An Environment for Machine Learning of Higher-Order Theorem  Proving  Kshitij Bansal * 1 Sarah Loos * 1 Markus Rabe * 1 Christian Szegedy * 1 Stewart Wilcox * 1  Abstract  We present an environment, benchmark, and deep learning driven automated theorem prover for higher-order logic. Higher-order interactive theo- rem provers enable the formalization of arbitrary mathematical theories and thereby present an in- teresting, open-ended challenge for deep learning. We provide an open-source f
Structured agents for physical construction  Victor Bapst * 1 Alvaro Sanchez-Gonzalez * 1 Carl Doersch 1 Kimberly L. Stachenfeld 1 Pushmeet Kohli 1  Peter W. Battaglia 1 Jessica B. Hamrick 1  Abstract  Physical construction—the ability to compose ob- jects, subject to physical dynamics, to serve some function—is fundamental to human intelligence. We introduce a suite of challenging physical con- struction tasks inspired by how children play with blocks, such as matching a target conﬁguration, st
Learning to Route in Similarity Graphs  Dmitry Baranchuk 1 2 Dmitry Persiyanov 3 Anton Sinitsin 1 4 Artem Babenko 1 4  Abstract  Recently similarity graphs became the leading paradigm for efﬁcient nearest neighbor search, outperforming traditional tree-based and LSH- based methods. Similarity graphs perform the search via greedy routing: a query traverses the graph and in each vertex moves to the adjacent vertex that is the closest to this query. In prac- tice, similarity graphs are often suscep
A Personalized Affective Memory Model for Improving Emotion Recognition  Pablo Barros 1 German I. Parisi 1 2 Stefan Wermter 1  Abstract  Recent models of emotion recognition strongly rely on supervised deep learning solutions for the distinction of general emotion expressions. How- ever, they are not reliable when recognizing on- line and personalized facial expressions, e.g., for person-speciﬁc affective understanding. In this paper, we present a neural model based on a con- ditional adversaria
Scale-free adaptive planning for deterministic dynamics & discounted rewards  Peter L. Bartlett1 Victor Gabillon2 Jennifer Healey3 Michal Valko4  Abstract  We address the problem of planning in an envi- ronment with deterministic dynamics and stochas- tic discounted rewards under a limited numeri- cal budget where the ranges of both rewards and noise are unknown. We introduce PlaTγPOOS, an adaptive, robust, and efﬁcient alternative to the OLOP (open-loop optimistic planning) algorithm. Whereas O
Pareto Optimal Streaming Unsupervised Classiﬁcation  Soumya Basu 1 Steven Gutstein 2 Brent Lance 2 Sanjay Shakkottai 1  Abstract  We study an online and streaming unsupervised classiﬁcation system. Our setting consists of a collection of classiﬁers (with unknown confusion matrices) each of which can classify one sam- ple per unit time, and which are accessed by a stream of unlabeled samples. Each sample is dis- patched to one or more classiﬁers, and depending on the labels collected from these c
Categorical Feature Compression via Submodular Optimization  MohammadHossein Bateni * 1 Lin Chen * 1 2 Hossein Esfandiari * 1 Thomas Fu * 1 Vahab S. Mirrokni * 1  Afshin Rostamizadeh * 1  Abstract  In the era of big data, learning from categorical features with very large vocabularies (e.g., 28 million for the Criteo click prediction dataset) has become a practical challenge for machine learning researchers and practitioners. We design a highly-scalable vocabulary compression algo- rithm that se
Noise2Self: Blind Denoising by Self-Supervision  Joshua Batson * 1 Loic Royer * 1  Abstract  We propose a general framework for denoising high-dimensional measurements which requires no prior on the signal, no estimate of the noise, and no clean training data. The only assumption is that the noise exhibits statistical independence across different dimensions of the measurement, while the true signal exhibits some correlation. For a broad class of functions (“J -invariant”), it is then possible t
Efﬁcient Optimization of Loops and Limits with Randomized Telescoping Sums  Alex Beatson 1 Ryan P. Adams 1  Abstract  We consider optimization problems in which the objective requires an inner loop with many steps or is the limit of a sequence of increasingly costly approximations. Meta-learning, training recurrent neural networks, and optimization of the solu- tions to differential equations are all examples of optimization problems with this character. In such problems, it can be expensive to 
Recurrent Kalman Networks:  Factorized Inference in High-Dimensional Deep Feature Spaces  Philipp Becker 1 2 3 Harit Pandya 4 Gregor Gebhardt 1 Cheng Zhao 5 James Taylor 6 Gerhard Neumann 4 2 3  Abstract  In order to integrate uncertainty estimates into deep time-series modelling, Kalman Filters (KFs) (Kalman et al., 1960) have been integrated with deep learning models, however, such approaches typically rely on approximate inference tech- niques such as variational inference which makes learnin
Switching Linear Dynamics for Variational Bayes Filtering  Philip Becker-Ehmck 1 2 Jan Peters 2 Patrick van der Smagt 1  Abstract  System identiﬁcation of complex and nonlinear systems is a central problem for model predictive control and model-based reinforcement learning. Despite their complexity, such systems can of- ten be approximated well by a set of linear dy- namical systems if broken into appropriate sub- sequences. This mechanism not only helps us ﬁnd good approximations of dynamics, b
Active Learning for Probabilistic Structured Prediction of Cuts and Matchings  Sima Behpour 1 2 Anqi Liu 3 Brian D. Ziebart 2  Abstract  Active learning methods, like uncertainty sam- pling, combined with probabilistic prediction tech- niques have achieved success in various prob- lems like image classiﬁcation and text classiﬁ- cation. For more complex multivariate predic- tion tasks, the relationships between labels play an important role in designing structured classi- ﬁers with better perform
Invertible Residual Networks  Jens Behrmann * 1 2 Will Grathwohl * 2 Ricky T. Q. Chen 2 David Duvenaud 2 J¨orn-Henrik Jacobsen * 2  Abstract  We show that standard ResNet architectures can be made invertible, allowing the same model to be used for classiﬁcation, density estimation, and generation. Typically, enforcing invertibility re- quires partitioning dimensions or restricting net- work architectures. In contrast, our approach only requires adding a simple normalization step dur- ing trainin
Greedy Layerwise Learning Can Scale to ImageNet  Eugene Belilovsky 1 Michael Eickenberg 2 Edouard Oyallon 3  Abstract  Shallow supervised 1-hidden layer neural net- works have a number of favorable properties that make them easier to interpret, analyze, and opti- mize than their deep counterparts, but lack their representational power. Here we use 1-hidden layer learning problems to sequentially build deep networks layer by layer, which can inherit proper- ties from shallow networks. Contrary to
Overcoming Multi-model Forgetting  Yassine Benyahia * † 1 Kaicheng Yu * 2 Kamil Bennani-Smires 3 Martin Jaggi 4 Anthony Davison 1  Mathieu Salzmann 2 Claudiu Musat 3  Abstract  We identify a phenomenon, which we refer to as multi-model forgetting, that occurs when se- quentially training multiple deep networks with partially-shared parameters; the performance of previously-trained models degrades as one opti- mizes a subsequent one, due to the overwriting of shared parameters. To overcome this, 
Optimal Kronecker-Sum Approximation of Real Time Recurrent Learning  Frederik Benzing * 1 Marcelo Matheus Gauy * 1 Asier Mujika 1 Anders Martinsson 1 Angelika Steger 1  Abstract  One of the central goals of Recurrent Neural Net- works (RNNs) is to learn long-term dependen- cies in sequential data. Nevertheless, the most popular training method, Truncated Backpropaga- tion through Time (TBPTT), categorically forbids learning dependencies beyond the truncation hori- zon. In contrast, the online tr
Adversarially Learned Representations for Information Obfuscation and  Inference  Martin Bertran * 1 Natalia Martinez * 1 Afroditi Papadaki 2 Qiang Qiu 1 Miguel Rodrigues 2 Galen Reeves 1  Guillermo Sapiro 1  Abstract  Data collection and sharing are pervasive aspects of modern society. This process can either be voluntary, as in the case of a person taking a fa- cial image to unlock his/her phone, or inciden- tal, such as trafﬁc cameras collecting videos on pedestrians. An undesirable side effe
Bandit Multiclass Linear Classiﬁcation:  Efﬁcient Algorithms for the Separable Case  Alina Beygelzimer* 1 D´avid P´al* 1 Bal´azs Sz¨or´enyi* 1 Devanathan Thiruvenkatachari* 2 Chen-Yu Wei* 3  Chicheng Zhang* 4  Abstract  We study the problem of efﬁcient online multi- class linear classiﬁcation with bandit feedback, where all examples belong to one of K classes and lie in the d-dimensional Euclidean space. Previous works have left open the challenge of designing efﬁcient algorithms with ﬁnite mist
Analyzing Federated Learning through an Adversarial Lens  Arjun Nitin Bhagoji * 1 Supriyo Chakraborty 2 Prateek Mittal 1 Seraphin Calo 2  Abstract  Federated learning distributes model training among a multitude of agents, who, guided by pri- vacy concerns, perform training using their lo- cal data but share only model parameter updates, for iterative aggregation at the server to train an overall global model. In this work, we explore how the federated learning setting gives rise to a new threat
Optimal Continuous DR-Submodular Maximization and Applications to Provable Mean Field Inference  Yatao A. Bian † 1 Joachim M. Buhmann 1 Andreas Krause 1  Abstract  Mean ﬁeld inference in probabilistic models is generally a highly nonconvex problem. Existing optimization methods, e.g., coordinate ascent al- gorithms, typically only ﬁnd local optima. In this work we propose provable mean ﬁeld methods for probabilistic log-submodular mod- els and its posterior agreement (PA) with strong approximati
More Efﬁcient Off-Policy Evaluation through Regularized Targeted Learning  Aur´elien F. Bibaut * 1 Ivana Malenica * 1 Nikos Vlassis 2 Mark J. van der Laan 1  Abstract  We study the problem of off-policy evaluation (OPE) in Reinforcement Learning (RL), where the aim is to estimate the performance of a new policy given historical data that may have been gener- ated by a different policy, or policies. In particular, we introduce a novel doubly-robust estimator for the OPE problem in RL, based on th
A Kernel Perspective for Regularizing Deep Neural Networks  Alberto Bietti * 1 Grégoire Mialon * 1 2 Dexiong Chen 1 Julien Mairal 1  Abstract  We propose a new point of view for regularizing deep neural networks by using the norm of a repro- ducing kernel Hilbert space (RKHS). Even though this norm cannot be computed, it admits upper and lower approximations leading to various prac- tical strategies. Speciﬁcally, this perspective (i) provides a common umbrella for many existing regularization pr
Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff  Yochai Blau 1 Tomer Michaeli 1  Abstract  Lossy compression algorithms are typically de- signed and analyzed through the lens of Shan- non’s rate-distortion theory, where the goal is to achieve the lowest possible distortion (e.g., low MSE or high SSIM) at any given bit rate. How- ever, in recent years, it has become increasingly accepted that “low distortion” is not a synonym for “high perceptual quality”, and in fact opti- 
Correlated bandits or: How to minimize mean-squared error online  Vinay Praneeth Boda 1 Prashanth L.A. 2  Abstract  While the objective in traditional multi-armed bandit problems is to ﬁnd the arm with the high- est mean, in many settings, ﬁnding an arm that best captures information about other arms is of interest. This objective, however, requires learn- ing the underlying correlation structure and not just the means of the arms. Sensors placement for industrial surveillance and cellular netwo
Adversarial Attacks on Node Embeddings via Graph Poisoning  Aleksandar Bojchevski 1 Stephan G¨unnemann 1  Abstract  The goal of network representation learning is to learn low-dimensional node embeddings that cap- ture the graph structure and are useful for solving downstream tasks. However, despite the prolifera- tion of such methods, there is currently no study of their robustness to adversarial attacks. We pro- vide the ﬁrst adversarial vulnerability analysis on the widely used family of meth
Online Variance Reduction with Mixtures  Zalán Borsos 1 Sebastian Curi 1 Kﬁr Y. Levy 1 Andreas Krause 1  Abstract  Adaptive importance sampling for stochastic opti- mization is a promising approach that offers im- proved convergence through variance reduction. In this work, we propose a new framework for variance reduction that enables the use of mixtures over predeﬁned sampling distributions, which can naturally encode prior knowledge about the data. While these sampling distributions are ﬁxed,
Compositional Fairness Constraints for Graph Embeddings  Avishek Joey Bose 1 2 William L. Hamilton 1 2 3  Abstract  Learning high-quality node embeddings is a key building block for machine learning models that operate on graph data, such as social networks and recommender systems. However, existing graph embedding techniques are unable to cope with fairness constraints, e.g., ensuring that the learned representations do not correlate with certain at- tributes, such as age or gender. Here, we in
Unreproducible Research is Reproducible  Xavier Bouthillier 1 C´esar Laurent 1 Pascal Vincent 1 2 3  Abstract  The apparent contradiction in the title is a word- play on the different meanings attributed to the word reproducible across different scientiﬁc ﬁelds. What we imply is that unreproducible ﬁndings can be built upon reproducible methods. With- out denying the importance of facilitating the re- production of methods, we deem important to reassert that reproduction of ﬁndings is a funda- m
Blended Conditional Gradients:  The Unconditioning of Conditional Gradients  G´abor Braun 1 Sebastian Pokutta 1 Dan Tu 1 Stephen Wright 2  Abstract  We present a blended conditional gradient ap- proach for minimizing a smooth convex function over a polytope P , combining the Frank–Wolfe algorithm (also called conditional gradient) with gradient-based steps, different from away steps and pairwise steps, but still achieving linear con- vergence for strongly convex functions, along with good practi
Coresets for Ordered Weighted Clustering  Vladimir Braverman 1 Shaofeng H.-C. Jiang 2 Robert Krauthgamer 2 Xuan Wu 1  Abstract  We design coresets for ORDERED k-MEDIAN, a generalization of classical clustering problems such as k-MEDIAN and k-CENTER. Its objective function is deﬁned via the Ordered Weighted Av- eraging (OWA) paradigm of Yager (1988), where data points are weighted according to a predeﬁned weight vector, but in order of their contribution to the objective (distance from the center
Target Tracking for Contextual Bandits: Application to Demand Side Management  Margaux Br´eg`ere 1 2 3 Pierre Gaillard 3 Yannig Goude 1 2 Gilles Stoltz 2  Abstract  We propose a contextual-bandit approach for de- mand side management by offering price incen- tives. More precisely, a target mean consumption is set at each round and the mean consumption is modeled as a complex function of the distribu- tion of prices sent and of some contextual vari- ables such as the temperature, weather, and so 
Active Manifolds: A non-linear analogue to Active Subspaces  Robert A. Bridges 1 Anthony D. Gruber 2 1 Christopher R. Felder 3 1 Miki E. Verma 1 Chelsey Hoff 1  Abstract  We present an approach to analyze C 1(Rm) func- tions that addresses limitations present in the Ac- tive Subspaces (AS) method of Constantine et al. (2015; 2014). Under appropriate hypotheses, our Ac- tive Manifolds (AM) method identiﬁes a 1-D curve in the domain (the active manifold) on which nearly all values of the unknown f
Conditioning by adaptive sampling for robust design  David H. Brookes 1 Hahnbeom Park 2 3 Jennifer Listgarten 4  Abstract  We present a method for design problems wherein the goal is to maximize or specify the value of one or more properties of interest (e. g., maximiz- ing the ﬂuorescence of a protein). We assume access to black box, stochastic “oracle” predictive functions, each of which maps from design space to a distribution over properties of interest. Be- cause many state-of-the-art predi
Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations  Daniel S. Brown * 1 Wonjoon Goo * 1 Prabhat Nagarajan 2 Scott Niekum 1  Abstract  A critical ﬂaw of existing inverse reinforcement learning (IRL) methods is their inability to sig- niﬁcantly outperform the demonstrator. This is because IRL typically seeks a reward function that makes the demonstrator appear near-optimal, rather than inferring the underlying intentions of the demonstrator that may
Deep Counterfactual Regret Minimization  Noam Brown * 1 2 Adam Lerer * 1 Sam Gross 1 Tuomas Sandholm 2 3  Abstract  Counterfactual Regret Minimization (CFR) is the leading framework for solving large imperfect- information games. It converges to an equilibrium by iteratively traversing the game tree. In order to deal with extremely large games, abstraction is typically applied before running CFR. The ab- stracted game is solved with tabular CFR, and its solution is mapped back to the full game. 
Understanding the Origins of Bias in Word Embeddings  Marc-Etienne Brunet 1 2 Colleen Alkalay-Houlihan 1 Ashton Anderson 1 2 Richard Zemel 1 2  Abstract  Popular word embedding algorithms exhibit stereotypical biases, such as gender bias. The widespread use of these algorithms in machine learning systems can thus amplify stereotypes in important contexts. Although some methods have been developed to mitigate this problem, how word embedding biases arise during training is poorly understood. In t
Low Latency Privacy Preserving Inference  Alon Brutzkus 1  Oren Elisha 2 Ran Gilad-Bachrach 3  Abstract  When applying machine learning to sensitive data, one has to ﬁnd a balance between accu- racy, information security, and computational- complexity. Recent studies combined Homomor- phic Encryption with neural networks to make inferences while protecting against information leakage. However, these methods are limited by the width and depth of neural networks that can be used (and hence the acc
Why do Larger Models Generalize Better? A Theoretical Perspective via the  XOR Problem  Alon Brutzkus 1 Amir Globerson 1  Abstract  Empirical evidence suggests that neural networks with ReLU activations generalize better with over- parameterization. However, there is currently no theoretical analysis that explains this observation. In this work, we provide theoretical and empir- ical evidence that, in certain cases, overparame- terized convolutional networks generalize better than small networks
Adversarial Examples from Computational Constraints  S´ebastien Bubeck 1 Yin Tat Lee 1 2 Eric Price 3 Ilya Razenshteyn 1  Abstract  Why are classiﬁers in high dimension vulnerable to “adversarial” perturbations? We show that it is likely not due to information theoretic limita- tions, but rather it could be due to computational constraints. First we prove that, for a broad set of classiﬁcation tasks, the mere existence of a robust classiﬁer implies that it can be found by a pos- sibly exponentia
Self-Similar Epochs: Value in Arrangement  Eliav Buchnik * 1 2 Edith Cohen * 2 1 Avinatan Hassidim 2 Yossi Matias 2  Abstract  Optimization of machine learning models is com- monly performed through stochastic gradient up- dates on randomly ordered training examples. This practice means that each fraction of an epoch comprises an independent random sample of the training data that may not preserve informative structure present in the full data. We hypothe- size that the training can be more effe
Learning Generative Models across Incomparable Spaces  Charlotte Bunne 1 David Alvarez-Melis 2 Andreas Krause 1 Stefanie Jegelka 2  Abstract  Generative Adversarial Networks have shown re- markable success in learning a distribution that faithfully recovers a reference distribution in its entirety. However, in some cases, we may want to only learn some aspects (e.g., cluster or manifold structure), while modifying others (e.g., style, ori- entation or dimension). In this work, we propose an appr
Rates of Convergence for Sparse Variational Gaussian Process Regression  David R. Burt 1 Carl Edward Rasmussen 1 2 Mark van der Wilk 2  Abstract  (cid:0)N 3(cid:1) scaling with dataset size N. (cid:0)N M 2(cid:1),  Excellent variational approximations to Gaussian process posteriors have been developed which avoid the O They reduce the computational cost to O with M (cid:28) N the number of inducing variables, which summarise the process. While the com- putational cost seems to be linear in N, th
What is the Effect of Importance Weighting in Deep Learning?  Jonathon Byrd 1 Zachary C. Lipton 1  Abstract  Importance-weighted risk minimization is a key ingredient in many machine learning algorithms for causal inference, domain adaptation, class im- balance, and off-policy reinforcement learning. While the effect of importance weighting is well- characterized for low-capacity misspeciﬁed mod- els, little is known about how it impacts over- parameterized, deep neural networks. Inspired by rec
A Quantitative Analysis of the Effect of Batch Normalization on Gradient  Descent  Yongqiang Cai 1 Qianxiao Li 1 2 Zuowei Shen 1  Abstract  Despite its empirical success and recent theoreti- cal progress, there generally lacks a quantitative analysis of the effect of batch normalization (BN) on the convergence and stability of gradient de- scent. In this paper, we provide such an analysis on the simple problem of ordinary least squares (OLS), where the precise dynamical properties of gradient de
Accelerated Linear Convergence of Stochastic Momentum Methods  in Wasserstein Distances  Bugra Can 1 Mert Gurbuzbalaban 1 Lingjiong Zhu 2  Abstract  Momentum methods such as Polyak’s heavy ball (HB) method, Nesterov’s accelerated gradient (AG) as well as accelerated projected gradient (APG) method have been commonly used in ma- chine learning practice, but their performance is quite sensitive to noise in the gradients. We study these methods under a ﬁrst-order stochastic ora- cle model where noi
Active Embedding Search via Noisy Paired Comparisons  Gregory H. Canal 1 Andrew K. Massimino 1 Mark A. Davenport 1 Christopher J. Rozell 1  Abstract  Suppose that we wish to estimate a user’s prefer- ence vector w from paired comparisons of the form “does user w prefer item p or item q?,” where both the user and items are embedded in a low-dimensional Euclidean space with distances that reﬂect user and item similarities. Such obser- vations arise in numerous settings, including psy- chometrics a
Dynamic Learning with Frequent New Product Launches:  A Sequential Multinomial Logit Bandit Problem  Junyu Cao 1 Wei Sun 2  Abstract  Motivated by the phenomenon that companies in- troduce new products to keep abreast with cus- tomers’ rapidly changing tastes, we consider a novel online learning setting where a proﬁt- maximizing seller needs to learn customers’ preferences through offering recommendations, which may contain existing products and new products that are launched in the middle of a 
Competing Against Equilibria in Zero-Sum Games with Evolving Payoffs  Adrian Rivera Cardoso 1 Jacob Abernethy 2 He Wang 1 Huan Xu 1  Abstract  We study the problem of repeated play in a zero- sum game in which the payoff matrix may change, in a possibly adversarial fashion, on each round; we call these Online Matrix Games. Finding the Nash Equilibrium (NE) of a two player zero-sum game is core to many problems in statistics, op- timization, and economics, and for a ﬁxed game matrix this can be e
Automated Model Selection with Bayesian Quadrature  Henry Chai 1 Jean-Franc¸ois Ton 2 Michael A. Osborne 3 4 Roman Garnett 1  Abstract  We present a novel technique for tailoring Bayesian quadrature (BQ) to model selection. The state-of-the-art for comparing the evidence of multiple models relies on Monte Carlo methods, which converge slowly and are unreliable for com- putationally expensive models. Although previ- ous research has shown that BQ offers sample efﬁciency superior to Monte Carlo in
Learning Action Representations for Reinforcement Learning  Yash Chandak 1 Georgios Theocharous 2 James E. Kostas 1 Scott M. Jordan 1 Philip S. Thomas 1  Abstract  Most model-free reinforcement learning methods leverage state representations (embeddings) for generalization, but either ignore structure in the space of actions or assume the structure is pro- vided a priori. We show how a policy can be decomposed into a component that acts in a low- dimensional space of action representations and a
Dynamic Measurement Scheduling for Event Forecasting Using Deep RL  Chun-Hao Chang * 1 2 3 Mingjie Mai * 1 2 3 Anna Goldenberg 1 2 3  Abstract  Imagine a patient in critical condition. What and when should be measured to forecast detri- mental events, especially under the budget con- straints? We answer this question by deep rein- forcement learning (RL) that jointly minimizes the measurement cost and maximizes predictive gain, by scheduling strategically-timed measure- ments. We learn our polic
On Symmetric Losses for Learning from Corrupted Labels  Nontawat Charoenphakdee 1 2 Jongyeong Lee 1 2 Masashi Sugiyama 2 1  Abstract  This paper aims to provide a better understand- ing of a symmetric loss. First, we emphasize that using a symmetric loss is advantageous in the balanced error rate (BER) minimization and area under the receiver operating characteristic curve (AUC) maximization from corrupted labels. Second, we prove general theoretical properties of symmetric losses, including a c
Online learning with kernel losses  Aldo Pacchiano * 1 Niladri S. Chatterji * 1 Peter L. Bartlett 1  Abstract  We present a generalization of the adversarial linear bandits framework, where the underlying losses are kernel functions (with an associated re- producing kernel Hilbert space) rather than linear functions. We study a version of the exponential weights algorithm and bound its regret in this set- ting. Under conditions on the eigen-decay of the kernel we provide a sharp characterization
Neural Network Attributions: A Causal Perspective  Aditya Chattopadhyay 1 Piyushi Manupriya 2 Anirban Sarkar 2 Vineeth N Balasubramanian 2  Abstract  We propose a new attribution method for neu- ral networks developed using ﬁrst principles of causality (to the best of our knowledge, the ﬁrst such). The neural network architecture is viewed as a Structural Causal Model, and a methodology to compute the causal effect of each feature on the output is presented. With reasonable assump- tions on the 
PAC Identiﬁcation of Many Good Arms in Stochastic Multi-Armed Bandits  Arghya Roy Chaudhuri 1 Shivaram Kalyanakrishnan 1  Abstract  We consider the problem of identifying any k out of the best m arms in an n-armed stochastic multi-armed bandit; framed in the PAC setting, this particular problem generalises both the prob- lem of “best subset selection” (Kalyanakrishnan & Stone, 2010) and that of selecting “one out of the best m” arms (Roy Chaudhuri & Kalyanakrishnan, 2017). We present a lower bou
Nearest Neighbor and Kernel Survival Analysis: Nonasymptotic Error Bounds  and Strong Consistency Rates  George H. Chen 1  Abstract  We establish the ﬁrst nonasymptotic error bounds for Kaplan-Meier-based nearest neighbor and ker- nel survival probability estimators where feature vectors reside in metric spaces. Our bounds imply rates of strong consistency for these nonparamet- ric estimators and, up to a log factor, match an existing lower bound for conditional CDF estima- tion. Our proof strat
Stein Point Markov Chain Monte Carlo  Wilson Ye Chen * 1 Alessandro Barp * 2 3 Franc¸ois-Xavier Briol 4 3 Jackson Gorham 5 Mark Girolami 4 3  Lester Mackey * 6 Chris. J. Oates 7 3  Abstract  An important task in machine learning and statis- tics is the approximation of a probability measure by an empirical measure supported on a discrete point set. Stein Points are a class of algorithms for this task, which proceed by sequentially min- imising a Stein discrepancy between the empir- ical measure 
Particle Flow Bayes’ Rule  Xinshi Chen * 1 Hanjun Dai * 2 Le Song 2 3  Abstract  We present a particle ﬂow realization of Bayes’ rule, where an ODE-based neural operator is used to transport particles from a prior to its posterior after a new observation. We prove that such an ODE operator exists. Its neural parameterization can be trained in a meta-learning framework, al- lowing this operator to reason about the effect of an individual observation on the posterior, and thus generalize across di
Proportionally Fair Clustering  Xingyu Chen 1 Brandon Fain 1 Liang Lyu 1 Kamesh Munagala 1  Abstract  We extend the fair machine learning literature by considering the problem of proportional centroid clustering in a metric context. For clustering n points with k centers, we deﬁne fairness as propor- tionality to mean that any n/k points are entitled to form their own cluster if there is another cen- ter that is closer in distance for all n/k points. We seek clustering solutions to which there a
Information-Theoretic Considerations in Batch Reinforcement Learning  Jinglin Chen 1 Nan Jiang 1  Abstract  Value-function approximation methods that oper- ate in batch mode have foundational importance to reinforcement learning (RL). Finite sample guar- antees for these methods often crucially rely on two types of assumptions: (1) mild distribution shift, and (2) representation conditions that are stronger than realizability. However, the necessity (“why do we need them?”) and the naturalness (
Generative Adversarial User Model for  Reinforcement Learning Based Recommendation System  Xinshi Chen 1 † Shuang Li 2 Hui Li 4 Shaohua Jiang 4 Yuan Qi 4 Le Song 3 4  Abstract  There are great interests as well as many chal- lenges in applying reinforcement learning (RL) to recommendation systems. In this setting, an online user is the environment; neither the reward function nor the environment dynamics are clearly deﬁned, making the application of RL challenging. In this paper, we propose a no
Understanding and Utilizing Deep Neural Networks  Trained with Noisy Labels  Pengfei Chen 1 2 Benben Liao 2 Guangyong Chen 2 Shengyu Zhang 1 2  Abstract  Noisy labels are ubiquitous in real-world datasets, which poses a challenge for robustly training deep neural networks (DNNs) as DNNs usually have the high capacity to memorize the noisy labels. In this paper, we ﬁnd that the test accuracy can be quantitatively characterized in terms of the noise ratio in datasets. In particular, the test accur
A Gradual, Semi-Discrete Approach to Generative Network Training  via Explicit Wasserstein Minimization  Yucheng Chen 1 Matus Telgarsky 1 Chao Zhang 1 Bolton Bailey 1 Daniel Hsu 2 Jian Peng 1  Abstract  This paper provides a simple procedure to ﬁt gen- erative networks to target distributions, with the goal of a small Wasserstein distance (or other op- timal transport cost). The approach is based on two principles: (a) if the source randomness of the network is a continuous distribution (the “se
FastIncrementalvonNeumannGraphEntropyComputation:Theory,Algorithm,andApplicationsPin-YuChen1LingfeiWu1SijiaLiu1IndikaRajapakse2AbstractThevonNeumanngraphentropy(VNGE)facili-tatesmeasurementofinformationdivergenceanddistancebetweengraphsinagraphsequence.Ithasbeensuccessfullyappliedtovariouslearn-ingtasksdrivenbynetwork-baseddata.Whileeffective,VNGEiscomputationallydemandingasitrequiresthefulleigenspectrumofthegraphLaplacianmatrix.Inthispaper,weproposeanewcomputationalframework,FastIncrementalvonN
Katalyst: Boosting Convex Katyusha  for Non-Convex Problems with a Large Condition Number  Zaiyi Chen 1 2 Yi Xu 3 Haoyuan Hu 1 Tianbao Yang 3  Abstract  An important class of non-convex objectives that has wide applications in machine learning consists of a sum of n smooth functions and a non-smooth convex function. Tremendous studies have been devoted to conquering these problems by lever- aging one of the two types of variance reduction techniques, i.e., SVRG-type that computes a full gradient
Multivariate-Information Adversarial Ensemble for Scalable  Joint Distribution Matching  Ziliang Chen * 1 Zhanfu Yang * 2 Xiaoxi Wang * 1 Xiaodan Liang 1 Xiaopeng Yan 1 Guanbin Li 1 Liang Lin 1  Abstract  A broad range of cross-m-domain generation re- searches boil down to matching a joint distribu- tion by deep generative models (DGMs). Hith- erto algorithms excel in pairwise domains while as m increases, remain struggling to scale them- selves to ﬁt a joint distribution. In this paper, we prop
Robust Decision Trees Against Adversarial Examples  Hongge Chen 1 Huan Zhang 2 Duane Boning 1 Cho-Jui Hsieh 2  Abstract  Although adversarial examples and model robust- ness have been extensively studied in the context of linear models and neural networks, research on this issue in tree-based models and how to make tree-based models robust against adversar- ial examples is still limited. In this paper, we show that tree based models are also vulnerable to adversarial examples and develop a novel
RaFM: Rank-Aware Factorization Machines  Xiaoshuang Chen 1 Yin Zheng 2 Jiaxing Wang 3 Wenye Ma 2 Junzhou Huang 2  Abstract  Fatorization machines (FM) are a popular model class to learn pairwise interactions by a low-rank approximation. Different from existing FM-based approaches which use a ﬁxed rank for all fea- tures, this paper proposes a Rank-Aware FM (RaFM) model which adopts pairwise interactions from embeddings with different ranks. The pro- posed model achieves a better performance on r
Control Regularization for Reduced Variance Reinforcement Learning  Richard Cheng 1 Abhinav Verma 2 G´abor Orosz 3 Swarat Chaudhuri 2 Yisong Yue 1 Joel W. Burdick 1  Abstract  Dealing with high variance is a signiﬁcant chal- lenge in model-free reinforcement learning (RL). Existing methods are unreliable, exhibiting high variance in performance from run to run using different initializations/seeds. Focusing on prob- lems arising in continuous control, we propose a functional regularization appro
Predictor-Corrector Policy Optimization  Ching-An Cheng 1 2 Xinyan Yan 1 Nathan Ratliff 2 Byron Boots 1 2  Abstract  We present a predictor-corrector framework, called PICCOLO, that can transform a ﬁrst-order model-free reinforcement or imitation learning al- gorithm into a new hybrid method that leverages predictive models to accelerate policy learning. The new “PICCOLOed” algorithm optimizes a policy by recursively repeating two steps: In the Prediction Step, the learner uses a model to pre- d
Variational Inference of Sparse Network from Count Data  Julien Chiquet 1 Mahendra Mariadassou 2 St´ephane Robin 1  Abstract  The problem of network reconstruction from con- tinuous data has been extensively studied and most state of the art methods rely on variants of Gaussian Graphical Models (GGM). GGM are unfortunately badly suited to sparse count data spanning several orders of magnitude. Most inference methods for count data (SparCC, RE- BACCA, SPIEC-EASI, gCoda, etc) ﬁrst trans- form coun
Random Walks on Hypergraphs with Edge-Dependent Vertex Weights  Uthsav Chitra 1 Benjamin J Raphael 1  Abstract  Hypergraphs are used in machine learning to model higher-order relationships in data. While spectral methods for graphs are well-established, spectral theory for hypergraphs remains an ac- tive area of research. In this paper, we use ran- dom walks to develop a spectral theory for hy- pergraphs with edge-dependent vertex weights: hypergraphs where every vertex v has a weight γe(v) for 
Neural Joint Source-Channel Coding  Kristy Choi 1 Kedar Tatwawadi 2 Aditya Grover 1 Tsachy Weissman 2 Stefano Ermon 1  Abstract  For reliable transmission across a noisy communi- cation channel, classical results from information theory show that it is asymptotically optimal to separate out the source and channel coding pro- cesses. However, this decomposition can fall short in the ﬁnite bit-length regime, as it requires non- trivial tuning of hand-crafted codes and assumes inﬁnite computational
Beyond Backprop: Online Alternating Minimization with Auxiliary Variables  Anna Choromanska* 1 Benjamin Cowen* 1 Sadhana Kumaravel* 2 Ronny Luss* 2 Mattia Rigotti* 2  Irina Rish* 2 Brian Kingsbury 2 Paolo DiAchille 2 Viatcheslav Gurev 2 Ravi Tejwani 3 Djallel Bouneffouf 2  Abstract  Despite signiﬁcant recent advances in deep neural networks, training them remains a challenge due to the highly non-convex nature of the objective function. State-of-the-art methods rely on error backpropagation, whi
Unifying Orthogonal Monte Carlo Methods  Krzysztof Choromanski * 1 Mark Rowland * 2 Wenyu Chen 3 Adrian Weller 2 4  Abstract  Many machine learning methods making use of Monte Carlo sampling in vector spaces have been shown to be improved by conditioning samples to be mutually orthogonal. Exact orthogonal cou- pling of samples is computationally intensive, hence approximate methods have been of great interest. In this paper, we present a unifying per- spective of many approximate methods by con-
Probability Functional Descent: A Unifying Perspective  on GANs, Variational Inference, and Reinforcement Learning  Casey Chu 1 Jose Blanchet 2 Peter Glynn 2  Abstract  The goal of this paper is to provide a unifying view of a wide range of problems of interest in machine learning by framing them as the mini- mization of functionals deﬁned on the space of probability measures. In particular, we show that generative adversarial networks, variational infer- ence, and actor-critic methods in reinfo
MeanSum : A Neural Model for Unsupervised Multi-Document Abstractive Summarization  Eric Chu * † 1 Peter J. Liu * 2  Abstract  Abstractive summarization has been studied us- ing neural sequence transduction methods with datasets of large, paired document-summary ex- amples. However, such datasets are rare and the models trained from them do not generalize to other domains. Recently, some progress has been made in learning sequence-to-sequence mappings with only unpaired examples. In our work, we
Weak Detection of Signal in the Spiked Wigner Model  Hye Won Chung 1 Ji Oon Lee 2  Abstract  We consider the problem of detecting the pres- ence of the signal in a rank-one signal-plus-noise data matrix. In case the signal-to-noise ratio is under the threshold below which a reliable detec- tion is impossible, we propose a hypothesis test based on the linear spectral statistics of the data matrix. When the noise is Gaussian, the error of the proposed test is optimal as it matches the error of the
New Results on Information Theoretic Clustering  Ferdinando Cicalese 1 Eduardo Laber 2 Lucas Murtinho 2  Abstract  We study the problem of optimizing the cluster- ing of a set of vectors when the quality of the clustering is measured by the Entropy or the Gini impurity measure. Our results contribute to the state of the art both in terms of best known approx- imation guarantees and inapproximability bounds: (i) we give the ﬁrst polynomial time algorithm for Entropy impurity based clustering with
Sensitivity Analysis of Linear Structural Causal Models  Carlos Cinelli 1 Daniel Kumor 2 Bryant Chen 3 Judea Pearl 1 Elias Bareinboim 2  Abstract  Causal inference requires assumptions about the data generating process, many of which are un- veriﬁable from the data. Given that some causal assumptions might be uncertain or disputed, for- mal methods are needed to quantify how sensitive research conclusions are to violations of those assumptions. Although an extensive literature exists on the topi
Dimensionality Reduction for Tukey Regression  Kenneth L. Clarkson * 1 Ruosong Wang * 2 David P. Woodruff * 2  Abstract  lem. The Tukey loss function (cid:107)y(cid:107)M =(cid:80)  We give the ﬁrst dimensionality reduction meth- ods for the overconstrained Tukey regression prob- i M (yi) has M (yi) ≈ |yi|p for residual errors yi smaller than a prescribed threshold τ, but M (yi) becomes constant for errors |yi| > τ. Our results de- pend on a new structural result, proven construc- tively, showin
On Medians of (Randomized) Pairwise Means  Pierre Laforgue 1 Stephan Cl´emenc¸on 1 Patrice Bertail 2  Abstract  Tournament procedures, recently introduced in Lugosi & Mendelson (2016), offer an appealing alternative, from a theoretical perspective at least, to the principle of Empirical Risk Minimization in machine learning. Statistical learning by Median- of-Means (MoM) basically consists in segmenting the training data into blocks of equal size and com- paring the statistical performance of ev
Quantifying Generalization in Reinforcement Learning  Karl Cobbe 1 Oleg Klimov 1 Chris Hesse 1 Taehoon Kim 1 John Schulman 1  Abstract  In this paper, we investigate the problem of over- ﬁtting in deep reinforcement learning. Among the most common benchmarks in RL, it is cus- tomary to use the same environments for both training and testing. This practice offers rela- tively little insight into an agent’s ability to gen- eralize. We address this issue by using proce- durally generated environmen
Empirical Analysis of Beam Search Performance Degradation  in Neural Sequence Models  Eldan Cohen 1 J. Christopher Beck 1  Abstract  Beam search is the most popular inference al- gorithm for decoding neural sequence models. Unlike greedy search, beam search allows for non-greedy local decisions that can potentially lead to a sequence with a higher overall probabil- ity. However, work on a number of applications has found that the quality of the highest proba- bility hypothesis found by beam sear
Learning Linear-Quadratic Regulators Efﬁciently with only  √  T Regret  Alon Cohen 1 2 Tomer Koren 3 Yishay Mansour 1 4  Abstract √  gorithm with (cid:101)O(  We present the ﬁrst computationally-efﬁcient al- T) regret for learning in Lin- ear Quadratic Control systems with unknown dy- namics. By that, we resolve an open question of Abbasi-Yadkori and Szepesv´ari (2011) and Dean, Mania, Matni, Recht, and Tu (2018).  1. Introduction Optimal control theory dates back to the 1950s, and has been appl
Certiﬁed Adversarial Robustness via Randomized Smoothing  Jeremy Cohen 1 Elan Rosenfeld 1 J. Zico Kolter 1 2  Abstract  We show how to turn any classiﬁer that classiﬁes well under Gaussian noise into a new classiﬁer that is certiﬁably robust to adversarial perturba- tions under the (cid:96)2 norm. While this “randomized smoothing” technique has been proposed before in the literature, we are the ﬁrst to provide a tight analysis, which establishes a close connection between (cid:96)2 robustness an
Gauge Equivariant Convolutional Networks and the Icosahedral CNN  Taco S. Cohen * 1 Maurice Weiler * 2 Berkay Kicanaoglu * 2 Max Welling 1  Abstract  The principle of equivariance to symmetry trans- formations enables a theoretically grounded ap- proach to neural network architecture design. Equivariant networks have shown excellent per- formance and data efﬁciency on vision and med- ical imaging problems that exhibit symmetries. Here we show how this principle can be extended beyond global symm
CURIOUS: Intrinsically Motivated Modular  Multi-Goal Reinforcement Learning  C´edric Colas 1 Pierre Fournier 2 Olivier Sigaud 2 Mohamed Chetouani 2 Pierre-Yves Oudeyer 1  Abstract  In open-ended environments, autonomous learn- ing agents must set their own goals and build their own curriculum through an intrinsically motivated exploration. They may consider a large diversity of goals, aiming to discover what is controllable in their environments, and what is not. Because some goals might prove e
A Fully Differentiable Beam Search Decoder  Ronan Collobert 1 Awni Hannun 1 Gabriel Synnaeve 1  Abstract  We introduce a new beam search decoder that is fully differentiable, making it possible to optimize at training time through the inference procedure. Our decoder allows us to combine models which operate at different granularities (e.g. acoustic and language models). It can be used when target se- quences are not aligned to input sequences by con- sidering all possible alignments between the
Scalable Metropolis–Hastings for Exact Bayesian Inference with Large Datasets  Rob Cornish 1 Paul Vanetti 1 Alexandre Bouchard-Cˆot´e 2 George Deligiannidis 1 3 Arnaud Doucet 1 3  Abstract  Bayesian inference via standard Markov Chain Monte Carlo (MCMC) methods is too computa- tionally intensive to handle large datasets, since the cost per step usually scales like Θ(n) in the number of data points n. We propose the Scal- able Metropolis–Hastings (SMH) kernel that ex- ploits Gaussian concentratio
Adjustment Criteria for Generalizing Experimental Findings  Juan D. Correa 1 Jin Tian 2 Elias Bareinboim 1  Abstract  Generalizing causal effects from a controlled ex- periment to settings beyond the particular study population is arguably one of the central tasks found in empirical circles. While a proper design and careful execution of the experiment would support, under mild conditions, the validity of inferences about the population in which the ex- periment was conducted, two challenges mak
Online Learning with Sleeping Experts and Feedback Graphs  Corinna Cortes 1 Giulia DeSalvo 1 Claudio Gentile 1 Mehryar Mohri 1 2 Scott Yang 3  Abstract  We consider the scenario of online learning with sleeping experts, where not all experts are avail- able at each round, and analyze the general frame- work of learning with feedback graphs, where the loss observations associated with each expert are characterized by a graph. A critical assumption in this framework is that the loss observations a
Active Learning with Disagreement Graphs  Corinna Cortes 1 Giulia DeSalvo 1 Claudio Gentile 1 Mehryar Mohri 1 2 Ningshan Zhang 3  Abstract  We present two novel enhancements of an on- line importance-weighted active learning algo- rithm IWAL, using the properties of disagreements among hypotheses. The ﬁrst enhancement, IWAL- D, prunes the hypothesis set with a more aggres- sive strategy based on the disagreement graph. We show that IWAL-D improves the generaliza- tion performance and the label c
Shape Constraints for Set Functions  Andrew Cotter 1 Maya R. Gupta 1 Heinrich Jiang 1 Erez Louidor 1 James Muller 1 Taman Narayan 1  Serena Wang 1 Tao Zhu 1  Abstract  Set functions predict a label from a permutation- invariant variable-size collection of feature vec- tors. We propose making set functions more un- derstandable and regularized by capturing domain knowledge through shape constraints. We show how prior work in monotonic constraints can be adapted to set functions, and then propose 
Training Well-Generalizing Classiﬁers for Fairness Metrics and Other  Data-Dependent Constraints  Andrew Cotter 1 Maya Gupta 1 Heinrich Jiang 1 Nathan Srebro 2 Karthik Sridharan 3 Serena Wang 1  Blake Woodworth 2 Seungil You 4  Abstract  Classiﬁers can be trained with data-dependent constraints to satisfy fairness goals, reduce churn, achieve a targeted false positive rate, or other pol- icy goals. We study the generalization perfor- mance for such constrained optimization prob- lems, in terms o
Monge blunts Bayes: Hardness Results for Adversarial Training  Zac Cranko 1 2 Aditya Krishna Menon 3 Richard Nock 2 1 4 Cheng Soon Ong 2 1 Zhan Shi 5 Christian Walder 2 1  Abstract  The last few years have seen a staggering number of empirical studies of the robustness of neural networks in a model of adversarial perturbations of their inputs. Most rely on an adversary which carries out local modiﬁcations within prescribed balls. None however has so far questioned the broader picture: how to fra
Boosted Density Estimation Remastered  Zac Cranko 1 2 Richard Nock 2 1 3  Abstract  There has recently been a steady increase in the number iterative approaches to density estimation. However, an accompanying burst of formal con- vergence guarantees has not followed; all results pay the price of heavy assumptions which are of- ten unrealistic or hard to check. The Generative Adversarial Network (GAN) literature — seem- ingly orthogonal to the aforementioned pursuit — has had the side effect of a
Submodular Cost Submodular Cover with an Approximate Oracle  Victoria G. Crawford 1 Alan Kuhnle 2 My T. Thai 1  Abstract  In this work, we study the Submodular Cost Sub- modular Cover problem, which is to minimize the submodular cost required to ensure that the sub- modular beneﬁt function exceeds a given thresh- old. Existing approximation ratios for the greedy algorithm assume a value oracle to the beneﬁt function. However, access to a value oracle is not a realistic assumption for many applic
Flexibly Fair Representation Learning by Disentanglement  Elliot Creager 1 2 David Madras 1 2 J¨orn-Henrik Jacobsen 2 Marissa A. Weis 2 3 Kevin Swersky 4  Toniann Pitassi 1 2 Richard Zemel 1 2  Abstract  We consider the problem of learning representa- tions that achieve group and subgroup fairness with respect to multiple sensitive attributes. Tak- ing inspiration from the disentangled represen- tation learning literature, we propose an algo- rithm for learning compact representations of dataset
Anytime Online-to-Batch, Optimism and Acceleration  Ashok Cutkosky 1  Abstract  A standard way to obtain convergence guaran- tees in stochastic convex optimization is to run an online learning algorithm and then output the average of its iterates: the actual iterates of the online learning algorithm do not come with indi- vidual guarantees. We close this gap by introduc- ing a black-box modiﬁcation to any online learn- ing algorithm whose iterates converge to the op- timum in stochastic scenario
Matrix-FreePreconditioninginOnlineLearningAshokCutkosky1TamasSarlos1AbstractWeprovideanonlineconvexoptimizationalgo-rithmwithregretthatinterpolatesbetweentheregretofanalgorithmusinganoptimalprecon-ditioningmatrixandoneusingadiagonalpre-conditioningmatrix.Ourregretboundisneverworsethanthatobtainedbydiagonalprecondi-tioning,andincertainsettingevensurpassesthatofalgorithmswithfull-matrixpreconditioning.Importantly,ouralgorithmrunsinthesametimeandspacecomplexityasonlinegradientdescent.Alongthewaywei
Minimal Achievable Sufﬁcient Statistic Learning  Milan Cvitkovic 1 G¨unther Koliander 2  Abstract  We introduce Minimal Achievable Sufﬁcient Statistic (MASS) Learning, a machine learning training objective for which the minima are min- imal sufﬁcient statistics with respect to a class of functions being optimized over (e.g., deep networks). In deriving MASS Learning, we also introduce Conserved Differential Informa- tion (CDI), an information-theoretic quantity that — unlike standard mutual info
Open Vocabulary Learning on Source Code with a Graph–Structured Cache  Milan Cvitkovic 1 Badal Singh 2 Anima Anandkumar 1  Abstract  Machine learning models that take computer pro- gram source code as input typically use Natural Language Processing (NLP) techniques. How- ever, a major challenge is that code is written using an open, rapidly changing vocabulary due to, e.g., the coinage of new variable and method names. Reasoning over such a vocabulary is not something for which most NLP methods 
The Value Function Polytope in Reinforcement Learning  Robert Dadashi 1 Adrien Ali Ta¨ıga 1 2 Nicolas Le Roux 1 Dale Schuurmans 1 3 Marc G. Bellemare 1  Abstract  We establish geometric and topological proper- ties of the space of value functions in ﬁnite state- action Markov decision processes. Our main con- tribution is the characterization of the nature of its shape: a general polytope (Aigner et al., 2010). To demonstrate this result, we exhibit several proper- ties of the structural relatio
Bayesian Optimization Meets Bayesian Optimal Stopping  Zhongxiang Dai 1 Haibin Yu 1 Bryan Kian Hsiang Low 1 Patrick Jaillet 2  Abstract  Bayesian optimization (BO) is a popular paradigm for optimizing the hyperparameters of machine learning (ML) models due to its sample efﬁciency. Many ML models require running an iterative training procedure (e.g., stochastic gradient de- scent). This motivates the question whether in- formation available during the training process (e.g., validation accuracy a
Policy Certiﬁcates: Towards Accountable Reinforcement Learning  Christoph Dann 1 Lihong Li 2 Wei Wei 2 Emma Brunskill 3  Abstract  The performance of a reinforcement learning algo- rithm can vary drastically during learning because of exploration. Existing algorithms provide little information about the quality of their current pol- icy before executing it, and thus have limited use in high-stakes applications like healthcare. We address this lack of accountability by proposing that algorithms o
Learning Fast Algorithms for Linear Transforms Using Butterﬂy Factorizations  Tri Dao 1 Albert Gu 1 Matthew Eichhorn 2 Atri Rudra 2 Christopher R´e 1  Abstract  Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix- vector multiplication, yet each has a specialized and highly efﬁcient (subquadratic) algorit
A Kernel Theory of Modern Data Augmentation  Tri Dao 1 Albert Gu 1 Alexander J. Ratner 1 Virginia Smith 2 Christopher De Sa 3 Christopher R´e 1  Abstract  Data augmentation, a technique in which a train- ing set is expanded with class-preserving transfor- mations, is ubiquitous in modern machine learn- ing pipelines. In this paper, we seek to establish a theoretical framework for understanding data aug- mentation. We approach this from two directions: First, we provide a general model of augment
TarMAC: Targeted Multi-Agent Communication  Abhishek Das 1 ‹ Théophile Gervet 2 Joshua Romoff 2 3  Dhruv Batra 1 3 Devi Parikh 1 3 Michael Rabbat 2 3 Joelle Pineau 2 3  Abstract  We propose a targeted communication archi- tecture for multi-agent reinforcement learning, where agents learn both what messages to send and whom to address them to while performing cooperative tasks in partially-observable environ- ments. This targeting behavior is learnt solely from downstream task-speciﬁc reward with
Teaching a black-box learner  Sanjoy Dasgupta 1 Daniel Hsu 2 Stefanos Poulis 1 3 Xiaojin Zhu 4  Abstract  One widely-studied model of teaching (Goldman & Kearns, 1995; Shinohara & Miyano, 1991; An- thony et al., 1992) calls for a teacher to provide the minimal set of labeled examples that uniquely speciﬁes a target concept. The assumption is that the teacher knows the learner’s hypothesis class, which is often not true of real-life teaching sce- narios. We consider the problem of teaching a lear
Stochastic Deep Networks  Gwendoline de Bie 1 Gabriel Peyr´e 2 1 Marco Cuturi 3 4  Abstract  Machine learning is increasingly targeting areas where input data cannot be accurately described by a single vector, but can be modeled instead us- ing the more ﬂexible concept of random vectors, namely probability measures or more simply point clouds of varying cardinality. Using deep architectures on measures poses, however, many challenging issues. Indeed, deep architectures are originally designed to
Learning-to-Learn Stochastic Gradient Descent with Biased Regularization  Giulia Denevi 1 2 Carlo Ciliberto 3 4 Riccardo Grazzi 1 4 Massimiliano Pontil 1 4  Abstract  We study the problem of learning-to-learn: infer- ring a learning algorithm that works well on a fam- ily of tasks sampled from an unknown distribution. As class of algorithms we consider Stochastic Gra- dient Descent (SGD) on the true risk regularized by the square euclidean distance from a bias vec- tor. We present an average exc
A Multitask Multiple Kernel Learning Algorithm for Survival Analysis  with Application to Cancer Biology  Onur Dereli 1 Ceyda O˘guz 2 Mehmet G¨onen 2 3 4  Abstract  Predictive performance of machine learning al- gorithms on related problems can be improved using multitask learning approaches. Rather than performing survival analysis on each data set to predict survival times of cancer patients, we de- veloped a novel multitask approach based on mul- tiple kernel learning (MKL). Our multitask MKL
Learning to Convolve: A Generalized Weight-Tying Approach  Nichita Diaconu 1 * Daniel Worrall 1 *  Abstract  Recent work (Cohen & Welling, 2016a) has shown that generalizations of convolutions, based on group theory, provide powerful inductive bi- ases for learning. In these generalizations, ﬁlters are not only translated but can also be rotated, ﬂipped, etc. However, coming up with exact mod- els of how to rotate a 3 × 3 ﬁlter on a square pixel-grid is difﬁcult. In this paper, we learn how to t
SEVER: A Robust Meta-Algorithm for Stochastic Optimization1  Ilias Diakonikolas * 1 Gautam Kamath * 2 Daniel M. Kane * 3 Jerry Li * 4 Jacob Steinhardt * 5 Alistair Stewart * 6  Abstract  In high dimensions, most machine learning meth- ods are brittle to even a small fraction of struc- tured outliers. To address this, we introduce a new meta-algorithm that can take in a base learner such as least squares or stochastic gradient de- scent, and harden the learner to be resistant to outliers. Our met
Approximated Oracle Filter Pruning for Destructive CNN Width Optimization  Xiaohan Ding 1 Guiguang Ding 1 Yuchen Guo 1 Jungong Han 2 Chenggang Yan 3  Abstract  It is not easy to design and run Convolutional Neu- ral Networks (CNNs) due to: 1) ﬁnding the opti- mal number of ﬁlters (i.e., the width) at each layer is tricky, given an architecture; and 2) the com- putational intensity of CNNs impedes the deploy- ment on computationally limited devices. Oracle Pruning is designed to remove the unimpo
Noisy Dual Principal Component Pursuit  Tianyu Ding * 1 Zhihui Zhu * 2 Tianjiao Ding 3 Yunchen Yang 3 Ren´e Vidal 2 Manolis C. Tsakiris 3 Daniel P. Robinson 1  Abstract  Dual Principal Component Pursuit (DPCP) is a recently proposed non-convex optimization based method for learning subspaces of high relative dimension from noiseless datasets contaminated by as many outliers as the square of the number of inliers. Experimentally, DPCP has proved to be robust to noise and outperform the popular RA
Finite-Time Analysis of Distributed TD(0) with Linear Function  Approximation for Multi-Agent Reinforcement Learning  Thinh T. Doan 1 2 Siva Theja Maguluri 1 Justin Romberg 2  Abstract  We study the policy evaluation problem in multi- agent reinforcement learning. In this problem, a group of agents work cooperatively to evaluate the value function for the global discounted ac- cumulative reward problem, which is composed of local rewards observed by the agents. Over a series of time steps, the a
Trajectory-Based Off-Policy Deep Reinforcement Learning  Andreas Doerr 1 2 3 Michael Volpp 1 Marc Toussaint 3 Sebastian Trimpe 2  Christian Daniel 1  Abstract  Policy gradient methods are powerful reinforce- ment learning algorithms and have been demon- strated to solve many complex tasks. However, these methods are also data-inefﬁcient, afﬂicted with high variance gradient estimates, and fre- quently get stuck in local optima. This work ad- dresses these weaknesses by combining recent improveme
Limitations of Adversarial Robustness: Strong No Free Lunch Theorem  Elvis Dohmatob 1  Abstract  This manuscript presents some new impossibil- ity results on adversarial robustness in machine learning, a very important yet largely open prob- lem. We show that if conditioned on a class label the data distribution satisﬁes the W2 Talagrand transportation-cost inequality (for example, this condition is satisﬁed if the conditional distribu- tion has density which is log-concave; is the uni- form mea
Width Provably Matters in Optimization for Deep Linear Neural Networks  Simon S. Du * 1 Wei Hu * 2  Abstract  We prove that for an L-layer fully-connected lin- ear neural network, if the width of every hidden  layer is e⌦L · r · dout · 3, where r and  are  the rank and the condition number of the input data, and dout is the output dimension, then gradi- ent descent with Gaussian random initialization converges to a global minimum at a linear rate. The number of iterations to ﬁnd an ✏-suboptima
Provably efﬁcient RL with Rich Observations via Latent State Decoding  Simon S. Du 1 Akshay Krishnamurthy 2 Nan Jiang 3 Alekh Agarwal 4 Miroslav Dud´ık 2 John Langford 2  Abstract  We study the exploration problem in episodic MDPs with rich observations generated from a small number of latent states. Under certain iden- tiﬁability assumptions, we demonstrate how to estimate a mapping from the observations to la- tent states inductively through a sequence of re- gression and clustering steps—wher
Gradient Descent Finds Global Minima of Deep Neural Networks  Simon S. Du * 1 Jason D. Lee * 2 Haochuan Li * 3 4 Liwei Wang * 5 4 Xiyu Zhai * 6  Abstract  Gradient descent ﬁnds a global minimum in training deep neural networks despite the objec- tive function being non-convex. The current pa- per proves gradient descent achieves zero train- ing loss in polynomial time for a deep over- parameterized neural network with residual con- nections (ResNet). Our analysis relies on the par- ticular struc
Incorporating Grouping Information into Bayesian Decision Tree Ensembles  Junliang Du 1 Antonio R. Linero 1  Abstract  We consider the problem of nonparametric re- gression in the high-dimensional setting in which P (cid:29) N. We study the use of overlapping group structures to improve prediction and variable se- lection. These structures arise commonly when analyzing DNA microarray data, where genes can naturally be grouped according to genetic path- ways. We incorporate overlapping group stru
Task-Agnostic Dynamics Priors for Deep Reinforcement Learning  Yilun Du 1 Karthik Narasimhan 2  Abstract  While model-based deep reinforcement learning (RL) holds great promise for sample efﬁciency and generalization, learning an accurate dynamics model is often challenging and requires substan- tial interaction with the environment. A wide variety of domains have dynamics that share com- mon foundations like the laws of classical me- chanics, which are rarely exploited by existing algorithms. I
Optimal Auctions through Deep Learning  Paul Dütting * 1 Zhe Feng * 2 Harikrishna Narasimham * 2 David C. Parkes * 2 Sai S. Ravindranath * 2  Abstract  Designing an incentive compatible auction that maximizes expected revenue is an intricate task. The single-item case was resolved in a seminal piece of work by Myerson in 1981. Even af- ter 30-40 years of intense research the problem remains unsolved for seemingly simple multi- bidder, multi-item settings. In this work, we initi- ate the explorat
Wasserstein of Wasserstein Loss for Learning Generative Models  Yonatan Dukler * 1 Wuchen Li * 1 Alex Tong Lin * 1 Guido Mont´ufar * 1 2 3  Abstract  The Wasserstein distance serves as a loss func- tion for unsupervised learning which depends on the choice of a ground metric on sample space. We propose to use a Wasserstein distance as the ground metric on the sample space of images. This ground metric is known as an effective dis- tance for image retrieval, since it correlates with human percept
Learning interpretable continuous-time models  of latent stochastic dynamical systems  Lea Duncker 1 Gerg˝o Bohner 1 Julien Boussard 2 Maneesh Sahani 1  Abstract  We develop an approach to learn an interpretable semi-parametric model of a latent continuous- time stochastic dynamical system, assuming noisy high-dimensional outputs sampled at uneven times. The dynamics are described by a nonlinear stochastic differential equation (SDE) driven by a Wiener process, with a drift evolution function dr
Autoregressive Energy Machines  Charlie Nash * 1 Conor Durkan * 1  Abstract  Neural density estimators are ﬂexible families of parametric models which have seen widespread use in unsupervised machine learning in recent years. Maximum-likelihood training typically dic- tates that these models be constrained to specify an explicit density. However, this limitation can be overcome by instead using a neural network to specify an energy function, or unnormalized density, which can subsequently be nor
Band-limited Training and Inference for Convolutional Neural Networks  Adam Dziedzic * 1 John Paparrizos * 1 Sanjay Krishnan 1 Aaron Elmore 1 Michael Franklin 1  Abstract  The convolutional layers are core building blocks of neural network architectures. In general, a con- volutional ﬁlter applies to the entire frequency spectrum of the input data. We explore artiﬁcially constraining the frequency spectra of these ﬁlters and data, called band-limiting, during training. The frequency domain const
Imitating Latent Policies from Observation  Ashley D. Edwards 1 Himanshu Sahni 1 Yannick Schroecker 1 Charles L. Isbell 1  Abstract  In this paper, we describe a novel approach to imitation learning that infers latent policies di- rectly from state observations. We introduce a method that characterizes the causal effects of la- tent actions on observations while simultaneously predicting their likelihood. We then outline an action alignment procedure that leverages a small amount of environment 
Semi-Cyclic Stochastic Gradient Descent  Hubert Eichner 1 Tomer Koren 1 H. Brendan McMahan 1 Nathan Srebro 2 Kunal Talwar 1  Abstract  We consider convex SGD updates with a block- cyclic structure, i.e., where each cycle consists of a small number of blocks, each with many samples from a possibly different, block-speciﬁc, distribution. This situation arises, e.g., in Fed- erated Learning where the mobile devices avail- able for updates at different times during the day have different characteris
GDPP: Learning Diverse Generations using Determinantal Point Processes  Mohamed Elfeki 1 Camille Couprie 2 Morgane Rivi`ere 2 Mohamed Elhoseiny 2 3  Abstract  Generative models have proven to be an outstand- ing tool for representing high-dimensional proba- bility distributions and generating realistic look- ing images. An essential characteristic of gen- erative models is their ability to produce multi- modal outputs. However, while training, they are often susceptible to mode collapse, that is
Sequential Facility Location: Approximate Submodularity  and Greedy Algorithm  Ehsan Elhamifar 1  Abstract  We develop and analyze a novel utility function and a fast optimization algorithm for subset se- lection in sequential data that incorporates the dynamic model of data. We propose a cardinality- constrained sequential facility location function that ﬁnds a ﬁxed number of representatives, where the sequence of representatives is compatible with the dynamic model and well encodes the data. A
Improved Convergence for `1 and `1 Regression via Iteratively Reweighted  Least Squares  Alina Ene * 1 Adrian Vladu * 1  Abstract  The iteratively reweighted least squares method (IRLS) is a popular technique used in practice for solving regression problems. Various versions of this method have been proposed, but their theoret- ical analyses failed to capture the good practical performance. In this paper we propose a simple and natural version of IRLS for solving `1 and `1 regres- sion, which pr
Exploring the Landscape of Spatial Robustness  Logan Engstrom * 1 Brandon Tran * 1 Dimitris Tsipras * 1 Ludwig Schmidt 1 Aleksander M ˛adry 1  Abstract  The study of adversarial robustness has so far largely focused on perturbations bound in (cid:96)p- norms. However, state-of-the-art models turn out to be also vulnerable to other, more natural classes of perturbations such as translations and rotations. In this work, we thoroughly investigate the vulnerability of neural network–based clas- siﬁe
Cross-Domain 3D Equivariant Image Embeddings  Carlos Esteves * 1 Avneesh Sud 2 Zhengyi Luo 1 Kostas Daniilidis 1 Ameesh Makadia 2  Abstract  Spherical convolutional networks have been intro- duced recently as tools to learn powerful feature representations of 3D shapes. Spherical CNNs are equivariant to 3D rotations making them ide- ally suited to applications where 3D data may be observed in arbitrary orientations. In this paper we learn 2D image embeddings with a similar equivariant structure:
On the Connection Between Adversarial Robustness and Saliency Map  Interpretability  Christian Etmann * 1 2 Sebastian Lunz * 3 Peter Maass 1 Carola-Bibiane Sch¨onlieb 3  Abstract  Recent studies on the adversarial vulnerability of neural networks have shown that models trained to be more robust to adversarial attacks exhibit more interpretable saliency maps than their non- robust counterparts. We aim to quantify this be- havior by considering the alignment between in- put image and saliency map.
Non-monotone Submodular Maximization with Nearly Optimal Adaptivity and Query Complexity  Matthew Fahrbach 1 Vahab Mirrokni 2 Morteza Zadimoghaddam 2  Abstract  Submodular maximization is a general optimiza- tion problem with a wide range of applications in machine learning (e.g., active learning, clus- tering, and feature selection). In large-scale op- timization, the parallel running time of an algo- rithm is governed by its adaptivity, which mea- sures the number of sequential rounds needed i
Multi-Frequency Vector Diffusion Maps  Yifeng Fan 1 Zhizhen Zhao 1  Abstract  We introduce multi-frequency vector diffusion maps (MFVDM), a new framework for organiz- ing and analyzing high dimensional datasets. The new method is a mathematical and algorithmic generalization of vector diffusion maps (VDM) and other non-linear dimensionality reduction methods. MFVDM combines different nonlin- ear embeddings of the data points deﬁned with multiple unitary irreducible representations of the alignme
Stable-Predictive Optimistic Counterfactual Regret Minimization  Gabriele Farina 1 Christian Kroer 2 Noam Brown 1 Tuomas Sandholm 1 3 4 5  Abstract  The CFR framework has been a powerful tool for solving large-scale extensive-form games in practice. However, the theoretical rate at which past CFR-based algorithms converge to the Nash equilibrium is on the order of O(T −1/2), where T is the number of iterations. In contrast, ﬁrst-order methods can be used to achieve a O(T −1) depen- dence on iter
Regret Circuits: Composability of Regret Minimizers  Gabriele Farina 1 Christian Kroer 2 Tuomas Sandholm 1 3 4 5  Abstract  Regret minimization is a powerful tool for solv- ing large-scale problems; it was recently used in breakthrough results for large-scale extensive- form game solving. This was achieved by com- posing simplex regret minimizers into an over- all regret-minimization framework for extensive- form game strategy spaces. In this paper we study the general composability of regret mi
Dead-ends and Secure Exploration in Reinforcement Learning  Mehdi Fatemi 1 Shikhar Sharma 1 Harm van Seijen 1 Samira Ebrahimi Kahou 2  Abstract  Many interesting applications of reinforcement learning (RL) involve MDPs that include numer- ous “dead-end” states. Upon reaching a dead-end state, the agent continues to interact with the envi- ronment in a dead-end trajectory before reaching an undesired terminal state, regardless of what- ever actions are chosen. The situation is even worse when exi
Invariant-EquivariantRepresentationLearningforMulti-ClassDataIlyaFeige1AbstractRepresentationslearntthroughdeepneuralnet-workstendtobehighlyinformative,butopaqueintermsofwhatinformationtheylearntoen-code.Weintroduceanapproachtoprobabilisticmodellingthatlearnstorepresentdatawithtwoseparatedeeprepresentations:aninvariantrep-resentationthatencodestheinformationoftheclassfromwhichthedatabelongs,andanequiv-ariantrepresentationthatencodesthesymmetrytransformationdeﬁningtheparticulardatapointwithinthec
The Advantages of Multiple Classes for Reducing Overﬁtting from Test Set Reuse  Vitaly Feldman 1 2 Roy Frostig 1 Moritz Hardt 3 4  Abstract  Excessive reuse of holdout data can lead to over- ﬁtting. Known results show that, in the worst- case, given the accuracies of k adaptively chosen classiﬁers on a dataset of size n, one can cre-  ate a classiﬁer with a bias of Θ((cid:112)k/n) for any per bound of ˜O(max{(cid:112)k log(n)/(mn), k/n}) bias of Ω((cid:112)k/(m2n)) and improves on previous  on t
Decentralized Exploration in Multi-Armed Bandits  Raphaël Féraud 1 Réda Alami 1 Romain Laroche 2  Abstract  We consider the decentralized exploration prob- lem: a set of players collaborate to identify the best arm by asynchronously interacting with the same stochastic environment. The objective is to ensure privacy in the best arm identiﬁcation prob- lem between asynchronous, collaborative, and thrifty players. In the context of a digital ser- vice, we advocate that this decentralized approach 
Almost surely constrained convex optimization  Olivier Fercoq 1 Ahmet Alacaoglu 2 Ion Necoara 3 Volkan Cevher 2  Abstract  We propose a stochastic gradient framework for solving stochastic composite convex optimiza- tion problems with (possibly) inﬁnite number of linear inclusion constraints that need to be satis- ﬁed almost surely. We use smoothing and homo- topy techniques to handle constraints without the √ need for matrix-valued projections. We show for our stochastic gradient algorithm O(lo
Online Meta-Learning  Chelsea Finn * 1 Aravind Rajeswaran * 2 Sham Kakade 2 Sergey Levine 1  Abstract  A central capability of intelligent systems is the ability to continuously build upon previous expe- riences to speed up and enhance learning of new tasks. Two distinct research paradigms have stud- ied this question. Meta-learning views this prob- lem as learning a prior over model parameters that is amenable for fast adaptation on a new task, but typically assumes the tasks are available toge
DL2: Training and Querying Neural Networks with Logic  Marc Fischer 1 Mislav Balunovi´c 1 Dana Drachsler-Cohen 1 Timon Gehr 1 Ce Zhang 1 Martin Vechev 1  Abstract  We present DL2, a system for training and query- ing neural networks with logical constraints. Us- ing DL2, one can declaratively specify domain knowledge constraints to be enforced during train- ing, as well as pose queries on the model to ﬁnd inputs that satisfy a set of constraints. DL2 works by translating logical constraints into
Bayesian Action Decoder for Deep Multi-Agent Reinforcement Learning  Jakob N. Foerster * 1 2 H. Francis Song * 3 Edward Hughes 3 Neil Burch 3 Iain Dunning 3 Shimon Whiteson 1  Matthew M. Botvinick 3 Michael Bowling 3  Abstract  When observing the actions of others, humans make inferences about why they acted as they did, and what this implies about the world; hu- mans also use the fact that their actions will be interpreted in this manner, allowing them to act in- formatively and thereby communi
Scalable Nonparametric Sampling from Multimodal Posteriors  with the Posterior Bootstrap  Edwin Fong 1 2 Simon Lyddon 1 Chris Holmes 1 2  Abstract  Increasingly complex datasets pose a number of challenges for Bayesian inference. Conventional posterior sampling based on Markov chain Monte Carlo can be too computationally intensive, is se- rial in nature and mixes poorly between posterior modes. Furthermore, all models are misspeciﬁed, which brings into question the validity of the conventional B
On Discriminative Learning of Prediction Uncertainty  Vojtech Franc 1 Daniel Prusa 1  Abstract  In classiﬁcation with a reject option, the classi- ﬁer is allowed in uncertain cases to abstain from prediction. The classical cost based model of an optimal classiﬁer with a reject option requires the cost of rejection to be deﬁned explicitly. An al- ternative bounded-improvement model, avoiding the notion of the reject cost, seeks for a classi- ﬁer with a guaranteed selective risk and maximal cover.
Learning Discrete Structures for Graph Neural Networks  Luca Franceschi 1 2 Mathias Niepert 3 Massimiliano Pontil 1 2 Xiao He 3  Abstract  Graph neural networks (GNNs) are a popular class of machine learning models whose major advantage is their ability to incorporate a sparse and discrete dependency structure between data points. Unfortunately, GNNs can only be used when such a graph-structure is available. In prac- tice, however, real-world graphs are often noisy and incomplete or might not be
Distributional Multivariate Policy Evaluation and Exploration with the  Bellman GAN  Dror Freirich 1 Tzahi Shimkin 1 Ron Meir 1 Aviv Tamar 2  Abstract  The recently proposed distributional approach to reinforcement learning (DiRL) is centered on learning the distribution of the reward-to-go, often referred to as the value distribution. In this work, we show that the distributional Bellman equation, which drives DiRL methods, is equivalent to a generative adversarial network (GAN) model. In this 
Approximating Orthogonal Matrices with Effective Givens Factorization  Thomas Frerix 1 Joan Bruna 2  Abstract  We analyze effective approximation of unitary matrices. In our formulation, a unitary matrix is represented as a product of rotations in two- dimensional subspaces, so-called Givens rota- tions. Instead of the quadratic dimension depen- dence when applying a dense matrix, applying such an approximation scales with the number factors, each of which can be implemented efﬁ- ciently. Conseq
Fast and Flexible Inference of Joint Distributions from their Marginals  Charlie Frogner 1 Tomaso Poggio 2  Abstract  Across the social sciences and elsewhere, practi- tioners frequently have to reason about relation- ships between random variables, despite lacking joint observations of the variables. This is some- times called an “ecological” inference; given sam- ples from the marginal distributions of the vari- ables, one attempts to infer their joint distribution. The problem is inherently i
Analyzing and Improving Representations with the Soft Nearest Neighbor Loss  Nicholas Frosst 1 Nicolas Papernot 1 Geoffrey Hinton 1  Abstract  We explore and expand the Soft Nearest Neighbor Loss to measure the entanglement of class mani- folds in representation space: i.e., how close pairs of points from the same class are relative to pairs of points from different classes. We demonstrate several use cases of the loss. As an analytical tool, it provides insights into the evolution of class sim-
Diagnosing Bottlenecks in Deep Q-learning Algorithms  Justin Fu * 1 Aviral Kumar * 1 Matthew Soh 1 Sergey Levine 1  Abstract  Q-learning methods are a common class of al- gorithms used in reinforcement learning (RL). However, their behavior with function approxi- mation, especially with neural networks, is poorly understood theoretically and empirically. In this work, we aim to experimentally investigate po- tential issues in Q-learning, by means of a ”unit testing” framework where we can utiliz
MetricGAN: Generative Adversarial Networks based Black-box Metric Scores  Optimization for Speech Enhancement  Szu-Wei Fu 1 2 Chien-Feng Liao 1 2 Yu Tsao 2 Shou-De Lin 1  Abstract  Adversarial loss in a conditional generative ad- versarial network (GAN) is not designed to di- rectly optimize evaluation metrics of a target task, and thus, may not always guide the generator in a GAN to generate data with improved metric scores. To overcome this issue, we propose a novel MetricGAN approach with an 
Beyond Adaptive Submodularity: Approximation Guarantees of  Greedy Policy with Adaptive Submodularity Ratio  Kaito Fujii 1 Shinsaku Sakaue 2  Abstract  We propose a new concept named adaptive sub- modularity ratio to study the greedy policy for sequential decision making. While the greedy policy is known to perform well for a wide variety of adaptive stochastic optimization problems in practice, its theoretical properties have been ana- lyzed only for a limited class of problems. We nar- row the
Off-Policy Deep Reinforcement Learning without Exploration  Scott Fujimoto 1 2 David Meger 1 2 Doina Precup 1 2  Abstract  Many practical applications of reinforcement learning constrain agents to learn from a ﬁxed batch of data which has already been gathered, without offering further possibility for data col- lection. In this paper, we demonstrate that due to errors introduced by extrapolation, standard off- policy deep reinforcement learning algorithms, such as DQN and DDPG, are incapable of 
Transfer Learning for Related Reinforcement Learning Tasks via  Image-to-Image Translation  Shani Gamrian 1 Yoav Goldberg 1 2  Abstract  Despite the remarkable success of Deep RL in learning control policies from raw pixels, the re- sulting models do not generalize. We demonstrate that a trained agent fails completely when fac- ing small visual changes, and that ﬁne-tuning— the common transfer learning paradigm—fails to adapt to these changes, to the extent that it is faster to re-train the mode
Breaking the Softmax Bottleneck via Learnable Monotonic Pointwise  Non-linearities  Octavian-Eugen Ganea 1 * Sylvain Gelly 2 Gary Bécigneul 1 Aliaksei Severyn 3  Abstract  The Softmax function on top of a ﬁnal linear layer is the de facto method to output probability distri- butions in neural networks. In many applications such as language models or text generation, this model has to produce distributions over large out- put vocabularies. Recently, this has been shown to have limited representat
Graph U-Nets  Hongyang Gao 1 Shuiwang Ji 1  Abstract  We consider the problem of representation learn- ing for graph data. Convolutional neural networks can naturally operate on images, but have sig- niﬁcant challenges in dealing with graph data. Given images are special cases of graphs with nodes lie on 2D lattices, graph embedding tasks have a natural correspondence with image pixel- wise prediction tasks such as segmentation. While encoder-decoder architectures like U-Nets have been successfu
Deep Generative Learning via Variational Gradient Flow  Yuan Gao 1 Yuling Jiao 2 Yang Wang 3 Yao Wang 4 Can Yang 3 Shunkang Zhang 3  Abstract  We propose a framework to learn deep generative models via Variational Gradient Flow (VGrow) on probability spaces. The evolving distribution that asymptotically converges to the target dis- tribution is governed by a vector ﬁeld, which is the negative gradient of the ﬁrst variation of the f-divergence between them. We prove that the evolving distribution
Rate Distortion For Model Compression:  From Theory To Practice  Weihao Gao 1 Yu-Han Liu 2 Chong Wang 3 Sewoong Oh 4  Abstract  The enormous size of modern deep neural net- works makes it challenging to deploy those mod- els in memory and communication limited scenar- ios. Thus, compressing a trained model without a signiﬁcant loss in performance has become an in- creasingly important task. Tremendous advances has been made recently, where the main techni- cal building blocks are pruning, quanti
Demystifying Dropout  Hongchang Gao 1 2 Jian Pei 3 4 Heng Huang 1 2  Abstract  Dropout is a popular technique to train large-scale deep neural networks to alleviate the overﬁtting problem. To disclose the underlying reason for its gain, numerous works have tried to explain it from different perspectives. In this paper, un- like existing works, we explore it from a new perspective to provide new insight into this line of research. In detail, we disentangle the forward and backward pass of dropout
Geometric Scattering for Graph Data Analysis  Feng Gao 1 2 Guy Wolf * 3 Matthew Hirn * 1 4  Abstract  We explore the generalization of scattering trans- forms from traditional (e.g., image or audio) sig- nals to graph data, analogous to the generalization of ConvNets in geometric deep learning, and the utility of extracted graph features in graph data analysis. In particular, we focus on the capacity of these features to retain informative variability and relations in the data (e.g., between ind
Multi-Frequency Phase Synchronization  Tingran Gao * 1 Zhizhen Zhao * 2  Abstract  We propose a novel formulation for phase syn- chronization—the statistical problem of jointly estimating alignment angles from noisy pairwise comparisons—as a nonconvex optimization prob- lem that enforces consistency among the pairwise comparisons in multiple frequency channels. In- spired by harmonic retrieval in signal processing, we develop a simple yet efﬁcient two-stage algo- rithm that leverages the multi-f
Optimal Mini-Batch and Step Sizes for SAGA  Nidham Gazagnadou 1 Robert M. Gower 1 Joseph Salmon 2  Abstract  Recently it has been shown that the step sizes of a family of variance reduced gradient meth- ods called the JacSketch methods depend on the expected smoothness constant. In particular, if this expected smoothness constant could be cal- culated a priori, then one could safely set much larger step sizes which would result in a much faster convergence rate. We ﬁll in this gap, and provide s
SelectiveNet: A Deep Neural Network with an Integrated Reject Option  Yonatan Geifman 1 Ran El-Yaniv 1  Abstract  We consider the problem of selective prediction (also known as reject option) in deep neural net- works, and introduce SelectiveNet, a deep neural architecture with an integrated reject option. Ex- isting rejection mechanisms are based mostly on a threshold over the prediction conﬁdence of a pre-trained network. In contrast, SelectiveNet is trained to optimize both classiﬁcation (or 
A Theory of Regularized Markov Decision Processes  Matthieu Geist 1 Bruno Scherrer 2 Olivier Pietquin 1  Abstract  Many recent successful (deep) reinforcement learning algorithms make use of regularization, generally based on entropy or Kullback-Leibler di- vergence. We propose a general theory of regular- ized Markov Decision Processes that generalizes these approaches in two directions: we consider a larger class of regularizers, and we consider the general modiﬁed policy iteration approach, e
DeepMDP: Learning Continuous Latent Space Models  for Representation Learning  Carles Gelada 1 Saurabh Kumar 1 Jacob Buckman 2 Oﬁr Nachum 1 Marc G. Bellemare 1  Abstract  Many reinforcement learning (RL) tasks provide the agent with high-dimensional observations that can be simpliﬁed into low-dimensional continu- ous states. To formalize this process, we introduce the concept of a DeepMDP, a parameterized latent space model that is trained via the minimization of two tractable latent space losse
Partially Linear Additive Gaussian Graphical Models  Sinong Geng 1 Minhao Yan 2 Mladen Kolar 3 Oluwasanmi Koyejo 4  Abstract  We propose a partially linear additive Gaussian graphical model (PLA-GGM) for the estimation of associations between random variables dis- torted by observed confounders. Model parame- ters are estimated using an L1-regularized max- imal pseudo-proﬁle likelihood estimator (MaP- PLE) for which we prove n-sparsistency. Im- portantly, our approach avoids parametric con- stra
Learning and Data Selection in Big Datasets  Hossein S. Ghadikolaei 1 Hadi Ghauch 1 2 Carlo Fischione 1 Mikael Skoglund 1  Abstract  Finding a dataset of minimal cardinality to char- acterize the optimal parameters of a model is of paramount importance in machine learning and distributed optimization over a network. This paper investigates the compressibility of large datasets. More speciﬁcally, we propose a frame- work that jointly learns the input-output mapping as well as the most representat
Improved Parallel Algorithms for Density-Based Network Clustering  Mohsen Ghaffari * 1 Silvio Lattanzi * 2 Slobodan Mitrovi´c * 3  Abstract  Clustering large-scale networks is a central topic in unsupervised learning with many applications in machine learning and data mining. A classic approach to cluster a network is to identify re- gions of high edge density, which in the literature is captured by two fundamental problems: the densest subgraph and the k-core decomposition problems. We design m
Recursive Sketches for Modular Deep  Learning  Badih Ghazi * 1 Rina Panigrahy * 1  Joshua R. Wang * 1  Abstract  We present a mechanism to compute a sketch (suc- cinct summary) of how a complex modular deep network processes its inputs. The sketch summa- rizes essential information about the inputs and outputs of the network and can be used to quickly identify key components and summary statistics of the inputs. Furthermore, the sketch is recursive and can be unrolled to identify sub-components 
An Instability in Variational Inference for Topic Models  Behrooz Ghorbani 1 Hamid Javadi 2 Andrea Montanari 1 3  Abstract  Naive mean ﬁeld variational methods are the state- of-the-art approach to inference in topic models. We show that these methods suffer from an in- stability that can produce misleading conclusions. Namely, for certain regimes of the model parame- ters, variational inference outputs a non-trivial de- composition into topics. However -–for the same parameter values-– the data
An Investigation into Neural Net Optimization via Hessian Eigenvalue Density  Behrooz Ghorbani 1 2 Shankar Krishnan 2 Ying Xiao 2  Abstract  To understand the dynamics of optimization in deep neural networks, we develop a tool to study the evolution of the entire Hessian spectrum throughout the optimization process. Using this, we study a number of hypotheses concerning smoothness, curvature, and sharpness in the deep learning literature. We then thoroughly analyze a crucial structural feature o
Data Shapley: Equitable Valuation of Data for Machine Learning  Amirata Ghorbani 1 James Zou 2  Abstract  As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic predictions and decisions. For example, in health- care and consumer markets, it has been suggested that individuals should be compensated for the data that they generate, but it is not clear what is an equitable valuation for individual data. In thi
Efﬁcient Dictionary Learning with Gradient Descent  Dar Gilboa 1 2 Sam Buchanan 3 2 John Wright 3 2  Abstract  Randomly initialized ﬁrst-order optimization al- gorithms are the method of choice for solving many high-dimensional nonconvex problems in machine learning, yet general theoretical guaran- tees cannot rule out convergence to critical points of poor objective value. For some highly struc- tured nonconvex problems however, the success of gradient descent can be understood by study- ing th
A Tree-Based Method for Fast Repeated Sampling of  Determinantal Point Processes  Jennifer Gillenwater 1 Alex Kulesza 1 Zelda Mariet 2 3 Sergei Vassilvitskii 1  Abstract  It is often desirable in recommender systems and other information retrieval applications to provide diverse results, and determinantal point processes (DPPs) have become a popular way to capture the trade-off between the quality of individual results and the diversity of the overall set. However, sam- pling from a DPP is inher
Learning to Groove with Inverse Sequence Transformations  Jon Gillick 1 2 Adam Roberts 2 Jesse Engel 2 Douglas Eck 2 David Bamman 1  Abstract  We explore models for translating abstract musi- cal ideas (scores, rhythms) into expressive perfor- mances using Seq2Seq and recurrent variational Information Bottleneck (VIB) models. Though Seq2Seq models usually require painstakingly aligned corpora, we show that it is possible to adapt an approach from the Generative Adversar- ial Network (GAN) litera
Adversarial Examples Are a Natural Consequence of Test Error in Noise  Nicolas Ford * 1 2 Justin Gilmer * 1 Nicholas Carlini 1 Ekin D. Cubuk 1  Abstract  Over the last few years, the phenomenon of ad- versarial examples — maliciously constructed in- puts that fool trained machine learning models — has captured the attention of the research commu- nity, especially when the adversary is restricted to small modiﬁcations of a correctly handled in- put. Less surprisingly, image classiﬁers also lack h
Discovering Conditionally Salient Features with Statistical Guarantees  Jaime Roquero Gimenez 1 James Zou 2  Abstract  The goal of feature selection is to identify im- portant features that are relevant to explain an outcome variable. Most of the work in this do- main has focused on identifying globally relevant features, which are features that are related to the outcome using evidence across the entire dataset. We study a more ﬁne-grained statistical problem: conditional feature selection, whe
Estimating Information Flow in Deep Neural Networks  Ziv Goldfeld 1 2 Ewout van den Berg 2 3 Kristjan Greenewald 2 3 Igor Melnyk 2 3 Nam Nguyen 2 3  Brian Kingsbury 2 3 Yury Polyanskiy 1 2  Abstract  We study the estimation of the mutual information I(X; T(cid:96)) between the input X to a deep neural network (DNN) and the output vector T(cid:96) of its (cid:96)th hidden layer (an “internal representation”). Focusing on feedforward networks with ﬁxed weights and noisy internal representations, w
Amortized Monte Carlo Integration  Adam Goli´nski * 1 2 Frank Wood 3 Tom Rainforth * 1  Abstract  Current approaches to amortizing Bayesian in- ference focus solely on approximating the pos- terior distribution. Typically, this approxima- tion is, in turn, used to calculate expectations for one or more target functions—a computational pipeline which is inefﬁcient when the target func- tion(s) are known upfront. In this paper, we ad- dress this inefﬁciency by introducing AMCI, a method for amorti
Online Algorithms for Rent-or-Buy with Expert Advice  Sreenivas Gollapudi * 1 Debmalya Panigrahi * 2  Abstract  We study the use of predictions by multiple ex- perts (such as machine learning algorithms) to improve the performance of online algorithms. In particular, we consider the classical rent-or-buy problem (also called ski rental), and obtain algo- rithms that provably improve their performance over the adversarial scenario by using these pre- dictions. We also prove matching lower bounds 
The Information-Theoretic Value of Unlabeled Data in Semi-Supervised  Learning  Alexander Golovnev 1 D´avid P´al 2 Bal´azs Sz¨or´enyi 2  Abstract  We quantify the separation between the numbers of labeled examples required to learn in two set- tings: Settings with and without the knowledge of the distribution of the unlabeled data. More speciﬁcally, we prove a separation by Θ(log n) multiplicative factor for the class of projections over the Boolean hypercube of dimension n. We prove that there 
Efﬁcient Training of BERT by Progressively Stacking  Linyuan Gong 1 Di He 1 Zhuohan Li 1 Tao Qin 2 Liwei Wang 1 3 Tie-Yan Liu 2  Abstract  Unsupervised pre-training is commonly used in natural language processing: a deep neural net- work trained with proper unsupervised prediction tasks are shown to be effective in many down- stream tasks. Because it is easy to create a large monolingual dataset by collecting data from the Web, we can train high-capacity models. There- fore, training efﬁciency b
Quantile Stein Variational Gradient Descent for Batch Bayesian Optimization  Chengyue Gong 1 Jian Peng 2 Qiang Liu 1  Abstract  Batch Bayesian optimization has been shown to be an efﬁcient and successful approach for black- box function optimization, especially when the evaluation of cost function is highly expensive but can be efﬁciently parallelized. In this paper, we introduce a novel variational framework for batch query optimization, based on the argument that the query batch should be sele
Obtaining Fairness using Optimal Transport Theory  Eustasio del Barrio * 1 Fabrice Gamboa * 2 Paula Gordaliza * 2 1 Jean-Michel Loubes * 2  Abstract  In the fair classiﬁcation setup, we recast the links between fairness and predictability in terms of probability metrics. We analyze repair methods based on mapping conditional distributions to the Wasserstein barycenter. We propose a Random Repair which yields a tradeoff between minimal information loss and a certain amount of fairness.  1. Introd
Combining Parametric and Nonparametric Models for Off-Policy Evaluation  Omer Gottesman 1 Yao Liu 2 Scott Sussex 1 Emma Brunskill 2 Finale Doshi-Velez 1  Abstract  We consider a model-based approach to per- form batch off-policy evaluation in reinforce- ment learning. Our method takes a mixture-of- experts approach to combine parametric and non- parametric models of the environment such that the ﬁnal value estimate has the least expected er- ror. We do so by ﬁrst estimating the local accu- racy 
Counterfactual Visual Explanations  Yash Goyal 1 Ziyan Wu 2 Jan Ernst 2 Dhruv Batra 1 Devi Parikh 1 Stefan Lee 1  Abstract  In this work, we develop a technique to pro- duce counterfactual visual explanations. Given a ‘query’ image I for which a vision system pre- dicts class c, a counterfactual visual explanation identiﬁes how I could change such that the sys- tem would output a different speciﬁed class c(cid:48). To do this, we select a ‘distractor’ image I(cid:48) that the system predicts as 
Adaptive Sensor Placement for Continuous Spaces  James A. Grant 1 2 Alexis Boukouvalas 1 Ryan-Rhys Grifﬁths 3 David S. Leslie 1 4 Sattar Vakili 1  Enrique Munoz de Cote 1  Abstract  We consider the problem of adaptively placing sensors along an interval to detect stochastically- generated events. We present a new formulation of the problem as a continuum-armed bandit prob- lem with feedback in the form of partial observa- tions of realisations of an inhomogeneous Pois- son process. We design a s
A Statistical Investigation of Long Memory in Language and Music  Alexander Greaves-Tunnell 1 Zaid Harchaoui 1  Abstract  Representation and learning of long-range depen- dencies is a central challenge confronted in mod- ern applications of machine learning to sequence data. Yet despite the prominence of this issue, the basic problem of measuring long-range depen- dence, either in a given data source or as repre- sented in a trained deep model, remains largely limited to heuristic tools. We cont
Automatic Posterior Transformation for Likelihood-free Inference  David S. Greenberg 1 Marcel Nonnenmacher 1 Jakob H. Macke 1  Abstract  How can one perform Bayesian inference on stochastic simulators with intractable likelihoods? A recent approach is to learn the posterior from adaptively proposed simulations using neural network-based conditional density estimators. However, existing methods are limited to a nar- row range of proposal distributions or require im- portance weighting that can li
Learning to Optimize Multigrid PDE Solvers  Daniel Greenfeld 1 Meirav Galun 1 Ron Kimmel 2 Irad Yavneh 2 Ronen Basri 1  Abstract  Constructing fast numerical solvers for partial dif- ferential equations (PDEs) is crucial for many scientiﬁc disciplines. A leading technique for solving large-scale PDEs is using multigrid meth- ods. At the core of a multigrid solver is the pro- longation matrix, which relates between different scales of the problem. This matrix is strongly problem-dependent, and it
Multi-Object Representation Learning with Iterative Variational Inference  Klaus Greff 1 2 Raphaël Lopez Kaufman 3 Rishabh Kabra 3 Nick Watters 3 Chris Burgess 3 Daniel Zoran 3  Loic Matthey 3 Matthew Botvinick 3 Alexander Lerchner 3  Abstract  Human perception is structured around objects which form the basis for our higher-level cogni- tion and impressive systematic generalization abil- ities. Yet most work on representation learning focuses on feature learning without even consider- ing multi
Graphite: Iterative Generative Modeling of Graphs  Aditya Grover 1 Aaron Zweig 1 Stefano Ermon 1  Abstract  Graphs are a fundamental abstraction for model- ing relational data. However, graphs are discrete and combinatorial in nature, and learning rep- resentations suitable for machine learning tasks poses statistical and computational challenges. In this work, we propose Graphite, an algorithmic framework for unsupervised learning of represen- tations over nodes in large graphs using deep la- t
Fast Algorithm for Generalized Multinomial Models  with Ranking Data  Jiaqi Gu 1 Guosheng Yin 1  Abstract  We develop a framework of generalized multi- nomial models, which includes both the popular Plackett–Luce model and Bradley–Terry model as special cases. From a theoretical perspective, we prove that the maximum likelihood estima- tor (MLE) under generalized multinomial models corresponds to the stationary distribution of an in- homogeneous Markov chain uniquely. Based on this property, we 
Towards a Deep and Uniﬁed Understanding of Deep Neural Models in NLP  1 Chaoyu Guan * 2 Xiting Wang * 2 Quanshi Zhang 1 Runjin Chen 1 Di He 3 Xing Xie 2  Abstract  We deﬁne a uniﬁed information-based measure to provide quantitative explanations on how inter- mediate layers of deep Natural Language Proces- sing (NLP) models leverage information of input words. Our method advances existing explanation methods by addressing issues in coherency and generality. Explanations generated by using our met
An Investigation of Model-Free Planning  Arthur Guez * 1 Mehdi Mirza * 1 Karol Gregor * 1 Rishabh Kabra * 1 Sébastien Racanière 1 Théophane Weber 1  David Raposo 1 Adam Santoro 1 Laurent Orseau 1 Tom Eccles 1 Greg Wayne 1 David Silver 1  Timothy Lillicrap 1  Abstract  The ﬁeld of reinforcement learning (RL) is facing increasingly challenging domains with combina- torial complexity. For an RL agent to address these challenges, it is essential that it can plan effectively. Prior work has typically
Humor in Word Embeddings:  Cockamamie Gobbledegook for Nincompoops  WARNING: This paper contains words that people rated humorous including many that are offensive in nature.  Limor Gultchin 1 Genevieve Patterson 2 Nancy Baym 3 Nathaniel Swinger 4 Adam Tauman Kalai 3  Abstract  While humor is often thought to be beyond the reach of Natural Language Processing, we show that several aspects of single-word humor corre- late with simple linear directions in Word Embed- dings. In particular: (a) the 
Exploring Interpretable LSTM Neural Networks over Multi-Variable Data  Tian Guo 1 Tao Lin 2 Nino Antulov-Fantulin 1  Abstract  For recurrent neural networks trained on time se- ries with target and exogenous variables, in ad- dition to accurate prediction, it is also desired to provide interpretable insights into the data. In this paper, we explore the structure of LSTM recur- rent neural networks to learn variable-wise hidden states, with the aim to capture different dynamics in multi-variable 
Learning to Exploit Long-term Relational Dependencies in Knowledge Graphs  Lingbing Guo 1 Zequn Sun 1 Wei Hu 1  Abstract  We study the problem of knowledge graph (KG) embedding. A widely-established assumption to this problem is that similar entities are likely to have similar relational roles. However, existing related methods derive KG embeddings mainly based on triple-level learning, which lack the ca- pability of capturing long-term relational depen- dencies of entities. Moreover, triple-lev
Memory-Optimal Direct Convolutions for Maximizing  Classiﬁcation Accuracy in Embedded Applications  Albert Gural 1 Boris Murmann 1  Abstract  In the age of Internet of Things (IoT), embed- ded devices ranging from ARM Cortex M0s with hundreds of KB of RAM to Arduinos with 2KB RAM are expected to perform increasingly so- phisticated classiﬁcation tasks, such as voice and gesture recognition, activity tracking, and biomet- ric security. While convolutional neural networks (CNNs), together with spe
IMEXnet - A Forward Stable Deep Neural Network  Eldad Haber * 1 2 Keegan Lensink * 1 2 Eran Treister 3 Lars Ruthotto 4  Abstract  Deep convolutional neural networks have revo- lutionized many machine learning and computer vision tasks, however, some remaining key chal- lenges limit their wider use. These challenges include improving the network’s robustness to perturbations of the input image and the limited “ﬁeld of view” of convolution operators. We intro- duce the IMEXnet that addresses these
On The Power of Curriculum Learning in Training Deep Networks  Guy Hacohen 1 2 Daphna Weinshall 1  Abstract  Training neural networks is traditionally done by providing a sequence of random mini-batches sampled uniformly from the entire training data. In this work, we analyze the effect of curriculum learning, which involves the non-uniform sam- pling of mini-batches, on the training of deep net- works, and speciﬁcally CNNs trained for image recognition. To employ curriculum learning, the traini
Trading Redundancy for Communication:  Speeding up Distributed SGD for Non-convex Optimization  Farzin Haddadpour 1 Mohammad Mahdi Kamani 2 Mehrdad Mahdavi 1 Viveck R. Cadambe 1  Abstract  Communication overhead is one of the key chal- lenges that hinders the scalability of distributed optimization algorithms to train large neural net- works. In recent years, there has been a great deal of research to alleviate communication cost by compressing the gradient vector or using local updates and peri
Learning Latent Dynamics for Planning from Pixels  Danijar Hafner 1 2 Timothy Lillicrap 3 Ian Fischer 4 Ruben Villegas 1 5  David Ha 1 Honglak Lee 1 James Davidson 1  Abstract  Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially i
Neural Separation of Observed and Unobserved Distributions  Tavi Halperin 1 Ariel Ephrat 2 Yedid Hoshen 1 3  Abstract  Separating mixed distributions is a long standing challenge for machine learning and signal process- ing. Most current methods either rely on making strong assumptions on the source distributions or rely on having training samples of each source in the mixture. In this work, we introduce a new method—Neural Egg Separation—to tackle the scenario of extracting a signal from an uno
Grid-Wise Control for Multi-Agent Reinforcement Learning in Video Game AI  Lei Han * 1 Peng Sun * 1 Yali Du * 2 3 Jiechao Xiong 1 Qing Wang 1 Xinghai Sun 1 Han Liu 4 Tong Zhang 5  Abstract  We consider the problem of multi-agent reinforce- ment learning (MARL) in video game AI, where the agents are located in a spatial grid-world en- vironment and the number of agents varies both within and across episodes. The challenge is to ﬂexibly control an arbitrary number of agents while achieving effecti
Dimension-Wise Importance Sampling Weight Clipping for Sample-Efﬁcient  Reinforcement Learning  Seungyul Han 1 Youngchul Sung 1  Abstract  In importance sampling (IS)-based reinforcement learning algorithms such as Proximal Policy Opti- mization (PPO), IS weights are typically clipped to avoid large variance in learning. However, pol- icy update from clipped statistics induces large bias in tasks with high action dimensions, and bias from clipping makes it difﬁcult to reuse old samples with larg
Complexity of Linear Regions in Deep Networks  Boris Hanin * 1 David Rolnick * 2  Abstract  It is well-known that the expressivity of a neural network depends on its architecture, with deeper networks expressing more complex functions. In the case of networks that compute piecewise lin- ear functions, such as those with ReLU activation, the number of distinct linear regions is a natural measure of expressivity. It is possible to construct networks with merely a single region, or for which the nu
Doubly-Competitive Distribution Estimation  Yi Hao 1 Alon Orlitsky 1  Abstract  1.1. Distribution Estimation  Distribution estimation is a statistical-learning cornerstone. Its classical min-max formulation minimizes the estimation error for the worst dis- tribution, hence under-performs for practical dis- tributions that, like power-law, are often rather simple. Modern research has therefore focused on two frameworks: structural estimation that improves learning accuracy by assuming a sim- ple 
Random Shufﬂing Beats SGD after Finite Epochs  Jeff HaoChen 1 Suvrit Sra 2  Abstract  A long-standing problem in optimization is proving that RANDOMSHUFFLE, the without- replacement version of SGD, converges faster than (the usual) with-replacement SGD. Building upon (G¨urb¨uzbalaban et al., 2015b), we present the ﬁrst non-asymptotic results for this problem, proving that after a reasonable number of epochs RANDOMSHUFFLE converges faster than SGD. Speciﬁcally, we prove that for strongly convex, 
Submodular Maximization beyond Non-negativity:  Guarantees, Fast Algorithms, and Applications  Christopher Harshaw 1 Moran Feldman 2 Justin Ward 3 Amin Karbasi 4  Abstract  (cid:15) log2 1  It is generally believed that submodular func- tions—and the more general class of γ-weakly submodular functions—may only be optimized under the non-negativity assumption f (S) ≥ 0. In this paper, we show that once the function is expressed as the difference f = g − c, where g is monotone, non-negative, and γ
Per-Decision Option Discounting  Anna Harutyunyan 1 2 Peter Vrancx 3 2 Philippe Hamel 1 Ann Now´e 2 Doina Precup 1  Abstract  In order to solve complex problems an agent must be able to reason over a sufﬁciently long hori- zon. Temporal abstraction, commonly modeled through options, offers the ability to reason at many timescales, but the horizon length is still determined by the discount factor of the under- lying Markov Decision Process. We propose a modiﬁcation to the options framework that n
Submodular Observation Selection and Information Gathering  for Quadratic Models  Abolfazl Hashemi 1 Mahsa Ghasemi 1 Haris Vikalo 1 Ufuk Topcu 1  Abstract  We study the problem of selecting most informa- tive subset of a large observation set to enable accurate estimation of unknown parameters. This problem arises in a variety of settings in machine learning and signal processing including feature selection, phase retrieval, and target localization. Since for quadratic measurement models the mo-
Understanding and Controlling Memory in Recurrent Neural Networks  Doron Haviv 1 2 Alexnader Rivkind 2 3 4 Omri Barak 2 3  Abstract  1. Introduction  To be effective in sequential data processing, Re- current Neural Networks (RNNs) are required to keep track of past events by creating memories. While the relation between memories and the network’s hidden state dynamics was established over the last decade, previous works in this di- rection were of a predominantly descriptive na- ture focusing m
On the Impact of the Activation Function on Deep Neural Networks Training  Souﬁane Hayou 1 Arnaud Doucet 1 Judith Rousseau 1  Abstract  The weight initialization and the activation func- tion of deep neural networks have a crucial impact on the performance of the training procedure. An inappropriate selection can lead to the loss of in- formation of the input during forward propagation and the exponential vanishing/exploding of gra- dients during back-propagation. Understanding the theoretical p
Provably Efﬁcient Maximum Entropy Exploration  Elad Hazan 1 2 Sham M. Kakade 3 4 2 Karan Singh 1 2 Abby Van Soest 1 2  Abstract  Suppose an agent is in a (possibly unknown) Markov Decision Process in the absence of a re- ward signal, what might we hope that an agent can efﬁciently learn to do? This work studies a broad class of objectives that are deﬁned solely as functions of the state-visitation frequencies that are induced by how the agent behaves. For exam- ple, one natural, intrinsically de
On the Long-term Impact of Algorithmic Decision Policies:  Effort Unfairness and Feature Segregation through Social Learning  Hoda Heidari * 1 Vedant Nanda * 2 Krishna P. Gummadi 2  Abstract  Most existing notions of algorithmic fairness are one-shot: they ensure some form of allocative equality at the time of decision making, but do not account for the adverse impact of the algo- rithmic decisions today on the long-term welfare and prosperity of certain segments of the popula- tion. We take a b
Graph Resistance and Learning from Pairwise Comparisons  Julien M. Hendrickx∗,† Alex Olshevsky† Venkatesh Saligrama†  Abstract  We consider the problem of learning the quali- ties of a collection of items by performing noisy comparisons among them. Following the standard paradigm, we assume there is a ﬁxed “compari- son graph” and every neighboring pair of items in this graph is compared k times according to the Bradley-Terry-Luce model (where the probability than an item wins a comparison is pr
Using Pre-Training Can Improve Model Robustness and Uncertainty  Dan Hendrycks 1 Kimin Lee 2 Mantas Mazeika 3  Abstract  He et al. (2018) have called into question the utility of pre-training by showing that train- ing from scratch can often yield similar per- formance to pre-training. We show that al- though pre-training may not improve perfor- mance on traditional classiﬁcation metrics, it im- proves model robustness and uncertainty esti- mates. Through extensive experiments on la- bel corrupt
Flow++: Improving Flow-Based Generative Models with Variational  Dequantization and Architecture Design  Jonathan Ho * 1 Xi Chen * 1 2 Aravind Srinivas 1 Yan Duan 2 Pieter Abbeel 1 2  Abstract  Flow-based generative models are powerful exact likelihood models with efﬁcient sampling and in- ference. Despite their computational efﬁciency, ﬂow-based models generally have much worse density modeling performance compared to state- of-the-art autoregressive models. In this paper, we investigate and im
Population Based Augmentation:  Efﬁcient Learning of Augmentation Policy Schedules  Daniel Ho 1 2 Eric Liang 1 Ion Stoica 1 Pieter Abbeel 1 3 Xi Chen 1 3  Abstract  A key challenge in leveraging data augmenta- tion for neural network training is choosing an effective augmentation policy from a large search space of candidate operations. Properly chosen augmentation policies can lead to signiﬁcant gen- eralization improvements; however, state-of-the- art approaches such as AutoAugment are compu- 
Collective Model Fusion for Multiple Black-Box Experts  Quang Minh Hoang * 1 Trong Nghia Hoang * 2 Bryan Kian Hsiang Low 3 Carl Kingsford 1  Abstract  Model fusion is a fundamental problem in collec- tive machine learning (ML) where independent experts with heterogeneous learning architectures are required to combine expertise to improve pre- dictive performance. This is particularly chal- lenging in information-sensitive domains where experts do not have access to each other’s internal architec
Connectivity-Optimized Representation Learning via Persistent Homology  Christoph D. Hofer 1 Roland Kwitt 1 Mandar Dixit 2 Marc Niethammer 3  Abstract  We study the problem of learning representations with controllable connectivity properties. This is beneﬁcial in situations when the imposed struc- ture can be leveraged upstream. In particular, we control the connectivity of an autoencoder’s latent space via a novel type of loss, operating on information from persistent homology. Un- der mild co
Better generalization with less data using robust gradient descent  Matthew J. Holland 1 Kazushi Ikeda 2  Abstract  For learning tasks where the data (or losses) may be heavy-tailed, algorithms based on empirical risk minimization may require a substantial num- ber of observations in order to perform well off- sample. In pursuit of stronger performance un- der weaker assumptions, we propose a technique which uses a cheap and robust iterative estimate of the risk gradient, which can be easily fed
Emerging Convolutions for Generative Normalizing Flows  Emiel Hoogeboom 1 Rianne van den Berg 2 Max Welling 1 3  Abstract  Generative ﬂows are attractive because they admit exact likelihood optimization and efﬁcient image synthesis. Recently, Kingma & Dhariwal (2018) demonstrated with Glow that generative ﬂows are capable of generating high quality images. We generalize the 1 × 1 convolutions proposed in Glow to invertible d × d convolutions, which are more ﬂexible since they operate on both cha
Nonconvex Variance Reduced Optimization with Arbitrary Sampling  Samuel Horváth 1 Peter Richtárik 1 2 3  Abstract  We provide the ﬁrst importance sampling vari- ants of variance-reduced algorithms for empirical risk minimization with non-convex loss functions. In particular, we analyze non-convex versions of SVRG, SAGA and SARAH. Our methods have the capacity to speed up the training process by an or- der of magnitude compared to the state of the art on real datasets. Moreover, we also improve u
Parameter-Efﬁcient Transfer Learning for NLP  Neil Houlsby 1 Andrei Giurgiu 1 * Stanisław Jastrze¸bski 2 * Bruna Morrone 1 Quentin de Laroussilhe 1  Andrea Gesmundo 1 Mona Attariyan 1 Sylvain Gelly 1  Abstract  Fine-tuning large pre-trained models is an effec- tive transfer mechanism in NLP. However, in the presence of many downstream tasks, ﬁne-tuning is parameter inefﬁcient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter mod
Stay With Me: Lifetime Maximization Through Heteroscedastic Linear Bandits With Reneging  Ping-Chun Hsieh * 1 Xi Liu * 1 Anirban Bhattacharya 2 P. R. Kumar 1  Abstract  Sequential decision making for lifetime maxi- mization is a critical problem in many real-world applications, such as medical treatment and port- folio selection. In these applications, a “reneg- ing” phenomenon, where participants may disen- gage from future interactions after observing an unsatisﬁable outcome, is rather prevale
Finding Mixed Nash Equilibria of Generative Adversarial Networks  Ya-Ping Hsieh 1 Chen Liu 1 Volkan Cevher 1  Abstract  Generative adversarial networks (GANs) are known to achieve the state-of-the-art performance on various generative tasks, but these results come at the expense of a notoriously difﬁcult training phase. Current training strategies typically draw a connection to optimization theory, whose scope is restricted to local convergence due to the pres- ence of non-convexity. In this wor
Classiﬁcation from Positive, Unlabeled and Biased Negative Data  Yu-Guan Hsieh ∗ 1 Gang Niu 2 Masashi Sugiyama 2 3  Abstract  In binary classiﬁcation, there are situations where negative (N) data are too diverse to be fully la- beled and we often resort to positive-unlabeled (PU) learning in these scenarios. However, col- lecting a non-representative N set that contains only a small portion of all possible N data can of- ten be much easier in practice. This paper studies a novel classiﬁcation fr
BayesianDeconditionalKernelMeanEmbeddingsKelvinHsu12FabioRamos13AbstractConditionalkernelmeanembeddingsformanat-tractivenonparametricframeworkforrepresent-ingconditionalmeansoffunctions,describingtheobservationprocessesformanycomplexmodels.However,therecoveryoftheoriginalunderlyingfunctionofinterestwhoseconditionalmeanwasobservedisachallenginginferencetask.Wefor-malizedeconditionalkernelmeanembeddingsasasolutiontothisinverseproblem,andshowthatitcanbenaturallyviewedasanonparametricBayes’rule.Crit
Faster Stochastic Alternating Direction Method of Multipliers  for Nonconvex Optimization  Feihu Huang 1 Songcan Chen 2 3 Heng Huang 1 4  Abstract  In this paper, we propose a faster stochastic alter- nating direction method of multipliers (ADMM) for nonconvex optimization by using a new stochastic path-integrated differential estimator (SPIDER), called as SPIDER-ADMM. More- over, we prove that the SPIDER-ADMM achieves a record-breaking incremental ﬁrst-order oracle (IFO) complexity of O(n + n1/
Unsupervised Deep Learning by Neighbourhood Discovery  Jiabo Huang 1 Qi Dong 1 Shaogang Gong 1 Xiatian Zhu 2  Abstract  Deep convolutional neural networks (CNNs) have demonstrated remarkable success in computer vi- sion by supervisedly learning strong visual feature representations. However, training CNNs relies heavily on the availability of exhaustive training data annotations, limiting signiﬁcantly their de- ployment and scalability in many application sce- narios. In this work, we introduce 
Detecting Overlapping and Correlated Communities without Pure Nodes:  Identiﬁability and Algorithm  Kejun Huang 1 Xiao Fu 2  Abstract  Many machine learning problems come in the form of networks with relational data between entities, and one of the key unsupervised learn- ing tasks is to detect communities in such a net- work. We adopt the mixed-membership stochastic blockmodel as the underlying probabilistic model, and give conditions under which the memberships of a subset of nodes can be uniq
HierarchicalImportanceWeightedAutoencodersChin-WeiHuang12KrisSankaran1EeshanDhekane1AlexandreLacoste2AaronCourville13AbstractImportanceweightedvariationalinference(Burdaetal.,2015)usesmultiplei.i.d.samplestohaveatightervariationallowerbound.Webelieveajointproposalhasthepotentialofreducingthenumberofredundantsamples,andintroduceahierarchicalstructuretoinducecorrelation.Thehopeisthattheproposalswouldcoordinatetomakeupfortheerrormadebyoneanothertoreducethevarianceoftheimportanceestimator.Theoretica
Stable and Fair Classiﬁcation  Lingxiao Huang 1 Nisheeth K. Vishnoi 2  Abstract  In a recent study, Friedler et al. (Friedler et al., 2019) observed that fair classiﬁcation algorithms may not be stable with respect to variations in the training dataset – a crucial consideration in sev- eral real-world applications. Motivated by their work, this paper initiates a study of designing classiﬁcation algorithms that are both fair and sta- ble. We propose an extended framework based on fair classiﬁcati
Addressing the Loss-Metric Mismatch with Adaptive Loss Alignment  Chen Huang 1 Shuangfei Zhai 1 Walter Talbott 1 Miguel Angel Bautista 1 Shih-Yu Sun 1 Carlos Guestrin 1  Josh Susskind 1  Abstract  In most machine learning training paradigms a ﬁxed, often handcrafted, loss function is assumed to be a good proxy for an underlying evaluation metric. In this work we assess this assumption by meta-learning an adaptive loss function to directly optimize the evaluation metric. We propose a sam- ple efﬁ
Causal Discovery and Forecasting in Nonstationary Environments with  State-Space Models  Biwei Huang 1 Kun Zhang 1 Mingming Gong 1 2 Clark Glymour 1  Abstract  In many scientiﬁc ﬁelds, such as economics and neuroscience, we are often faced with nonstation- ary time series, and concerned with both ﬁnd- ing causal relations and forecasting the values of variables of interest, both of which are particu- larly challenging in such nonstationary environ- ments. In this paper, we study causal discovery
Composing Entropic Policies using Divergence Correction  Jonathan J Hunt 1 Andre Barreto 1 Timothy P Lillicrap 1 Nicolas Heess 1  Abstract  Composing skills mastered in one task to solve novel tasks promises dramatic improvements in the data efﬁciency of reinforcement learning. Here, we analyze two recent works composing behaviors represented in the form of action-value functions and show that they perform poorly in some situations. As part of this analysis, we ex- tend an important generalizati
HexaGAN: Generative Adversarial Nets for Real World Classification  Uiwon Hwang 1 Dahuin Jung 1 Sungroh Yoon 1 2  Abstract  Most deep learning classification studies assume clean data. However, when dealing with the real world data, we encounter three problems such as 1) missing data, 2) class imbalance, and 3) miss- ing label problems. These problems undermine the performance of a classifier. Various prepro- cessing techniques have been proposed to mitigate one of these problems, but an algorit
Overcoming Mean-Field Approximations in Recurrent Gaussian Process Models  Alessandro Davide Ialongo 1 2 Mark van der Wilk 3 James Hensman 3 Carl Edward Rasmussen 1  Abstract  We identify a new variational inference scheme for dynamical systems whose transition function is modelled by a Gaussian process. Inference in this setting has either employed computationally intensive MCMC methods, or relied on factorisa- tions of the variational posterior. As we demon- strate in our experiments, the fact
Learning Structured Decision Problems with Unawareness  Craig Innes 1 Alex Lascarides 1  Abstract  Structured models of decision making often as- sume an agent is aware of all possible states and actions in advance. This assumption is sometimes untenable. In this paper, we learn inﬂuence dia- grams from both domain exploration and expert assertions in a way which guarantees convergence to optimal behaviour, even when the agent starts unaware of actions or belief variables that are crit- ical to 
Phase transition in PCA with missing data: Reduced signal-to-noise ratio, not sample size!  Niels Bruun Ipsen 1 Lars Kai Hansen 1  Abstract  How does missing data affect our ability to learn signal structures? It has been shown that learning signal structure in terms of principal components is dependent on the ratio of sample size and di- mensionality and that a critical number of obser- vations is needed before learning starts (Biehl and Mietzner, 1993). Here we generalize this analysis to incl
Actor-Attention-Critic for Multi-Agent Reinforcement Learning  Shariq Iqbal 1 Fei Sha 1 2  Abstract  Reinforcement learning in multi-agent scenar- ios is important for real-world applications but presents challenges beyond those seen in single- agent settings. We present an actor-critic algo- rithm that trains decentralized policies in multi- agent settings, using centrally computed crit- ics that share an attention mechanism which se- lects relevant information for each agent at every timestep.
Complementary-Label Learning for Arbitrary Losses and Models  Takashi Ishida 1 2 Gang Niu 2 Aditya Krishna Menon 3 Masashi Sugiyama 2 1  Abstract  In contrast to the standard classiﬁcation paradigm where the true class is given to each training pattern, complementary-label learning only uses training patterns each equipped with a comple- mentary label, which only speciﬁes one of the classes that the pattern does not belong to. The goal of this paper is to derive a novel framework of complementar
Causal Identiﬁcation under Markov Equivalence:  Completeness Results  Amin Jaber 1 Jiji Zhang 2 Elias Bareinboim 1  Abstract  Causal effect identiﬁcation is the task of determin- ing whether a causal distribution is computable from the combination of an observational distribu- tion and substantive knowledge about the domain under investigation. One of the most studied ver- sions of this problem assumes that knowledge is articulated in the form of a fully known causal diagram, which is arguably a
Learning from a Learner  Alexis D. Jacq 1 2 Matthieu Geist 1 Ana Paiva 2 Olivier Pietquin 1  Abstract  In this paper, we propose a novel setting for Inverse Reinforcement Learning (IRL), namely “Learning from a Learner” (LfL). As opposed to standard IRL, it does not consist in learning a reward by observing an optimal agent, but from observations of another learning (and thus sub- optimal) agent. To do so, we leverage the fact that the observed agent’s policy is assumed to improve over time. The
Differentially Private Fair Learning  Matthew Jagielski 1 Michael Kearns 2 Jieming Mao 2 Alina Oprea 1 Aaron Roth 2 Saeed Shariﬁ-Malvajerdi 2  Jonathan Ullman 1  Abstract  Motivated by settings in which predictive models may be required to be non-discriminatory with re- spect to certain attributes (such as race), but even collecting the sensitive attribute may be forbidden or restricted, we initiate the study of fair learn- ing under the constraint of differential privacy. Our ﬁrst algorithm is 
Sum-of-Squares Polynomial Flow  Priyank Jaini 1 2 3 Kira A. Selby 1 2 Yaoliang Yu 1 2  Abstract  Triangular map is a recent construct in probability theory that allows one to transform any source probability density function to any target density function. Based on triangular maps, we propose a general framework for high-dimensional den- sity estimation, by specifying one-dimensional transformations (equivalently conditional densi- ties) and appropriate conditioner networks. This framework (a) r
DBSCAN++: Towards fast and scalable density clustering  Jennifer Jang 1 Heinrich Jiang 2  Abstract  DBSCAN is a classical density-based clustering procedure with tremendous practical relevance. However, DBSCAN implicitly needs to compute the empirical density for each sample point, lead- ing to a quadratic worst-case time complexity, which is too slow on large datasets. We propose DBSCAN++, a simple modiﬁcation of DBSCAN which only requires computing the densities for a chosen subset of points. 
Learning What and Where to Transfer  Yunhun Jang * 1 2 Hankook Lee * 1 Sung Ju Hwang 3 4 5 Jinwoo Shin 1 4 5  Abstract  As the application of deep learning has expanded to real-world problems with insufﬁcient volume of training data, transfer learning recently has gained much attention as means of improving the performance in such small-data regime. However, when existing methods are applied between het- erogeneous architectures and tasks, it becomes more important to manage their detailed conﬁg
Social Inﬂuence as Intrinsic Motivation  for Multi-Agent Deep Reinforcement Learning  Natasha Jaques 1 2 Angeliki Lazaridou 2 Edward Hughes 2 Caglar Gulcehre 2 Pedro A. Ortega 2 DJ Strouse 3  Joel Z. Leibo 2 Nando de Freitas 2  Abstract  We propose a uniﬁed mechanism for achieving coordination and communication in Multi-Agent Reinforcement Learning (MARL), through rewarding agents for having causal inﬂuence over other agents’ actions. Causal inﬂuence is assessed using counterfactual reasoning. A
A Deep Reinforcement Learning Perspective on Internet Congestion Control  Nathan Jay * 1 Noga H. Rotman * 2 P. Brighten Godfrey 1 Michael Schapira 2 Aviv Tamar 3  Abstract  We present and investigate a novel and timely application domain for deep reinforcement learn- ing (RL): Internet congestion control. Congestion control is the core networking task of modulat- ing trafﬁc sources’ data-transmission rates to efﬁ- ciently utilize network capacity, and is the subject of extensive attention in lig
Graph Neural Network for Music Score Data and Modeling Expressive Piano Performance  Dasaem Jeong 1 Taegyun Kwon 1 Yoojin Kim 1 Juhan Nam 1  Abstract  Music score is often handled as one-dimensional sequential data. Unlike words in a text document, notes in music score can be played simultaneously by the polyphonic nature and each of them has its own duration. In this paper, we represent the unique form of musical score using graph neu- ral network and apply it for rendering expressive piano per
Ladder Capsule Network  Taewon Jeong 1 Youngmin Lee 1 Heeyoung Kim 1  Abstract  We propose a new architecture of the capsule net- work called the ladder capsule network, which has an alternative building block to the dynamic routing algorithm in the capsule network (Sabour et al., 2017). Motivated by the need for using only important capsules during training for ro- bust performance, we ﬁrst introduce a new layer called the pruning layer, which removes irrele- vant capsules. Based on the selecte
Training CNNs with Selective Allocation of Channels  Jongheon Jeong 1 Jinwoo Shin 1 2 3  Abstract  Recent progress in deep convolutional neural net- works (CNNs) have enabled a simple paradigm of architecture design: larger models typically achieve better accuracy. Due to this, in modern CNN architectures, it becomes more important to design models that generalize well under certain resource constraints, e.g. the number of parame- ters. In this paper, we propose a simple way to improve the capac
Learning Discrete and Continuous Factors of Data  via Alternating Disentanglement  Yeonwoo Jeong 1 Hyun Oh Song 1  Abstract  We address the problem of unsupervised disentan- glement of discrete and continuous explanatory factors of data. We ﬁrst show a simple proce- dure for minimizing the total correlation of the continuous latent variables without having to use a discriminator network or perform importance sampling, via cascading the information ﬂow in the β-vae framework. Furthermore, we prop
Improved Zeroth-Order Variance Reduced Algorithms and Analysis for  Nonconvex Optimization  Kaiyi Ji 1 Zhe Wang 1 Yi Zhou 2 Yingbin Liang 1  Abstract  Two types of zeroth-order stochastic algorithms have recently been designed for nonconvex op- timization respectively based on the ﬁrst-order techniques SVRG and SARAH/SPIDER. This pa- per addresses several important issues that are still open in these methods. First, all existing SVRG- type zeroth-order algorithms suffer from worse function query
Neural Logic Reinforcement Learning  Zhengyao Jiang 1 Shan Luo 1  Abstract  Deep reinforcement learning (DRL) has achieved signiﬁcant breakthroughs in various tasks. How- ever, most DRL algorithms suffer a problem of generalising the learned policy, which makes the policy performance largely affected even by mi- nor modiﬁcations of the training environment. Except that, the use of deep neural networks makes the learned policies hard to be inter- pretable. To address these two challenges, we prop
Finding Options that Minimize Planning Time  Yuu Jinnai 1 David Abel 1 D Ellis Hershkowitz 2 Michael L. Littman 1 George Konidaris 1  Abstract  We formalize the problem of selecting the optimal set of options for planning as that of computing the smallest set of options so that planning con- verges in less than a given maximum of value- iteration passes. We ﬁrst show that the problem is NP-hard, even if the task is constrained to be deterministic—the ﬁrst such complexity result for option discov
Discovering Options for Exploration by Minimizing Cover Time  Yuu Jinnai 1 Jee Won Park 1 David Abel 1 George Konidaris 1  Abstract  One of the main challenges in reinforcement learn- ing is solving tasks with sparse reward. We show that the difﬁculty of discovering a distant reward- ing state in an MDP is bounded by the expected cover time of a random walk over the graph in- duced by the MDP’s transition dynamics. We therefore propose to accelerate exploration by constructing options that minim
Kernel Mean Matching for Content Addressability of GANs  Wittawat Jitkrittum * 1 Patsorn Sangkloy * 2 Muhammad Waleed Gondal 1 Amit Raj 2  James Hays 2 Bernhard Schölkopf 1  Abstract  We propose a novel procedure which adds “content-addressability” to any given uncondi- tional implicit model e.g., a generative adversar- ial network (GAN). The procedure allows users to control the generative process by specifying a set (arbitrary size) of desired examples based on which similar samples are genera
GOODE: A Gaussian Off-The-Shelf Ordinary Differential Equation Solver  David N. John 1 2 Vincent Heuveline 2 Michael Schober 3  Abstract  There are two types of ordinary differential equa- tions (ODEs): initial value problems (IVPs) and boundary value problems (BVPs). While many probabilistic numerical methods for the solution of IVPs have been presented to-date, there exists no efﬁcient probabilistic general-purpose solver for nonlinear BVPs. Our method based on iter- ated Gaussian process (GP)
Bilinear Bandits with Low-rank Structure  Kwang-Sung Jun 1 Rebecca Willett 2 Stephen Wright 3 Robert Nowak 3  Abstract  We introduce the bilinear bandit problem with low-rank structure in which an action takes the form of a pair of arms from two different en- tity types, and the reward is a bilinear function of the known feature vectors of the arms. The unknown in the problem is a d1 by d2 matrix  ⇥∗ that deﬁnes the reward, and has low rank r￿ min{d1, d2}. Determination of ⇥∗ with this  low-rank
Statistical Foundations of Virtual Democracy  Anson Kahng 1 Min Kyung Lee 1 Ritesh Noothigattu 1 Ariel D. Procaccia 1 Alexandros Psomas 1  Abstract  Virtual democracy is an approach to automating decisions, by learning models of the preferences of individual people, and, at runtime, aggregat- ing the predicted preferences of those people on the dilemma at hand. One of the key questions is which aggregation method — or voting rule — to use; we offer a novel statistical viewpoint that provides gui
Molecular Hypergraph Grammar  with Its Application to Molecular Optimization  Hiroshi Kajino 1  Abstract  problem is to obtain m(cid:63) ∈ M such that,  Molecular optimization aims to discover novel molecules with desirable properties, and its two fundamental challenges are: (i) it is not trivial to generate valid molecules in a controllable way due to hard chemical constraints such as the valency conditions, and (ii) it is often costly to evaluate a property of a novel molecule, and therefore, 
Robust Inﬂuence Maximization for Hyperparametric Models  Dimitris Kalimeris 1 Gal Kaplun 1 Yaron Singer 1  Abstract  In this paper we study the problem of robust in- ﬂuence maximization in the independent cascade model under a hyperparametric assumption. In social networks users inﬂuence and are inﬂuenced by individuals with similar characteristics and as such they are associated with some features. A recent surging research direction in inﬂuence maximization focuses on the case where the edge p
Classifying Treatment Responders Under Causal Effect Monotonicity  Nathan Kallus 1  Abstract  In the context of individual-level causal inference, we study the problem of predicting whether some- one will respond or not to a treatment based on their features and past examples of features, treat- ment indicator (e.g., drug/no drug), and a binary outcome (e.g., recovery from disease). As a classi- ﬁcation task, the problem is made difﬁcult by not knowing the example outcomes under the oppo- site t
Trainable Decoding of Sets of Sequences for Neural Sequence Models  Ashwin Kalyan 1 Peter Anderson 1 Stefan Lee 1 Dhruv Batra 1 2  Abstract  Many sequence prediction tasks admit multiple correct outputs and so, it is often useful to decode a set of outputs that maximize some task-speciﬁc set-level metric. However, retooling standard se- quence prediction procedures tailored towards pre- dicting single best outputs tends to produce sets containing very similar sequences; failing to cap- ture the 
Myopic Posterior Sampling for Adaptive Goal Oriented Design of Experiments  Kirthevasan Kandasamy 1 Willie Neiswanger 1 Reed Zhang 1 Akshay Krishnamurthy 2  Jeff Schneider 1 Barnab´as P´oczos 1  Abstract  Bayesian methods for adaptive decision-making, such as Bayesian optimisation, active learning, and active search have seen great success in rele- vant applications. However, real world data collec- tion tasks are more broad and complex, as we may need to achieve a combination of the above goals
Differentially Private Learning of Geometric Concepts  Haim Kaplan 1 2 Yishay Mansour 1 2 Yossi Matias 2 Uri Stemmer 3 4  Abstract  Pr[A(S) ∈ T ] ≤ eε · Pr[A(S(cid:48)) ∈ T ] + δ.  We present differentially private efﬁcient al- gorithms for learning union of polygons in the plane (which are not necessarily convex). Our algorithms achieve (α, β)-PAC learning and (ε, δ)-differential privacy using a sample of size  ˜O(cid:0) 1 αε k log d(cid:1), where the domain is [d] × [d] and  k is the number of
Policy Consolidation for Continual Reinforcement Learning  Christos Kaplanis 1 2 Murray Shanahan 1 3 Claudia Clopath 2  Abstract  We propose a method for tackling catastrophic forgetting in deep reinforcement learning that is agnostic to the timescale of changes in the dis- tribution of experiences, does not require knowl- edge of task boundaries, and can adapt in con- tinuously changing environments. In our policy consolidation model, the policy network interacts with a cascade of hidden networ
Error Feedback Fixes SignSGD and other Gradient Compression Schemes  Sai Praneeth Karimireddy 1 Quentin Rebjock 1 Sebastian U. Stich 1 Martin Jaggi 1  Abstract  Sign-based algorithms (e.g. SIGNSGD) have been proposed as a biased gradient compression tech- nique to alleviate the communication bottleneck in training large neural networks across multi- ple workers. We show simple convex counter- examples where signSGD does not converge to the optimum. Further, even when it does converge, signSGD ma
Riemannian adaptive stochastic gradient algorithms on matrix manifolds  Hiroyuki Kasai 1 Pratik Jawanpuria 2 Bamdev Mishra 2  Abstract  Adaptive stochastic gradient algorithms in the Eu- clidean space have attracted much attention lately. Such explorations on Riemannian manifolds, on the other hand, are relatively new, limited, and challenging. This is because of the intrinsic non- linear structure of the underlying manifold and the absence of a canonical coordinate system. In ma- chine learning
Neural Inverse Knitting: From Images to Manufacturing Instructions  Alexandre Kaspar * 1 Tae-Hyun Oh * 1 Liane Makatura 1 Petr Kellnhofer 1 Wojciech Matusik 1  Abstract  Motivated by the recent potential of mass cus- tomization brought by whole-garment knitting machines, we introduce the new problem of auto- matic machine instruction generation using a sin- gle image of the desired physical product, which we apply to machine knitting. We propose to tackle this problem by directly learning to syn
Processing Megapixel Images with Deep Attention-Sampling Models  Angelos Katharopoulos 1 2 Franc¸ois Fleuret 1 2  Abstract  Existing deep architectures cannot operate on very large signals such as megapixel images due to computational and memory constraints. To tackle this limitation, we propose a fully differentiable end-to-end trainable model that samples and pro- cesses only a fraction of the full resolution input image. The locations to process are sampled from an attention distribution comp
Robust Estimation of Tree Structured Gaussian Graphical Models  Ashish Katiyar 1 Jessica Hoffmann 1 Constantine Caramanis 1  Abstract  Consider jointly Gaussian random variables whose conditional independence structure is spec- iﬁed by a graphical model. If we observe realiza- tions of the variables, we can compute the covari- ance matrix, and it is well known that the support of the inverse covariance matrix corresponds to the edges of the graphical model. Instead, sup- pose we only have noisy 
Shallow-Deep Networks: Understanding and Mitigating Network Overthinking  Yigitcan Kaya 1 Sanghyun Hong 1 Tudor Dumitras,  1  Abstract  We characterize a prevalent weakness of deep neural networks (DNNs)—overthinking—which occurs when a DNN can reach correct predic- tions before its ﬁnal layer. Overthinking is com- putationally wasteful, and it can also be destruc- tive when, by the ﬁnal layer, a correct prediction changes into a misclassiﬁcation. Understanding overthinking requires studying how
Submodular Streaming in All Its Glory:  Tight Approximation, Minimum Memory and Low Adaptive Complexity  Ehsan Kazemi 1 Marko Mitrovic 1 Morteza Zadimoghaddam 2 Silvio Lattanzi 2 Amin Karbasi 1  Abstract  Streaming algorithms are generally judged by the quality of their solution, memory footprint, and computational complexity. In this paper, we study the problem of maximizing a mono- tone submodular function in the streaming set- ting with a cardinality constraint k. We ﬁrst propose SIEVE-STREAM
Adaptive Scale-Invariant Online Algorithms for Learning Linear Models  Michał Kempka 1 Wojciech Kotłowski 1 Manfred K. Warmuth 2  Abstract  We consider online learning with linear models, where the algorithm predicts on sequentially re- vealed instances (feature vectors), and is com- pared against the best linear function (compara- tor) in hindsight. Popular algorithms in this frame- work, such as Online Gradient Descent (OGD), have parameters (learning rates), which ideally should be tuned base
CHiVE: Varying Prosody in Speech Synthesis with a Linguistically Driven  Dynamic Hierarchical Conditional Variational Network  Vincent Wan 1 Chun-an Chan 1 Tom Kenter 1 Jakub Vit 2 Rob Clark 1  Abstract  The prosodic aspects of speech signals produced by current text-to-speech systems are typically averaged over training material, and as such lack the variety and liveliness found in natural speech. To avoid monotony and averaged prosody con- tours, it is desirable to have a way of modeling the v
Collaborative Evolutionary Reinforcement Learning  Shauharda Khadka 1 2 Somdeb Majumdar 1 Tarek Nassar 1 Zach Dwiel 1 Evren Tumer 1 Santiago Miret 1  Yinyin Liu 1 Kagan Tumer 2  Abstract  Deep reinforcement learning algorithms have been successfully applied to a range of challeng- ing control tasks. However, these methods typi- cally struggle with achieving effective exploration and are extremely sensitive to the choice of hyper- parameters. One reason is that most approaches use a noisy version
Geometry Aware Convolutional Filters for Omnidirectional Images  Representation  Renata Khasanova 1 Pascal Frossard 1  Abstract  Due to their wide ﬁeld of view, omnidirectional cameras are frequently used by autonomous vehi- cles, drones and robots for navigation and other computer vision tasks. The images captured by such cameras, are often analyzed and classiﬁed with techniques designed for planar images that unfortunately fail to properly handle the native geometry of such images and therefor
EMI: Exploration with Mutual Information  Hyoungseok Kim * 1 2 Jaekyeom Kim * 1 2 Yeonwoo Jeong 1 2 Sergey Levine 3 Hyun Oh Song 1 2  Abstract  Reinforcement learning algorithms struggle when the reward signal is very sparse. In these cases, naive random exploration methods essentially rely on a random walk to stumble onto a rewarding state. Recent works utilize intrinsic motivation to guide the exploration via generative models, predictive forward models, or discriminative mod- eling of novelty
FloWaveNet : A Generative Flow for Raw Audio  Sungwon Kim 1 Sang-gil Lee 1 Jongyoon Song 1 Jaehyeon Kim 2 Sungroh Yoon 1 3  Abstract  Most modern text-to-speech architectures use a WaveNet vocoder for synthesizing high-ﬁdelity waveform audio, but there have been limitations, such as high inference time, in its practical appli- cation due to its ancestral sampling scheme. The recently suggested Parallel WaveNet and Clar- iNet have achieved real-time audio synthesis ca- pability by incorporating i
Curiosity-Bottleneck: Exploration by Distilling Task-Speciﬁc Novelty  Youngjin Kim 1 2 Wontae Nam∗ 3 Hyunwoo Kim∗ 2 Ji-Hoon Kim 4 Gunhee Kim 2  Abstract  Exploration based on state novelty has brought great success in challenging reinforcement learn- ing problems with sparse rewards. However, ex- isting novelty-based strategies become inefﬁcient in real-world problems where observation con- tains not only task-dependent state novelty of our interest but also task-irrelevant information that shou
Contextual Multi-armed Bandit Algorithm for Semiparametric Reward Model  Gi-Soo Kim 1 Myunghee Cho Paik 1  Abstract  Contextual multi-armed bandit (MAB) algorithms have been shown promising for maximizing cumu- lative rewards in sequential decision tasks such as news article recommendation systems, web page ad placement algorithms, and mobile health. However, most of the proposed contextual MAB algorithms assume linear relationships between the reward and the context of the action. This paper pr
Uniform Convergence Rate of the Kernel Density Estimator  Adaptive to Intrinsic Volume Dimension  Jisu Kim 1 Jaehyeok Shin 2 Alessandro Rinaldo 2 Larry Wasserman 2  Abstract  We derive concentration inequalities for the supre- mum norm of the difference between a kernel density estimator (KDE) and its point-wise expec- tation that hold uniformly over the selection of the bandwidth and under weaker conditions on the kernel and the data generating distribution than previously used in the literatur
Bit-Swap: Recursive Bits-Back Coding for Lossless Compression with  Hierarchical Latent Variables  Friso H. Kingma 1 Pieter Abbeel 1 Jonathan Ho 1  Abstract  The bits-back argument suggests that latent vari- able models can be turned into lossless compres- sion schemes. Translating the bits-back argument into efﬁcient and practical lossless compression schemes for general latent variable models, how- ever, is still an open problem. Bits-Back with Asymmetric Numeral Systems (BB-ANS), re- cently p
CompILE: Compositional Imitation Learning and Execution  Thomas Kipf 1 † Yujia Li 2 Hanjun Dai 3 † Vinicius Zambaldi 2 Alvaro Sanchez-Gonzalez 2  Edward Grefenstette 4 # Pushmeet Kohli 2 Peter Battaglia 2  Abstract  We introduce Compositional Imitation Learn- ing and Execution (CompILE): a framework for learning reusable, variable-length segments of hierarchically-structured behavior from demon- stration data. CompILE uses a novel unsuper- vised, fully-differentiable sequence segmentation module
Adaptive and Safe Bayesian Optimization in High Dimensions via  One-Dimensional Subspaces  Johannes Kirschner 1 Mojm´ır Mutn´y 1 Nicole Hiller 2 Rasmus Ischebeck 2 Andreas Krause 1  Abstract  Bayesian optimization is known to be difﬁcult to scale to high dimensions, because the acquisition step requires solving a non-convex optimization problem in the same search space. In order to scale the method and keep its beneﬁts, we pro- pose an algorithm (LINEBO) that restricts the problem to a sequence 
AUCµ: A Performance Metric for Multi-Class Machine Learning Models  Ross S. Kleiman 1 David Page 1 2  Abstract  The area under the receiver operating character- istic curve (AUC) is arguably the most common metric in machine learning for assessing the qual- ity of a two-class classiﬁcation model. As the number and complexity of machine learning ap- plications grows, so too does the need for mea- sures that can gracefully extend to classiﬁcation models trained for more than two classes. Prior wor
Fair k-Center Clustering for Data Summarization  Matth¨aus Kleindessner 1 Pranjal Awasthi 1 Jamie Morgenstern 2  Abstract  In data summarization we want to choose k proto- types in order to summarize a data set. We study a setting where the data set comprises several de- mographic groups and we are restricted to choose ki prototypes belonging to group i. A common approach to the problem without the fairness con- straint is to optimize a centroid-based clustering objective such as k-center. A nat
Guarantees for Spectral Clustering with Fairness Constraints  Matth¨aus Kleindessner 1 Samira Samadi 2 Pranjal Awasthi 1 Jamie Morgenstern 2  Abstract  Given the widespread popularity of spectral clus- tering (SC) for partitioning graph data, we study a version of constrained SC in which we try to incorporate the fairness notion proposed by Chierichetti et al. (2017). According to this no- tion, a clustering is fair if every demographic group is approximately proportionally represented in each c
POPQORN: Quantifying Robustness of Recurrent Neural Networks  Ching-Yun Ko * 1 Zhaoyang Lyu * 2 Tsui-Wei Weng 3 Luca Daniel 3 Ngai Wong 1 Dahua Lin 2  Abstract  The vulnerability to adversarial attacks has been a critical issue for deep neural networks. Address- ing this issue requires a reliable way to evaluate the robustness of a network. Recently, several methods have been developed to compute robust- ness quantiﬁcation for neural networks, namely, certiﬁed lower bounds of the minimum adversa
Decentralized Stochastic Optimization and Gossip Algorithms  with Compressed Communication  Anastasia Koloskova * 1 Sebastian U. Stich * 1 Martin Jaggi 1  Abstract  We consider decentralized stochastic optimiza- tion with the objective function (e.g. data samples for machine learning tasks) being distributed over n machines that can only communicate to their neighbors on a ﬁxed communication graph. To address the communication bottleneck, the nodes compress (e.g. quantize or sparsify) their mode
Robust Learning from Untrusted Sources  Nikola Konstantinov 1 Christoph H. Lampert 1  Abstract  Modern machine learning methods often require more data for training than a single expert can provide. Therefore, it has become a standard procedure to collect data from multiple external sources, e.g. via crowdsourcing. Unfortunately, the quality of these sources is not always guaran- teed. As further complications, the data might be stored in a distributed way, or might even have to remain private. 
Stochastic Beams and Where to Find Them:  The Gumbel-Top-k Trick for Sampling Sequences Without Replacement  Wouter Kool 1 2 Herke van Hoof 1 Max Welling 1 3  Abstract  The well-known Gumbel-Max trick for sampling from a categorical distribution can be extended to sample k elements without replacement. We show how to implicitly apply this ‘Gumbel-Top-k’ trick on a factorized distribution over sequences, allowing to draw exact samples without replace- ment using a Stochastic Beam Search. Even for
LIT: Learned Intermediate Representation Training for Model Compression  Animesh Koratana * 1 Daniel Kang * 1 Peter Bailis 1 Matei Zaharia 1  Abstract  Researchers have proposed a range of model com- pression techniques to reduce the computational and memory footprint of deep neural networks In this work, we introduce Learned (DNNs). Intermediate representation Training (LIT), a novel model compression technique that outper- forms a range of recent model compression tech- niques by leveraging th
Similarity of Neural Network Representations Revisited  Simon Kornblith 1 Mohammad Norouzi 1 Honglak Lee 1 Geoffrey Hinton 1  Abstract  Recent work has sought to understand the behav- ior of neural networks by comparing representa- tions between layers and between different trained models. We examine methods for comparing neu- ral network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring mul- tivariate similarity, but 
On the Complexity of Approximating Wasserstein Barycenters  Alexey Kroshnin 1 2 3 Darina Dvinskikh 4 1 Pavel Dvurechensky 4 1 Alexander Gasnikov 5 1 2 Nazarii Tupitsa 1 5  C´esar A. Uribe 6  Abstract  We study the complexity of approximating the Wasserstein barycenter of m discrete measures, or histograms of size n, by contrasting two al- ternative approaches that use entropic regulariza- tion. The ﬁrst approach is based on the Iterative Bregman Projections (IBP) algorithm for which our novel an
Estimate Sequences for Variance-Reduced Stochastic Composite Optimization  Andrei Kulunchakov 1 Julien Mairal 1  Abstract  In this paper, we propose a uniﬁed view of gradient-based algorithms for stochastic convex composite optimization by extending the con- cept of estimate sequence introduced by Nesterov. This point of view covers the stochastic gradi- ent descent method, variants of the approaches SAGA, SVRG, and has several advantages: (i) we provide a generic proof of convergence for the af
Faster Algorithms for Binary Matrix Factorization  Ravi Kumar 1 Rina Panigrahy 1 Ali Rahimi 1 David P. Woodruff 2  Abstract  We give faster approximation algorithms for well- studied variants of Binary Matrix Factorization (BMF), where we are given a binary m× n matrix A and would like to ﬁnd binary rank-k matrices U, V to minimize the Frobenius norm of U·V −A. In the ﬁrst setting, U · V denotes multi- plication over Z, and we give a constant- factor approximation algorithm that runs in 2O(k2 lo
Loss Landscapes of Regularized Linear Autoencoders  Daniel Kunin * 1 Jonathan M. Bloom * 2 Aleksandrina Goeva 2 Cotton Seed 2  Abstract  Autoencoders are a deep learning model for repre- sentation learning. When trained to minimize the distance between the data and its reconstruction, linear autoencoders (LAEs) learn the subspace spanned by the top principal directions but can- not learn the principal directions themselves. In this paper, we prove that L2-regularized LAEs are symmetric at all cr
Geometry and Symmetry in Short-and-Sparse Deconvolution  Han-Wen Kuo 1 2 Yuqian Zhang 3 Yenson Lau 1 2 John Wright 1 2 4  Abstract  We study the Short-and-Sparse (SaS) deconvo- lution problem of recovering a short signal a0 and a sparse signal x0 from their convolution. We propose a method based on nonconvex opti- mization, which under certain conditions recovers the target short and sparse signals, up to signed shift symmetry which is intrinsic to this model. This symmetry plays a central role 
A Large-Scale Study on Regularization and Normalization in GANs  Karol Kurach * 1 Mario Lucic * 1 Xiaohua Zhai 1 Marcin Michalski 1 Sylvain Gelly 1  Abstract  Generative adversarial networks (GANs) are a class of deep generative models which aim to learn a target distribution in an unsupervised fash- ion. While they were successfully applied to many problems, training a GAN is a notoriously chal- lenging task and requires a signiﬁcant number of hyperparameter tuning, neural architecture en- gine
Making Decisions that Reduce Discriminatory Impact  Matt J. Kusner 1 2 Chris Russell 1 3 Joshua R. Loftus 4 Ricardo Silva 1 5  Abstract  As machine learning algorithms move into real- world settings, it is crucial to ensure they are aligned with societal values. There has been much work on one aspect of this, namely the discriminatory prediction problem: How can we reduce discrimination in the predictions them- selves? While an important question, solutions to this problem only apply in a restri
GarbageIn,RewardOut:BootstrappingExplorationinMulti-ArmedBanditsBranislavKveton1CsabaSzepesv´ari23SharanVaswani4ZhengWen5MohammadGhavamzadeh6TorLattimore2AbstractWeproposeabanditalgorithmthatexploresbyrandomizingitshistoryofrewards.Speciﬁcally,itpullsthearmwiththehighestmeanrewardinanon-parametricbootstrapsampleofitshistorywithpseudorewards.Wedesignthepseudore-wardssuchthatthebootstrapmeanisoptimisticwithasufﬁcientlyhighprobability.WecallouralgorithmGiro,whichstandsforgarbagein,re-wardout.Weanal
Characterizing Well-Behaved vs. Pathological Deep Neural Networks  Antoine Labatie 1  Abstract  We introduce a novel approach, requiring only mild assumptions, for the characterization of deep neural networks at initialization. Our approach applies both to fully-connected and convolutional networks and easily incorporates batch normal- ization and skip-connections. Our key insight is to consider the evolution with depth of statistical moments of signal and noise, thereby characteriz- ing the pre
State-Reiﬁcation Networks: Improving Generalization by Modeling the Distribution of Hidden Representations  Alex Lamb 1 * Jonathan Binas 1 Anirudh Goyal 1 Sandeep Subramanian 1 Denis Kazakov 2  Ioannis Mitliagkas 1 Yoshua Bengio 1 3 Michael Mozer 4 *  Abstract  Machine learning promises methods that gener- alize well from ﬁnite labeled data. However, the brittleness of existing neural net approaches is revealed by notable failures, such as the existence of adversarial examples that are misclassi
A Recurrent Neural Cascade-based Model for Continuous-Time Diffusion  Sylvain Lamprier 1  Abstract  Many works have been proposed in the literature to capture the dynamics of diffusion in networks. While some of them deﬁne graphical Markovian models to extract temporal relationships between node infections in networks, others consider dif- fusion episodes as sequences of infections via recurrent neural models. In this paper we propose a model at the crossroads of these two extremes, which embeds
Projection onto Minkowski Sums with Application to Constrained Learning  Joong-Ho Won 1 Jason Xu 2 Kenneth Lange 3  Abstract  We introduce block descent algorithms for pro- jecting onto Minkowski sums of sets. Projection onto such sets is a crucial step in many statistical learning problems, and may regularize complexity of solutions to an optimization problem or arise in dual formulations of penalty methods. We show that projecting onto the Minkowski sum admits simple, efﬁcient algorithms when 
Safe Policy Improvement with Baseline Bootstrapping  Romain Laroche 1 Paul Trichelair 1 Remi Tachet des Combes 1  Abstract  This paper considers Safe Policy Improvement (SPI) in Batch Reinforcement Learning (Batch RL): from a ﬁxed dataset and without direct ac- cess to the true environment, train a policy that is guaranteed to perform at least as well as the baseline policy used to collect the data. Our ap- proach, called SPI with Baseline Bootstrapping (SPIBB), is inspired by the knows-what-it-
A Better k-means++ Algorithm via Local Search  Silvio Lattanzi * 1 Christian Sohler * 1  Abstract  In this paper, we develop a new variant of k- means++ seeding that in expectation achieves a constant approximation guarantee. We obtain this result by a simple combination of k-means++ sam- pling with a local search strategy. We evaluate our algorithm empirically and show that it also improves the quality of the solution in practice.  1. Introduction As a central problem in unsupervised learning c
Lorentzian Distance Learning for Hyperbolic Representations  Marc T. Law 1 2 3 Renjie Liao 1 2 Jake Snell 1 2 Richard S. Zemel 1 2  Abstract  We introduce an approach to learn representations based on the Lorentzian distance in hyperbolic ge- ometry. Hyperbolic geometry is especially suited to hierarchically-structured datasets, which are prevalent in the real world. Current hyperbolic representation learning methods compare exam- ples with the Poincar´e distance. They try to min- imize the dist
DP-GP-LVM: A Bayesian Non-Parametric Model for  Learning Multivariate Dependency Structures  Andrew R. Lawrence 1 Carl Henrik Ek 2 Neill D. F. Campbell 1  Abstract  We present a non-parametric Bayesian latent vari- able model capable of learning dependency struc- tures across dimensions in a multivariate setting. Our approach is based on ﬂexible Gaussian pro- cess priors for the generative mappings and in- terchangeable Dirichlet process priors to learn the structure. The introduction of the Dir
POLITEX: Regret Bounds for Policy Iteration Using Expert Prediction  Yasin Abbasi-Yadkori 1 Peter L. Bartlett 2 Kush Bhatia 2 Nevena Lazi´c 3 Csaba Szepesvári 4 Gellért Weisz 4  Abstract  We present POLITEX (POLicy ITeration with EX- pert advice), a variant of policy iteration where each policy is a Boltzmann distribution over the sum of action-value function estimates of the pre- vious policies, and analyze its regret in continuing RL problems. We assume that the value func- tion error after ru
Batch Policy Learning under Constraints  Hoang M. Le 1 Cameron Voloshin 1 Yisong Yue 1  Abstract  When learning policies for real-world domains, two important questions arise: (i) how to efﬁ- ciently use pre-collected off-policy, non-optimal behavior data; and (ii) how to mediate among dif- ferent competing objectives and constraints. We thus study the problem of batch policy learning un- der multiple constraints, and offer a systematic so- lution. We ﬁrst propose a ﬂexible meta-algorithm that a
Target-Based Temporal-Difference Learning  Donghwan Lee 1 Niao He 2  Abstract  The use of target networks has been a popular and key component of recent deep Q-learning al- gorithms for reinforcement learning, yet little is known from the theory side. In this work, we introduce a new family of target-based temporal difference (TD) learning algorithms that main- tain two separate learning parameters – the target variable and online variable. We propose three members in the family, the averaging T
Functional Transparency for Structured Data: a Game-Theoretic Approach  Guang-He Lee 1 Wengong Jin 1 David Alvarez-Melis 1 Tommi S. Jaakkola 1  Abstract  We provide a new approach to training neural models to exhibit transparency in a well-deﬁned, functional manner. Our approach naturally oper- ates over structured data and tailors the predictor, functionally, towards a chosen family of (local) witnesses. The estimation problem is setup as a co-operative game between an unrestricted predic- tor 
Self-Attention Graph Pooling  Junhyun Lee * 1 Inyeop Lee * 1 Jaewoo Kang 1  Abstract  Advanced methods of applying deep learning to structured data such as graphs have been proposed in recent years. In particular, studies have fo- cused on generalizing convolutional neural net- works to graph data, which includes redeﬁning the convolution and the downsampling (pooling) operations for graphs. The method of generaliz- ing the convolution operation to graphs has been proven to improve performance a
Set Transformer: A Framework for Attention-based  Permutation-Invariant Neural Networks  Juho Lee 1 2 Yoonho Lee 3 Jungtaek Kim 4 Adam R. Kosiorek 1 5 Seungjin Choi 4 Yee Whye Teh 1  Abstract  Many machine learning tasks such as multiple instance learning, 3D shape recognition and few- shot image classiﬁcation are deﬁned on sets of in- stances. Since solutions to such problems do not depend on the order of elements of the set, mod- els used to address them should be permutation invariant. We pre
First-Order Algorithms Converge Faster than O(1/k) on Convex Problems  Ching-pei Lee 1 Stephen J. Wright 1  Abstract  It is well known that both gradient descent and stochastic coordinate descent achieve a global convergence rate of O(1/k) in the objective value, when applied to a scheme for minimiz- ing a Lipschitz-continuously differentiable, un- constrained convex function. In this work, we improve this rate to o(1/k). We extend the re- sult to proximal gradient and proximal coordinate descen
Robust Inference via Generative Classiﬁers for Handling Noisy Labels  Kimin Lee 1 Sukmin Yun 1 Kibok Lee 2 Honglak Lee 3 2 Bo Li 4 Jinwoo Shin 1 5  Abstract  Large-scale datasets may contain signiﬁcant pro- portions of noisy (incorrect) class labels, and it is well-known that modern deep neural networks (DNNs) poorly generalize from such noisy train- ing datasets. To mitigate the issue, we propose a novel inference method, termed Robust Genera- tive classiﬁer (RoG), applicable to any discrimina-
Sublinear Time Nearest Neighbor Search over Generalized Weighted Space  Yifan Lei 1 Qiang Huang 1 Mohan Kankanhalli 1 Anthony K. H. Tung 1  Abstract  Nearest Neighbor Search (NNS) over generalized weighted space is a fundamental problem which has many applications in various ﬁelds. However, to the best of our knowledge, there is no sublin- ear time solution to this problem. Based on the idea of Asymmetric Locality-Sensitive Hashing (ALSH), we introduce a novel spherical asymmet- ric transformati
MONK – Outlier-Robust Mean Embedding Estimation by Median-of-Means  Matthieu Lerasle 1 2 Zolt´an Szab´o 3 Timoth´ee Mathieu 1 Guillaume Lecu´e 4  Abstract  Mean embeddings provide an extremely ﬂexible and powerful tool in machine learning and statis- tics to represent probability distributions and de- ﬁne a semi-metric (MMD, maximum mean dis- crepancy; also called N-distance or energy dis- tance), with numerous successful applications. The representation is constructed as the expec- tation of th
Cheap Orthogonal Constraints in Neural Networks:  A Simple Parametrization of the Orthogonal and Unitary Group  Mario Lezcano-Casado 1 David Mart´ınez-Rubio 2  Abstract  We introduce a novel approach to perform ﬁrst- order optimization with orthogonal and uni- tary constraints. This approach is based on a parametrization stemming from Lie group theory through the exponential map. The parametrization transforms the constrained optimization problem into an unconstrained one over a Euclidean space,
Are Generative Classiﬁers More Robust to Adversarial Attacks?  Yingzhen Li 1 John Bradshaw 2 3 Yash Sharma 4  Abstract  There is a rising interest in studying the robust- ness of deep neural network classiﬁers against adversaries, with both advanced attack and de- fence techniques being actively developed. How- ever, most recent work focuses on discriminative classiﬁers, which only model the conditional dis- tribution of the labels given the inputs. In this paper, we propose and investigate the 
Sublinear Quantum Algorithms for Training Linear and  Kernel-based Classiﬁers  Tongyang Li 1 Shouvanik Chakrabarti 1 Xiaodi Wu 1  Abstract  We investigate quantum algorithms for classiﬁca- tion, a fundamental problem in machine learning, with provable guarantees. Given n d-dimensional data points, the state-of-the-art (and optimal) classical algorithm for training classiﬁers with constant margin (Clarkson et al., 2012) runs in ˜O(n + d)1. We design sublinear quantum algo- rithms for the same tas
LGM-Net: Learning to Generate Matching Networks for Few-Shot Learning  Huaiyu Li 1 2 Weiming Dong 1 Xing Mei 3 Chongyang Ma 4 Feiyue Huang 5 Bao-Gang Hu 1  Abstract  In this work, we propose a novel meta-learning approach for few-shot classiﬁcation, which learns transferable prior knowledge across tasks and directly produces network parameters for similar unseen tasks with training samples. Our approach, called LGM-Net, includes two key modules, namely, TargetNet and MetaNet. The TargetNet modul
Graph Matching Networks for  Learning the Similarity of Graph Structured Objects  Yujia Li 1 Chenjie Gu 1 Thomas Dullien 2 Oriol Vinyals 1 Pushmeet Kohli 1  Abstract  This paper addresses the challenging problem of retrieval and matching of graph structured ob- jects, and makes two key contributions. First, we demonstrate how Graph Neural Networks (GNN), which have emerged as an effective model for var- ious supervised prediction problems deﬁned on structured data, can be trained to produce embe
Area Attention  Yang Li 1 Lukasz Kaiser 1 Samy Bengio 1 Si Si 1  Abstract  Existing attention mechanisms are trained to at- tend to individual items in a collection (the mem- ory) with a predeﬁned, ﬁxed granularity, e.g., a word token or an image grid. We propose area attention: a way to attend to areas in the memory, where each area contains a group of items that are structurally adjacent, e.g., spatially for a 2D memory such as images, or temporally for a 1D memory such as natural language sen
Online Learning to Rank with Features  Shuai Li 1 Tor Lattimore 2 Csaba Szepesvári 2  Abstract  We introduce a new model for online ranking in which the click probability factors into an exami- nation and attractiveness function and the attrac- tiveness function is a linear function of a feature vector and an unknown parameter. Only relatively mild assumptions are made on the examination function. A novel algorithm for this setup is anal- ysed, showing that the dependence on the number of items 
N ATTACK: Learning the Distributions of Adversarial Examples for an  Improved Black-Box Attack on Deep Neural Networks  Yandong Li * 1 Lijun Li * 1 Liqiang Wang 1 Tong Zhang 2 Boqing Gong 3  Abstract  Powerful adversarial attack methods are vital for understanding how to construct robust deep neural networks (DNNs) and thoroughly testing defense techniques. In this paper, we propose a black-box adversarial attack algorithm that can defeat both vanilla DNNs and those generated by various de- fens
Bayesian Joint Spike-and-Slab Graphical Lasso  Zehang Richard Li 1 Tyler H. McCormick 2 3 Samuel J. Clark 4  Abstract  In this article, we propose a new class of priors for Bayesian inference with multiple Gaussian graph- ical models. We introduce Bayesian treatments of two popular procedures, the group graphical lasso and the fused graphical lasso, and extend them to a continuous spike-and-slab framework to allow self-adaptive shrinkage and model selection simultaneously. We develop an EM algor
Exploiting Worker Correlation for Label Aggregation in Crowdsourcing  Yuan Li 1 Benjamin I. P. Rubinstein 1 Trevor Cohn 1  Abstract  Crowdsourcing has emerged as a core component of data science pipelines. From collected noisy worker labels, aggregation models that incorpo- rate worker reliability parameters aim to infer a latent true annotation. In this paper, we argue that existing crowdsourcing approaches do not sufﬁciently model worker correlations observed in practical settings; we propose 
Adversarial camera stickers:  A physical camera-based attack on deep learning systems  Juncheng B. Li 1 2 Frank R. Schmidt 1 J. Zico Kolter 1 2  Abstract  Recent work has documented the susceptibility of deep learning systems to adversarial examples, but most such attacks directly manipulate the dig- ital input to a classiﬁer. Although a smaller line of work considers physical adversarial attacks, in all cases these involve manipulating the object of interest, e.g., putting a physical sticker on
Towards a Uniﬁed Analysis of Random Fourier Features  Zhu Li 1 Jean-François Ton 1 Dino Oglic 2 Dino Sejdinovic 1  Abstract  Random Fourier features is a widely used, sim- ple, and effective technique for scaling up ker- nel methods. The existing theoretical analysis of the approach, however, remains focused on spe- ciﬁc learning tasks and typically gives pessimistic bounds which are at odds with the empirical re- sults. We tackle these problems and provide the ﬁrst uniﬁed risk analysis of learn
Towards a Uniﬁed Analysis of Random Fourier Features  Zhu Li 1 Jean-François Ton 1 Dino Oglic 2 Dino Sejdinovic 1  Abstract  Random Fourier features is a widely used, sim- ple, and effective technique for scaling up ker- nel methods. The existing theoretical analysis of the approach, however, remains focused on spe- ciﬁc learning tasks and typically gives pessimistic bounds which are at odds with the empirical re- sults. We tackle these problems and provide the ﬁrst uniﬁed risk analysis of learn
Learn to Grow: A Continual Structure Learning Framework for Overcoming  Catastrophic Forgetting  Xilai Li * † 1 Yingbo Zhou * 2 Tianfu Wu 1 Richard Socher 2 Caiming Xiong 2  Abstract  Addressing catastrophic forgetting is one of the key challenges in continual learning where ma- chine learning systems are trained with sequential or streaming tasks. Despite recent remarkable progress in state-of-the-art deep learning, deep neural networks (DNNs) are still plagued with the catastrophic forgetting 
Alternating Minimizations Converge to Second-Order Optimal Solutions  Qiuwei Li * 1 Zhihui Zhu * 2 Gongguo Tang 1  Abstract  This work studies the second-order convergence for both standard alternating minimization and proximal alternating minimization. We show that under mild assumptions on the (nonconvex) ob- jective function, both algorithms avoid strict sad- dles almost surely from random initialization. To- gether with known ﬁrst-order convergence results, this implies that both algorithms 
Cautious Regret Minimization:  Online Optimization with Long-Term Budget Constraints  Nikolaos Liakopoulos 1 2 Apostolos Destounis 1 Georgios Paschos 1 Thrasyvoulos Spyropoulos 2  Panayotis Mertikopoulos 3  Abstract  We study a class of online convex optimization problems with long-term budget constraints that arise naturally as reliability guarantees or total consumption constraints. In this general setting, prior work by Mannor et al. (2009) has shown that achieving no regret is impossible if 
Regularization in Directable Environments with Application to Tetris  Jan Malte Lichtenberg 1  ¨Ozg¨ur S¸ims¸ek 1  Abstract  Learning from small data sets is difﬁcult in the ab- sence of speciﬁc domain knowledge. We present a regularized linear model called STEW, which ben- eﬁts from a generic and prevalent form of prior knowledge: feature directions. STEW shrinks weights toward each other, converging to an equal- weights solution in the limit of inﬁnite regulariza- tion. We provide theoretical 
Inference and Sampling of K33-free Ising Models  Valerii Likhosherstov 1 Yury Maximov 1 2 Michael Chertkov 1 2 3  Abstract  We call an Ising model tractable when it is possi- ble to compute its partition function value (statisti- cal inference) in polynomial time. The tractability also implies an ability to sample conﬁgurations of this model in polynomial time. The notion of tractability extends the basic case of planar zero-ﬁeld Ising models. Our starting point is to describe algorithms for the
Kernel-Based Reinforcement Learning in Robust Markov Decision Processes  Shiau Hong Lim 1 Arnaud Autef 2  Abstract  The robust Markov Decision Process (MDP) framework aims to address the problem of param- eter uncertainty due to model mismatch, approx- imation errors or even adversarial behaviors. It is especially relevant when deploying the learned policies in real-world applications. Scaling up the robust MDP framework to large or continu- ous state space remains a challenging problem. The use
On Efﬁcient Optimal Transport: An Analysis of Greedy and Accelerated  Mirror Descent Algorithms  Tianyi Lin * 1 Nhat Ho * 2 Michael I. Jordan 3  Abstract  We provide theoretical analyses for two algo- rithms that solve the regularized optimal transport (OT) problem between two discrete probability measures with at most n atoms. We show that a greedy variant of the classical Sinkhorn algo- rithm, known as the Greenkhorn algorithm, can  be improved to (cid:101)O(cid:0)n2/ε2(cid:1), improving on th
Fast and Simple Natural-Gradient Variational Inference  with Mixture of Exponential-family Approximations  Wu Lin 1 Mohammad Emtiyaz Khan 2 Mark Schmidt 1  Abstract  Natural-gradient methods enable fast and simple algorithms for variational inference, but due to computational difﬁculties, their use is mostly lim- ited to minimal exponential-family (EF) approx- imations. In this paper, we extend their applica- tion to estimate structured approximations such as mixtures of EF distributions. Such a
Acceleration of SVRG and Katyusha X by Inexact Preconditioning  Yanli Liu 1 Fei Feng 1 Wotao Yin 1  Abstract  Empirical risk minimization is an important class of optimization problems with many popular ma- chine learning applications, and stochastic vari- ance reduction methods are popular choices for solving them. Among these methods, SVRG and Katyusha X (a Nesterov accelerated SVRG) achieve fast convergence without substantial memory requirement. In this paper, we propose to accelerate these 
Transferable Adversarial Training:  A General Approach to Adapting Deep Classiﬁers  Hong Liu 1 2 Mingsheng Long 1 3 Jianmin Wang 1 3 Michael I. Jordan 4  Abstract  Domain adaptation enables knowledge transfer from a labeled source domain to an unlabeled tar- get domain. A mainstream approach is adversarial feature adaptation, which learns domain-invariant representations through aligning the feature distri- butions of both domains. However, a theoretical prerequisite of domain adaptation is the 
Rao-Blackwellized Stochastic Gradients for Discrete Distributions  Runjing Liu 1 Jeffrey Regier 2 Nilesh Tripuraneni 2 Michael I. Jordan 1 2 Jon McAuliffe 1 3  Abstract  We wish to compute the gradient of an expec- tation over a ﬁnite or countably inﬁnite sample space having K ≤ ∞ categories. When K is in- deed inﬁnite, or ﬁnite but very large, the relevant summation is intractable. Accordingly, various stochastic gradient estimators have been proposed. In this paper, we describe a technique tha
Sparse Extreme Multi-label Learning with Oracle Property  Weiwei Liu 1 Xiaobo Shen 2  Abstract  The pioneering work of sparse local embeddings for extreme classiﬁcation (SLEEC) (Bhatia et al., 2015) has shown great promise in multi-label learning. Unfortunately, the statistical rate of con- vergence and oracle property of SLEEC are still not well understood. To ﬁll this gap, we present a uniﬁed framework for SLEEC with nonconvex penalty. Theoretically, we rigorously prove that our proposed estim
Data Poisoning Attacks on Stochastic Bandits  Fang Liu 1 Ness Shroff 1 2  Abstract  Stochastic multi-armed bandits form a class of on- line learning problems that have important appli- cations in online recommendation systems, adap- tive medical treatment, and many others. Even though potential attacks against these learning al- gorithms may hijack their behavior, causing catas- trophic loss in real-world applications, little is known about adversarial attacks on bandit algo- rithms. In this pap
The Implicit Fairness Criterion of Unconstrained Learning  Lydia T. Liu * 1 Max Simchowitz * 1 Moritz Hardt 1  Abstract  We clarify what fairness guarantees we can and cannot expect to follow from unconstrained ma- chine learning. Speciﬁcally, we show that in many settings, unconstrained learning on its own im- plies group calibration, that is, the outcome vari- able is conditionally independent of group mem- bership given the score. A lower bound conﬁrms the optimality of our upper bound. Moreo
Taming MAML: Efﬁcient Unbiased Meta-Reinforcement Learning  Hao Liu 1 Richard Socher 1 Caiming Xiong 1  Abstract  While meta reinforcement learning (Meta-RL) methods have achieved remarkable success, ob- taining correct and low variance estimates for policy gradients remains a signiﬁcant challenge. In particular, estimating a large Hessian, poor sample efﬁciency and unstable training continue to make Meta-RL difﬁcult. We propose a surro- gate objective function named, Taming MAML (TMAML), that a
On Certifying Non-uniform Bounds against Adversarial Attacks  Chen Liu 1 Ryota Tomioka 2 Volkan Cevher 1  Abstract  This work studies the robustness certiﬁcation problem of neural network models, which aims to ﬁnd certiﬁed adversary-free regions as large as possible around data points. In contrast to the existing approaches that seek regions bounded uniformly along all input features, we consider non-uniform bounds and use it to study the de- cision boundary of neural network models. We formulat
Understanding and Accelerating Particle-Based Variational Inference  Chang Liu 1 Jingwei Zhuo 1 Pengyu Cheng 2 Ruiyi Zhang 2 Jun Zhu 1 Lawrence Carin 2  Abstract  Particle-based variational inference methods (ParVIs) have gained attention in the Bayesian inference literature, for their capacity to yield ﬂexible and accurate approximations. We explore ParVIs from the perspective of Wasserstein gradi- ent ﬂows, and make both theoretical and practical contributions. We unify various ﬁnite-particle 
Understanding MCMC Dynamics as Flows on the Wasserstein Space  Chang Liu 1 Jingwei Zhuo 1 Jun Zhu 1  Abstract  It is known that the Langevin dynamics used in MCMC is the gradient ﬂow of the KL divergence on the Wasserstein space, which helps conver- gence analysis and inspires recent particle-based variational inference methods (ParVIs). But no more MCMC dynamics is understood in this way. In this work, by developing novel concepts, we propose a theoretical framework that recognizes a general MC
Sliced-Wasserstein Flows: Nonparametric Generative Modeling via Optimal  Transport and Diffusions  Antoine Liutkus 1 Umut S¸ims¸ekli 2 Szymon Majewski 3 Alain Durmus 4 Fabian-Robert St¨oter 1  Abstract  By building upon the recent theory that estab- lished the connection between implicit generative modeling (IGM) and optimal transport, in this study, we propose a novel parameter-free algo- rithm for learning the underlying distributions of complicated datasets and sampling from them. The propose
Challenging Common Assumptions in the Unsupervised Learning of  Disentangled Representations  Francesco Locatello 1 2 Stefan Bauer 2 Mario Lucic 3 Gunnar Rätsch 1 Sylvain Gelly 3 Bernhard Schölkopf 2  Olivier Bachem 3  Abstract  The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progr
Bayesian Counterfactual Risk Minimization  Ben London 1 Ted Sandler 1  Abstract  We present a Bayesian view of counterfactual risk minimization (CRM) for ofﬂine learning from logged bandit feedback. Using PAC- Bayesian analysis, we derive a new generaliza- tion bound for the truncated inverse propensity score estimator. We apply the bound to a class of Bayesian policies, which motivates a novel, potentially data-dependent, regularization tech- nique for CRM. Experimental results indicate that th
PA-GD: On the Convergence of Perturbed Alternating Gradient Descent to Second-Order Stationary Points for Structured Nonconvex Optimization  Songtao Lu 1 Mingyi Hong 1 Zhengdao Wang 2  Abstract  Alternating gradient descent (A-GD) is a simple but popular algorithm in machine learning, which updates two blocks of variables in an alternating manner using gradient descent steps. In this paper, we consider a smooth unconstrained nonconvex optimization problem, and propose a perturbed A-GD (PA-GD) wh
Neurally-Guided Structure Inference  Sidi Lu * 1 Jiayuan Mao * 2 3 Joshua B. Tenenbaum 2 4 5 Jiajun Wu 2  Abstract  Most structure inference methods either rely on exhaustive search or are purely data-driven. Ex- haustive search robustly infers the structure of arbitrarily complex data, but it is slow. Data- driven methods allow efﬁcient inference, but do not generalize when test data have more complex structures than training data. In this paper, we pro- pose a hybrid inference algorithm, the N
Optimal Algorithms for Lipschitz Bandits with Heavy-tailed Rewards  Shiyin Lu 1 Guanghui Wang 1 Yao Hu 2 Lijun Zhang 1  Abstract  We study Lipschitz bandits, where a learner re- peatedly plays one arm from an inﬁnite arm set and then receives a stochastic reward whose ex- pectation is a Lipschitz function of the chosen arm. Most of existing work assume the reward distri- butions are bounded or at least sub-Gaussian, and thus do not apply to heavy-tailed rewards arising in many real-world scenari
CoT: Cooperative Training for Generative Modeling of Discrete Data  Sidi Lu 1 Lantao Yu 2 Siyuan Feng 1 Yaoming Zhu 1 Weinan Zhang 1 Yong Yu 1  Abstract  In this paper, we study the generative models of sequential discrete data. To tackle the exposure bias problem inherent in maximum likelihood es- timation (MLE), generative adversarial networks (GANs) are introduced to penalize the unrealis- tic generated samples. To exploit the supervi- sion signal from the discriminator, most previ- ous model
Generalized Approximate Survey Propagation  for High-Dimensional Estimation  Luca Saglietti 1 2 Yue M. Lu 3 Carlo Lucibello 4 1  Abstract  In Generalized Linear Estimation (GLE) prob- lems, we seek to estimate a signal that is ob- served through a linear transform followed by a component-wise, possibly nonlinear and noisy, channel. In the Bayesian optimal setting, Gener- alized Approximate Message Passing (GAMP) is known to achieve optimal performance for GLE. However, its performance can signiﬁ
High-Fidelity Image Generation With Fewer Labels  Mario Lucic * 1 Michael Tschannen * 2 Marvin Ritter * 1 Xiaohua Zhai 1 Olivier Bachem 1 Sylvain Gelly 1  Abstract  Deep generative models are becoming a cor- nerstone of modern machine learning. Recent work on conditional generative adversarial net- works has shown that learning complex, high- dimensional distributions over natural images is within reach. While the latest models are able to generate high-ﬁdelity, diverse natural images at high re
LeveragingLow-RankRelationsBetweenSurrogateTasksinStructuredPredictionGiuliaLuise1DimitrisStamos1MassimilianoPontil12CarloCiliberto13AbstractWestudytheinterplaybetweensurrogatemeth-odsforstructuredpredictionandtechniquesfrommultitasklearningdesignedtoleveragerelation-shipsbetweensurrogateoutputs.Weproposeanefﬁcientalgorithmbasedontracenormregular-izationwhich,differentlyfrompreviousmethods,doesnotrequireexplicitknowledgeofthecod-ing/decodingfunctionsofthesurrogateframe-work.Asaresult,ouralgorith
Differentiable Dynamic Normalization for Learning Deep Representation  Ping Luo * 1 2 Zhanglin Peng * 3 Wenqi Shao 2 3 Ruimao Zhang 2 3 Jiamin Ren 3 Lingyun Wu 3  Abstract  This work presents Dynamic Normalization (DN), which is able to learn arbitrary normalization operations for different convolutional layers in a deep ConvNet. Unlike existing normalization approaches that predeﬁned computations of the statistics (mean and variance), DN learns to estimate them. DN has several appealing beneﬁts
Disentangled Graph Convolutional Networks  Jianxin Ma 1 Peng Cui 1 Kun Kuang 1 Xin Wang 1 Wenwu Zhu 1  Abstract  The formation of a real-world graph typically arises from the highly complex interaction of many latent factors. The existing deep learning methods for graph-structured data neglect the en- tanglement of the latent factors, rendering the learned representations non-robust and hardly ex- plainable. However, learning representations that disentangle the latent factors poses great chal- 
Variational Implicit Processes  Chao Ma 1 Yingzhen Li 2 José Miguel Hernández-Lobato 1 2  Abstract  We introduce the implicit processes (IPs), a stochastic process that places implicitly deﬁned multivariate distributions over any ﬁnite collec- tions of random variables. IPs are therefore highly ﬂexible implicit priors over functions, with ex- amples including data simulators, Bayesian neu- ral networks and non-linear transformations of stochastic processes. A novel and efﬁcient approx- imate inf

Bayesian Leave-One-Out Cross-Validation for Large Data  M˚ans Magnusson 1 Michael Riis Andersen 1 2 Johan Jonasson 3 Aki Vehtari 1  Abstract  Model inference, such as model comparison, model checking, and model selection, is an impor- tant part of model development. Leave-one-out cross-validation (LOO-CV) is a general approach for assessing the generalizability of a model, but unfortunately, LOO-CV does not scale well to large datasets. We propose a combination of using approximate inference tec
Composable Core-sets for Determinant Maximization:  A Simple Near-Optimal Algorithm  Piotr Indyk * 1 Sepideh Mahabadi * 2 Shayan Oveis Gharan * 3 Alireza Rezaei * 3  Abstract  “Composable core-sets” are an efﬁcient frame- work for solving optimization problems in mas- sive data models. In this work, we consider efﬁ- cient construction of composable core-sets for the determinant maximization problem. This can also be cast as the MAP inference task for determi- nantal point processes, that have re
Guided evolutionary strategies:  Augmenting random search with surrogate gradients  Niru Maheswaranathan 1 Luke Metz 1 George Tucker 1 Dami Choi 1 Jascha Sohl-Dickstein 1  Abstract  Many applications in machine learning require optimizing a function whose true gradient is in- accessible, but where surrogate gradient informa- tion (directions that may be correlated with, but not necessarily identical to, the true gradient) is available instead. This arises when an approxi- mate gradient is easier
Universal Multi-Party Poisoning Attacks  Saeed Mahloujifar 1 Mohammad Mahmoody 1 Ameer Mohammed 2  Abstract  In this work, we demonstrate universal multi- party poisoning attacks that adapt and apply to any multi-party learning process with arbitrary interaction pattern between the parties. More gen- erally, we introduce and study (k, p)-poisoning attacks in which an adversary controls k ∈ [m] of the parties, and for each corrupted party Pi, the adversary submits some poisoned data T (cid:48) i 
Traditional and Heavy Tailed Self Regularization in Neural Network Models  Charles H. Martin 1 Michael W. Mahoney 2  Abstract  Random Matrix Theory (RMT) is applied to an- alyze the weight matrices of Deep Neural Net- works (DNNs), including both production quality, pre-trained models such as AlexNet and Incep- tion, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet. Empir- ical and theoretical results clearly indicate that the empirical spectral density (ESD) of DN
Curvature-Exploiting Acceleration of Elastic Net Computations  Vien V. Mai 1 Mikael Johansson 1  Abstract  This paper introduces an efﬁcient second-order method for solving the elastic net problem. Its key innovation is a computationally efﬁcient technique for injecting curvature information in the optimization process which admits a strong theoretical performance guarantee. In particular, we show improved run time over popular ﬁrst- order methods and quantify the speed-up in terms of statistica
BreakingthegridlockinMixture-of-Experts:ConsistentandEfﬁcientAlgorithmsAshokVardhanMakkuva1SewoongOh2SreeramKannan3PramodViswanath1AbstractMixture-of-Experts(MoE)isawidelypopularmodelforensemblelearningandisabasicbuild-ingblockofhighlysuccessfulmodernneuralnet-worksaswellasacomponentinGatedRecurrentUnits(GRU)andAttentionnetworks.However,presentalgorithmsforlearningMoE,includingtheEMalgorithmandgradientdescent,areknowntogetstuckinlocaloptima.Fromatheoreticalviewpoint,ﬁndinganefﬁcientandprovablyco
Calibrated Model-Based Deep Reinforcement Learning  Ali Malik * 1 Volodymyr Kuleshov * 1 2 Jiaming Song 1 Danny Nemer 2 Harlan Seymour 2 Stefano Ermon 1  Abstract  Estimates of predictive uncertainty are important for accurate model-based planning and reinforce- ment learning. However, predictive uncertainties — especially ones derived from modern deep learn- ing systems — can be inaccurate and impose a bottleneck on performance. This paper explores which uncertainties are needed for model-based
Learning from Delayed Outcomes via Proxies with Applications to  Recommender Systems  Timothy A. Mann * 1 Sven Gowal * 1 András György 1 Ray Jiang 1 Huiyi Hu 1 Balaji Lakshminarayanan 1  Prav Srinivasan 1  Abstract  Predicting delayed outcomes is an important prob- lem in recommender systems (e.g., if customers will ﬁnish reading an ebook). We formalize the problem as an adversarial, delayed online learn- ing problem and consider how a proxy for the delayed outcome (e.g., if customers read a thi
Descent Algorithms and Local Minima in Spiked Matrix-Tensor Models  Passed & Spurious:  Stefano Sarao Mannelli 1 Florent Krzakala 2 Pierfrancesco Urbani 1 Lenka Zdeborov´a 1  Abstract  In this work we analyse quantitatively the inter- play between the loss landscape and performance of descent algorithms in a prototypical inference problem, the spiked matrix-tensor model. We study a loss function that is the negative log- likelihood of the model. We analyse the number of local minima at a ﬁxed di
A Baseline for Any Order Gradient Estimation  in Stochastic Computation Graphs  Jingkai Mao * 1 Jakob Foerster * 2 Tim Rockt¨aschel 3 Maruan Al-Shedivat 4 Gregory Farquhar 2  Shimon Whiteson 2  Abstract  By enabling correct differentiation in stochastic computation graphs (SCGs), the inﬁnitely differ- entiable Monte-Carlo estimator (DiCE) can gen- erate correct estimates for the higher order gradi- ents that arise in, e.g., multi-agent reinforcement learning and meta-learning. However, the base-
Adversarial Generation of Time-Frequency Features  with application in audio synthesis  Andr´es Maraﬁoti 1 Nicki Holighaus 1 Nathana¨el Perraudin 2 Piotr Majdak 1  Abstract  Time-frequency (TF) representations provide powerful and intuitive features for the analysis of time series such as audio. But still, generative modeling of audio in the TF domain is a sub- tle matter. Consequently, neural audio synthesis widely relies on directly modeling the waveform and previous attempts at unconditionall
On the Universality of Invariant Networks  Haggai Maron 1 Ethan Fetaya 2 3 Nimrod Segol 1 Yaron Lipman 1  Abstract  Constraining linear layers in neural networks to respect symmetry transformations from a group G is a common design principle for invariant net- works that has found many applications in ma- chine learning. In this paper, we consider a fun- damental question that has received little atten- tion to date: Can these networks approximate any (continuous) invariant function? We tackle t
Decomposing feature-level variation with  Covariate Gaussian Process Latent Variable Models  Kaspar Märtens 1 Kieran R Campbell 2 3 4 Christopher Yau 5 6  Abstract  The interpretation of complex high-dimensional data typically requires the use of dimensionality reduction techniques to extract explanatory low- dimensional representations. However, in many real-world problems these representations may not be sufﬁcient to aid interpretation on their own, and it would be desirable to interpret the m
Fairness-Aware Learning for Continuous Attributes and Treatments  J´er´emie Mary 1 Cl´ement Calauz`enes 1 Noureddine El Karoui 1 2  Abstract  We address the problem of algorithmic fairness: ensuring that the outcome of a classiﬁer is not biased towards certain values of sensitive vari- ables such as age, race or gender. As common fairness metrics can be expressed as measures of (conditional) independence between variables, we propose to use the R´enyi maximum correlation coefﬁcient to generalize
Optimal Minimal Margin Maximization with Boosting  Allan Grønlund * 1 Kasper Green Larsen * 1 Alexander Mathiasen * 1  Abstract  Boosting algorithms iteratively produce linear combinations of more and more base hypotheses and it has been observed experimentally that the generalization error keeps improving even after achieving zero training error. One popular expla- nation attributes this to improvements in margins. A common goal in a long line of research, is to maximize the smallest margin usi
Disentangling Disentanglement in Variational Autoencoders  Emile Mathieu * 1 Tom Rainforth * 1 N. Siddharth * 2 Yee Whye Teh 1  Abstract  We develop a generalisation of disentanglement in variational autoencoders (VAEs)—decomposition of the latent representation—characterising it as the fulﬁlment of two factors: a) the latent encod- ings of the data having an appropriate level of overlap, and b) the aggregate encoding of the data conforming to a desired structure, represented through the prior. 
MIWAE: Deep Generative Modelling and Imputation of Incomplete Data Sets  Pierre-Alexandre Mattei 1 Jes Frellsen 1  Abstract  We consider the problem of handling missing data with deep latent variable models (DLVMs). First, we present a simple technique to train DLVMs when the training set contains missing-at-random data. Our approach, called MIWAE, is based on the importance-weighted autoencoder (IWAE), and maximises a potentially tight lower bound of the log-likelihood of the observed data. Com
Distributional Reinforcement Learning for Efﬁcient Exploration  Borislav Mavrin 1 2 Hengshuai Yao 3 Linglong Kong 1 2 Kaiwen Wu 4 Yaoliang Yu 4  Abstract  In distributional reinforcement learning (RL), the estimated distribution of value function models both the parametric and intrinsic uncertainties. We propose a novel and efﬁcient exploration method for deep RL that has two components. The ﬁrst is a decaying schedule to suppress the intrinsic uncertainty. The second is an exploration bonus cal
Graphical-model based estimation and inference for differential privacy  Ryan McKenna 1 Daniel Sheldon 1 2 Gerome Miklau 1  Abstract  Many privacy mechanisms reveal high-level in- formation about a data distribution through noisy measurements. It is common to use this informa- tion to estimate the answers to new queries. In this work, we provide an approach to solve this estima- tion problem efﬁciently using graphical models, which is particularly effective when the distribu- tion is high-dimens
Efﬁcient Amortised Bayesian Inference  for Hierarchical and Nonlinear Dynamical Systems  Geoffrey Roeder 1 2 Paul K Grant 1 Andrew Phillips 1 Neil Dalchau 1 Edward Meeds 1  Abstract  We introduce a ﬂexible, scalable Bayesian infer- ence framework for nonlinear dynamical systems characterised by distinct and hierarchical variabil- ity at the individual, group, and population levels. Our model class is a generalisation of nonlinear mixed-effects (NLME) dynamical systems, the statistical workhorse 
Toward Controlling Discrimination in Online Ad Auctions  L. Elisa Celis 1 Anay Mehrotra 2 Nisheeth K. Vishnoi 3  Abstract  Online advertising platforms are thriving due to the customizable audiences they offer advertisers. However, recent studies show that advertisements can be discriminatory with respect to the gender or race of the audience that sees the ad, and may inadvertently cross ethical and/or legal bound- aries. To prevent this, we propose a constrained ad auction framework that maximi
Stochastic Blockmodels meet Graph Neural Networks  Nikhil Mehta∗ 1 Lawrence Carin 1 Piyush Rai 2  Abstract  Stochastic blockmodels (SBM) and their variants, e.g., mixed-membership and overlapping stochas- tic blockmodels, are latent variable based gener- ative models for graphs. They have proven to be successful for various tasks, such as discovering the community structure and link prediction on graph-structured data. Recently, graph neural net- works, e.g., graph convolutional networks, have a
Imputing Missing Events in Continuous-Time Event Streams  Hongyuan Mei 1 Guanghui Qin 2 Jason Eisner 1  Abstract  Events in the world may be caused by other, unobserved events. We consider sequences of events in continuous time. Given a probability model of complete sequences, we propose parti- cle smoothing—a form of sequential importance sampling—to impute the missing events in an incomplete sequence. We develop a trainable family of proposal distributions based on a type of bidirectional cont
Same, Same But Different: Recovering Neural Network Quantization Error  Through Weight Factorization  Meller Eldad 1 Finkelstein Alexander 1 Almog Uri 1 Grobman Mark 1  Abstract  Quantization of neural networks has become com- mon practice, driven by the need for efﬁcient im- plementations of deep neural networks on em- bedded devices. In this paper, we exploit an oft-overlooked degree of freedom in most net- works - for a given layer, individual output chan- nels can be scaled by any factor pro
The Wasserstein Transform  Facundo M´emoli * 1 2 Zane Smith * 3 Zhengchao Wan * 1  Abstract  We introduce the Wasserstein transform, a method for enhancing and denoising datasets deﬁned on general metric spaces. The construction draws in- spiration from Optimal Transportation ideas. We establish the stability of our method under data perturbation and, when the dataset is assumed to be Euclidean, we also exhibit a precise connec- tion between the Wasserstein transform and the mean shift family of
Ithemal: Accurate, Portable and Fast Basic Block Throughput Estimation  using Deep Neural Networks  Charith Mendis 1 Alex Renda 1 Saman Amarasinghe 1 Michael Carbin 1  Abstract  Predicting the number of clock cycles a processor takes to execute a block of assembly instructions in steady state (the throughput) is important for both compiler designers and performance engi- neers. Building an analytical model to do so is especially complicated in modern x86-64 Com- plex Instruction Set Computer (CI
Geometric Losses for Distributional Learning  Arthur Mensch 1 2 Mathieu Blondel 3 Gabriel Peyr´e 1 2  Abstract  Building upon recent advances in entropy- regularized optimal transport, and upon Fenchel duality between measures and continuous func- tions, we propose a generalization of the lo- gistic loss that incorporates a metric or cost between classes. Unlike previous attempts to use optimal transport distances for learning, our loss results in unconstrained convex objective functions, suppor
Spectral Clustering of Signed Graphs via Matrix Power Means  Pedro Mercado 1 2 Francesco Tudisco 3 Matthias Hein 2  Abstract  Signed graphs encode positive (attractive) and negative (repulsive) relations between nodes. We extend spectral clustering to signed graphs via the one-parameter family of Signed Power Mean Laplacians, deﬁned as the matrix power mean of normalized standard and signless Laplacians of positive and negative edges. We provide a thorough analysis of the proposed approach in th
Simple Stochastic Gradient Methods for  Non-Smooth Non-Convex Regularized Optimization  Michael R. Metel 1 Akiko Takeda 1 2  Abstract  Our work focuses on stochastic gradient methods for optimizing a smooth non-convex loss func- tion with a non-smooth non-convex regularizer. Research on this class of problem is quite lim- ited, and until recently no non-asymptotic conver- gence results have been reported. We present two simple stochastic gradient algorithms, for ﬁnite- sum and general stochastic
Reinforcement Learning in Conﬁgurable Continuous Environments  Alberto Maria Metelli 1 Emanuele Ghelﬁ 1 Marcello Restelli 1  Abstract  Conﬁgurable Markov Decision Processes (Conf- MDPs) have been recently introduced as an ex- tension of the usual MDP model to account for the possibility of conﬁguring the environment to improve the agent’s performance. Currently, there is still no suitable algorithm to solve the learning problem for real-world Conf-MDPs. In this pa- per, we ﬁll this gap by propos
Understanding and correcting pathologies in the training of learned optimizers  Luke Metz 1 Niru Maheswaranathan 1 Jeremy Nixon 1 C. Daniel Freeman 1 Jascha Sohl-Dickstein 1  Abstract  Deep learning has shown that learned func- tions can dramatically outperform hand-designed functions on perceptual tasks. Analogously, this suggests that learned optimizers may simi- larly outperform current hand-designed optimiz- ers, especially for speciﬁc problems. How- ever, learned optimizers are notoriously 
OptimalityImpliesKernelSumClassiﬁersareStatisticallyEfﬁcientRaphaelA.Meyer1JeanHonorio1AbstractWeproposeanovelcombinationofoptimizationtoolswithlearningtheoryboundsinordertoan-alyzethesamplecomplexityofoptimalkernelsumclassiﬁers.Thiscontraststhetypicallearningtheoreticresultswhichholdforall(potentiallysuboptimal)classiﬁers.Ourworkalsojustiﬁesassumptionsmadeinpriorworkonmultipleker-nellearning.Asabyproductofouranalysis,wealsoprovideanewformofRademachercomplex-ityforhypothesisclassescontainingonly
On Dropout and Nuclear Norm Regularization  Poorya Mianjy 1 Raman Arora 1  Abstract  We give a formal and complete characterization of the explicit regularizer induced by dropout in deep linear networks with squared loss. We show that (a) the explicit regularizer is composed of an (cid:96)2-path regularizer and other terms that are also re- scaling invariant, (b) the convex envelope of the induced regularizer is the squared nuclear norm of the network map, and (c) for a sufﬁciently large dropout
Discriminative Regularization for Latent Variable Models  with Applications to Electrocardiography  Andrew C. Miller 1 Ziad Obermeyer 2 John P. Cunningham 3 Sendhil Mullainathan 4  Abstract  Generative models often use latent variables to represent structured variation in high-dimensional data, such as images and medical waveforms. However, these latent variables may ignore subtle, yet meaningful features in the data. Some features may predict an outcome of interest (e.g. heart attack) but accou
Formal Privacy for Functional Data with Gaussian Perturbations  Ardalan Mirshani 1 Matthew Reimherr 1 Aleksandra Slavkovic 1  Abstract  Motivated by the rapid rise in statistical tools in Functional Data Analysis, we consider the Gaus- sian mechanism for achieving differential privacy (DP) with parameter estimates taking values in a, potentially inﬁnite-dimensional, separable Ba- nach space. Using classic results from probabil- ity theory, we show how densities over function spaces can be utiliz
Co-manifold learning with missing data  Gal Mishne 1 Eric C. Chi 2 Ronald R. Coifman 1  Abstract  Representation learning is typically applied to only one mode of a data matrix, either its rows or columns. Yet in many applications, there is an underlying geometry to both the rows and the columns. We propose utilizing this coupled struc- ture to perform co-manifold learning: uncovering the underlying geometry of both the rows and the columns of a given matrix, where we focus on a missing data set
Agnostic Federated Learning  Mehryar Mohri 1 2 Gary Sivek 1 Ananda Theertha Suresh 1  Abstract  A key learning scenario in large-scale applications is that of federated learning, where a centralized model is trained based on data originating from a large number of clients. We argue that, with the existing training and inference, federated models can be biased towards different clients. Instead, we propose a new framework of agnostic feder- ated learning, where the centralized model is opti- mize
Flat Metric Minimization with Applications in Generative Modeling  Thomas M¨ollenhoff 1 Daniel Cremers 1  Abstract  We take the novel perspective to view data not as a probability distribution but rather as a cur- rent. Primarily studied in the ﬁeld of geomet- ric measure theory, k-currents are continuous lin- ear functionals acting on compactly supported smooth differential forms and can be understood as a generalized notion of oriented k-dimensional manifold. By moving from distributions (whic
Parsimonious Black-Box Adversarial Attacks  via Efﬁcient Combinatorial Optimization  Seungyong Moon * 1 2 Gaon An * 1 2 Hyun Oh Song 1 2  Abstract  Solving for adversarial examples with projected gradient descent has been demonstrated to be highly effective in fooling the neural network based classiﬁers. However, in the black-box setting, the attacker is limited only to the query access to the network and solving for a successful adversarial example becomes much more difﬁcult. To this end, recen
Parameter Efﬁcient Training of Deep Convolutional  Neural Networks by Dynamic Sparse Reparameterization  Hesham Mostafa 1 Xin Wang 1 2  Abstract  Modern deep neural networks are typically highly overparameterized. Pruning techniques are able to remove a signiﬁcant fraction of network pa- rameters with little loss in accuracy. Recently, techniques based on dynamic reallocation of non- zero parameters have emerged, allowing direct training of sparse networks without having to pre- train a large de
A Dynamical Systems Perspective on Nesterov Acceleration  Michael Muehlebach 1 Michael I. Jordan 1  Abstract  We present a dynamical system framework for understanding Nesterov’s accelerated gradient method. In contrast to earlier work, our derivation does not rely on a vanishing step size argument. We show that Nesterov acceleration arises from discretizing an ordinary differential equation with a semi-implicit Euler integration scheme. We an- alyze both the underlying differential equation as 
Relational Pooling for Graph Representations  Ryan L. Murphy 1 Balasubramaniam Srinivasan 2 Vinayak Rao 1 Bruno Ribeiro 2  Abstract  This work generalizes graph neural networks (GNNs) beyond those based on the Weisfeiler- Lehman (WL) algorithm, graph Laplacians, and diffusions. Our approach, denoted Relational Pooling (RP), draws from the theory of ﬁnite partial exchangeability to provide a framework with maximal representation power for graphs. RP can work with existing graph representation mod
Learning Optimal Fair Policies  Razieh Nabi 1 Daniel Malinsky 1 Ilya Shpitser 1  Abstract  Systematic discriminatory biases present in our society inﬂuence the way data is collected and stored, the way variables are deﬁned, and the way scientiﬁc ﬁndings are put into practice as policy. Automated decision procedures and learning al- gorithms applied to such data may serve to perpet- uate existing injustice or unfairness in our society. In this paper, we consider how to make optimal but fair decis
Lexicographic and Depth-Sensitive Margins in  Homogeneous and Non-Homogeneous Deep Models  Mor Shpigel Nacson 1 Suriya Gunasekar 2 Jason D. Lee 3 Nathan Srebro 2 Daniel Soudry 1  Abstract  With an eye toward understanding complexity control in deep learning, we study how inﬁnites- imal regularization or gradient descent optimiza- tion lead to margin maximizing solutions in both homogeneous and non homogeneous models, ex- tending previous work that focused on inﬁnitesi- mal regularization only in
A Wrapped Normal Distribution on Hyperbolic Space  for Gradient-Based Learning  Yoshihiro Nagano 1 Shoichiro Yamaguchi 2 Yasuhiro Fujita 2 Masanori Koyama 2  (a) A tree representation of the training dataset  (b) Vanilla VAE ((cid:12) = 1:0)  (c) Hyperbolic VAE  Abstract  Hyperbolic space is a geometry that is known to be well-suited for representation learning of data with an underlying hierarchical structure. In this paper, we present a novel hyperbolic distri- bution called hyperbolic wrapped
Dropout as a Structured Shrinkage Prior  Eric Nalisnick 1 José Miguel Hernández-Lobato 1 2 3 Padhraic Smyth 4  Abstract  Dropout regularization of deep neural networks has been a mysterious yet effective tool to prevent overﬁtting. Explanations for its success range from the prevention of "co-adapted" weights to it being a form of cheap Bayesian inference. We propose a novel framework for understanding multiplicative noise in neural networks, consider- ing continuous distributions as well as Ber
Hybrid Models with Deep and Invertible Features  Eric Nalisnick * 1 Akihiro Matsukawa * 1 Yee Whye Teh 1 Dilan Gorur 1 Balaji Lakshminarayanan 1  Abstract  We propose a neural hybrid model consisting of a linear model deﬁned on a set of features com- puted by a deep, invertible transformation (i.e. a normalizing ﬂow). An attractive property of our model is that both p(features), the density of the features, and p(targets|features), the predictive distribution, can be computed ex- actly in a sing
Learning Context-Dependent  Label Permutations for Multi-Label Classiﬁcation  Jinseok Nam 1 Young-Bum Kim 1 Eneldo Loza Menc´ıa 2 Sunghyun Park 1 Ruhi Sarikaya 1  Johannes F¨urnkranz 2  Abstract  A key problem in multi-label classiﬁcation is to utilize dependencies among the labels. Chaining classiﬁers are a simple technique for addressing this problem but current algorithms all assume a ﬁxed, static label ordering. In this work, we pro- pose a multi-label classiﬁcation approach which allows to 
Zero-Shot Knowledge Distillation in Deep Networks  Gaurav Kumar Nayak * 1 Konda Reddy Mopuri * 2 Vaisakh Shaj * 3 R. Venkatesh Babu 1  Anirban Chakraborty 1  Abstract  Knowledge distillation deals with the problem of training a smaller model (Student) from a high ca- pacity source model (Teacher) so as to retain most of its performance. Existing approaches use either the training data or meta-data extracted from it in order to train the Student. However, accessing the dataset on which the Teache
A Framework for Bayesian Optimization in Embedded Subspaces  Alexander Munteanu 1 * Amin Nayebi 2 * Matthias Poloczek 3 2  Abstract  We present a theoretically founded approach for high-dimensional Bayesian optimization based on low-dimensional subspace embeddings. We prove that the error in the Gaussian process model is bounded tightly when going from the origi- nal high-dimensional search domain to the low- dimensional embedding. This implies that the optimization process in the low-dimensiona
Phaseless PCA: Low-Rank Matrix Recovery from Column-wise Phaseless  Measurements  Seyedehsara Nayer 1 Praneeth Narayanamurthy 1 Namrata Vaswani 1  Abstract  This work proposes the ﬁrst set of simple, prac- tically useful, and provable algorithms for two inter-related problems. (i) The ﬁrst is low-rank matrix recovery from magnitude-only (phaseless) linear projections of each of its columns. This ﬁnds important applications in phaseless dynamic imaging, e.g., Fourier ptychographic imaging of live
Safe Grid Search with Optimal Complexity  Eugene Ndiaye 1 Tam Le 1 Olivier Fercoq 2 Joseph Salmon 3 Ichiro Takeuchi 4  Abstract  Popular machine learning estimators involve reg- ularization parameters that can be challenging to tune, and standard strategies rely on grid search for this task. In this paper, we revisit the tech- niques of approximating the regularization path up to predeﬁned tolerance (cid:15) in a uniﬁed frame- work and show that its complexity is O(1/ d√(cid:15)) for uniformly c
On Connected Sublevel Sets in Deep Learning  Quynh Nguyen 1  Abstract  This paper shows that every sublevel set of the loss function of a class of deep over- parameterized neural nets with piecewise linear activation functions is connected and unbounded. This implies that the loss has no bad local valleys and all of its global minima are connected within a unique and potentially very large global valley.  1. Introduction It has been commonly observed in deep learning that over- parameterization 
Anomaly Detection With Multiple-Hypotheses Predictions  Duc Tam Nguyen 1 2 Zhongyu Lou 2 Michael Klar 2 Thomas Brox 1  Abstract  In one-class-learning tasks, only the normal case (foreground) can be modeled with data, whereas the variation of all possible anomalies is too er- ratic to be described by samples. Thus, due to the lack of representative data, the wide-spread dis- criminative approaches cannot cover such learn- ing tasks, and rather generative models, which attempt to learn the input 
Non-Asymptotic Analysis of Fractional Langevin Monte Carlo for  Non-Convex Optimization  Thanh Huy Nguyen 1 Umut S¸ims¸ekli 1 Ga¨el Richard 1  Abstract  Recent studies on diffusion-based sampling meth- ods have shown that Langevin Monte Carlo (LMC) algorithms can be beneﬁcial for non- convex optimization, and rigorous theoretical guarantees have been proven for both asymp- totic and ﬁnite-time regimes. Algorithmically, LMC-based algorithms resemble the well-known gradient descent (GD) algorithm,
Rotation Invariant Householder Parameterization for Bayesian PCA  Rajbir S. Nirwan 1 Nils Bertschinger 1 2  Abstract  We consider probabilistic PCA and related fac- tor models from a Bayesian perspective. These models are in general not identiﬁable as the like- lihood has a rotational symmetry. This gives rise to complicated posterior distributions with contin- uous subspaces of equal density and thus hinders efﬁciency of inference as well as interpretation of obtained parameters. In particular,
Lossless or Quantized Boosting with Integer Arithmetic  Richard Nock 1 2 3 Robert C. Williamson 2 1  Abstract  In supervised learning, efﬁciency often starts with the choice of a good loss: support vector machines popularised Hinge loss, Adaboost popularised the exponential loss, etc. Recent trends in machine learning have highlighted the necessity for train- ing routines to meet tight requirements on commu- nication, bandwidth, energy, operations, encoding, among others. Fitting the often decad
Training Neural Networks with Local Error Signals  Arild Nøkland * 1 Lars H. Eidnes * 2  Abstract  Supervised training of neural networks for classi- ﬁcation is typically performed with a global loss function. The loss function provides a gradient for the output layer, and this gradient is back- propagated to hidden layers to dictate an update direction for the weights. An alternative approach is to train the network with layer-wise loss func- tions. In this paper we demonstrate, for the ﬁrst ti
Remember and Forget for Experience Replay  Guido Novati 1 Petros Koumoutsakos 1  Abstract  Experience replay (ER) is a fundamental com- ponent of off-policy deep reinforcement learning (RL). ER recalls experiences from past iterations to compute gradient estimates for the current pol- icy, increasing data-efﬁciency. However, the ac- curacy of such updates may deteriorate when the policy diverges from past behaviors and can un- dermine the performance of ER. Many algorithms mitigate this issue by
Learning to Infer Program Sketches  Maxwell Nye 1 2 Luke Hewitt 1 2 3 Joshua Tenenbaum 1 2 4 Armando Solar-Lezama 2  Abstract  Our goal is to build systems which write code automatically from the kinds of speciﬁcations hu- mans can most easily provide, such as examples and natural language instruction. The key idea of this work is that a ﬂexible combination of pattern recognition and explicit reasoning can be used to solve these complex programming problems. We propose a method for dynamically i
Tensor Variable Elimination for Plated Factor Graphs  Fritz Obermeyer * 1 Eli Bingham * 1 Martin Jankowiak * 1 Justin Chiu 2 Neeraj Pradhan 1 Alexander M. Rush 2  Noah Goodman 1 3  Abstract  A wide class of machine learning algorithms can be reduced to variable elimination on factor graphs. While factor graphs provide a unifying notation for these algorithms, they do not provide a compact way to express repeated structure when compared to plate diagrams for directed graph- ical models. To exploi
Counterfactual Off-Policy Evaluation with Gumbel-Max Structural Causal Models  Michael Oberst 1 David Sontag 1  Abstract  We introduce an off-policy evaluation procedure for highlighting episodes where applying a rein- forcement learned (RL) policy is likely to have produced a substantially different outcome than the observed policy. In particular, we introduce a class of structural causal models (SCMs) for generating counterfactual trajectories in ﬁnite partially observable Markov Decision Proc
Model Based Conditional Gradient Method with Armijo-like Line Search  Yura Malitsky * 1 Peter Ochs * 2  Abstract  The Conditional Gradient Method is generalized to a class of non-smooth non-convex optimiza- tion problems with many applications in machine learning. The proposed algorithm iterates by min- imizing so-called model functions over the con- straint set. Complemented with an Armijo line search procedure, we prove that subsequences converge to a stationary point. The abstract frame- work
TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing  Augustus Odena 1 Catherine Olsson 2 David G. Andersen 1 Ian Goodfellow 3  Abstract  Neural networks are difﬁcult to interpret and debug. We introduce testing techniques for neural networks that can discover errors occurring only for rare inputs. Speciﬁcally, we develop coverage-guided fuzzing (CGF) methods for neural networks. In CGF, random mutations of inputs are guided by a coverage metric toward the goal of satisfying user-s
Scalable Learning in Reproducing Kernel Kre˘ın Spaces  Dino Oglic 1 Thomas Gärtner 2  Abstract  We provide the ﬁrst mathematically complete derivation of the Nyström method for low-rank ap- proximation of indeﬁnite kernels and propose an efﬁcient method for ﬁnding an approximate eigen- decomposition of such kernel matrices. Building on this result, we devise highly scalable methods for learning in reproducing kernel Kre˘ın spaces. The devised approaches provide a principled and theoretically wel
Approximation and Non-parametric Estimation of  ResNet-type Convolutional Neural Networks  Kenta Oono 1 2 Taiji Suzuki 1 3  Abstract  Convolutional neural networks (CNNs) have been shown to achieve optimal approximation and es- timation error rates (in minimax sense) in sev- eral function classes. However, previous analyzed optimal CNNs are unrealistically wide and dif- ﬁcult to obtain via optimization due to sparse constraints in important function classes, includ- ing the H¨older class. We sho
Orthogonal Random Forest for Causal Inference  Miruna Oprescu 1 Vasilis Syrgkanis 1 Zhiwei Steven Wu 2  Abstract  We propose the orthogonal random forest, an al- gorithm that combines Neyman-orthogonality to reduce sensitivity with respect to estimation er- ror of nuisance parameters with generalized ran- dom forests (Athey et al., 2017)—a ﬂexible non- parametric method for statistical estimation of conditional moment models using random forests. We provide a consistency rate and establish asymp
Inferring Heterogeneous Causal Effects in Presence of Spatial Confounding  Muhammad Osama 1 Dave Zachariah 1 Thomas B. Sch¨on 1  Abstract  We address the problem of inferring the causal effect of an exposure on an outcome across space, using observational data. The data is pos- sibly subject to unmeasured confounding vari- ables which, in a standard approach, must be adjusted for by estimating a nuisance function. Here we develop a method that eliminates the nuisance function, while mitigating t
Overparameterized Nonlinear Learning: Gradient Descent Takes the Shortest Path?  Samet Oymak 1 Mahdi Soltanolkotabi 2  Abstract  Many modern learning tasks involve ﬁtting non- linear models which are trained in an overparame- terized regime where the parameters of the model exceed the size of the training dataset. Due to this overparameterization, the training loss may have inﬁnitely many global minima and it is criti- cal to understand the properties of the solutions found by ﬁrst-order optimiz
Multiplicative Weights Update as a Distributed Constrained Optimization Algorithm: Convergence to Second-order Stationary Points Almost Always  Ioannis Panageas 1 Georgios Piliouras 1 Xiao Wang 1  Abstract  Non-concave maximization has been the subject of much recent study in the optimization and ma- chine learning communities, speciﬁcally in deep learning. Recent papers (Ge et al., 2015), (Lee et al., 2017) and references therein indicate that ﬁrst order methods work well and avoid saddle point
Improving Adversarial Robustness via Promoting Ensemble Diversity  Tianyu Pang 1 Kun Xu 1 Chao Du 1 Ning Chen 1 Jun Zhu 1  Abstract  Though deep neural networks have achieved sig- niﬁcant progress on various tasks, often enhanced by model ensemble, existing high-performance models can be vulnerable to adversarial attacks. Many efforts have been devoted to enhancing the robustness of individual networks and then con- structing a straightforward ensemble, e.g., by di- rectly averaging the outputs,
Nonparametric Bayesian Deep Networks with Local Competition  Konstantinos P. Panousis * 1 Sotirios Chatzis * 2 Sergios Theodoridis 1 3  Abstract  The aim of this work is to enable inference of deep networks that retain high accuracy for the least possible model complexity, with the latter de- duced from the data during inference. To this end, we revisit deep networks that comprise competing linear units, as opposed to nonlinear units that do not entail any form of (local) competition. In this co
Optimistic Policy Optimization via Multiple Importance Sampling  Matteo Papini 1 Alberto Maria Metelli 1 Lorenzo Lupo 1 Marcello Restelli 1  Abstract  Policy Search (PS) is an effective approach to Reinforcement Learning (RL) for solving con- trol tasks with continuous state-action spaces. In this paper, we address the exploration-exploitation trade-off in PS by proposing an approach based on Optimism in the Face of Uncertainty. We cast the PS problem as a suitable Multi Armed Bandit (MAB) probl
Deep Residual Output Layers for Neural Language Generation  Nikolaos Pappas 1 James Henderson 1  Abstract  Many tasks, including language generation, bene- ﬁt from learning the structure of the output space, particularly when the space of output labels is large and the data is sparse. State-of-the-art neu- ral language models indirectly capture the output space structure in their classiﬁer weights since they lack parameter sharing across output labels. Learning shared output label mappings helps
Measurements of Three-Level Hierarchical Structure in the Outliers in the Spectrum of Deepnet Hessians  Vardan Papyan 1  Abstract  We expose a structure in deep classifying neural networks in the derivative of the logits with re- spect to the parameters of the model, which is used to explain the existence of outliers in the spectrum of the Hessian. Previous works decom- posed the Hessian into two components, attribut- ing the outliers to one of them, the so-called Co- variance of gradients. We s
Generalized Majorization-Minimization  Sobhan Naderi 1 Kun He 2 Reza Aghajani 3 Stan Sclaroff 4 Pedro Felzenszwalb 5  Abstract  Non-convex optimization is ubiquitous in ma- chine learning. Majorization-Minimization (MM) is a powerful iterative procedure for op- timizing non-convex functions that works by op- timizing a sequence of bounds on the function. In MM, the bound at each iteration is required to touch the objective function at the optimizer of the previous bound. We show that this touchi
Variational Laplace Autoencoders  Yookoon Park 1 Chris Dongjoo Kim 1 Gunhee Kim 1  Abstract  Variational autoencoders (Kingma & Welling, 2014) employ an amortized inference model to ap- proximate the posterior of latent variables. How- ever, such amortized variational inference faces two challenges: (1) the limited posterior expres- siveness of fully-factorized Gaussian assumption and (2) the amortization error of the inference model. We present a novel approach that ad- dresses both challenges.
The Effect of Network Width on Stochastic Gradient Descent  and Generalization: an Empirical Study  Daniel S. Park 1 2 Jascha Sohl-Dickstein 1 Quoc V. Le 1 Samuel L. Smith 3  Abstract  We investigate how the ﬁnal parameters found by stochastic gradient descent are inﬂuenced by over- parameterization. We generate families of models by increasing the number of channels in a base net- work, and then perform a large hyper-parameter search to study how the test error depends on learning rate, batch s
Spectral Approximate Inference  Sejun Park 1 Eunho Yang 2 3 4 Se-Young Yun 3 5 Jinwoo Shin 1 3 4  Abstract  Given a graphical model (GM), computing its partition function is the most essential inference task, but it is computationally intractable in gen- eral. To address the issue, iterative approxi- mation algorithms exploring certain local struc- ture/consistency of GM have been investigated as popular choices in practice. However, due to their local/iterative nature, they often output poor ap
Self-Supervised Exploration via Disagreement  Deepak Pathak * 1 Dhiraj Gandhi * 2 Abhinav Gupta 2 3  Abstract  Efﬁcient exploration is a long-standing problem in sensorimotor learning. Major advances have been demonstrated in noise-free, non-stochastic domains such as video games and simulation. However, most of these formulations either get stuck in environments with stochastic dynamics or are too inefﬁcient to be scalable to real robotics setups. In this paper, we propose a formulation for exp
Subspace Robust Wasserstein Distances  Franc¸ois-Pierre Paty 1 Marco Cuturi 2 1  Abstract  Making sense of Wasserstein distances between discrete measures in high-dimensional settings re- mains a challenge. Recent work has advocated a two-step approach to improve robustness and facilitate the computation of optimal transport, us- ing for instance projections on random real lines, or a preliminary quantization of the measures to reduce the size of their support. We propose in this work a “max-min
Fingerprint Policy Optimisation  for Robust Reinforcement Learning  Supratik Paul 1 Michael A. Osborne 2 Shimon Whiteson 1  Abstract  Policy gradient methods ignore the potential value of adjusting environment variables: unobservable state features that are randomly determined by the environment in a physical setting, but are con- trollable in a simulator. This can lead to slow learning, or convergence to suboptimal policies, if the environment variable has a large impact on the transition dynam
COMIC: Multi-view Clustering Without Parameter Selection  Xi Peng 1 Zhenyu Huang 1 Jianchen Lv 1 Hongyuan Zhu 2 Joey Tianyi Zhou 3  Abstract  In this paper, we study two challenges in cluster- ing analysis, namely, how to cluster multi-view data and how to perform clustering without pa- rameter selection on cluster size. To this end, we propose a novel objective function to project raw data into one space in which the projection em- braces the geometric consistency (GC) and the cluster assignmen
Domain Agnostic Learning with Disentangled Representations  Xingchao Peng 1 Zijun Huang 2 Ximeng Sun 1 Kate Saenko 1  Abstract  Unsupervised model transfer has the potential to greatly improve the generalizability of deep mod- els to novel domains. Yet the current literature assumes that the separation of target data into dis- tinct domains is known as a priori. In this paper, we propose the task of Domain-Agnostic Learn- ing (DAL): How to transfer knowledge from a labeled source domain to unlab
Collaborative Channel Pruning for Deep Networks  Hanyu Peng 1 Jiaxiang Wu 2 Shifeng Chen 1 Junzhou Huang 3  Abstract  Deep networks have achieved impressive perfor- mance in various domains, but their applications are largely limited by the prohibitive computa- tional overhead. In this paper, we propose a novel algorithm, namely collaborative channel pruning (CCP), to reduce the computational overhead with negligible performance degradation. The joint impact of pruned/preserved channels on the l
Exploiting Structure of Uncertainty for Efﬁcient Matroid Semi-Bandits  Pierre Perrault 1 2 Vianney Perchet 2 3 Michal Valko 1 4  Abstract  We improve the efﬁciency of algorithms for stochastic combinatorial semi-bandits. In most interesting problems, state-of-the-art algorithms take advantage of structural properties of rewards, such as independence. However, while being optimal in terms of asymptotic regret, these al- gorithms are inefﬁcient. In our paper, we ﬁrst reduce their implementation to
Cognitive Model Priors for Predicting Human Decisions  David D. Bourgin * 1 Joshua C. Peterson * 2 Daniel Reichman 2 Stuart J. Russell 1 Thomas L. Grifﬁths 2  Abstract  Human decision-making underlies all economic behavior. For the past four decades, human decision-making under uncertainty has continued to be explained by theoretical models based on prospect theory, a framework that was awarded the Nobel Prize in Economic Sciences. However, theoretical models of this kind have developed slowly, 
Towards Understanding Knowledge Distillation  Mary Phuong 1 Christoph H. Lampert 1  Abstract  Knowledge distillation, i.e. one classiﬁer being trained on the outputs of another classiﬁer, is an empirically very successful technique for knowl- edge transfer between classiﬁers. It has even been observed that classiﬁers learn much faster and more reliably if trained with the outputs of another classiﬁer as soft labels, instead of from ground truth data. So far, however, there is no satisfactory the
Temporal Gaussian Mixture Layer for Videos  AJ Piergiovanni 1 Michael S. Ryoo 1  Abstract  We introduce a new convolutional layer named the Temporal Gaussian Mixture (TGM) layer and present how it can be used to efﬁciently capture longer-term temporal information in continuous activity videos. The TGM layer is a temporal con- volutional layer governed by a much smaller set of parameters (e.g., location/variance of Gaussians) that are fully differentiable. We present our fully convolutional video
Voronoi Boundary Classiﬁcation: A High-Dimensional Geometric Approach  via Weighted Monte Carlo Integration  Vladislav Polianskii 1 Florian T. Pokorny 1  Abstract  Voronoi cell decompositions provide a classical avenue to classiﬁcation. Typical approaches how- ever only utilize point-wise cell-membership in- formation by means of nearest neighbor queries and do not utilize further geometric information about Voronoi cells since the computation of Voronoi diagrams is prohibitively expensive in hi
On Variational Bounds of Mutual Information  Ben Poole 1 Sherjil Ozair 1 2 A¨aron van den Oord 3 Alexander A. Alemi 1 George Tucker 1  Abstract  Estimating and optimizing Mutual Informa- tion (MI) is core to many problems in machine learning; however, bounding MI in high dimen- sions is challenging. To establish tractable and scalable objectives, recent work has turned to vari- ational bounds parameterized by neural networks, but the relationships and tradeoffs between these bounds remains uncle
Hiring Under Uncertainty  Manish Raghavan 1 Manish Purohit 2 Sreenivas Gollapudi 2  Abstract  In this paper we introduce the hiring under un- certainty problem to model the questions faced by hiring committees in large enterprises and uni- versities alike. Given a set of n eligible candi- dates, the decision maker needs to choose the sequence of candidates to make offers so as to hire the k best candidates. However, candidates may choose to reject an offer (for instance, due to a competing offer
SAGA with Arbitrary Sampling  Xun Qian 1 Zheng Qu 2 Peter Rich´arik 1 3  Abstract  We study the problem of minimizing the average of a very large number of smooth functions, which is of key importance in training supervised learn- ing models. One of the most celebrated methods in this context is the SAGA algorithm of Defazio et al. (2014). Despite years of research on the topic, a general-purpose version of SAGA—one that would include arbitrary importance sampling and minibatching schemes—does n
SGD: General Analysis and Improved Rates  Robert M. Gower 1 Nicolas Loizou 2 Xun Qian 3 Alibek Sailanbayev 3 Egor Shulgin 4 Peter Richt´arik 3 2 4  Abstract  We propose a general yet simple theorem describ- ing the convergence of SGD under the arbitrary sampling paradigm. Our theorem describes the convergence of an inﬁnite array of variants of SGD, each of which is associated with a speciﬁc probability law governing the data selection rule used to form minibatches. This is the ﬁrst time such an 
AUTOVC: Zero-Shot Voice Style Transfer with Only Autoencoder Loss  Kaizhi Qian * 1 Yang Zhang * 2 3 Shiyu Chang 2 3 Xuesong Yang 1 Mark Hasegawa-Johnson 1  Abstract  Non-parallel many-to-many voice conversion, as well as zero-shot voice conversion, remain under- explored areas. Deep style transfer algorithms, such as generative adversarial networks (GAN) and conditional variational autoencoder (CVAE), are being applied as new solutions in this ﬁeld. However, GAN training is sophisticated and dif
Fault Tolerance in Iterative-Convergent Machine Learning  Aurick Qiao 1 2 Bryon Aragam 3 Bingjing Zhang 1 Eric P. Xing 1 2 3  Abstract  Machine learning (ML) training algorithms often possess an inherent self-correcting behavior due to their iterative- convergent nature. Recent systems exploit this property to achieve adaptability and efficiency in unreliable computing environments by relaxing the consistency of execution and allowing calculation errors to be self-corrected during training. Howe
Imperceptible, Robust, and Targeted  Adversarial Examples for Automatic Speech Recognition  Yao Qin 1 Nicholas Carlini 2 Ian Goodfellow 2 Garrison Cottrell 1 Colin Raffel 2  Abstract  Adversarial examples are inputs to machine learn- ing models designed by an adversary to cause an incorrect output. So far, adversarial examples have been studied most extensively in the image domain. In this domain, adversarial examples can be constructed by imperceptibly modifying images to cause misclassiﬁcation
GMNN: Graph Markov Neural Networks  Meng Qu 1 2 Yoshua Bengio 1 2 3 Jian Tang 1 3 4  Abstract  This paper studies semi-supervised object classiﬁ- cation in relational data, which is a fundamental problem in relational data modeling. The prob- lem has been extensively studied in the literature of both statistical relational learning (e.g. rela- tional Markov networks) and graph neural net- works (e.g. graph convolutional networks). Statis- tical relational learning methods can effectively model t
Nonlinear Distributional Gradient Temporal-Difference Learning  Chao Qu 1 Shie Mannor 2 Huan Xu 3 4  Abstract  We devise a distributional variant of gradient temporal-difference (TD) learning. Distributional reinforcement learning has been demonstrated to outperform the regular one in the recent study (Bellemare et al., 2017a). In the policy evaluation setting, we design two new algorithms called dis- tributional GTD2 and distributional TDC using the Cram´er distance on the distributional versio
Learning to Collaborate in Markov Decision Processes  Goran Radanovic 1 Rati Devidze 2 David C. Parkes 1 Adish Singla 2  Abstract  novel  design  O(cid:16)  We consider a two-agent MDP framework where agents repeatedly solve a task in a collaborative setting. We study the problem of designing a learning algorithm for the ﬁrst agent (A1) that facilitates successful collaboration even in cases when the second agent (A2) is adapting its pol- icy in an unknown way. The key challenge in our setting i
Meta-Learning Neural Bloom Filters  Jack W Rae 1 2 Sergey Bartunov 1 Timothy P Lillicrap 1 2  Abstract  There has been a recent trend in training neural networks to replace data structures that have been crafted by hand, with an aim for faster execution, better accuracy, or greater compression. In this setting, a neural data structure is instantiated by training a network over many epochs of its inputs until convergence. In applications where inputs arrive at high throughput, or are ephemeral, t
Direct Uncertainty Prediction for Medical Second Opinions  Maithra Raghu * 1 2 Katy Blumer * 2 Rory Sayres 2 Ziad Obermeyer 3 Robert Kleinberg 1  Sendhil Mullainathan 4 Jon Kleinberg 1  Abstract  The issue of disagreements amongst human ex- perts is a ubiquitous one in both machine learning and medicine. In medicine, this often corresponds to doctor disagreements on a patient diagnosis. In this work, we show that machine learning mod- els can be trained to give uncertainty scores to data instanc
Game Theoretic Optimization via Gradient-based Nikaido-Isoda Function  Arvind U. Raghunathan 1 Anoop Cherian 1 Devesh K. Jha 1  Abstract  Computing Nash equilibrium (NE) of multi- player games has witnessed renewed interest due to recent advances in generative adversarial net- works. However, computing equilibrium efﬁ- ciently is challenging. To this end, we introduce the Gradient-based Nikaido-Isoda (GNI) func- tion which serves: (i) as a merit function, vanish- ing only at the ﬁrst-order stati
Look Ma, No Latent Variables: Accurate Cutset Networks via Compilation  Tahrima Rahman * 1 Shasha Jin * 1 Vibhav Gogate 1  Abstract  Tractable probabilistic models obviate the need for unreliable approximate inference approaches and as a result often yield accurate query an- swers in practice. However, most tractable models that achieve state-of-the-art generalization perfor- mance (measured using test set likelihood score) use latent variables. Such models admit poly- time marginal (MAR) infere
Does Data Augmentation Lead to Positive Margin?  Shashank Rajput * 1 Zhili Feng * 1 Zachary Charles 2 Po-Ling Loh 3 Dimitris Papailiopoulos 2  Abstract  Data augmentation (DA) is commonly used dur- ing model training, as it signiﬁcantly improves test error and model robustness. DA artiﬁcially expands the training set by applying random noise, rotations, crops, or even adversarial perturbations to the input data. Although DA is widely used, its capacity to provably improve robustness is not fully
Efﬁcient Off-Policy Meta-Reinforcement Learning via  Probabilistic Context Variables  Kate Rakelly 1 * Aurick Zhou 1 * Deirdre Quillen 1 Chelsea Finn 1 Sergey Levine 1  Abstract  Deep reinforcement learning algorithms require large amounts of experience to learn an individual task. While meta-reinforcement learning (meta- RL) algorithms can enable agents to learn new skills from small amounts of experience, several major challenges preclude their practicality. Cur- rent methods rely heavily on o
Screening Rules for Lasso with Non-Convex Sparse Regularizers  Alain Rakotomamonjy 1 2 Gilles Gasso 1 Joseph Salmon 3  Abstract  Leveraging on the convexity of the Lasso prob- lem, screening rules help in accelerating solvers by discarding irrelevant variables, during the op- timization process. However, because they pro- vide better theoretical guarantees in identifying relevant variables, several non-convex regulariz- ers for the Lasso have been proposed in the lit- erature. This work is the ﬁ
Topological Data Analysis of Decision Boundaries  with Application to Model Selection  Karthikeyan Natesan Ramamurthy 1 Kush R. Varshney 1 Krishnan Mody 1 2  Abstract  We propose the labeled ˇCech complex, the plain labeled Vietoris-Rips complex, and the locally scaled labeled Vietoris-Rips complex to perform persistent homology inference of decision bound- aries in classiﬁcation tasks. We provide theoreti- cal conditions and analysis for recovering the ho- mology of a decision boundary from sam
HyperGAN: A Generative Model for Diverse, Performant Neural Networks  Neale Ratzlaff 1 Li Fuxin 1  Abstract  Standard neural networks are often overconﬁdent when presented with data outside the training distribution. We introduce HyperGAN, a new generative model for learning a distribution of neural network parameters. HyperGAN does not require restrictive assumptions on priors, and net- works sampled from it can be used to quickly create very large and diverse ensembles. Hyper- GAN employs a no
Efﬁcient On-Device Models using Neural Projections  Sujith Ravi 1  Abstract  Many applications involving visual and language understanding can be effectively solved using deep neural networks. Even though these tech- niques achieve state-of-the-art results, it is very challenging to apply them on devices with limited memory and computational capacity such as mo- bile phones, smart watches and IoT. We propose a neural projection approach for training compact on-device neural networks. We introduc
A Block Coordinate Descent Proximal Method  for Simultaneous Filtering and Parameter Estimation  Ramin Raziperchikolaei 1 2 Harish S. Bhat 3 4  Abstract  We propose and analyze a block coordinate de- scent proximal algorithm (BCD-prox) for simulta- neous ﬁltering and parameter estimation of ODE models. As we show on ODE systems with up to d = 40 dimensions, as compared to state-of- the-art methods, BCD-prox exhibits increased ro- bustness (to noise, parameter initialization, and hyperparameters)
Do ImageNet Classiﬁers Generalize to ImageNet?  Benjamin Recht⇤ 1 Rebecca Roelofs 1 Ludwig Schmidt 1 Vaishaal Shankar 1  Abstract  We build new test sets for the CIFAR-10 and Ima- geNet datasets. Both benchmarks have been the focus of intense research for almost a decade, rais- ing the danger of overﬁtting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classiﬁcation models generalize to new data. We evaluate a broad 
Fast Rates for a kNN Classiﬁer Robust to Unknown Asymmetric Label Noise  Henry W. J. Reeve 1 Ata Kab´an 1  Abstract  We consider classiﬁcation in the presence of class- dependent asymmetric label noise with unknown noise probabilities. In this setting, identiﬁability conditions are known, but additional assumptions were shown to be required for ﬁnite sample rates, and so far only the parametric rate has been ob- tained. Assuming these identiﬁability conditions, together with a measure-smoothness
Almost Unsupervised Text to Speech and Automatic Speech Recognition  Yi Ren * 1 Xu Tan * 2 Tao Qin 2 Sheng Zhao 3 Zhou Zhao 1 Tie-Yan Liu 2  Abstract  Text to speech (TTS) and automatic speech recog- nition (ASR) are two dual tasks in speech pro- cessing and both achieve impressive performance thanks to the recent advance in deep learning and large amount of aligned speech and text data. However, the lack of aligned data poses a ma- jor practical problem for TTS and ASR on low- resource language
Adaptive Antithetic Sampling for Variance Reduction  Hongyu Ren * 1 Shengjia Zhao * 1 Stefano Ermon 1  Abstract  Variance reduction is crucial in stochastic esti- mation and optimization problems. Antithetic sampling reduces the variance of a Monte Carlo estimator by drawing correlated, rather than in- dependent, samples. However, designing an ef- fective correlation structure is challenging and application speciﬁc, thus limiting the practical applicability of these methods. In this paper, we pr
Adversarial Online Learning with noise  Alon Resler 1 Yishay Mansour 1 2  Abstract  We present and study models of adversarial on- line learning where the feedback observed by the learner is noisy, and the feedback is either full information feedback or bandit feedback. Specif- ically, we consider binary losses xored with the noise, which is a Bernoulli random variable. We consider both a constant noise rate and a vari- able noise rate. Our main results are tight regret bounds for learning with 
A Polynomial Time MCMC Method for  Sampling from Continuous Determinantal Point Processes  Alireza Rezaei 1 Shayan Oveis Gharan 1  Abstract  We study the Gibbs sampling algorithm for dis- crete and continuous k-determinantal point pro- cesses. We show that in both cases, the spectral gap of the chain is bounded by a polynomial of k and it is independent of the size of the domain. As an immediate corollary, we obtain sublinear time algorithms for sampling from discrete k-DPPs given access to poly
A Persistent Weisfeiler–Lehman Procedure for Graph Classiﬁcation  Bastian Rieck * 1 Christian Bock * 1 Karsten Borgwardt 1  Abstract  The Weisfeiler–Lehman graph kernel exhibits competitive performance in many graph classiﬁ- cation tasks. However, its subtree features are not able to capture connected components and cycles, topological features known for character- ising graphs. To extract such features, we lever- age propagated node label information and trans- form unweighted graphs into metri
Efﬁcient learning of smooth probability functions from Bernoulli tests  with guarantees  Paul Rolland 1 Ali Kavis 1 Alex Immer 1 Adish Singla 2 Volkan Cevher 1  Abstract  We study the fundamental problem of learning an unknown, smooth probability function via point- wise Bernoulli tests. We provide a scalable algo- rithm for efﬁciently solving this problem with rig- orous guarantees. In particular, we prove the con- vergence rate of our posterior update rule to the true probability function in L
Separating value functions across time-scales  Joshua Romoff * 1 2 Peter Henderson * 3 Ahmed Touati 4 2 Emma Brunskill 3 Joelle Pineau 1 2 Yann Ollivier 2  Abstract  In many ﬁnite horizon episodic reinforcement learning (RL) settings, it is desirable to optimize for the undiscounted return – in settings like Atari, for instance, the goal is to collect the most points while staying alive in the long run. Yet, it may be difﬁcult (or even intractable) mathematically to learn with this target. As su
Online Convex Optimization in Adversarial Markov Decision Processes  Aviv Rosenberg 1 Yishay Mansour 1 2  Abstract  We consider online learning in episodic loop- free Markov decision processes (MDPs), where the loss function can change arbitrarily between episodes, and the transition function is not known  to the learner. We show ˜O(L|X|(cid:112)|A|T ) regret  bound, where T is the number of episodes, X is the state space, A is the action space, and L is the length of each episode. Our online al
Good Initializations of Variational Bayes for Deep Models  Simone Rossi 1 Pietro Michiardi 1 Maurizio Filippone 1  Abstract  Stochastic variational inference is an established way to carry out approximate Bayesian inference for deep models ﬂexibly and at scale. While there have been effective proposals for good initializa- tions for loss minimization in deep learning, far less attention has been devoted to the issue of initialization of stochastic variational inference. We address this by propos
The Odds are Odd:  A Statistical Test for Detecting Adversarial Examples  Kevin Roth * 1 Yannic Kilcher * 1 Thomas Hofmann 1  Abstract  We investigate conditions under which test statis- tics exist that can reliably detect examples, which have been adversarially manipulated in a white- box attack. These statistics can be easily com- puted and calibrated by randomly corrupting in- puts. They exploit certain anomalies that adversar- ial attacks introduce, in particular if they follow the paradigm 
Global convergence of neuron birth-death dynamics  Grant M. Rotskoff 1 Samy Jelassi 2 3 Joan Bruna 1 2 Eric Vanden-Eijnden 1  Abstract  Neural networks with a large number of units ad- mit a mean-ﬁeld description, which has recently served as a theoretical explanation for the favor- able training properties of “overparameterized” models. In this regime, gradient descent obeys a deterministic partial differential equation (PDE) that converges to a globally optimal solution for networks with a sin
Iterative Linearized Control: Stable Algorithms and Complexity Guarantees  Vincent Roulet 1 Siddhartha Srinivasa 2 Dmitriy Drusvyatskiy 3 Zaid Harchaoui 1  Abstract  We examine popular gradient-based algorithms for nonlinear control in the light of the mod- ern complexity analysis of ﬁrst-order optimiza- tion algorithms. The examination reveals that the complexity bounds can be clearly stated in terms of calls to a computational oracle related to dynamic programming and implementable by gradient
Statistics and Samples in Distributional Reinforcement Learning  Mark Rowland 1 Robert Dadashi 2 Saurabh Kumar 2 R´emi Munos 1 Marc G. Bellemare 2 Will Dabney 1  Abstract  We present a unifying framework for designing and analysing distributional reinforcement learn- ing (DRL) algorithms in terms of recursively es- timating statistics of the return distribution. Our key insight is that DRL algorithms can be de- composed as the combination of some statistical estimator and a method for imputing a
A Contrastive Divergence for Combining Variational Inference and MCMC  Francisco J. R. Ruiz 1 2 Michalis K. Titsias 3  Abstract  We develop a method to combine Markov chain Monte Carlo (MCMC) and variational inference (VI), leveraging the advantages of both inference approaches. Speciﬁcally, we improve the varia- tional distribution by running a few MCMC steps. To make inference tractable, we introduce the vari- ational contrastive divergence (VCD), a new diver- gence that replaces the standard 
Plug-and-Play Methods Provably Converge with Properly Trained Denoisers  Ernest K. Ryu 1 Jialin Liu 1 Sicheng Wang 2 Xiaohan Chen 2 Zhangyang Wang 2 Wotao Yin 1  Abstract  Plug-and-play (PnP) is a non-convex framework that integrates modern denoising priors, such as BM3D or deep learning-based denoisers, into ADMM or other proximal algorithms. An ad- vantage of PnP is that one can use pre-trained denoisers when there is not sufﬁcient data for end-to-end training. Although PnP has been re- cently
White-box vs Black-box: Bayes Optimal Strategies for Membership Inference  Alexandre Sablayrolles 1 2 Matthijs Douze 2 Yann Ollivier 2 Cordelia Schmid 1 Herv´e J´egou 2  Abstract  Membership inference determines, given a sam- ple and trained parameters of a machine learn- ing model, whether the sample was part of the training set. In this paper, we derive the optimal strategy for membership inference with a few as- sumptions on the distribution of the parameters. We show that optimal attacks onl
An Optimal Private Stochastic-MAB Algorithm  Based on an Optimal Private Stopping Rule  Touqir Sajed * 1 Or Sheffet * 1  Abstract  We present a provably optimal differentially pri- vate algorithm for the stochastic multi-arm ban- dit problem, as opposed to the private analogue of the UCB-algorithm (Mishra and Thakurta, 2015; Tossou and Dimitrakakis, 2016) which doesn’t meet the recently discovered lower- bound of Ω (K log(T )/(cid:15)) (Shariff and Sheffet, 2018). Our construction is based on a 
Deep Gaussian Processes with Importance-Weighted Variational Inference  Hugh Salimbeni 1 2 Vincent Dutordoir 2 James Hensman 2 Marc Peter Deisenroth 1 2  Abstract  Deep Gaussian processes (DGPs) can model com- plex marginal densities as well as complex map- pings. Non-Gaussian marginals are essential for modelling real-world data, and can be generated from the DGP by incorporating uncorrelated vari- ables to the model. Previous work on DGP mod- els has introduced noise additively and used vari- 
Multivariate Submodular Optimization  Richard Santiago * 1 F. Bruce Shepherd * 2  Abstract  Submodular functions have found a wealth of new applications in data science and machine learn- ing models in recent years. This has been coupled with many algorithmic advances in the area of sub- modular optimization: (SO) min / max f (S) : S ∈ F, where F is a given family of feasible sets over a ground set V and f : 2V → R is sub- modular. Our focus is on a more general class of multivariate submodular 
Near optimal ﬁnite time identiﬁcation of arbitrary linear dynamical systems  Tuhin Sarkar 1 Alexander Rakhlin 2  Abstract  We derive ﬁnite time error bounds for estimating general linear time-invariant (LTI) systems from a single observed trajectory using the method of least squares. We provide the ﬁrst analysis of the general case when eigenvalues of the LTI system are arbitrarily distributed in three regimes: stable, marginally stable, and explosive. Our analysis yields sharp upper bounds for 
Breaking Inter-Layer Co-Adaptation by Classiﬁer Anonymization  Ikuro Sato 1 Kohta Ishikawa 1 Guoqing Liu 1 Masayuki Tanaka 2  Abstract  This study addresses an issue of co-adaptation between a feature extractor and a classiﬁer in a neural network. A na¨ıve joint optimization of a feature extractor and a classiﬁer often brings situations in which an excessively com- plex feature distribution adapted to a very spe- ciﬁc classiﬁer degrades the test performance. We introduce a method called Feature-
A Theoretical Analysis of Contrastive Unsupervised Representation Learning  Sanjeev Arora 1 2 Hrishikesh Khandeparkar 1 Mikhail Khodak 3 Orestis Plevrakis 1 Nikunj Saunshi 1  Abstract  Recent empirical works have successfully used unlabeled data to learn feature representations that are broadly useful in downstream classiﬁca- tion tasks. Several of these methods are remi- niscent of the well-known word2vec embedding algorithm: leveraging availability of pairs of se- mantically “similar” data poi
Locally Private Bayesian Inference for Count Models  Aaron Schein 1 Zhiwei Steven Wu 2 Alexandra Schoﬁeld 3 Mingyuan Zhou 4 Hanna Wallach 5  Abstract  We present a general and modular method for privacy-preserving Bayesian inference for Pois- son factorization, a broad class of models that includes some of the most widely used models in the social sciences. Our method satisﬁes limited- precision local privacy, a generalization of local differential privacy that we introduce to formulate appropri
Weakly-Supervised Temporal Localization via Occurrence Count Learning  Julien Schroeter 1 Kirill Sidorov 1 David Marshall 1  Abstract  We propose a novel model for temporal detection and localization which allows the training of deep neural networks using only counts of event occur- rences as training labels. This powerful weakly- supervised framework alleviates the burden of the imprecise and time-consuming process of anno- tating event locations in temporal data. Unlike existing methods, in wh
Discovering Context Effects from Raw Choice Data  Arjun Seshadri 1 Alexander Peysakhovich 2 Johan Ugander 1  Abstract  Many applications in preference learning assume that decisions come from the maximization of a stable utility function. Yet a large experimental literature shows that individual choices and judge- ments can be affected by “irrelevant” aspects of the context in which they are made. An important class of such contexts is the composition of the choice set. In this work, our goal is
On the Feasibility of Learning, Rather than Assuming,  Human Biases for Reward Inference  Rohin Shah 1 Noah Gundotra 1 Pieter Abbeel 1 Anca D. Dragan 1  Abstract  Our goal is for agents to optimize the right re- ward function, despite how difﬁcult it is for us to specify what that is. Inverse Reinforcement Learning (IRL) enables us to infer reward func- tions from demonstrations, but it usually assumes that the expert is noisily optimal. Real people, on the other hand, often have systematic bias
Exploration Conscious Reinforcement Learning Revisited  Lior Shani * 1 Yonathan Efroni * 1 Shie Mannor 1  Abstract  The Exploration-Exploitation tradeoff arises in Reinforcement Learning when one cannot tell if a policy is optimal. Then, there is a constant need to explore new actions instead of exploiting past ex- perience. In practice, it is common to resolve the tradeoff by using a ﬁxed exploration mechanism, such as (cid:15)-greedy exploration or by adding Gaus- sian noise, while still tryin
Compressed Factorization: Fast and Accurate  Low-Rank Factorization of Compressively-Sensed Data  Vatsal Sharan * 1 Kai Sheng Tai * 1 Peter Bailis 1 Gregory Valiant 1  Abstract  What learning algorithms can be run directly on compressively-sensed data? In this work, we con- sider the question of accurately and efﬁciently computing low-rank matrix or tensor factoriza- tions given data compressed via random projec- tions. We examine the approach of ﬁrst perform- ing factorization in the compressed
Conditional Independence in Testing Bayesian Networks  Yujia Shen 1 Haiying Huang 1 Arthur Choi 1 Adnan Darwiche 1  Abstract  Testing Bayesian Networks (TBNs) were intro- duced recently to represent a set of distributions, one of which is selected based on the given evi- dence and used for reasoning. TBNs are more ex- pressive than classical Bayesian Networks (BNs): Marginal queries correspond to multi-linear func- tions in BNs and to piecewise multi-linear func- tions in TBNs. Moreover, TBN que
Learning to Clear the Market  Weiran Shen 1 S´ebastien Lahaie 2 Renato Paes Leme 2  Abstract  The problem of market clearing is to set a price for an item such that quantity demanded equals quan- tity supplied. In this work, we cast the problem of predicting clearing prices into a learning frame- work and use the resulting models to perform rev- enue optimization in auctions and markets with contextual information. The economic intuition behind market clearing allows us to obtain ﬁne- grained co
Mixture Models for Diverse Machine Translation: Tricks of the Trade  Tianxiao Shen * 1 Myle Ott * 2 Michael Auli 2 Marc’Aurelio Ranzato 2  Abstract  Mixture models trained via EM are among the simplest, most widely used and well understood latent variable models in the machine learning literature. Surprisingly, these models have been hardly explored in text generation applications such as machine translation. In principle, they pro- vide a latent variable to control generation and pro- duce a di
Hessian Aided Policy Gradient  Zebang Shen 1 Hamed Hassani 2 Chao Mi 1 Hui Qian 1 Alejandro Ribeiro 2  Abstract  Reducing the variance of estimators for policy gradient has long been the focus of reinforcement learning research. While classic algorithms like REINFORCE ﬁnd an (cid:15)-approximate ﬁrst-order stationary point in O(1/(cid:15)4) random trajectory simulations, no provable improvement on the complexity has been made so far. This paper presents a Hessian aided policy gradient method wit
Learning with Bad Training Data via Iterative Trimmed Loss Minimization  Yanyao Shen 1 Sujay Sanghavi 1  Abstract  In this paper, we study a simple and generic frame- work to tackle the problem of learning model pa- rameters when a fraction of the training samples are corrupted. We ﬁrst make a simple observation: in a variety of such settings, the evolution of train- ing accuracy (as a function of training epochs) is different for clean and bad samples. Based on this we propose to iteratively mi
Replica Conditional Sequential Monte Carlo  Alexander Y. Shestopaloff 1 2 Arnaud Doucet 3 2  Abstract  density given by  We propose a Markov chain Monte Carlo (MCMC) scheme to perform state inference in non-linear non-Gaussian state-space models. Cur- rent state-of-the-art methods to address this prob- lem rely on particle MCMC techniques and its variants, such as the iterated conditional Sequen- tial Monte Carlo (cSMC) scheme, which uses a Sequential Monte Carlo (SMC) type proposal within MCMC.
Scalable Training of Inference Networks for Gaussian-Process Models  Jiaxin Shi 1 Mohammad Emtiyaz Khan 2 Jun Zhu 1  Abstract  Inference in Gaussian process (GP) models is computationally challenging for large data, and often difﬁcult to approximate with a small number of inducing points. We explore an alternative ap- proximation that employs stochastic inference net- works for a ﬂexible inference. Unfortunately, for such networks, minibatch training is difﬁcult to be able to learn meaningful co
Fast Direct Search in an Optimally Compressed Continuous Target Space for  Efﬁcient Multi-Label Active Learning  Weishi Shi 1 Qi Yu 1  Abstract  Active learning for multi-label classiﬁcation poses fundamental challenges given the complex label correlations and a potentially large and sparse la- bel space. We propose a novel CS-BPCA process that integrates compressed sensing and Bayesian principal component analysis to perform a two- level label transformation, resulting in an opti- mally compres
Model-Based Active Exploration  Pranav Shyam 1 Wojciech Ja´skowski 1 Faustino Gomez 1  Abstract  Efﬁcient exploration is an unsolved problem in Reinforcement Learning which is usually ad- dressed by reactively rewarding the agent for for- tuitously encountering novel situations. This pa- per introduces an efﬁcient active exploration algo- rithm, Model-Based Active eXploration (MAX), which uses an ensemble of forward models to plan to observe novel events. This is carried out by optimizing agent 
Rehashing Kernel Evaluation in High Dimensions  Paris Siminelakis * 1 Kexin Rong * 1 Peter Bailis 1 Moses Charikar 1 Philip Levis 1  Abstract  Kernel methods are effective but do not scale well to large scale data, especially in high dimensions where the geometric data structures used to accel- erate kernel evaluation suffer from the curse of dimensionality. Recent theoretical advances have proposed fast kernel evaluation algorithms lever- aging hashing techniques with worst-case asymp- totic im
Revisiting Precision and Recall Deﬁnition for Generative Model Evaluation  Lo¨ıc Simon 1 Ryan Webster 1 Julien Rabin 1  Abstract  In this article we revisit the deﬁnition of Precision- Recall (PR) curves for generative models pro- posed by (Sajjadi et al., 2018). Rather than pro- viding a scalar for generative quality, PR curves distinguish mode-collapse (poor recall) and bad quality (poor precision). We ﬁrst generalize their formulation to arbitrary measures, hence remov- ing any restriction to
First-order Adversarial Vulnerability of Neural Networks and Input Dimension  Carl-Johann Simon-Gabriel 1 2 Yann Ollivier 2 Bernhard Schölkopf 1 Léon Bottou 2 David Lopez-Paz 2  Abstract  Over the past few years, neural networks were proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gra- dients of the training objective when viewed as a function of the inputs
Reﬁned Complexity of PCA with Outliers  Fedor Fomin * 1 Petr Golovach * 1 Fahad Panolan * 1 Kirill Simonov * 1  Abstract  low-rank approximation of data matrix M by solving  Principal component analysis (PCA) is one of the most fundamental procedures in exploratory data analysis and is the basic step in applications rang- ing from quantitative ﬁnance and bioinformatics to image analysis and neuroscience. However, it is well-documented that the applicability of PCA in many real scenarios could be
A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks  Umut S¸ims¸ekli 1 Levent Sagun 2 Mert G¨urb¨uzbalaban 3  Abstract  The gradient noise (GN) in the stochastic gra- dient descent (SGD) algorithm is often consid- ered to be Gaussian in the large data regime by assuming that the classical central limit theo- rem (CLT) kicks in. This assumption is often made for mathematical convenience, since it en- ables SGD to be analyzed as a stochastic differ- ential equation (SDE) dri
Non-Parametric Priors For Generative Adversarial Networks  Rajhans Singh 1 Pavan Turaga 1 2 Suren Jayasuriya 1 2 Ravi Garg 3 Martin W. Braun 3  Abstract  The advent of generative adversarial networks (GAN) has enabled new capabilities in synthe- sis, interpolation, and data augmentation hereto- fore considered very challenging. However, one of the common assumptions in most GAN archi- tectures is the assumption of simple parametric latent-space distributions. While easy to imple- ment, a simple 
Understanding Impacts of High-Order Loss Approximations and Features in  Deep Learning Interpretation  Sahil Singla 1 Eric Wallace 1 Shi Feng 1 Soheil Feizi 1  Abstract  Current saliency map interpretations for neural networks generally rely on two key assumptions. First, they use ﬁrst-order approximations of the loss function, neglecting higher-order terms such as the loss curvature. Second, they evaluate each feature’s importance in isolation, ignoring feature interdependencies. This work stud
kernelPSI: a Post-Selection Inference Framework for Nonlinear Variable  Selection  Lotﬁ Slim 1 2 Cl´ement Chatelain 1 Chlo´e-Agathe Azencott 2 3 Jean-Philippe Vert 2 4  Abstract  Model selection is an essential task for many ap- plications in scientiﬁc discovery. The most com- mon approaches rely on univariate linear mea- sures of association between each feature and the outcome. Such classical selection procedures fail to take into account nonlinear effects and in- teractions between features. 
GEOMetrics: Exploiting Geometric Structure for Graph-Encoded Objects  Edward J. Smith 1 Scott Fujimoto 1 2 Adriana Romero 3 1 David Meger 1  Abstract  Mesh models are a promising approach for en- coding the structure of 3D objects. Current mesh reconstruction systems predict uniformly distributed vertex locations of a predetermined graph through a series of graph convolutions, lead- ing to compromises with respect to performance or resolution. In this paper, we argue that the graph representatio
The Evolved Transformer  David R. So 1 Chen Liang 1 Quoc V. Le 1  Abstract  Recent works have highlighted the strength of the Transformer architecture on sequence tasks while, at the same time, neural architecture search (NAS) has begun to outperform human-designed models. Our goal is to apply NAS to search for a better alternative to the Transformer. We ﬁrst construct a large search space inspired by the re- cent advances in feed-forward sequence models and then run evolutionary architecture se
QTRAN: Learning to Factorize with Transformation for  Cooperative Multi-Agent Reinforcement learning  Kyunghwan Son 1 Daewoo Kim 1 Wan Ju Kang 1 David Hostallero 1 Yung Yi 1  Abstract  We explore value-based solutions for multi-agent reinforcement learning (MARL) tasks in the cen- tralized training with decentralized execution (CTDE) regime popularized recently. However, VDN and QMIX are representative examples that use the idea of factorization of the joint action- value function into individua
Distribution Calibration for Regression  Hao Song 1 Tom Diethe 2 Meelis Kull 3 Peter Flach 1 4  Abstract  We are concerned with obtaining well-calibrated output distributions from regression models. Such distributions allow us to quantify the uncertainty that the model has regarding the predicted tar- get value. We introduce the novel concept of distribution calibration, and demonstrate its ad- vantages over the existing deﬁnition of quantile calibration. We further propose a post-hoc ap- proach
SELFIE: Refurbishing Unclean Samples for Robust Deep Learning  Hwanjun Song 1 Minseok Kim 1 Jae-Gil Lee 1  Abstract  Owing to the extremely high expressive power of deep neural networks, their side effect is to totally memorize training data even when the la- bels are extremely noisy. To overcome overﬁtting on the noisy labels, we propose a novel robust training method called SELFIE. Our key idea is to selectively refurbish and exploit unclean sam- ples that can be corrected with high precision,
Revisiting the Softmax Bellman Operator:  New Beneﬁts and New Perspective  Zhao Song 1 * Ronald E. Parr 1 Lawrence Carin 1  Abstract  The impact of softmax on the value function itself in reinforcement learning (RL) is often viewed as problematic because it leads to sub-optimal value (or Q) functions and interferes with the contrac- tion properties of the Bellman operator. Surpris- ingly, despite these concerns, and independent of its effect on exploration, the softmax Bellman operator when comb
MASS: Masked Sequence to Sequence Pre-training for Language Generation  Kaitao Song * 1 Xu Tan * 2 Tao Qin 2 Jianfeng Lu 1 Tie-Yan Liu 2  Abstract  Pre-training and ﬁne-tuning, e.g., BERT (De- vlin et al., 2018), have achieved great success in language understanding by transferring knowl- edge from rich-resource pre-training task to the low/zero-resource downstream tasks. Inspired by the success of BERT, we propose MAsked Sequence to Sequence pre-training (MASS) for encoder-decoder based languag
Dual Entangled Polynomial Code: Three-Dimensional Coding  for Distributed Matrix Multiplication  Pedro Soto 1 Jun Li 1 Xiaodi Fan 1  Abstract  Matrix multiplication is a fundamental building block in various machine learning algorithms. When the matrix comes from a large dataset, the multiplication can be split into multiple tasks which calculate the multiplication of submatri- ces on different nodes. As some nodes may be stragglers, coding schemes have been proposed to tolerate stragglers in su
Compressing Gradient Optimizers via Count-Sketches  Ryan Spring 1 * Anastasios Kyrillidis 1 Vijai Mohan 2 Anshumali Shrivastava 1 2  Abstract  Many popular ﬁrst-order optimization methods accelerate the convergence rate of deep learning models. However, these algorithms require aux- iliary variables, which cost additional memory proportional to the number of parameters in the model. The problem is becoming more severe as models grow larger to learn from complex, large- scale datasets. Our propos
Escaping Saddle Points with Adaptive Gradient Methods  Matthew Staib 1 2 Sashank Reddi 3 Satyen Kale 3 Sanjiv Kumar 3 Suvrit Sra 1  Abstract  Adagrad updates the parameters in the following manner:  Adaptive methods such as Adam and RMSProp are widely used in deep learning but are not well understood. In this paper, we seek a crisp, clean and precise characterization of their behavior in nonconvex settings. To this end, we ﬁrst provide a novel view of adaptive methods as preconditioned SGD, wher
Faster Attend-Infer-Repeat with Tractable Probabilistic Models  Karl Stelzner 1 Robert Peharz 2 Kristian Kersting 1 3  Abstract  The recent Attend-Infer-Repeat (AIR) framework marks a milestone in structured probabilistic mod- eling, as it tackles the challenging problem of unsupervised scene understanding via Bayesian inference. AIR expresses the composition of vi- sual scenes from individual objects, and uses vari- ational autoencoders to model the appearance of those objects. However, inferen
Insertion Transformer:  Flexible Sequence Generation via Insertion Operations  Mitchell Stern 1 2 William Chan 1 Jamie Kiros 1 Jakob Uszkoreit 1  Abstract  We present the Insertion Transformer, an itera- tive, partially autoregressive model for sequence generation based on insertion operations. Unlike typical autoregressive models which rely on a ﬁxed, often left-to-right ordering of the output, our approach accommodates arbitrary orderings by allowing for tokens to be inserted anywhere in the s
BERT and PALs: Projected Attention Layers for  Efﬁcient Adaptation in Multi-Task Learning  Asa Cooper Stickland 1 Iain Murray 1  Abstract  Multi-task learning shares information between related tasks, sometimes reducing the number of parameters required. State-of-the-art results across multiple natural language understanding tasks in the GLUE benchmark have previously used transfer from a single large task: unsuper- vised pre-training with BERT, where a separate BERT model was ﬁne-tuned for each
Learning Optimal Linear Regularizers  Matthew Streeter 1  Abstract  We present algorithms for efﬁciently learning reg- ularizers that improve generalization. Our ap- proach is based on the insight that regularizers can be viewed as upper bounds on the general- ization gap, and that reducing the slack in the bound can improve performance on test data. For a broad class of regularizers, the hyperparameters that give the best upper bound can be computed us- ing linear programming. Under certain Bay
CAB: Continuous Adaptive Blending for Policy Evaluation and Learning  Yi Su * 1 Lequn Wang * 1 Michele Santacatterina 2 Thorsten Joachims 1  Abstract  The ability to perform ofﬂine A/B-testing and off-policy learning using logged contextual ban- dit feedback is highly desirable in a broad range of applications, including recommender systems, search engines, ad placement, and personalized health care. Both ofﬂine A/B-testing and off- policy learning require a counterfactual estimator that evaluat
Learning Distance for Sequences by Learning a Ground Metric  Bing Su 1 Ying Wu 2  Abstract  Learning distances that operate directly on multi- dimensional sequences is challenging because such distances are structural by nature and the vectors in sequences are not independent. Gen- erally, distances for sequences heavily depend on the ground metric between the vectors in se- quences. We propose to learn the distance for se- quences through learning a ground Mahalanobis metric for the vectors in 
Contextual Memory Trees  Wen Sun 1 Alina Beygelzimer 2 Hal Daum´e III 3 John Langford 3 Paul Mineiro 4  Abstract  We design and study a Contextual Memory Tree (CMT), a learning memory controller that inserts new memories into an experience store of un- bounded size. It is designed to efﬁciently query for memories from that store, supporting logarith- mic time insertion and retrieval operations. Hence CMT can be integrated into existing statistical learning algorithms as an augmented memory unit 
Provably Efﬁcient Imitation Learning from Observation Alone  Wen Sun 1 Anirudh Vemula 1 Byron Boots 2 J. Andrew Bagnell 3  Abstract  We study Imitation Learning (IL) from Observa- tions alone (ILFO) in large-scale MDPs. While most IL algorithms rely on an expert to directly provide actions to the learner, in this setting the expert only supplies sequences of observations. We design a new model-free algorithm for ILFO, Forward Adversarial Imitation Learning (FAIL), which learns a sequence of time
Active Learning for Decision-Making from Imbalanced Observational Data  Iiris Sundin 1 Peter Schulam * 2 Eero Siivola * 1 Aki Vehtari 1 Suchi Saria 2 Samuel Kaski 1  Abstract  Machine learning can help personalized decision support by learning models to predict individual treatment effects (ITE). This work studies the re- liability of prediction-based decision-making in a task of deciding which action a to take for a target unit after observing its covariates ˜x and predicted outcomes ˆp(˜y | ˜x
Robustly Disentangled Causal Mechanisms:  Validating Deep Representations for Interventional Robustness  Raphael Suter 1 Dorde Miladinovi´c 1 Bernhard Sch¨olkopf 2 Stefan Bauer 2  Abstract  The ability to learn disentangled representations that split underlying sources of variation in high dimensional, unstructured data is important for data efﬁcient and robust use of neural networks. While various approaches aiming towards this goal have been proposed in recent times, a com- monly accepted deﬁn
Hyperbolic Disk Embeddings for Directed Acyclic Graphs  Ryota Suzuki 1 Ryusuke Takahama 1 Shun Onoda 1  Abstract  Obtaining continuous representations of struc- tural data such as directed acyclic graphs (DAGs) has gained attention in machine learning and arti- ﬁcial intelligence. However, embedding complex DAGs in which both ancestors and descendants of nodes are exponentially increasing is difﬁcult. Tackling in this problem, we develop Disk Embeddings, which is a framework for embed- ding DAGs
Accelerated Flow for Probability Distributions  Amirhossein Taghvaei 1 Prashant G. Mehta 1  Abstract  This paper presents a methodology and numerical algorithms for constructing accelerated gradient ﬂows on the space of probability distributions. In particular, we extend the recent variational formu- lation of accelerated methods in (Wibisono et al., 2016) from vector valued variables to probability distributions. The variational problem is modeled as a mean-ﬁeld optimal control problem. A quan-
Equivariant Transformer Networks  Kai Sheng Tai 1 Peter Bailis 1 Gregory Valiant 1  Abstract  How can prior knowledge on the transforma- tion invariances of a domain be incorporated into the architecture of a neural network? We propose Equivariant Transformers (ETs), a fam- ily of differentiable image-to-image mappings that improve the robustness of models towards pre-deﬁned continuous transformation groups. Through the use of specially-derived canonical co- ordinate systems, ETs incorporate fun
Making Deep Q-learning Methods Robust to Time Discretization  Corentin Tallec 1 L´eonard Blier 1 2 Yann Ollivier 2  Abstract  Despite remarkable successes, Deep Reinforce- ment Learning (DRL) is not robust to hyperparam- eterization, implementation details, or small envi- ronment changes (Henderson et al. 2017, Zhang et al. 2018). Overcoming such sensitivity is key to making DRL applicable to real world problems. In this paper, we identify sensitivity to time dis- cretization in near continuous-
EfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks  Mingxing Tan 1 Quoc V. Le 1  Abstract  Convolutional Neural Networks (ConvNets) are commonly developed at a ﬁxed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we sys- tematically study model scaling and identify that carefully balancing network depth, width, and res- olution can lead to better performance. Based on this observation, we propose a new scaling method th
Hierarchical Decompositional Mixtures of Variational Autoencoders  Ping Liang Tan 1 2 Robert Peharz 1  Abstract  Variational autoencoders (VAEs) have received considerable attention, since they allow us to learn expressive neural density estimators effectively and efﬁciently. However, learning and inference in VAEs is still problematic due to the sensitive interplay between the generative model and the inference network. Since these problems become generally more severe in high dimensions, we pr
Mallows Ranking Models: Maximum Likelihood Estimate and Regeneration  Wenpin Tang 1  Abstract  This paper is concerned with various Mallows ranking models. We study the statistical proper- ties of the MLE of Mallows’ φ model. We also make connections of various Mallows ranking models, encompassing recent progress in math- ematics. Motivated by the inﬁnite top-t ranking model, we propose an algorithm to select the model size t automatically. The key idea relies on the renewal property of such an 
Correlated Variational Auto-Encoders  Da Tang 1 Dawen Liang 2 Tony Jebara 1 2 Nicholas Ruozzi 3  Abstract  Variational Auto-Encoders (VAEs) are capable of learning latent representations for high dimen- sional data. However, due to the i.i.d. assump- tion, VAEs only optimize the singleton variational distributions and fail to account for the correla- tions between data points, which might be crucial for learning latent representations from datasets where a priori we know correlations exist. We p
The Variational Predictive Natural Gradient  Da Tang 1 Rajesh Ranganath 2  Abstract  Variational inference transforms posterior infer- ence into parametric optimization thereby en- abling the use of latent variable models where otherwise impractical. However, variational in- ference can be ﬁnicky when different variational parameters control variables that are strongly cor- related under the model. Traditional natural gradi- ents based on the variational approximation fail to correct for correla
DOUBLESQUEEZE: Parallel Stochastic Gradient Descent with Double-pass  Error-Compensated Compression  Hanlin Tang 1 Xiangru Lian 1 Chen Yu 1 Tong Zhang 2 Ji Liu 3 1  Abstract  A standard approach in large scale machine learn- ing is distributed stochastic gradient training, which requires the computation of aggregated stochastic gradients over multiple nodes on a net- work. Communication is a major bottleneck in such applications, and in recent years, compressed stochastic gradient methods such a
Adaptive Neural Trees  Ryutaro Tanno 1 Kai Arulkumaran 2 Daniel C. Alexander 1 Antonio Criminisi 3 Aditya Nori 3  Abstract  Deep neural networks and decision trees oper- ate on largely separate paradigms; typically, the former performs representation learning with pre- speciﬁed architectures, while the latter is charac- terised by learning hierarchies over pre-speciﬁed features with data-driven architectures. We unite the two via adaptive neural trees (ANTs) that incorporates representation lear
Variational Annealing of GANs: A Langevin Perspective  Chenyang Tao 1 Shuyang Dai 1 Liqun Chen 1 Ke Bai 1 Junya Chen 1 2 Chang Liu 1 3 Ruiyi Zhang 1  Georgiy Bobashev 4 Lawrence Carin 1  Abstract  The generative adversarial network (GAN) has re- ceived considerable attention recently as a model for data synthesis, without an explicit speciﬁca- tion of a likelihood function. There has been commensurate interest in leveraging likelihood estimates to improve GAN training. To enrich the understandin
Predicate Exchange: Inference with Declarative Knowledge  Zenna Tavares 1 Javier Burroni 2 Edgar Minasyan 3 Armando Solar Lezama 1 Rajesh Ranganath 4  Abstract  Programming languages allow us to express com- plex predicates, but existing inference methods are unable to condition probabilistic models on most of them. To support a broader class of predi- cates, we develop an inference procedure called predicate exchange, which softens predicates. A soft predicate quantiﬁes the extent to which val-
The Natural Language of Actions  Guy Tennenholtz 1 Shie Mannor 1  Abstract  We introduce Act2Vec, a general framework for learning context-based action representation for Reinforcement Learning. Representing actions in a vector space help reinforcement learning al- gorithms achieve better performance by grouping similar actions and utilizing relations between different actions. We show how prior knowledge of an environment can be extracted from demon- strations and injected into action vector re
Kernel Normalized Cut: a Theoretical Revisit  Yoshikazu Terada 1 2 Michio Yamamoto 3 2  Abstract  In this paper, we study the theoretical properties of clustering based on the kernel normalized cut. Our ﬁrst contribution is to derive a nonasymptotic upper bound on the expected distortion rate of the kernel normalized cut. From this result, we show that the solution of the kernel normalized cut con- verges to that of the population-level weighted k-means clustering on a certain reproducing ker- n
Action Robust Reinforcement Learning and Applications in Continuous  Control  Chen Tessler * 1 Yonathan Efroni * 1 Shie Mannor 1  Abstract  A policy is said to be robust if it maximizes the re- ward while considering a bad, or even adversarial, model. In this work we formalize two new criteria of robustness to action uncertainty. Speciﬁcally, we consider two scenarios in which the agent at- tempts to perform an action a, and (i) with prob- ability α, an alternative adversarial action ¯a is taken
Concentration Inequalities for Conditional Value at Risk  Philip S. Thomas 1 Erik Learned-Miller 1  Abstract  In this paper we derive new concentration inequal- ities for the conditional value at risk (CVaR) of a random variable, and compare them to the pre- vious state of the art (Brown, 2007). We show analytically that our lower bound is strictly tighter than Brown’s, and empirically that this difference is signiﬁcant. While our upper bound may be looser than Brown’s in some cases, we show em-
Combating Label Noise in Deep Learning Using Abstention  Sunil Thulasidasan 1 2 Tanmoy Bhattacharya 1 Jeffrey Bilmes 2 Gopinath Chennupati 1 Jamaludin Mohd-Yusof 1  Abstract  We introduce a novel method to combat label noise when training deep neural networks for clas- siﬁcation. We propose a loss function that permits abstention during training thereby allowing the DNN to abstain on confusing samples while con- tinuing to learn and improve classiﬁcation perfor- mance on the non-abstained sample
ELF OpenGo: An Analysis and Open Reimplementation of AlphaZero  Yuandong Tian 1 Jerry Ma * 1 Qucheng Gong * 1 Shubho Sengupta * 1 Zhuoyuan Chen 1 James Pinkerton 1  C. Lawrence Zitnick 1  Abstract  The AlphaGo, AlphaGo Zero, and AlphaZero series of algorithms are remarkable demonstra- tions of deep reinforcement learning’s capabili- ties, achieving superhuman performance in the complex game of Go with progressively increas- ing autonomy. However, many obstacles remain in the understanding of and
Random Matrix Improved Covariance Estimation for a Large Class of Metrics  Malik Tiomoko 1 2 Florent Bouchard 3 Guillaume Ginolhac 3 Romain Couillet 2 1  Abstract  Relying on recent advances in statistical estima- tion of covariance distances based on random matrix theory, this article proposes an improved covariance and precision matrix estimation for a wide family of metrics. The method is shown to largely outperform the sample covariance ma- trix estimate and to compete with state-of-the-art 
Transfer of Samples in Policy Search via Multiple Importance Sampling  Andrea Tirinzoni 1 Mattia Salvini 1 Marcello Restelli 1  Abstract  We consider the transfer of experience samples in reinforcement learning. Most of the previous works in this context focused on value-based set- tings, where transferring instances conveniently reduces to the transfer of (s, a, s(cid:48), r) tuples. In this paper, we consider the more complex case of reusing samples in policy search methods, in which the agent
Optimal Transport for structured data with application on graphs  Titouan Vayer 1 Laetitia Chapel 1 R´emi Flamary 2 Romain Tavenard 3 Nicolas Courty 1  Abstract  This work considers the problem of computing distances between structured objects such as undi- rected graphs, seen as probability distributions in a speciﬁc metric space. We consider a new transportation distance (i.e. that minimizes a total cost of transporting probability masses) that unveils the geometric nature of the structured ob
Discovering Latent Covariance Structures for Multiple Time Series  Anh Tong 1 Jaesik Choi 1  Abstract  Analyzing multivariate time series data is impor- tant to predict future events and changes of com- plex systems in ﬁnance, manufacturing, and ad- ministrative decisions. The expressiveness power of Gaussian Process (GP) regression methods has been signiﬁcantly improved by compositional co- variance structures. In this paper, we present a new GP model which naturally handles multiple time serie
Bayesian Generative Active Deep Learning  Toan Tran 1 Thanh-Toan Do 2 Ian Reid 1 Gustavo Carneiro 1  Abstract  Deep learning models have demonstrated out- standing performance in several problems, but their training process tends to require immense amounts of computational and human resources for training and labeling, constraining the types of problems that can be tackled. Therefore, the design of effective training methods that require small labeled training sets is an important research direc
DeepNose: Using artiﬁcial neural networks to represent the space of odorants  Ngoc B. Tran 1 Daniel R. Kepple 1 Sergey A. Shuvaev 1 Alexei A. Koulakov 1  Abstract  The olfactory system employs an ensemble of odorant receptors (ORs) to sense odorants and to derive olfactory percepts. We trained artiﬁcial neural networks to represent the chemical space of odorants and used this representation to predict human olfactory percepts. We hypothesized that ORs may be considered 3D convolutional ﬁlters th
LR-GLM: High-Dimensional Bayesian Inference Using Low-Rank Data  Approximations  Brian L. Trippe 1 Jonathan H. Huggins 2 Raj Agrawal 1 Tamara Broderick 1  Abstract  Due to the ease of modern data collection, applied statisticians often have access to a large set of co- variates that they wish to relate to some observed outcome. Generalized linear models (GLMs) of- fer a particularly interpretable framework for such an analysis. In these high-dimensional problems, the number of covariates is ofte
Learning Hawkes Processes Under Synchronization Noise  William Trouleau 1 Jalal Etesami 2 Matthias Grossglauser 1 Negar Kiyavash 3 4 Patrick Thiran 1  Abstract  Multivariate Hawkes processes (MHP) are widely used in a variety of ﬁelds to model the occurrence of discrete events. Prior work on learning MHPs has only focused on inference in the presence of perfect traces without noise. We address the problem of learning the causal structure of MHPs when observations are subject to an unknown de- la
Homomorphic sensing  Manolis C. Tsakiris 1 Liangzu Peng 1  Abstract  A recent line of research termed unlabeled sens- ing and shufﬂed linear regression has been ex- ploring under great generality the recovery of signals from subsampled and permuted measure- ments; a challenging problem in diverse ﬁelds of data science and machine learning. In this pa- per we introduce an abstraction of this problem which we call homomorphic sensing. Given a linear subspace and a ﬁnite set of linear trans- format
Metropolis-Hastings Generative Adversarial Networks  Ryan Turner 1 Jane Hung 1 Eric Frank 1 Yunus Saatci 1 Jason Yosinski 1  Abstract  We introduce the Metropolis-Hastings generative adversarial network (MH-GAN), which combines aspects of Markov chain Monte Carlo and GANs. The MH-GAN draws samples from the distribu- tion implicitly deﬁned by a GAN’s discriminator- generator pair, as opposed to standard GANs which draw samples from the distribution deﬁned only by the generator. It uses the discri
Distributed, Egocentric Representations of Graphs for  Detecting Critical Structures  Ruo-Chun Tzeng 1 Shan-Hung Wu 2  Alkane  Alcohol  ...  ...  Abstract  We study the problem of detecting critical struc- tures using a graph embedding model. Existing graph embedding models lack the ability to pre- cisely detect critical structures that are speciﬁc to a task at the global scale. In this paper, we pro- pose a novel graph embedding model, called the Ego-CNNs, that employs the ego-convolutions conv
Fairness without Harm:  Decoupled Classiﬁers with Preference Guarantees  Berk Ustun 1 Yang Liu 2 David C. Parkes 1  Abstract  In domains such as medicine, it can be acceptable for machine learning models to include sensitive attributes such as gender and ethnicity. In this work, we argue that when there is this kind of treatment disparity then it should be in the best interest of each group. Drawing on ethical prin- ciples such as beneﬁcence (“do the best”) and non-maleﬁcence (“do no harm”), we 
Large-Scale Sparse Kernel Canonical Correlation Analysis  Viivi Uurtio 1 2 Sahely Bhadra 3 Juho Rousu 1 2  Abstract  This paper presents gradKCCA, a large-scale sparse non-linear canonical correlation method. Like Kernel Canonical Correlation Analysis (KCCA), our method ﬁnds non-linear relations through kernel functions, but it does not rely on a kernel matrix, a known bottleneck for scaling up kernel methods. gradKCCA corresponds to solving KCCA with the additional constraint that the canonical
Characterization of Convex Objective Functions and Optimal Expected  Convergence Rates for SGD  Marten van Dijk 1 Lam M. Nguyen 2 Phuong Ha Nguyen 1 Dzung T. Phan 2  Abstract  Algorithm 1 Stochastic Gradient Descent (SGD) Method  We study Stochastic Gradient Descent (SGD) with diminishing step sizes for convex objective functions. We introduce a deﬁnitional framework and theory that deﬁnes and characterizes a core property, called curvature, of convex objective functions. In terms of curvature w
Composing Value Functions in Reinforcement Learning  Benjamin van Niekerk * 1 Steven James * 1 Adam Earle 1 Benjamin Rosman 1 2  Abstract An important property for lifelong-learning agents is the ability to combine existing skills to solve new unseen tasks. In general, however, it is unclear how to compose existing skills in a principled manner. Under the assumption of de- terministic dynamics, we prove that optimal value function composition can be achieved in entropy- regularised reinforcement
Model Comparison for Semantic Grouping  Francisco Vargas 1 Kamen Brestnichki 1 Nils Hammerla 1  Abstract  We introduce a probabilistic framework for quanti- fying the semantic similarity between two groups of embeddings. We formulate the task of semantic similarity as a model comparison task in which we contrast a generative model which jointly mod- els two sentences versus one that does not. We illustrate how this framework can be used for the Semantic Textual Similarity tasks using clear as- s
Learning Dependency Structures for Weak Supervision Models  Paroma Varma * 1 Frederic Sala * 2 Ann He 2 Alexander Ratner 2 Christopher R´e 2  Abstract  Labeling training data is a key bottleneck in the modern machine learning pipeline. Recent weak supervision approaches combine labels from mul- tiple noisy sources by estimating their accuracies without access to ground truth labels; however, estimating the dependencies among these sources is a critical challenge. We focus on a robust PCA- based 
Probabilistic Neural-symbolic Models for Interpretable Visual Question Answering  Ramakrishna Vedantam * 1 Karan Desai 2 Stefan Lee 2 Marcus Rohrbach 1 Dhruv Batra 1 2 Devi Parikh 1 2  Abstract  We propose a new class of probabilistic neural- symbolic models, that have symbolic functional programs as a latent, stochastic variable. Instan- tiated in the context of visual question answer- ing, our probabilistic formulation offers two key conceptual advantages over prior neural-symbolic models for 
Manifold Mixup: Better Representations by Interpolating Hidden States  Vikas Verma * 1 2 Alex Lamb * 2 Christopher Beckham 2 Amir Najaﬁ 3 Ioannis Mitliagkas 2 David Lopez-Paz 4  Yoshua Bengio 2  Abstract  Deep neural networks excel at learning the train- ing data, but often provide incorrect and conﬁdent predictions when evaluated on slightly different test examples. This includes distribution shifts, outliers, and adversarial examples. To address these issues, we propose Manifold Mixup, a sim- 
Maximum Likelihood Estimation for Learning Populations of Parameters  Ramya Korlakai Vinayak 1 Weihao Kong 2 Gregory Valiant 2 Sham Kakade 1  Abstract  Consider a setting with N independent individu- als, each with an unknown parameter, pi ∈ [0, 1] drawn from some unknown distribution P (cid:63). Af- ter observing the outcomes of t independent Bernoulli trials, i.e., Xi ∼ Binomial(t, pi) per individual, our objective is to accurately estimate P (cid:63). This problem arises in numerous domains, 
Understanding Priors in Bayesian Neural Networks at the Unit Level  Mariia Vladimirova 1 2 Jakob Verbeek 1 Pablo Mesejo 3 Julyan Arbel 1  Abstract  We investigate deep Bayesian neural networks with Gaussian weight priors and a class of ReLU- like nonlinearities. Bayesian neural networks with Gaussian priors are well known to induce an L2, “weight decay”, regularization. Our results char- acterize a more intricate regularization effect at the level of the unit activations. Our main result establi
On the Design of Estimators for Bandit Off-Policy Evaluation  Nikos Vlassis 1 Aurelien Bibaut 2 Maria Dimakopoulou 1 Tony Jebara 1  Abstract  Off-policy evaluation is the problem of estimat- ing the value of a target policy using data col- lected under a different policy. We describe a framework for designing estimators for bandit off- policy evaluation. Given a base estimator and a parametrized class of control variates, we seek a control variate in that class that reduces the risk of the base 
Learning to Select for a Predeﬁned Ranking  Aleksei Ustimenko * 1 2 3 Aleksandr Vorobev * 1 Gleb Gusev 1 4 Pavel Serdyukov 1  Abstract  In this paper, we formulate a novel problem of learning to select a set of items maximizing the quality of their ordered list, where the order is predeﬁned by some explicit rule. Unlike the clas- sic information retrieval problem, in our setting, the predeﬁned order of items in the list may not correspond to their quality in general. For exam- ple, this is a dom
On the Limitations of Representing Functions on Sets  Edward Wagstaff * 1 Fabian B. Fuchs * 1 Martin Engelcke * 1 Ingmar Posner 1 Michael Osborne 1  Abstract  Recent work on the representation of functions on sets has considered the use of summation in a latent space to enforce permutation invariance. In particular, it has been conjectured that the di- mension of this latent space may remain ﬁxed as the cardinality of the sets under consideration increases. However, we demonstrate that the ana- 
Graph Convolutional Gaussian Processes  Ian Walker 1 Ben Glocker 1  Abstract  We propose a novel Bayesian nonparametric method to learn translation-invariant relationships on non-Euclidean domains. The resulting graph convolutional Gaussian processes can be applied to problems in machine learning for which the input observations are functions with domains on general graphs. The structure of these models al- lows for high dimensional inputs while retaining expressibility, as is the case with conv
Gaining Free or Low-Cost Transparency with Interpretable Partial Substitute  Tong Wang 1  Abstract  This work addresses the situation where a black- box model with good predictive performance is chosen over its interpretable competitors, and we show interpretability is still achievable in this case. Our solution is to ﬁnd an interpretable substitute on a subset of data where the black-box model is overkill or nearly overkill while leaving the rest to the black-box. This transparency is obtained 
Convolutional Poisson Gamma Belief Network  Chaojie Wang 1 Bo Chen 1 Sucheng Xiao 1 Mingyuan Zhou 2  Abstract  For text analysis, one often resorts to a lossy rep- resentation that either completely ignores word order or embeds each word as a low-dimensional dense feature vector. In this paper, we propose convolutional Poisson factor analysis (CPFA) that directly operates on a lossless representa- tion that processes the words in each document as a sequence of high-dimensional one-hot vec- tors.
Differentially Private Empirical Risk Minimization with Non-convex Loss  Functions  Di Wang 1 Changyou Chen 1 Jinhui Xu 1  Abstract  We study the problem of Empirical Risk Mini- mization (ERM) with (smooth) non-convex loss functions under the differential-privacy (DP) model. We ﬁrst study the expected excess em- pirical (or population) risk, which was primarily used as the utility to measure the quality for con- vex loss functions. Speciﬁcally, we show that the excess empirical (or population) r
Random Expert Distillation: Imitation Learning via Expert Policy Support  Estimation  Ruohan Wang 1 Carlo Ciliberto 1 Pierluigi V. Amadori 1 Yiannis Demiris 1  Abstract  We consider the problem of imitation learning from a ﬁnite set of expert trajectories, without access to reinforcement signals. The classical approach of extracting the expert’s reward func- tion via inverse reinforcement learning, followed by reinforcement learning is indirect and may be computationally expensive. Recent genera
SATNet: Bridging deep learning and logical reasoning using a differentiable  satisﬁability solver  Po-Wei Wang 1 Priya L. Donti 1 2 Bryan Wilder 3 Zico Kolter 1 4  Abstract  Integrating logical reasoning within deep learning architectures has been a major goal of modern AI systems. In this paper, we propose a new direction toward this goal by introducing a differentiable (smoothed) maximum satisﬁability (MAXSAT) solver that can be integrated into the loop of larger deep learning systems. Our (ap
Improving Neural Language Modeling via Adversarial Training  Dilin Wang * 1 Chengyue Gong * 1 Qiang Liu 1  Abstract  Recently, substantial progress has been made in language modeling by using deep neural net- works. However, in practice, large scale neural language models have been shown to be prone to overﬁtting. In this paper, we present a simple yet highly effective adversarial training mech- anism for regularizing neural language models. The idea is to introduce adversarial noise to the outp
EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis  Chaoqi Wang 1 2 Roger Grosse 1 2 Sanja Fidler 1 2 3 Guodong Zhang 1 2  Abstract  Reducing the test time resource requirements of a neural network while preserving test accu- racy is crucial for running inference on resource- constrained devices. To achieve this goal, we in- troduce a novel network reparameterization based on the Kronecker-factored eigenbasis (KFE), and then apply Hessian-based structured prun- ing methods in t
Nonlinear Stein Variational Gradient Descent  for Learning Diversiﬁed Mixture Models  Dilin Wang 1 Qiang Liu 1  Abstract  Diversiﬁcation has been shown to be a powerful mechanism for learning robust models in non- convex settings. A notable example is learn- ing mixture models, in which enforcing diver- sity between the different mixture components allows us to prevent the model collapsing phe- nomenon and capture more patterns from the ob- served data. In this work, we present a varia- tional a
On the Convergence and Robustness of Adversarial Training  Yisen Wang * 1 Xingjun Ma * 2 James Bailey 2 Jinfeng Yi 1 Bowen Zhou 1 Quanquan Gu 3  Abstract  Improving the robustness of deep neural networks (DNNs) to adversarial examples is an important yet challenging problem for secure deep learning. Across existing defense techniques, adversarial training with Projected Gradient Decent (PGD) is amongst the most effective. Adversarial training solves a min-max optimization problem, with the inner
State-Regularized Recurrent Neural Networks  Cheng Wang 1 Mathias Niepert 1  Abstract  Recurrent neural networks are a widely used class of neural architectures with two shortcomings. First, it is difﬁcult to understand what exactly they learn. Second, they tend to work poorly on sequences requiring long-term memorization, de- spite having this capacity in principle. We aim to address both shortcomings with a class of recur- rent networks that use a stochastic state transition mechanism between 
Deep Factors for Forecasting  Yuyang Wang 1 Alex Smola 1 Danielle C. Maddix 1 Jan Gasthaus 1 Dean Foster 1 Tim Januschowski 1  Abstract  Producing probabilistic forecasts for large collec- tions of similar and/or dependent time series is a practically relevant and challenging task. Clas- sical time series models fail to capture complex patterns in the data, and multivariate techniques struggle to scale to large problem sizes. Their reliance on strong structural assumptions makes them data-efﬁcie
Repairing without Retraining:  Avoiding Disparate Impact with Counterfactual Distributions  Hao Wang 1 Berk Ustun 1 Flavio P. Calmon 1  Abstract  When the performance of a machine learning model varies over groups deﬁned by sensitive attributes (e.g., gender or ethnicity), the perfor- mance disparity can be expressed in terms of the probability distributions of the input and output variables over each group. In this paper, we ex- ploit this fact to reduce the disparate impact of a ﬁxed classiﬁca
On Sparse Linear Regression in the Local Differential Privacy Model  Di Wang 1 Jinhui Xu 1  Abstract  In this paper, we study the sparse linear regres- sion problem under the Local Differential Privacy (LDP) model. We ﬁrst show that polynomial de- pendency on the dimensionality 𝑝 of the space is unavoidable for the estimation error in both non-interactive and sequential interactive local models, if the privacy of the whole dataset needs to be preserved. Similar limitations also exist for other t
Doubly Robust Joint Learning for Recommendation  on Data Missing Not at Random  Xiaojie Wang 1 Rui Zhang 1 Yu Sun 2 Jianzhong Qi 1  Abstract  In recommender systems, usually the ratings of a user to most items are missing and a critical prob- lem is that the missing ratings are often missing not at random (MNAR) in reality. It is widely ac- knowledged that MNAR ratings make it difﬁcult to accurately predict the ratings and unbiasedly estimate the performance of rating prediction. Re- cent approa
On the Generalization Gap in Reparameterizable Reinforcement Learning  Huan Wang 1 Stephan Zheng 1 Caiming Xiong 1 Richard Socher 1  Abstract  Understanding generalization in reinforcement learning (RL) is a signiﬁcant challenge, as many common assumptions of traditional supervised learning theory do not apply. We focus on the special class of reparameterizable RL problems, where the trajectory distribution can be decom- posed using the reparametrization trick. For this problem class, estimating
Bias Also Matters: Bias Attribution for Deep Neural Network Explanation  Shengjie Wang * 1 Tianyi Zhou * 1 Jeffery A. Bilmes 2  Abstract  The gradient of a deep neural network (DNN) w.r.t. the input provides information that can be used to explain the output prediction in terms of the input features and has been widely studied to assist in interpreting DNNs. In a linear model (i.e., g(x) = wx + b), the gradient corresponds to the weights w. Such a model can reasonably locally-linearly approximat
Jumpout : Improved Dropout for Deep Neural Networks with ReLUs  Shengjie Wang * 1 Tianyi Zhou * 1 Jeff A. Bilmes 2  Abstract  We discuss three novel insights about dropout for DNNs with ReLUs: 1) dropout encourages each local linear piece of a DNN to be trained on data points from nearby regions; 2) the same dropout rate results in different (effective) deactivation rates for layers with different portions of ReLU- deactivated neurons; and 3) the rescaling factor of dropout causes a normalizatio
Generalized Linear Rule Models  Dennis Wei 1 Sanjeeb Dash 1 Tian Gao 1 Oktay G¨unl¨uk 1  Abstract  This paper considers generalized linear models using rule-based features, also referred to as rule ensembles, for regression and probabilistic clas- siﬁcation. Rules facilitate model interpretation while also capturing nonlinear dependences and interactions. Our problem formulation accord- ingly trades off rule set complexity and predic- tion accuracy. Column generation is used to op- timize over a
On the statistical rate of nonlinear recovery in generative models with  heavy-tailed data  Xiaohan Wei 1 Zhuoran Yang 2 Zhaoran Wang 3  Abstract  We consider estimating a high-dimensional vec- tor from non-linear measurements where the un- known vector is represented by a generative model G : Rk ! Rd with k ⌧ d. Such a model poses structural priors on the unknown vector without having a dedicated basis, and in particu- lar allows new and efﬁcient approaches solving recovery problems with number
CapsAndRuns: An Improved Method for Approximately Optimal Algorithm  Conﬁguration  Gell´ert Weisz 1 Andr´as Gy¨orgy 1 2 Csaba Szepesv´ari 1 3  Abstract  We consider the problem of conﬁguring general- purpose solvers to run efﬁciently on problem in- stances drawn from an unknown distribution, a problem of major interest in solver autoconﬁgu- ration. Following previous work, we focus on de- signing algorithms that ﬁnd a conﬁguration with near-optimal expected capped runtime while do- ing the least
Non-Monotonic Sequential Text Generation  Sean Welleck 1 Kiant´e Brantley 2 Hal Daum´e III 2 3 Kyunghyun Cho 1 4 5  Abstract  Standard sequential generation methods assume a pre-speciﬁed generation order, such as text gener- ation methods which generate words from left to right. In this work, we propose a framework for training models of text generation that operate in non-monotonic orders; the model directly learns good orders, without any additional annotation. Our framework operates by genera
PROVEN: Verifying Robustness of Neural Networks with a Probabilistic  Approach  Tsui-Wei Weng 1 Pin-Yu Chen * 2 Lam M. Nguyen * 2 Mark S. Squillante * 2 Akhilan Boopathy 1  Ivan Oseledets 3 Luca Daniel 1  Abstract  We propose a novel framework PROVEN to PRObabilistically VErify Neural network’s ro- bustness with statistical guarantees. PROVEN provides probability certiﬁcates of neural net- work robustness when the input perturbation follow distributional characterization. Notably, PROVEN is deri
Learning Deep Kernels for Exponential Family Densities  Li K. Wenliang * 1 Dougal J. Sutherland * 1 Heiko Strathmann 1 Arthur Gretton 1  Abstract  The kernel exponential family is a rich class of distributions, which can be ﬁt efﬁciently and with statistical guarantees by score matching. Being required to choose a priori a simple kernel such as the Gaussian, however, limits its practical applica- bility. We provide a scheme for learning a kernel parameterized by a deep network, which can ﬁnd com
Improving Model Selection by Employing the Test Data  Max Westphal 1 Werner Brannath 1  Abstract  Model selection and evaluation are usually strictly separated by means of data splitting to enable an unbiased estimation and a simple statistical infer- ence for the unknown generalization performance of the ﬁnal prediction model. We investigate the properties of novel evaluation strategies, namely when the ﬁnal model is selected based on em- pirical performances on the test data. To guard against 
Automatic Classiﬁers as Scientiﬁc Instruments: One Step Further Away from Ground-Truth  Jacob Whitehill 1 Anand Ramakrishnan 1  Abstract  Automatic machine learning-based detectors of various psychological and social phenomena (e.g., emotion, stress, engagement) have great poten- tial to advance basic science. However, when a detector d is trained to approximate an existing measurement tool (e.g., a questionnaire, obser- vation protocol), then care must be taken when interpreting measurements co
Moment-Based Variational Inference for Markov Jump Processes  Christian Wildner 1 Heinz Koeppl 1  Abstract  We propose moment-based variational inference as a ﬂexible framework for approximate smooth- ing of latent Markov jump processes. The main ingredient of our approach is to partition the set of all transitions of the latent process into classes. This allows to express the Kullback-Leibler di- vergence between the approximate and the exact posterior process in terms of a set of moment func- 
End-to-End Probabilistic Inference for Nonstationary Audio Analysis  William J. Wilkinson 1 2 Michael Riis Andersen 2 3 Joshua D. Reiss 1 Dan Stowell 1 Arno Solin 2  Abstract  GP spectrogram = NMF weights (W) × positive modulator GPs (gn(t))  A typical audio signal processing pipeline in- cludes multiple disjoint analysis stages, includ- ing calculation of a time-frequency representation followed by spectrogram-based feature analysis. We show how time-frequency analysis and non- negative matrix 
FairnessRiskMeasuresRobertC.Williamson1AdityaKrishnaMenon2AbstractEnsuringthatclassiﬁersarenon-discriminatoryorfairwithrespecttoasensitivefeature(e.g.,raceorgender)isatopicalproblem.Progressinthistaskrequiresﬁxingadeﬁnitionoffairness,andtherehavebeenseveralproposalsinthisre-gardoverthepastfewyears.Severalofthese,however,assumeeitherbinarysensitivefeatures(thusprecludingcategoricalorreal-valuedsensi-tivegroups),orresultinnon-convexobjectives(thusadverselyaffectingtheoptimisationland-scape).Inthis
Partially Exchangeable Networks and Architectures for Learning Summary  Statistics in Approximate Bayesian Computation  Samuel Wiqvist∗ 1 Pierre-Alexandre Mattei∗ 2 Umberto Picchini 3 Jes Frellsen 2  Abstract  We present a novel family of deep neural archi- tectures, named partially exchangeable networks (PENs) that leverage probabilistic symmetries. By design, PENs are invariant to block-switch transformations, which characterize the partial ex- changeability properties of conditionally Marko- 
Wasserstein Adversarial Examples via Projected Sinkhorn Iterations  Eric Wong 1 Frank R. Schmidt 2 J. Zico Kolter 3 4  Abstract  A rapidly growing area of work has studied the ex- istence of adversarial examples, datapoints which have been perturbed to fool a classiﬁer, but the vast majority of these works have focused primar- ily on threat models deﬁned by (cid:96)p norm-bounded perturbations. In this paper, we propose a new threat model for adversarial attacks based on the Wasserstein distance
Imitation Learning from Imperfect Demonstration  Yueh-Hua Wu 1 2 Nontawat Charoenphakdee 3 2 Han Bao 3 2 Voot Tangkaratt 2 Masashi Sugiyama 2 3  Abstract  Imitation learning (IL) aims to learn an opti- mal policy from demonstrations. However, such demonstrations are often imperfect since collect- ing optimal ones is costly. To effectively learn from imperfect demonstrations, we propose a novel approach that utilizes conﬁdence scores, which describe the quality of demonstrations. More speciﬁcally
Learning a Compressed Sensing Measurement Matrix via Gradient Unrolling  Shanshan Wu 1 Alexandros G. Dimakis 1 Sujay Sanghavi 1 Felix X. Yu 2 Daniel Holtmann-Rice 2  Dmitry Storcheus 2 Afshin Rostamizadeh 2 Sanjiv Kumar 2  Abstract  Linear encoding of sparse vectors is widely popu- lar, but is commonly data-independent – missing any possible extra (but a priori unknown) structure beyond sparsity. In this paper we present a new method to learn linear encoders that adapt to data, while still perfo
Heterogeneous Model Reuse via Optimizing Multiparty Multiclass Margin  Xi-Zhu Wu 1 Song Liu 2 3 Zhi-Hua Zhou 1  Abstract  Nowadays, many problems require learning a model from data owned by different participants who are restricted to share their examples due to privacy concerns, which is referred to as multipar- ty learning in the literature. In conventional mul- tiparty learning, a global model is usually trained from scratch via a communication protocol, ig- noring the fact that each party ma
Deep Compressed Sensing  Yan Wu 1 Mihaela Rosca 1 Timothy Lillicrap 1  Abstract  Compressed sensing (CS) provides an elegant framework for recovering sparse signals from compressed measurements. For example, CS can exploit the structure of natural images and recover an image from only a few random measurements. CS is ﬂexible and data efﬁcient, but its applica- tion has been restricted by the strong assumption of sparsity and costly reconstruction process. A recent approach that combines CS with 
Simplifying Graph Convolutional Networks  Felix Wu * 1 Tianyi Zhang * 1 Amauri Holanda de Souza Jr. * 1 2 Christopher Fifty 1 Tao Yu 1  Kilian Q. Weinberger 1  Abstract  Graph Convolutional Networks (GCNs) and their variants have experienced signiﬁcant attention and have become the de facto methods for learning graph representations. GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complex- ity and redundant computation. In this pap
Domain Adaptation with Asymmetrically-Relaxed Distribution Alignment  Yifan Wu 1 Ezra Winston 1 Divyansh Kaushik 1 Zachary C. Lipton 1  Abstract  Domain adaptation addresses the common sit- uation in which the target distribution generat- ing our test data differs from the source distri- bution generating our training data. While ab- sent assumptions, domain adaptation is impossi- ble, strict conditions, e.g. covariate or label shift, enable principled algorithms. Recently-proposed domain-advers
On Scalable and Efﬁcient Computation of Large Scale  Optimal Transport  Yujia Xie 1 Minshuo Chen 1 Haoming Jiang 1 Tuo Zhao 1 Hongyuan Zha 1  Abstract  Optimal Transport (OT) naturally arises in many machine learning applications, yet the heavy com- putational burden limits its wide-spread uses. To address the scalability issue, we propose an im- plicit generative learning-based framework called SPOT (Scalable Push-forward of Optimal Trans- port). Speciﬁcally, we approximate the optimal transpor
Zeno: Distributed Stochastic Gradient Descent with Suspicion-based  Fault-tolerance  Cong Xie 1 Oluwasanmi Koyejo 1 Indranil Gupta 1  variance and magnitude, making them hard to distinguish. It is also possible that in different iterations, different groups of workers are faulty, which means that we can not simply identify workers which are always faulty.  Abstract  We present Zeno, a technique to make distributed machine learning, particularly Stochastic Gradi- ent Descent (SGD), tolerant to an
Differentiable Linearized ADMM  Xingyu Xie * 1 Jianlong Wu * 1 Zhisheng Zhong 1 Guangcan Liu 2 Zhouchen Lin 1  Abstract  Recently, a number of learning-based optimiza- tion methods that combine data-driven architec- tures with the classical optimization algorithms have been proposed and explored, showing supe- rior empirical performance in solving various ill- posed inverse problems, but there is still a scarcity of rigorous analysis about the convergence behav- iors of learning-based optimizati
Calibrated Approximate Bayesian Inference  Hanwen Xing 1 Geoff K. Nicholls 1 Jeong Eun Lee 2  Abstract  We give a computational framework for estimat- ing the bias in coverage resulting from making approximations in Bayesian inference. Coverage is the probability credible sets cover prior parame- ter values. We show how to estimate the coverage an approximation scheme achieves when the ideal but intractable observation model and the prior can be simulated, but have been replaced, in the Monte Ca
Power k-Means Clustering  Jason Xu 1 Kenneth Lange 2  Abstract  Clustering is a fundamental task in unsupervised machine learning. Lloyd’s 1957 algorithm for k- means clustering remains one of the most widely used due to its speed and simplicity, but the greedy approach is sensitive to initialization and often falls short at a poor solution. This paper explores an alternative to Lloyd’s algorithm that retains its simplicity and mitigates its tendency to get trapped by local minima. Called power 
Gromov-Wasserstein Learning for Graph Matching and Node Embedding  Hongteng Xu 1 2 Dixin Luo 2 Hongyuan Zha 3 Lawrence Carin 2  Abstract  A novel Gromov-Wasserstein learning framework is proposed to jointly match (align) graphs and learn embedding vectors for the associated graph nodes. Using Gromov-Wasserstein discrepancy, we measure the dissimilarity between two graphs and ﬁnd their correspondence, according to the learned optimal transport. The node embeddings associated with the two graphs a
Stochastic Optimization for DC Functions and Non-smooth Non-convex  Regularizers with Non-asymptotic Convergence  Yi Xu 1 Qi Qi 1 Qihang Lin 2 Rong Jin 3 Tianbao Yang 1  Abstract  Difference of convex (DC) functions cover a broad family of non-convex and possibly non- smooth and non-differentiable functions, and have wide applications in machine learning and statis- tics. Although deterministic algorithms for DC functions have been extensively studied, stochas- tic optimization that is more suit
Learning a Prior over Intent  via Meta-Inverse Reinforcement Learning  Kelvin Xu 1 Ellis Ratner 1 Anca Dragan 1 Sergey Levine 1 Chelsea Finn 1  Abstract  A signiﬁcant challenge for the practical appli- cation of reinforcement learning to real world problems is the need to specify an oracle reward function that correctly deﬁnes a task. Inverse re- inforcement learning (IRL) seeks to avoid this challenge by instead inferring a reward function from expert demonstrations. While appealing, it can be 
Variational Russian Roulette for Deep Bayesian Nonparametrics  Kai Xu 1 Akash Srivastava 1 2 Charles Sutton 1 3 4  Abstract  Bayesian nonparametric models provide a princi- pled way to automatically adapt the complexity of a model to the amount of the data available, but computation in such models is difﬁcult. Amor- tized variational approximations are appealing be- cause of their computational efﬁciency, but cur- rent methods rely on a ﬁxed ﬁnite truncation of the inﬁnite model. This truncation
Supervised Hierarchical Clustering with Exponential Linkage  Nishant Yadav 1 Ari Kobren 1 Nicholas Monath 1 Andrew McCallum 1  Abstract  In supervised clustering, standard techniques for learning a pairwise dissimilarity function often suffer from a discrepancy between the training and clustering objectives, leading to poor cluster quality. Rectifying this discrepancy necessitates matching the procedure for training the dissimi- larity function to the clustering algorithm. In this paper, we intr
Learning to Prove Theorems via Interacting with Proof Assistants  Kaiyu Yang 1 Jia Deng 1  Abstract  Humans prove theorems by relying on substan- tial high-level reasoning and problem-speciﬁc in- sights. Proof assistants offer a formalism that resembles human mathematical reasoning, repre- senting theorems in higher-order logic and proofs as high-level tactics. However, human experts have to construct proofs manually by entering tactics into the proof assistant. In this paper, we study the probl
Sample-Optimal Parametric Q-Learning Using Linearly Additive Features  Lin F. Yang 1 Mengdi Wang 1  Abstract  Consider a Markov decision process (MDP) that admits a set of state-action features, which can linearly express the process’s probabilistic transi- tion model. We propose a parametric Q-learning algorithm that ﬁnds an approximate-optimal pol- icy using a sample size proportional to the feature dimension K and invariant with respect to the size of the state space. To further improve its s
LegoNet: Efﬁcient Convolutional Neural Networks with Lego Filters  Zhaohui Yang 1 2 * Yunhe Wang 2 Hanting Chen 1 2 * Chuanjian Liu 2  Boxin Shi 3 4 Chao Xu 1 Chunjing Xu 2 Chang Xu 5  Abstract  This paper aims to build efﬁcient convolutional neural networks using a set of Lego ﬁlters. Many successful building blocks, e.g. inception and residual modules, have been designed to refresh state-of-the-art records of CNNs on visual recog- nition tasks. Beyond these high-level modules, we suggest that 
SWALP: Stochastic Weight Averaging in Low-Precision Training  Guandao Yang 1 Tianyi Zhang 1 Polina Kirichenko 1 Junwen Bai 1  Andrew Gordon Wilson 1 Christopher De Sa 1  Abstract  Low precision operations can provide scalabil- ity, memory savings, portability, and energy ef- ﬁciency. This paper proposes SWALP, an ap- proach to low precision training that averages low- precision SGD iterates with a modiﬁed learning rate schedule. SWALP is easy to implement and can match the performance of full-pr
ME-Net: Towards Effective Adversarial Robustness with Matrix Estimation  Yuzhe Yang 1 Guo Zhang 1 Dina Katabi 1 Zhi Xu 1  Abstract  Deep neural networks are vulnerable to adver- sarial attacks. The literature is rich with algo- rithms that can easily craft successful adversarial examples. In contrast, the performance of de- fense techniques still lags behind. This paper pro- poses ME-Net, a defense method that leverages matrix estimation (ME). In ME-Net, images are preprocessed using two steps: 
Efﬁcient Nonconvex Regularized Tensor Completion  with Structure-aware Proximal Iterations  Quanming Yao 1 2 James T. Kwok 2 Bo Han 3  Abstract  Nonconvex regularizers have been successful- ly used in low-rank matrix learning. In this paper, we extend this to the more challenging problem of low-rank tensor completion. Based on the proximal average algorithm, we develop an efﬁcient solver that avoids expensive tensor folding and unfolding. A special “sparse plus low-rank” structure, which is esse
Hierarchically Structured Meta-learning  Huaxiu Yao† 1 Ying Wei 2 Junzhou Huang 2 Zhenhui Li 1  Abstract  In order to learn quickly with few samples, meta- learning utilizes prior knowledge learned from previous tasks. However, a critical challenge in meta-learning is task uncertainty and heterogene- ity, which can not be handled via globally sharing knowledge among tasks. In this paper, based on gradient-based meta-learning, we propose a hier- archically structured meta-learning (HSML) algo- ri
Tight Kernel Query Complexity of Kernel Ridge Regression  and Kernel k-means Clustering  Manuel Fern´andez * 1 David P. Woodruff * 1 Taisuke Yasuda * 2  Abstract  Kernel methods generalize machine learning al- gorithms that only depend on the pairwise in- ner products of the data set by replacing inner products with kernel evaluations, a function that passes input points through a nonlinear feature map before taking the inner product in a higher dimensional space. In this work, we present tight 
Understanding Geometry of Encoder-Decoder CNNs  Jong Chul Ye 1 2 Woon Kyoung Sung 2  Abstract  Encoder-decoder networks using convolutional neural network (CNN) architecture have been ex- tensively used in deep learning literatures thanks to its excellent performance for various inverse problems. However, it is still difﬁcult to obtain coherent geometric view why such an architecture gives the desired performance. Inspired by recent theoretical understanding on generalizability, ex- pressivity a
Defending Against Saddle Point Attack in Byzantine-Robust Distributed  Learning  Dong Yin 1 Yudong Chen 2 Kannan Ramchandran 1 Peter Bartlett 1 3  Abstract  We study robust distributed learning that involves minimizing a non-convex loss function with sad- dle points. We consider the Byzantine setting where some worker machines have abnormal or even arbitrary and adversarial behavior, and in this setting, the Byzantine machines may create fake local minima near a saddle point that is far away fro
Rademacher Complexity for Adversarially Robust Generalization  Dong Yin 1 Kannan Ramchandran 1 Peter Bartlett 1 2  Abstract  Many machine learning models are vulnerable to adversarial attacks; for example, adding ad- versarial perturbations that are imperceptible to humans can often make machine learning models produce wrong predictions with high conﬁdence; moreover, although we may obtain robust models on the training dataset via adversarial training, in some problems the learned models cannot 
ARSM: Augment-REINFORCE-Swap-Merge Estimator for Gradient Backpropagation Through Categorical Variables  Mingzhang Yin * 1 Yuguang Yue * 1 Mingyuan Zhou 2  Abstract  To address the challenge of backpropagating the gradient through categorical variables, we propose the augment-REINFORCE-swap-merge (ARSM) gradient estimator that is unbiased and has low variance. ARSM ﬁrst uses vari- able augmentation, REINFORCE, and Rao- Blackwellization to re-express the gradient as an expectation under the Diric
NAS-Bench-101: Towards Reproducible Neural Architecture Search  Chris Ying * 1 Aaron Klein * 2 Esteban Real 1 Eric Christiansen 1 Kevin Murphy 1 Frank Hutter 2  Abstract  Recent advances in neural architecture search (NAS) demand tremendous computational re- sources, which makes it difﬁcult to reproduce experiments and imposes a barrier-to-entry to re- searchers without access to large-scale computa- tion. We aim to ameliorate these problems by in- troducing NAS-Bench-101, the ﬁrst public archi-
TapNet: Neural Network Augmented with Task-Adaptive Projection for  Few-Shot Learning  Sung Whan Yoon 1 Jun Seo 1 Jaekyun Moon 1  Abstract  Handling previously unseen tasks after given only a few training examples continues to be a tough challenge in machine learning. We propose TapNets, neural networks augmented with task- adaptive projection for improved few-shot learn- ing. Here, employing a meta-learning strategy with episode-based training, a network and a set of per-class reference vectors
Towards Accurate Model Selection in Deep Unsupervised Domain Adaptation  Kaichao You 1 2 Ximei Wang 1 2 Mingsheng Long 1 2 Michael I. Jordan 3  Abstract  Deep unsupervised domain adaptation (Deep UDA) methods successfully leverage rich labeled data in a source domain to boost the performance on related but unlabeled data in a target domain. However, algorithm comparison is cumbersome in Deep UDA due to the absence of accurate and standardized model selection method, posing an obstacle to further
Position-aware Graph Neural Networks  Jiaxuan You 1 Rex Ying 1 Jure Leskovec 1  Abstract  Learning node embeddings that capture a node’s position within the broader graph structure is cru- cial for many prediction tasks on graphs. How- ever, existing Graph Neural Network (GNN) ar- chitectures have limited power in capturing the position/location of a given node with respect to all other nodes of the graph. Here we pro- pose Position-aware Graph Neural Networks (P- GNNs), a new class of GNNs for 
Learning Neurosymbolic Generative Models via Program Synthesis  Halley Young 1 Osbert Bastani 1 Mayur Naik 1  Abstract  Generative models have become signiﬁcantly more powerful in recent years. However, these models continue to have difﬁculty capturing global structure in data. For example, images of buildings typically contain spatial patterns such as windows repeating at regular intervals, but state- of-the-art models have difﬁculty generating these patterns. We propose to address this problem
DAG-GNN: DAG Structure Learning with Graph Neural Networks  Yue Yu * 1 Jie Chen * 2 3 Tian Gao 3 Mo Yu 3  Abstract  Learning a faithful directed acyclic graph (DAG) from samples of a joint distribution is a chal- lenging combinatorial problem, owing to the in- tractable search space superexponential in the number of graph nodes. A recent breakthrough formulates the problem as a continuous opti- mization with a structural constraint that ensures acyclicity (Zheng et al., 2018). The authors ap- pl
How does Disagreement Help Generalization against Label Corruption?  Xingrui Yu 1 Bo Han 2 Jiangchao Yao 3 Gang Niu 2 Ivor W. Tsang 1 Masashi Sugiyama 2 4  Abstract  Learning with noisy labels is one of the hottest problems in weakly-supervised learning. Based on memorization effects of deep neural networks, training on small-loss instances becomes very promising for handling noisy labels. This fosters the state-of-the-art approach “Co-teaching” that cross-trains two deep neural networks using t
On the Computation and Communication Complexity of Parallel SGD with  Dynamic Batch Sizes for Stochastic Non-Convex Optimization  Hao Yu 1 Rong Jin 1  Abstract  For SGD based distributed stochastic optimiza- tion, computation complexity, measured by the convergence rate in terms of the number of stochastic gradient calls, and communication com- plexity, measured by the number of inter-node communication rounds, are two most important performance metrics. The classical data-parallel implementatio
On the Linear Speedup Analysis of Communication Efﬁcient Momentum SGD  for Distributed Non-Convex Optimization  Hao Yu 1 Rong Jin 1 Sen Yang 1  Abstract  Recent developments on large-scale distributed machine learning applications, e.g., deep neu- ral networks, beneﬁt enormously from the ad- vances in distributed non-convex optimization techniques, e.g., distributed Stochastic Gradient Descent (SGD). A series of recent works study the linear speedup property of distributed SGD variants with redu
Multi-Agent Adversarial Inverse Reinforcement Learning  Lantao Yu 1 Jiaming Song 1 Stefano Ermon 1  Abstract  Reinforcement learning agents are prone to unde- sired behaviors due to reward mis-speciﬁcation. Finding a set of reward functions to properly guide agent behaviors is particularly challeng- ing in multi-agent scenarios. Inverse reinforce- ment learning provides a framework to automati- cally acquire suitable reward functions from ex- pert demonstrations. Its extension to multi-agent set
Distributed Learning over Unreliable Networks  Chen Yu 1 Hanlin Tang 1 Cedric Renggli 2 Simon Kassing 2 Ankit Singla 2 Dan Alistarh 3 Ce Zhang 2 Ji Liu 4 1  Abstract  Most of today’s distributed machine learning sys- tems assume reliable networks: whenever two machines exchange information (e.g., gradients or models), the network should guarantee the de- livery of the message. At the same time, recent work exhibits the impressive tolerance of machine learning algorithms to errors or noise arisin
Online Adaptive Principal Component Analysis and Its extensions  Jianjun Yuan 1 Andrew Lamperski 1  Abstract  We propose algorithms for online principal com- ponent analysis (PCA) and variance minimiza- tion for adaptive settings. Previous literature has focused on upper bounding the static adversarial regret, whose comparator is the optimal ﬁxed ac- tion in hindsight. However, static regret is not an appropriate metric when the underlying envi- ronment is changing. Instead, we adopt the adap- t
Generative Modeling of Inﬁnite Occluded Objects for  Compositional Scene Representation  Jinyang Yuan 1 Bin Li 1 Xiangyang Xue 1  Abstract  We present a deep generative model which explic- itly models object occlusions for compositional scene representation. Latent representations of objects are disentangled into location, size, shape, and appearance, and the visual scene can be gen- erated compositionally by integrating these rep- resentations and an inﬁnite-dimensional binary vector indicating
Differential Inclusions for Modeling Nonsmooth ADMM Variants:  A Continuous Limit Theory  Huizhuo Yuan 1 Yuren Zhou 2 Chris Junchi Li 3 Qingyun Sun 4  Abstract  Recently, there has been a great deal of research attention on understanding the convergence be- havior of ﬁrst-order methods. One line of this research focuses on analyzing the convergence be- havior of ﬁrst-order methods using tools from con- tinuous dynamical systems such as ordinary differ- ential equations and differential inclusion
Statistical Analysis, Optimization, and Applications to Deep Learning  Trimming the (cid:96)1 Regularizer:  Jihun Yun 1 Peng Zheng 2 Eunho Yang 1 3 Aur´elie C. Lozano 4 Aleksandr Aravkin 2  Abstract  We study high-dimensional estimators with the trimmed (cid:96)1 penalty, which leaves the h largest pa- rameter entries penalty-free. While optimization techniques for this nonconvex penalty have been studied, the statistical properties have not yet been analyzed. We present the ﬁrst statistical ana
Bayesian Nonparametric Federated Learning of Neural Networks  Mikhail Yurochkin 1 2 Mayank Agarwal 1 2 Soumya Ghosh 1 2 3 Kristjan Greenewald 1 2 Trong Nghia Hoang 1 2  Yasaman Khazaeni 1 2  Abstract  In federated learning problems, data is scattered across different servers and exchanging or pooling it is often impractical or prohibited. We develop a Bayesian nonparametric framework for federated learning with neural networks. Each data server is assumed to provide local neural network weights,
Dirichlet Simplex Nest and Geometric Inference  Mikhail Yurochkin 1 2 * Aritra Guha 3 * Yuekai Sun 3 XuanLong Nguyen 3  Abstract  We propose Dirichlet Simplex Nest, a class of probabilistic models suitable for a variety of data types, and develop fast and provably accurate in- ference algorithms by accounting for the model’s convex geometry and low dimensional simplicial structure. By exploiting the connection to Voronoi tessellation and properties of Dirichlet distribu- tion, the proposed infer
A Conditional-Gradient-Based Augmented Lagrangian Framework  Alp Yurtsever 1 Olivier Fercoq 2 Volkan Cevher 1  Abstract  This paper considers a generic convex minimiza- tion template with afﬁne constraints over a com- pact domain, which covers key semideﬁnite pro- gramming applications. The existing conditional gradient methods either do not apply to our tem- plate or are too slow in practice. To this end, we propose a new conditional gradient method, based on a uniﬁed treatment of smoothing and
Conditional Gradient Methods via  Stochastic Path-Integrated Differential Estimator  Alp Yurtsever 1 Suvrit Sra 2 Volkan Cevher 1  Abstract  We propose a class of novel variance-reduced stochastic conditional gradient methods. By adopting the recent stochastic path-integrated dif- ferential estimator technique (SPIDER) of Fang et al. (2018) for the classical Frank-Wolfe (FW) method, we introduce SPIDER-FW for ﬁnite-sum minimization as well as the more general expecta- tion minimization problems.
Context-Aware Zero-Shot Learning for Object Recognition  ´Eloi Zablocki * 1 Patrick Bordes * 1 Benjamin Piwowarski 1 Laure Soulier 1 Patrick Gallinari 1 2  Abstract  Zero-Shot Learning (ZSL) aims at classifying unlabeled objects by leveraging auxiliary knowl- edge, such as semantic representations. A limita- tion of previous approaches is that only intrinsic properties of objects, e.g. their visual appearance, are taken into account while their context, e.g. the surrounding objects in the image,
Tighter Problem-Dependent Regret Bounds in Reinforcement Learning  without Domain Knowledge using Value Function Bounds  Andrea Zanette 1 Emma Brunskill 2  Abstract  Strong worst-case performance bounds for episodic reinforcement learning exist but fortu- nately in practice RL algorithms perform much better than such bounds would predict. Algo- rithms and theory that provide strong problem- dependent bounds could help illuminate the key features of what makes a RL problem hard and reduce the bar
Global Convergence of Block Coordinate Descent in Deep Learning  Jinshan Zeng 1 2 * Tim Tsz-Kit Lau 3 * Shao-Bo Lin 4 Yuan Yao 2  Abstract  Deep learning has aroused extensive attention due to its great empirical success. The efﬁciency of the block coordinate descent (BCD) methods has been recently demonstrated in deep neural net- work (DNN) training. However, theoretical stud- ies on their convergence properties are limited due to the highly nonconvex nature of DNN train- ing. In this paper, we
Making Convolutional Networks Shift-Invariant Again  Richard Zhang 1  Abstract  Modern convolutional networks are not shift- invariant, as small input shifts or translations can cause drastic changes in the output. Com- monly used downsampling methods, such as max-pooling, strided-convolution, and average- pooling, ignore the sampling theorem. The well- known signal processing ﬁx is anti-aliasing by low-pass ﬁltering before downsampling. How- ever, simply inserting this module into deep net- wor
Warm-starting Contextual Bandits:  Robustly Combining Supervised and Bandit Feedback  Chicheng Zhang 1 Alekh Agarwal 1 Hal Daumé III 1 2 John Langford 1 Sahand N Negahban 3  Abstract  We investigate the feasibility of learning from a mix of both fully-labeled supervised data and contextual bandit data. We speciﬁcally consider settings in which the underlying learning signal may be different between these two data sources. Theoretically, we state and prove no-regret algo- rithms for learning that
When Samples Are Strategically Selected  Hanrui Zhang 1 Yu Cheng 1 Vincent Conitzer 1  Abstract  In standard classiﬁcation problems, the assump- tion is that the entity making the decision (the principal) has access to all the samples. How- ever, in many contexts, she either does not have direct access to the samples, or can inspect only a limited set of samples and does not know which are the most relevant ones. In such cases, she must rely on another party (the agent) to either provide the sam
Self-Attention Generative Adversarial Networks  Han Zhang 1 2 Ian Goodfellow 2 Dimitris Metaxas 1 Augustus Odena 2  Abstract  In this paper, we propose the Self-Attention Gen- erative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution de- tails as a function of only spatially local points in lower-resolution feature maps. In SAGAN, de- tails can be generated using cues from 
Circuit-GNN: Graph Neural Networks for Distributed Circuit Design  Guo Zhang * 1 Hao He * 1 Dina Katabi 1  Abstract  We present Circuit-GNN, a graph neural network (GNN) model for designing distributed circuits. Today, designing distributed circuits is a slow pro- cess that can take months from an expert engi- neer. Our model both automates and speeds up the process. The model learns to simulate the electromagnetic (EM) properties of distributed circuits. Hence, it can be used to replace tradi- 
LatentGNN: Learning Efﬁcient Non-local Relations for Visual Recognition  Songyang Zhang 1 Shipeng Yan 1 Xuming He 1  Abstract  Capturing long-range dependencies in feature rep- resentations is crucial for many visual recognition tasks. Despite recent successes of deep convolu- tional networks, it remains challenging to model non-local context relations between visual fea- tures. A promising strategy is to model the feature context by a fully-connected graph neural network (GNN), which augments t
Neural Collaborative Subspace Clustering  Tong Zhang 1 2 Pan Ji 3 Mehrtash Harandi 4 Wenbing Huang 5 Hongdong Li 2  Abstract  We introduce the Neural Collaborative Subspace Clustering, a neural model that discovers clus- ters of data points drawn from a union of low- dimensional subspaces. In contrast to previous at- tempts, our model runs without the aid of spectral clustering. This makes our algorithm one of the kinds that can gracefully scale to large datasets. At its heart, our neural model 
Incremental Randomized Sketching for Online Kernel Learning  Xiao Zhang 1 Shizhong Liao 1  Abstract  Randomized sketching has been used in ofﬂine kernel learning, but it cannot be applied directly to online kernel learning due to the lack of incremen- tal maintenances for randomized sketches with regret guarantees. To address these issues, we propose a novel incremental randomized sketch- ing approach for online kernel learning, which has efﬁcient incremental maintenances with theo- retical guar
Bridging Theory and Algorithm for Domain Adaptation  Yuchen Zhang * 1 2 Tianle Liu * 1 3 Mingsheng Long 1 2 Michael I. Jordan 4  Abstract  This paper addresses the problem of unsupervised domain adaption from theoretical and algorithmic perspectives. Existing domain adaptation theories naturally imply minimax optimization algorithms, which connect well with the domain adaptation methods based on adversarial learning. However, several disconnections still exist and form the gap between theory and
Adaptive Regret of Convex and Smooth Functions  Lijun Zhang 1 Tie-Yan Liu 2 Zhi-Hua Zhou 1  Abstract  We investigate online convex optimization in changing environments, and choose the adaptive regret as the performance measure. The goal is to achieve a small regret over every interval so that the comparator is allowed to change over time. Different from previous works that only utilize the convexity condition, this paper furt- her exploits smoothness to improve the adaptive regret. To this end,
Random Function Priors for Correlation Modeling  Aonan Zhang 1 John Paisley 1  Abstract  The likelihood model of high dimensional data Xn can often be expressed as p(Xn|Zn, θ), where θ := (θk)k∈[K] is a collection of hidden features shared across objects, indexed by n, and Zn is a non-negative factor loading vector with K en- tries where Znk indicates the strength of θk used to express Xn. In this paper, we introduce ran- dom function priors for Zn for modeling correla- tions among its K dimensi
Co-Representation Network for Generalized Zero-Shot Learning  Fei Zhang 1 Guangming Shi 1  Abstract  Generalized zero-shot learning is a signiﬁcant topic but faced with bias problem, which leads to unseen classes being easily misclassiﬁed into seen classes. Hence we propose an embedding model called co-representation network to learn a more uniform visual embedding space that ef- fectively alleviates the bias problem and helps with classiﬁcation. We mathematically analyze our model and ﬁnd it le
SOLAR: Deep Structured Representations for  Model-Based Reinforcement Learning  Marvin Zhang * 1 Sharad Vikram * 2 Laura Smith 1 Pieter Abbeel 1 Matthew J. Johnson 3 Sergey Levine 1  Abstract  Model-based reinforcement learning (RL) has proven to be a data efﬁcient approach for learning control tasks but is difﬁcult to utilize in domains with complex observations such as images. In this paper, we present a method for learning represen- tations that are suitable for iterative model-based policy i
A Composite Randomized Incremental Gradient Method  Junyu Zhang 1  Lin Xiao 2  Abstract  We consider the problem of minimizing the com- position of a smooth function (which can be non- convex) and a smooth vector mapping, where both of them can be express as the average of a large number of components. We propose a composite randomized incremental gradient method based on SAGA type of construction. The gradient sample complexity of our method matches that of several recently developed methods ba
Fast and Stable Maximum Likelihood Estimation  for Incomplete Multinomial Models  Chenyang Zhang 1 Guosheng Yin 1  Abstract  We propose a ﬁxed-point iteration approach to the maximum likelihood estimation for the in- complete multinomial model, which provides a uniﬁed framework for ranking data analysis. In- complete observations typically fall in a subset of categories, and thus cannot be distinguished as belonging to a unique category. We develop a minorization–maximization (MM) type of algo- 
Theoretically Principled Trade-off between Robustness and Accuracy  Hongyang Zhang 1 2 Yaodong Yu 3 Jiantao Jiao 4 Eric P. Xing 1 5 Laurent El Ghaoui 4 Michael I. Jordan 4  Abstract  We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decompose the prediction
Learning Novel Policies For Tasks  Yunbo Zhang 1 Wenhao Yu 1 Greg Turk 1  Abstract  In this work, we present a reinforcement learning algorithm that can ﬁnd a variety of policies (novel policies) for a task that is given by a task reward function. Our method does this by creating a sec- ond reward function that recognizes previously seen state sequences and rewards those by novelty, which is measured using autoencoders that have been trained on state sequences from previously discovered policies
Greedy Orthogonal Pivoting Algorithm for Non-negative Matrix Factorization  Kai Zhang 1 Jun Liu 2 Jie Zhang 3 Jun Wang 1  Abstract  Non-negative matrix factorization is a powerful tool for learning useful representations in the da- ta and has been widely applied in many problems such as data mining and signal processing. Or- thogonal NMF, which can further improve the locality of decomposition, has drawn consider- able interest in clustering problems. However, imposing simultaneous non-negative 
Interpreting Adversarially Trained Convolutional Neural Networks  Tianyuan Zhang 1 Zhanxing Zhu 2 3 4  Abstract  We attempt to interpret how adversarially trained convolutional neural networks (AT-CNNs) recog- nize objects. We design systematic approaches to interpret AT-CNNs in both qualitative and quan- titative ways and compare them with normally trained models. Surprisingly, we ﬁnd that adver- sarial training alleviates the texture bias of stan- dard CNNs when trained on object recognition t
Adaptive Monte Carlo Multiple Testing via Multi-Armed Bandits  Martin J. Zhang 1 James Zou 1 2 3 David Tse 1  Abstract  Monte Carlo (MC) permutation test is considered the gold standard for statistical hypothesis testing, especially when standard parametric assumptions are not clear or likely to fail. However, in mod- ern data science settings where a large number of hypothesis tests need to be performed simul- taneously, it is rarely used due to its prohibitive computational cost. In genome-wid
On Learning Invariant Representations for Domain Adaptation  Han Zhao 1 Remi Tachet des Combes 2 Kun Zhang 1 Geoffrey J. Gordon 1 2  Abstract  Due to the ability of deep neural nets to learn rich representations, recent advances in unsuper- vised domain adaptation have focused on learning domain-invariant features that achieve a small er- ror on the source domain. The hope is that the learnt representation, together with the hypothesis learnt from the source domain, can generalize to the target 
Metric-Optimized Example Weights  Sen Zhao * 1 Mahdi Milani Fard * 1 Harikrishna Narasimhan 1 Maya Gupta 1  Abstract  Real-world machine learning applications often have complex test metrics, and may have train- ing and test data that are not identically dis- tributed. Motivated by known connections be- tween complex test metrics and cost-weighted learning, we propose addressing these issues by using a weighted loss function with a standard loss, where the weights on the training examples are le
Improving Neural Network Quantization without Retraining using  Outlier Channel Splitting  Ritchie Zhao 1 Yuwei Hu 1 Jordan Dotzel 1 Christopher De Sa 1 Zhiru Zhang 1  Abstract  Quantization can improve the execution latency and energy efﬁciency of neural networks on both commodity GPUs and specialized accelerators. The majority of existing literature focuses on train- ing quantized DNNs, while this work examines the less-studied topic of quantizing a ﬂoating- point model without (re)training. D
Maximum Entropy-Regularized Multi-Goal Reinforcement Learning  Rui Zhao 1 2 Xudong Sun 1 Volker Tresp 1 2  Abstract  In Multi-Goal Reinforcement Learning, an agent learns to achieve multiple goals with a goal- conditioned policy. During learning, the agent ﬁrst collects the trajectories into a replay buffer, and later these trajectories are selected randomly for replay. However, the achieved goals in the re- play buffer are often biased towards the behavior policies. From a Bayesian perspective,
Stochastic Iterative Hard Thresholding for Graph-structured Sparsity  Optimization  Baojian Zhou 1 Feng Chen 1 Yiming Ying 2  Abstract  Stochastic optimization algorithms update models with cheap per-iteration costs sequentially, which makes them amenable for large-scale data anal- ysis. Such algorithms have been widely studied for structured sparse models where the sparsity information is very speciﬁc, e.g., convex sparsity- inducing norms or (cid:96)0-norm. However, these norms cannot be direc
Lower Bounds for Smooth Nonconvex Finite-Sum Optimization  Dongruo Zhou 1 Quanquan Gu 1  Abstract  Smooth ﬁnite-sum optimization has been widely studied in both convex and nonconvex settings. However, existing lower bounds for ﬁnite-sum op- timization are mostly limited to the setting where each component function is (strongly) convex, while the lower bounds for nonconvex ﬁnite-sum optimization remain largely unsolved. In this pa- per, we study the lower bounds for smooth non- convex ﬁnite-sum o
Lipschitz Generative Adversarial Nets  Zhiming Zhou 1 Jiadong Liang 2 Yuxuan Song 1 Lantao Yu 3 Hongwei Wang 3 Weinan Zhang 1 Yong Yu 1  Zhihua Zhang 2  Abstract  In this paper we show that generative adversarial networks (GANs) without restriction on the dis- criminative function space commonly suffer from the problem that the gradient produced by the discriminator is uninformative to guide the gen- erator. By contrast, Wasserstein GAN (WGAN), where the discriminative function is restricted to 
Towards Understanding the Importance of Noise in Training Neural Networks  Mo Zhou * 1 Tianyi Liu * 2 Yan Li 2 Dachao Lin 1 Enlu Zhou 2 Tuo Zhao 2  Abstract  Numerous empirical evidence has corroborated that noise plays a crucial rule in effective and ef- ﬁcient training of neural networks. The theory behind, however, is still largely unknown. This paper studies this fundamental problem through training a simple two-layer convolutional neural network model. Although training such a network requi
BayesNAS: A Bayesian Approach for Neural Architecture Search  Hongpeng Zhou 1 * Minghao Yang 1 * Jun Wang 2 Wei Pan 1  Abstract  One-Shot Neural Architecture Search (NAS) is a promising method to signiﬁcantly reduce search time without any separate training. It can be treated as a Network Compression prob- lem on the architecture parameters from an over- parameterized network. However, there are two issues associated with most one-shot NAS meth- ods. First, dependencies between a node and its pr
Transferable Clean-Label Poisoning Attacks on Deep Neural Nets  Chen Zhu * 1 W. Ronny Huang * 1 Ali Shafahi 1 Hengduo Li 1 Gavin Taylor 2 Christoph Studer 3  Tom Goldstein 1  Abstract  Clean-label poisoning attacks inject innocuous looking (and “correctly” labeled) poison images into training data, causing a model to misclassify a targeted image after being trained on this data. We consider transferable poisoning attacks that succeed without access to the victim network’s outputs, architecture, 
Improved Dynamic Graph Learning through Fault-Tolerant Sparsiﬁcation  Chun Jiang Zhu 1 Sabine Storandt 2 Kam-Yiu Lam 3 Song Han 1 Jinbo Bi 1  Abstract  Graph sparsiﬁcation has been used to improve the computational cost of learning over graphs, e.g., Laplacian-regularized estimation, graph semi- supervised learning (SSL) and spectral clustering (SC). However, when graphs vary over time, re- peated sparsiﬁcation requires polynomial order computational cost per update. We propose a new type of gra
Poisson Subsampled Renyi Differential Privacy  Yuqing Zhu 1 Yu-Xiang Wang 1  Abstract the  by  under  problem of  subsampling”  "privacy- We consider ampliﬁcation the Renyi Differential Privacy (RDP) framework (Mironov, 2017). This is the main workhorse underlying the moments accountant approach for differentially private deep learning (Abadi et al., 2016). Complementing a recent result on this problem that deals with “Sampling without Replacement” (Wang et al., 2019), we address the “Poisson su
Learning Classiﬁers for Target Domain with Limited or No Labels  Pengkai Zhu * 1 Hanxiao Wang * 1 Venkatesh Saligrama 1  Abstract  In computer vision applications, such as domain adaptation (DA), few shot learning (FSL) and zero-shot learning (ZSL), we encounter new ob- jects and environments, for which insufﬁcient ex- amples exist to allow for training “models from scratch,” and methods that adapt existing models, trained on the presented training environment, to the new scenario are required. 
Surrogate Losses for Online Learning of Stepsizes in Stochastic Non-Convex  Optimization  Zhenxun Zhuang 1 Ashok Cutkosky 2 Francesco Orabona 1 3  Abstract  Stochastic Gradient Descent (SGD) has played a central role in machine learning. However, it requires a carefully hand-picked stepsize for fast convergence, which is notoriously tedious and time-consuming to tune. Over the last several years, a plethora of adaptive gradient-based algo- rithms have emerged to ameliorate this problem. In this 
Latent Normalizing Flows for Discrete Sequences  Zachary M. Ziegler 1 Alexander M. Rush 1  Abstract  Normalizing ﬂows are a powerful class of gen- erative models for continuous random variables, showing both strong model ﬂexibility and the po- tential for non-autoregressive generation. These beneﬁts are also desired when modeling discrete random variables such as text, but directly ap- plying normalizing ﬂows to discrete sequences poses signiﬁcant additional challenges. We pro- pose a VAE-based 
Beating Stochastic and Adversarial Semi-bandits  Optimally and Simultaneously  Julian Zimmert 1 Haipeng Luo 2 Chen-Yu Wei 2  Abstract  We develop the ﬁrst general semi-bandit algo- rithm that simultaneously achieves O(log T ) re- gret for stochastic environments and O(√T ) re- gret for adversarial environments without prior knowledge of the regime or the number of rounds T . The leading problem-dependent constants of our bounds are not only optimal in a certain worst- case sense studied previous
Fast Context Adaptation via Meta-Learning  Luisa Zintgraf 1 Kyriacos Shiarlis 1 2 Vitaly Kurin 1 2 Katja Hofmann 3 Shimon Whiteson 1 2  Abstract  We propose CAVIA for meta-learning, a simple extension to MAML that is less prone to meta- overﬁtting, easier to parallelise, and more inter- pretable. CAVIA partitions the model parame- ters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained an
Natural Analysts in Adaptive Data Analysis  Tijana Zrnic 1 Moritz Hardt 1  Abstract  Adaptive data analysis is frequently criticized for its pessimistic generalization guarantees. The source of these pessimistic bounds is a model that permits arbitrary, possibly adversarial ana- lysts that optimally use information to bias re- sults. While being a central issue in the ﬁeld, still lacking are notions of natural analysts that allow for more optimistic bounds faithful to the reality that typical an
Gibbs Max-Margin Topic Models with Fast Sampling Algorithms  Jun Zhu Ning Chen Hugh Perkins Bo Zhang Dept. of Comp. Sci & Tech; TNLIST Lab; State Key Lab of Intell. Tech & Sys., Beijing, 100084 China  dcszj@mail.tsinghua.edu.cn ningchen@mail.tsinghua.edu.cn ngls11@mails.tsinghua.edu.cn dcszb@mail.tsinghua.edu.cn  Abstract  Existing max-margin supervised topic mod- els rely on an iterative procedure to solve multiple latent SVM subproblems with addi- tional mean-ﬁeld assumptions on the desired po
Cost-Sensitive Tree of Classiﬁers  Zhixiang (Eddie) Xu Matt J. Kusner Kilian Q. Weinberger Minmin Chen Washington University, One Brookings Dr., St. Louis, MO 63130 USA  xuzx@cse.wustl.edu mkusner@wustl.edu kilian@wustl.edu mchen@wustl.edu  Abstract  ings and reductions of greenhouse gas emissions.  Recently, machine learning algorithms have successfully entered large-scale real-world in- dustrial applications (e.g. search engines and email spam ﬁlters). Here, the CPU cost during test-time must 
Maximum Variance Correction with Application to A∗ Search  Wenlin Chen Kilian Q. Weinberger Yixin Chen Washington University, One Brookings Dr., St. Louis, MO 63130 USA  wenlinchen@wustl.edu kilian@wustl.edu chen@cse.wustl.edu  Abstract  In this paper we introduce Maximum Vari- ance Correction (MVC), which ﬁnds large- scale feasible solutions to Maximum Variance Unfolding (MVU) by post-processing embed- dings from any manifold learning algorithm. It increases the scale of MVU embeddings by sever
Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization  Martin Jaggi CMAP, ´Ecole Polytechnique, Palaiseau, France  jaggi@cmap.polytechnique.fr  Abstract  We provide stronger and more general primal-dual convergence results for Frank- Wolfe-type algorithms (a.k.a. conditional gradient) for constrained convex optimiza- tion, enabled by a simple framework of du- ality gap certiﬁcates. Our analysis also holds if the linear subproblems are only solved ap- proximately (as well as if the g
Sparse projections onto the simplex  Anastasios Kyrillidis Stephen Becker Volkan Cevher Christoph Koch  Abstract  Most learning methods with rank or spar- sity constraints use convex relaxations, which lead to optimization with the nuclear norm or the (cid:96)1-norm. However, several important learning applications cannot beneﬁt from this approach as they feature these convex norms as constraints in addition to the non-convex rank and sparsity constraints. In this setting, we derive eﬃcient spar
A proximal Newton framework for composite minimization: Graph  learning without Cholesky decompositions and matrix inversions  Quoc Tran Dinh Anastasios Kyrillidis Volkan Cevher LIONS, ´Ecole Polytechnique F´ed´erale de Lausanne, Switzerland  quoc.trandinh@epfl.ch anastasios.kyrillidis@epfl.ch volkan.cevher@epfl.ch  Abstract  We propose an algorithmic framework for convex minimization problems of composite functions with two terms: a self-concordant part and a possibly nonsmooth regularization p
Domain Adaptation under Target and Conditional Shift  Kun Zhang Bernhard Sch¨olkopf Krikamol Muandet Zhikun Wang Max Plank Institute for Intelligent Systems, T¨ubingen, Germany  kzhang@tuebingen.mpg.de bs@tuebingen.mpg.de krikamol@tuebingen.mpg.de zhikun@tuebingen.mpg.de  Abstract  Let X denote the feature and Y the tar- get. We consider domain adaptation under three possible scenarios: (1) the marginal PY changes, while the conditional PX|Y stays the same (target shift), (2) the marginal PY is 
FastMax-MarginMatrixFactorizationwithDataAugmentationMinjieXuxuj-10@mails.tsinghua.edu.cnJunZhudcszj@mail.tsinghua.edu.cnBoZhangdcszb@mail.tsinghua.edu.cnDept.ofComp.Sci.&Tech.,LITSLab,TNListLab,TsinghuaUniversity,Beijing100084,ChinaAbstractExistingmax-marginmatrixfactorization(M3F)methodseitherarecomputationallyineﬃcientorneedamodelselectionproce-duretodeterminethenumberoflatentfac-tors.Inthispaperwepresentaprobabilis-ticM3FmodelthatadmitsahighlyeﬃcientGibbssamplingalgorithmthroughdataaug-menta
On the importance of initialization and momentum in deep learning  Ilya Sutskever1 James Martens George Dahl Geo↵rey Hinton  Abstract  Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful mod- els that were considered to be almost impos- sible to train using stochastic gradient de- scent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule fo
Structure Discovery in Nonparametric Regression through  Compositional Kernel Search  David Duvenaud∗† James Robert Lloyd∗† Roger Grosse‡ Joshua B. Tenenbaum‡ Zoubin Ghahramani†  Abstract  Despite its importance, choosing the struc- tural form of the kernel in nonparametric regression remains a black art. We deﬁne a space of kernel structures which are built compositionally by adding and multiplying a small number of base kernels. We present a method for searching over this space of struc- tures
Uncovering Causality from Multivariate Hawkes Integrated Cumulants  Massil Achab 1 Emmanuel Bacry 1 Stéphane Gaïffas 1 Iacopo Mastromatteo 2 Jean-François Muzy 1 3  Abstract  We design a new nonparametric method that al- lows one to estimate the matrix of integrated ker- nels of a multivariate Hawkes process. This ma- trix not only encodes the mutual inﬂuences of each node of the process, but also disentangles the causality relationships between them. Our approach is the ﬁrst that leads to an es
A Uniﬁed Maximum Likelihood Approach for Estimating Symmetric  Properties of Discrete Distributions  Jayadev Acharya 1 Hirakendu Das 2 Alon Orlitsky 3 Ananda Theertha Suresh 4  Abstract  Symmetric distribution properties such as sup- port size, support coverage, entropy, and prox- imity to uniformity, arise in many applications. Recently, researchers applied different estima- tors and analysis tools to derive asymptotically sample-optimal approximations for each of these properties. We show that
Constrained Policy Optimization  Joshua Achiam 1 David Held 1 Aviv Tamar 1 Pieter Abbeel 1 2  Abstract  For many applications of reinforcement learn- ing it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. For example, systems that physically interact with or around humans should satisfy safety constraints. Recent advances in policy search algorithms (Mnih et al., 2016; Schul- man et al., 2015; Lillicrap 
The Price of Differential Privacy for Online Learning  Naman Agarwal 1 Karan Singh 1  Abstract  We design differentially private algorithms for the problem of online linear optimization in the full information and bandit settings with optimal ˜O(pT )1 regret bounds. In the full-information setting, our results demonstrate that "-differential privacy may be ensured for free – in particular, the regret bounds scale as O(pT ) + ˜O 1 ". For pT⌘, posed algorithm achieves a regret of ˜O⇣ 1 was ˜O⇣ 1  
Local Bayesian Optimization of Motor Skills  Riad Akrour 1 Dmitry Sorokin 1 Jan Peters 1 2 Gerhard Neumann 1 3  Abstract  Bayesian optimization is renowned for its sam- ple efﬁciency but its application to higher dimen- sional tasks is impeded by its focus on global optimization. To scale to higher dimensional problems, we leverage the sample efﬁciency of Bayesian optimization in a local context. The optimization of the acquisition function is re- stricted to the vicinity of a Gaussian search di
Connected Subgraph Detection with Mirror Descent on SDPs  Cem Aksoylar 1 Lorenzo Orecchia 1 Venkatesh Saligrama 1  Abstract  We propose a novel, computationally efﬁcient mirror-descent based optimization framework for subgraph detection in graph-structured data. Our aim is to discover anomalous patterns present in a connected subgraph of a given graph. This problem arises in many applications such as de- tection of network intrusions, community detec- tion, detection of anomalous events in surve
Learning from Clinical Judgments: Semi-Markov-Modulated Marked  Hawkes Processes for Risk Prognosis  Ahmed M. Alaa, 1 Scott Hu, 1 Mihaela van der Schaar 1 2 3  Abstract  Critically ill patients in regular wards are vulner- able to unanticipated adverse events which re- quire prompt transfer to the intensive care unit (ICU). To allow for accurate prognosis of deteri- orating patients, we develop a novel continuous- time probabilistic model for a monitored pa- tient’s temporal sequence of physiolo
A Semismooth Newton Method for Fast, Generic Convex Programming  Alnur Ali * 1 Eric Wong * 1 J. Zico Kolter 2  Abstract  We introduce Newton-ADMM, a method for fast conic optimization. The basic idea is to view the residuals of consecutive iterates generated by the alternating direction method of multipliers (ADMM) as a set of ﬁxed point equations, and then use a nonsmooth Newton method to ﬁnd a solution; we apply the basic idea to the Split- ting Cone Solver (SCS), a state-of-the-art method for
Learning Continuous Semantic Representations of Symbolic Expressions  Miltiadis Allamanis 1 Pankajan Chanthirasegaran 2 Pushmeet Kohli 3 Charles Sutton 2 4  Abstract  Combining abstract, symbolic reasoning with con- tinuous neural reasoning is a grand challenge of representation learning. As a step in this direc- tion, we propose a new architecture, called neural equivalence networks, for the problem of learn- ing continuous semantic representations of alge- braic and logical expressions. These 
Natasha: Faster Non-Convex Stochastic Optimization  via Strongly Non-Convex Parameter  Zeyuan Allen-Zhu 1  Abstract  Given a non-convex function f (x) that is an av- erage of n smooth functions, we design stochas- tic ﬁrst-order methods to ﬁnd its approximate stationary points. The performance of our new methods depend on the smallest (negative) eigen- value −σ of the Hessian. This parameter σ captures how strongly non-convex f (x) is, and is analogous to the strong convexity parameter for conve
Doubly Accelerated Methods for Faster CCA  and Generalized Eigendecomposition  Zeyuan Allen-Zhu * 1 Yuanzhi Li * 2  Abstract  We study k-GenEV, the problem of ﬁnding the top k generalized eigenvectors, and k-CCA, the problem of ﬁnding the top k vectors in canonical- correlation analysis. We propose algorithms LazyEV and LazyCCA to solve the two problems with running times linearly dependent on the in- put size and on k. Furthermore, our algorithms are doubly-accelerated: our running times de- pe
Faster Principal Component Regression  and Stable Matrix Chebyshev Approximation  Zeyuan Allen-Zhu * 1 Yuanzhi Li * 2  Abstract  We solve principal component regression (PCR), up to a multiplicative accuracy 1+γ, by reducing  the problem to (cid:101)O(γ−1) black-box calls of ridge  regression. Therefore, our algorithm does not re- quire any explicit construction of the top prin- cipal components, and is suitable for large-scale In contrast, previous result re- PCR instances.  quires (cid:101)O(γ
Follow the Compressed Leader:  Faster Online Learning of Eigenvectors and Faster MMWU  Zeyuan Allen-Zhu * 1 Yuanzhi Li * 2  Abstract  √  is fundamental  The online problem of computing the top eigen- vector to machine learning. The famous matrix-multiplicative-weight-update (MMWU) framework solves this online prob- lem and gives optimal regret. However, since MMWU runs very slow due to the computa- tion of matrix exponentials, researchers proposed the follow-the-perturbed-leader (FTPL) frame- d 
Near-OptimalDesignofExperimentsviaRegretMinimizationZeyuanAllen-Zhu*1YuanzhiLi*2AartiSingh*3YiningWang*3AbstractWeconsidercomputationallytractablemethodsfortheexperimentaldesignproblem,wherekoutofndesignpointsofdimensionpareselectedsothatcertainoptimalitycriteriaareapproxi-matelysatisﬁed.Ouralgorithmﬁndsa(1+ε)-approximateoptimaldesignwhenkisalinearfunctionofp;incontrast,existingresultsrequirektobesuper-linearinp.Ouralgorithmalsohan-dlesallpopularoptimalitycriteria,whileexistingonesonlyhandleoneo
OptNet: Differentiable Optimization as a Layer in Neural Networks  Brandon Amos 1 J. Zico Kolter 1  Abstract  This paper presents OptNet, a network architec- ture that integrates optimization problems (here, speciﬁcally in the form of quadratic programs) as individual layers in larger end-to-end train- able deep networks. These layers encode con- straints and complex dependencies between the hidden states that traditional convolutional and fully-connected layers often cannot capture. In this pap
Input Convex Neural Networks  Brandon Amos 1 Lei Xu 2 * J. Zico Kolter 1  Abstract  This paper presents the input convex neural net- work architecture. These are scalar-valued (po- tentially deep) neural networks with constraints on the network parameters such that the output of the network is a convex function of (some of) the inputs. The networks allow for efﬁcient inference via optimization over some inputs to the network given others, and can be applied to settings including structured predi
An Efﬁcient, Sparsity-Preserving, Online Algorithm for Low-Rank  Approximation  David Anderson * 1 Ming Gu * 1  Abstract  Low-rank matrix approximation is a fundamental tool in data analysis for processing large datasets, reducing noise, and ﬁnding important signals. In this work, we present a novel truncated LU factorization called Spectrum-Revealing LU (SRLU) for effective low-rank matrix approxi- mation, and develop a fast algorithm to compute an SRLU factorization. We provide both matrix and
Modular Multitask Reinforcement Learning with Policy Sketches  Jacob Andreas 1 Dan Klein 1 Sergey Levine 1  Abstract  We describe a framework for multitask deep re- inforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them—speciﬁcally not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g. int
Averaged-DQN: Variance Reduction and Stabilization for  Deep Reinforcement Learning  Oron Anschel 1 Nir Baram 1 Nahum Shimkin 1  Abstract  Instability and variability of Deep Reinforcement Learning (DRL) algorithms tend to adversely af- fect their performance. Averaged-DQN is a sim- ple extension to the DQN algorithm, based on averaging previously learned Q-values estimates, which leads to a more stable training procedure and improved performance by reducing approxi- mation error variance in the
A Simple Multi-Class Boosting Framework  with Theoretical Guarantees and Empirical Proﬁciency  Ron Appel 1 Pietro Perona 1  Abstract  There is a need for simple yet accurate white-box learning systems that train quickly and with lit- tle data. To this end, we showcase REBEL, a multi-class boosting method, and present a novel family of weak learners called localized similar- ities. Our framework provably minimizes the training error of any dataset at an exponential rate. We carry out experiments 
Deep Voice: Real-time Neural Text-to-Speech  Sercan ¨O. Arık * 1 Mike Chrzanowski * 1 Adam Coates * 1 Gregory Diamos * 1 Andrew Gibiansky * 1 Yongguo Kang * 2 Xian Li * 2 John Miller * 1 Andrew Ng * 1 Jonathan Raiman * 1 Shubho Sengupta * 1  Mohammad Shoeybi * 1  Abstract  We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises ﬁve ma- jor bu
Oracle Complexity of Second-Order Methods for Finite-Sum Problems  Yossi Arjevani 1 Ohad Shamir 1  Abstract  Finite-sum optimization problems are ubiquitous in machine learning, and are commonly solved using ﬁrst-order methods which rely on gra- dient computations. Recently, there has been growing interest in second-order methods, which rely on both gradients and Hessians. In prin- ciple, second-order methods can require much fewer iterations than ﬁrst-order methods, and hold the promise for mor
Wasserstein Generative Adversarial Networks  Martin Arjovsky 1 Soumith Chintala 2 L´eon Bottou 1 2  Abstract  We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the cor- responding optimization problem is sound, and provide extensive the
Generalization and Equilibrium in Generative Adversarial Nets (GANs)  Sanjeev Arora 1 Rong Ge 2 Yingyu Liang 1 Tengyu Ma 1 Yi Zhang 1  Abstract  It is shown that training of generative adversar- ial network (GAN) may not have good gener- alization properties; e.g., training may appear successful but the trained distribution may be far from target distribution in standard metrics. However, generalization does occur for a weaker metric called neural net distance. It is also shown that an approxima
A Closer Look at Memorization in Deep Networks  Devansh Arpit * 1 2 Stanisław Jastrz˛ebski * 3 Nicolas Ballas * 1 2 David Krueger * 1 2 Emmanuel Bengio 4  Maxinder S. Kanwal 5 Tegan Maharaj 1 6 Asja Fischer 7 Aaron Courville 1 2 8 Yoshua Bengio 1 2 9  Simon Lacoste-Julien 1 2  Abstract  We examine the role of memorization in deep learning, drawing connections to capacity, gen- eralization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest t
An Alternative Softmax Operator for Reinforcement Learning  Kavosh Asadi 1 Michael L. Littman 1  Abstract  A softmax operator applied to a set of values acts somewhat like the maximization function and somewhat like an average. In sequential decision making, softmax is often used in settings where it is necessary to maximize utility but also to hedge against problems that arise from putting all of one’s weight behind a single maximum utility decision. The Boltzmann softmax operator is the most c
Random Fourier Features for Kernel Ridge Regression:  Approximation Bounds and Statistical Guarantees  Haim Avron 1 Michael Kapralov 2 Cameron Musco 3  Christopher Musco 3 Ameya Velingker 2 Amir Zandieh 2  Abstract  Random Fourier features is one of the most pop- ular techniques for scaling up kernel methods, such as kernel ridge regression. However, de- spite impressive empirical results, the statistical properties of random Fourier features are still not well understood. In this paper we take 
Minimax Regret Bounds for Reinforcement Learning  Mohammad Gheshlaghi Azar 1 Ian Osband 1 Rémi Munos 1  Abstract  √  √  √  HSAT +H 2S2A+H  bound of (cid:101)O( ous known bound (cid:101)O(HS (cid:101)O(  We consider the problem of provably optimal exploration in reinforcement learning for ﬁnite horizon MDPs. We show that an optimistic modiﬁcation to value iteration achieves a regret T ) where H is the time horizon, S the number of states, A the number of actions and T the number of time- steps. T
Learning the Structure of Generative Models without Labeled Data  Stephen H. Bach 1 Bryan He 1 Alexander Ratner 1 Christopher R´e 1  Abstract  Curating labeled training data has become the primary bottleneck in machine learning. Re- cent frameworks address this bottleneck with generative models to synthesize labels at scale from weak supervision sources. The generative model’s dependency structure directly affects the quality of the estimated labels, but selecting a structure automatically witho
Uniform Deviation Bounds for k-Means Clustering  Olivier Bachem 1 Mario Lucic 1 S. Hamed Hassani 1 Andreas Krause 1  Abstract  Uniform deviation bounds limit the difference be- tween a model’s expected loss and its loss on a random sample uniformly for all models in a learn- ing problem. In this paper, we provide a novel framework to obtain uniform deviation bounds for unbounded loss functions. As a result, we obtain competitive uniform deviation bounds for k-Means clustering under weak assumpti
Distributed and Provably Good Seedings for k-Means in Constant Rounds  Olivier Bachem 1 Mario Lucic 1 Andreas Krause 1  Abstract  The k-means++ algorithm is the state of the art algorithm to solve k-Means clustering problems as the computed clusterings are O(log k) com- petitive in expectation. However, its seeding step requires k inherently sequential passes through the full data set making it hard to scale to mas- sive data sets. The standard remedy is to use the k-means(cid:107) algorithm whi
Learning Algorithms for Active Learning  Philip Bachman * 1 Alessandro Sordoni * 1 Adam Trischler 1  Abstract  We introduce a model that learns active learning algorithms via metalearning. For a distribution of related tasks, our model jointly learns: a data representation, an item selection heuristic, and a prediction function. Our model uses the item selection heuristic to construct a labeled support set for training the prediction function. Using the Omniglot and MovieLens datasets, we test o
Improving Viterbi is Hard:  Better Runtimes Imply Faster Clique Algorithms  Arturs Backurs 1 Christos Tzamos 1  Abstract  The classic algorithm of Viterbi computes the most likely path in a Hidden Markov Model (HMM) that results in a given sequence of ob- servations. It runs in time O(T n2) given a se- quence of T observations from a HMM with n states. Despite signiﬁcant interest in the prob- lem and prolonged effort by different communi- ties, no known algorithm achieves more than a polylogarit
Differentially Private Clustering in High-Dimensional Euclidean Spaces  Maria-Florina Balcan 1 Travis Dick 1 Yingyu Liang 2 Wenlong Mou 3 Hongyang Zhang 1  Abstract  We study the problem of clustering sensitive data while preserving the privacy of individu- als represented in the dataset, which has broad applications in practical machine learning and data analysis tasks. Although the problem has been widely studied in the context of low- dimensional, discrete spaces, much remains un- known conce
Strongly-Typed Agents are Guaranteed to Interact Safely  David Balduzzi 1  Abstract  As artiﬁcial agents proliferate, it is becoming in- creasingly important to ensure that their interac- tions with one another are well-behaved. In this paper, we formalize a common-sense notion of when algorithms are well-behaved: an algorithm is safe if it does no harm. Motivated by recent progress in deep learning, we focus on the spe- ciﬁc case where agents update their actions ac- cording to gradient descent
The Shattered Gradients Problem:  If resnets are the answer, then what is the question?  David Balduzzi 1 Marcus Frean 1 Lennox Leary 1 JP Lewis 1 2 Kurt Wan-Duo Ma 1 Brian McWilliams 3  Abstract  A long-standing obstacle to progress in deep learning is the problem of vanishing and ex- ploding gradients. Although, the problem has largely been overcome via carefully constructed initializations and batch normalization, archi- tectures incorporating skip-connections such as highway and resnets perf
Neural Taylor Approximations:  Convergence and Exploration in Rectiﬁer Networks  David Balduzzi 1 Brian McWilliams 2 Tony Butler-Yeoman 1  Abstract  Modern convolutional networks, incorporating rectiﬁers and max-pooling, are neither smooth nor convex; standard guarantees therefore do not apply. Nevertheless, methods from convex opti- mization such as gradient descent and Adam are widely used as building blocks for deep learning algorithms. This paper provides the ﬁrst conver- gence guarantee app
Spectral Learning from a Single Trajectory under Finite-State Policies  Borja Balle 1 Odalric-Ambrym Maillard 2  Abstract  We present spectral methods of moments for learning sequential models from a single trajec- tory, in stark contrast with the classical litera- ture that assumes the availability of multiple i.i.d. trajectories. Our approach leverages an efﬁcient SVD-based learning algorithm for weighted au- tomata and provides the ﬁrst rigorous analysis for learning many important models usi
Lost Relatives of the Gumbel Trick  Matej Balog 1 2 Nilesh Tripuraneni 3 Zoubin Ghahramani 1 4 Adrian Weller 1 5  Abstract  The Gumbel trick is a method to sample from a discrete probability distribution, or to estimate its normalizing partition function. The method re- lies on repeatedly applying a random perturba- tion to the distribution in a particular way, each time solving for the most likely conﬁguration. We derive an entire family of related methods, of which the Gumbel trick is one memb
Dynamic Word Embeddings  Robert Bamler 1 Stephan Mandt 1  Abstract  We present a probabilistic language model for time-stamped text data which tracks the se- mantic evolution of individual words over time. The model represents words and contexts by latent trajectories in an embedding space. At each moment in time, the embedding vectors are inferred from a probabilistic version of word2vec (Mikolov et al., 2013b). These em- bedding vectors are connected in time through a latent diffusion process.
End-to-End Differentiable Adversarial Imitation Learning  Nir Baram 1 Oron Anschel 1 Itai Caspi 1 Shie Mannor 1  Abstract  Generative Adversarial Networks (GANs) have been successfully applied to the problem of pol- icy imitation in a model-free setup. However, the computation graph of GANs, that include a stochastic policy as the generative model, is no longer differentiable end-to-end, which re- quires the use of high-variance gradient estima- tion. In this paper, we introduce the Model- based
Emulating the Expert: Inverse Optimization through Online Learning  Andreas Bärmann * 1 Sebastian Pokutta * 2 Oskar Schneider * 1  Abstract  In this paper, we demonstrate how to learn the objective function of a decision maker while only observing the problem input data and the deci- sion maker’s corresponding decisions over mul- tiple rounds. Our approach is based on online learning techniques and works for linear objec- tives over arbitrary sets for which we have a lin- ear optimization oracle
Unimodal Probability Distributions for Deep Ordinal Classiﬁcation  Christopher Beckham 1 Christopher Pal 1  Abstract  Probability distributions produced by the cross- entropy loss for ordinal classiﬁcation problems can possess undesired properties. We propose a straightforward technique to constrain discrete ordinal probability distributions to be unimodal via the use of the Poisson and binomial proba- bility distributions. We evaluate this approach in the context of deep learning on two large o
Globally Induced Forest: A Prepruning Compression Scheme  Jean-Michel Begon 1 Arnaud Joly 1 Pierre Geurts 1  Abstract  Tree-based ensemble models are heavy memory- wise. An undesired state of affairs consider- ing nowadays datasets, memory-constrained en- vironment and ﬁtting/prediction times. In this paper, we propose the Globally Induced Forest (GIF) to remedy this problem. GIF is a fast prepruning approach to build lightweight ensem- bles by iteratively deepening the current forest. It mixes 
End-to-End Learning for Structured Prediction Energy Networks  David Belanger 1 Bishan Yang 2 Andrew McCallum 1  Abstract  minimization (LeCun et al., 2006):  Structured Prediction Energy Networks (SPENs) are a simple, yet expressive family of struc- tured prediction models (Belanger & McCal- lum, 2016). An energy function over candidate structured outputs is given by a deep network, and predictions are formed by gradient-based optimization. This paper presents end-to-end learning for SPENs, whe
Learning to Discover Sparse Graphical Models  Eugene Belilovsky 1 2 3 Kyle Kastner 4 Gael Varoquaux 2 Matthew B. Blaschko 1  Abstract  We consider structure discovery of undirected graphical models from observational data. Infer- ring likely structures from few examples is a com- plex task often requiring the formulation of priors and sophisticated inference procedures. Popular methods rely on estimating a penalized maximum likelihood of the precision matrix. However, in these approaches structu
ADistributionalPerspectiveonReinforcementLearningMarcG.Bellemare*1WillDabney*1R´emiMunos1AbstractInthispaperweargueforthefundamentalimpor-tanceofthevaluedistribution:thedistributionoftherandomreturnreceivedbyareinforcementlearningagent.Thisisincontrasttothecom-monapproachtoreinforcementlearningwhichmodelstheexpectationofthisreturn,orvalue.Althoughthereisanestablishedbodyofliter-aturestudyingthevaluedistribution,thusfarithasalwaysbeenusedforaspeciﬁcpurposesuchasimplementingrisk-awarebehaviour.Web
Learning Texture Manifolds with the Periodic Spatial GAN  Urs Bergmann * 1 Nikolay Jetchev * 1 Roland Vollgraf 1  Abstract  This paper introduces a novel approach to tex- ture synthesis based on generative adversarial networks (GAN) (Goodfellow et al., 2014), and call this technique Periodic Spatial GAN (PS- GAN). The PSGAN has several novel abilities which surpass the current state of the art in tex- ture synthesis. First, we can learn multiple tex- tures, periodic or non-periodic, from dataset
Differentially Private Learning of Undirected Graphical Models Using  Collective Graphical Models  Garrett Bernstein 1 Ryan McKenna 1 Tao Sun 1 Daniel Sheldon 1 2 Michael Hay 3 Gerome Miklau 1  Abstract  We investigate the problem of learning discrete, undirected graphical models in a differentially private way. We show that the approach of releas- ing noisy sufﬁcient statistics using the Laplace mechanism achieves a good trade-off between privacy, utility, and practicality. A naive learn- ing a
Efﬁcient Online Bandit Multiclass Learning with ˜O(√T ) Regret  Alina Beygelzimer 1 Francesco Orabona 2 Chicheng Zhang 3  Abstract  We present an efﬁcient second-order algorithm √T )1 regret for the bandit online mul- with ˜O( 1 η ticlass problem. The regret bound holds simulta- neously with respect to a family of loss functions parameterized by η, for a range of η restricted by the norm of the competitor. The family of loss functions ranges from hinge loss (η = 0) to squared hinge loss (η = 1).
Guarantees for Greedy Maximization of  Non-submodular Functions with Applications  Andrew An Bian 1 Joachim M. Buhmann 1 Andreas Krause 1 Sebastian Tschiatschek 1  Abstract  We investigate the performance of the standard GREEDY algorithm for cardinality constrained maximization of non-submodular nondecreasing set functions. While there are strong theoretical guarantees on the performance of GREEDY for maximizing submodular functions, there are few guarantees for non-submodular ones. However, GRE
Robust Submodular Maximization:  A Non-Uniform Partitioning Approach  Ilija Bogunovic 1 Slobodan Mitrovi´c 2 Jonathan Scarlett 1 Volkan Cevher 1  Abstract  We study the problem of maximizing a monotone submodular function subject to a cardinality con- straint k, with the added twist that a number of items ⌧ from the returned set may be removed. We focus on the worst-case setting considered in (Orlin et al., 2016), in which a constant-factor ap- proximation guarantee was given for ⌧ = o(pk). In t
Unsupervised Learning by Predicting Noise  Piotr Bojanowski 1 Armand Joulin 1  Abstract  Convolutional neural networks provide visual features that perform well in many computer vi- sion applications. However, training these net- works requires large amounts of supervision; this paper introduces a generic framework to train such networks, end-to-end, with no supervision. We propose to ﬁx a set of target representations, called Noise As Targets (NAT), and to constrain the deep features to align t
Adaptive Neural Networks for Efﬁcient Inference  Tolga Bolukbasi 1 Joseph Wang 2 Ofer Dekel 3 Venkatesh Saligrama 1  Abstract  We present an approach to adaptively utilize deep neural networks in order to reduce the evaluation time on new examples without loss of accuracy. Rather than attempting to redesign or approxi- mate existing networks, we propose two schemes that adaptively utilize networks. We ﬁrst pose an adaptive network evaluation scheme, where we learn a system to adaptively choose t
Compressed Sensing using Generative Models  Ashish Bora 1 Ajil Jalal 2 Eric Price 1 Alexandros G. Dimakis 2  Abstract  The goal of compressed sensing is to estimate a vector from an underdetermined system of noisy linear measurements, by making use of prior knowledge on the structure of vectors in the rel- evant domain. For almost all results in this lit- erature, the structure is represented by sparsity in a well-chosen basis. We show how to achieve guarantees similar to standard compressed sen
Programming with a Differentiable Forth Interpreter  Matko Boˇsnjak 1 Tim Rockt¨aschel 2 Jason Naradowsky 3 Sebastian Riedel 1  Abstract  Given that in practice training data is scarce for all but a small set of problems, a core question is how to incorporate prior knowledge into a model. In this paper, we consider the case of prior procedural knowledge for neural networks, such as knowing how a program should traverse a sequence, but not what local actions should be performed at each step. To t
Practical Gauss-Newton Optimisation for Deep Learning  Aleksandar Botev 1 Hippolyt Ritter 1 David Barber 1 2  Abstract  We present an efﬁcient block-diagonal approxi- mation to the Gauss-Newton matrix for feedfor- ward neural networks. Our resulting algorithm is competitive against state-of-the-art ﬁrst-order optimisation methods, with sometimes signiﬁ- cant improvement in optimisation performance. Unlike ﬁrst-order methods, for which hyperpa- rameter tuning of the optimisation parameters is oft
Lazifying Conditional Gradient Algorithms  G´abor Braun * 1 Sebastian Pokutta * 1 Daniel Zink * 1  Abstract  Conditional gradient algorithms (also often called Frank-Wolfe algorithms) are popular due to their simplicity of only requiring a linear optimization oracle and more recently they also gained signif- icant traction for online learning. While simple in principle, in many cases the actual implemen- tation of the linear optimization oracle is costly. We show a general method to lazify vario
Clustering High Dimensional Dynamic Data Streams  Vladimir Braverman 1 Gereon Frahling 2 Harry Lang 1 Christian Sohler 3 Lin F. Yang 1  Abstract  We present data streaming algorithms for the k- median problem in high-dimensional dynamic geometric data streams, i.e. streams allowing both insertions and deletions of points from a discrete Euclidean space {1, 2, . . . ∆}d. Our al- gorithms use k(cid:15)−2poly(d log ∆) space/time and maintain with high probability a small weighted set of points (a c
On the Sampling Problem for Kernel Quadrature  Franc¸ois-Xavier Briol 1 2 Chris J. Oates 3 4 Jon Cockayne 1 Wilson Ye Chen 5 Mark Girolami 2 4  Abstract  The standard Kernel Quadrature method for nu- merical integration with random point sets (also called Bayesian Monte Carlo) is known to con- verge in root mean square error at a rate de- termined by the ratio s/d, where s and d en- code the smoothness and dimension of the in- tegrand. However, an empirical investigation reveals that the rate co
Reduced Space and Faster Convergence in Imperfect-Information Games via Pruning  Noam Brown 1 Tuomas Sandholm 1  Abstract  Iterative algorithms such as Counterfactual Regret Minimization (CFR) are the most popular way to solve large zero-sum imperfect-information games. In this paper we introduce Best-Response Pruning (BRP), an improvement to iterative algo- rithms such as CFR that allows poorly-performing actions to be temporarily pruned. We prove that when using CFR in zero-sum games, adding B
Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs  Alon Brutzkus 1 Amir Globerson 1  Abstract  Deep learning models are often successfully trained using gradient descent, despite the worst case hardness of the underlying non-convex op- timization problem. The key question is then un- der what conditions can one prove that optimiza- tion will succeed. Here we provide a strong re- sult of this kind. We consider a neural net with one hidden layer and a convolutional structure wit
Deep Tensor Convolution on Multicores  David Budden 1 Alexander Matveev 1 Shibani Santurkar 1 Shraman Ray Chaudhuri 1 Nir Shavit 1  Abstract  Deep convolutional neural networks (ConvNets) of 3-dimensional kernels allow joint modeling of spatiotemporal features. These networks have improved performance of video and volumetric image analysis, but have been limited in size due to the low memory ceiling of GPU hardware. Ex- isting CPU implementations overcome this con- straint but are impractically 
Multi-objective Bandits: Optimizing the Generalized Gini Index  R´obert Busa-Fekete 1 Bal´azs Sz¨or´enyi 2 3 Paul Weng 4 5 Shie Mannor 3  Abstract  We study the multi-armed bandit (MAB) problem where the agent receives a vectorial feedback that encodes many possibly competing objectives to be optimized. The goal of the agent is to ﬁnd a policy, which can optimize these objectives simultaneously in a fair way. This multi-objective online optimization problem is formalized by us- ing the Generaliz
Priv’IT: Private and Sample Efﬁcient Identity Testing  Bryan Cai * 1 Constantinos Daskalakis * 1 Gautam Kamath * 1  Abstract  We develop differentially private hypothesis test- ing methods for the small sample regime. Given a sample D from a categorical distribution p over some domain Σ, an explicitly described distri- bution q over Σ, some privacy parameter ε, ac- curacy parameter α, and requirements βI and βII for the type I and type II errors of our test, the goal is to distinguish between p 
Second-Order Kernel Online Convex Optimization with Adaptive Sketching  Daniele Calandriello 1 Alessandro Lazaric 1 Michal Valko 1  Abstract  Kernel online convex optimization (KOCO) is a framework combining the expressiveness of non- parametric kernel models with the regret guaran- tees of online learning. First-order KOCO meth- ods such as functional gradient descent require only O(t) time and space per iteration, and, when √ the only information on the losses is their con- vexity, achieve a m
“Convex Until Proven Guilty”: Dimension-Free Acceleration  of Gradient Descent on Non-Convex Functions  Yair Carmon John C. Duchi Oliver Hinder Aaron Sidford 1  Abstract  We develop and analyze a variant of Nesterov’s accelerated gradient descent (AGD) for mini- mization of smooth non-convex functions. We prove that one of two cases occurs: either our AGD variant converges quickly, as if the function was convex, or we produce a cer- tiﬁcate that the function is “guilty” of being non-convex. This
Sliced Wasserstein Kernel for Persistence Diagrams  Mathieu Carri`ere 1 Marco Cuturi 2 Steve Oudot 1  Abstract  Persistence diagrams (PDs) play a key role in topological data analysis (TDA), in which they are routinely used to describe topological prop- erties of complicated shapes. PDs enjoy strong stability properties and have proven their utility in various learning contexts. They do not, how- ever, live in a space naturally endowed with a Hilbert structure and are usually compared with non-H
Multiple Clustering Views from Multiple Uncertain Experts  Yale Chang 1 Junxiang Chen 1 Michael H. Cho 2 Peter J. Castaldi 2 Edwin K. Silverman 2 Jennifer G. Dy 1  Abstract  Expert input can improve clustering perfor- mance. In today’s collaborative environment, the availability of crowdsourced multiple expert in- put is becoming common. Given multiple ex- perts’ inputs, most existing approaches can only discover one clustering structure. However, data is multi-faceted by nature and can be clust
Uncertainty Assessment and False Discovery Rate Control  in High-Dimensional Granger Causal Inference  Aditya Chaudhry 1 Pan Xu 2 Quanquan Gu 2  Abstract  Causal inference among high-dimensional time series data proves an important research problem in many ﬁelds. While in the classical regime one often establishes causality among time series via a concept known as “Granger causality,” exist- ing approaches for Granger causal inference in high-dimensional data lack the means to char- acterize the
Active Heteroscedastic Regression  Kamalika Chaudhuri 1 Prateek Jain 2 Nagarajan Natarajan 2  Abstract  An active learner is given a model class Θ, a large sample of unlabeled data drawn from an under- lying distribution and access to a labeling oracle that can provide a label for any of the unlabeled instances. The goal of the learner is to ﬁnd a model θ ∈ Θ that ﬁts the data to a given accuracy while making as few label queries to the oracle as possible. In this work, we consider a theoretical
Combining Model-Based and Model-Free Updates for Trajectory-Centric  Reinforcement Learning  Yevgen Chebotar* 1 2 Karol Hausman* 1 Marvin Zhang* 3 Gaurav Sukhatme 1 Stefan Schaal 1 2 Sergey Levine 3  Abstract  learning algorithms for  Reinforcement real- world robotic applications must be able to han- dle complex, unknown dynamical systems while maintaining data-efﬁcient learning. These re- quirements are handled well by model-free and model-based RL approaches, respectively. In this work, we ai
Robust Structured Estimation with Single-Index Models  Sheng Chen 1 Arindam Banerjee 1  Abstract  In this paper, we investigate general single-index models (SIMs) in high dimensions. Based on U-statistics, we propose two types of robust es- timators for the recovery of model parameters, which can be viewed as generalizations of sev- eral existing algorithms for one-bit compressed sensing (1-bit CS). With minimal assumption on noise, the statistical guarantees are established for the generalized 
Adaptive Multiple-Arm Identiﬁcation  Jiecao Chen * 1 Xi Chen * 2 Qin Zhang * 1 Yuan Zhou * 1  Abstract  We study the problem of selecting K arms with the highest expected rewards in a stochastic n- armed bandit game. This problem has a wide range of applications, e.g., A/B testing, crowd- sourcing, simulation optimization. Our goal is to develop a PAC algorithm, which, with prob- ability at least 1 − δ, identiﬁes a set of K arms with the aggregate regret at most (cid:15). The notion of aggregate
Dueling Bandits with Weak Regret  Bangrui Chen 1 Peter I. Frazier 1  Abstract  We consider online content recommendation with implicit feedback through pairwise comparisons, formalized as the so-called dueling bandit prob- lem. We study the dueling bandit problem in the Condorcet winner setting, and consider two notions of regret: the more well-studied strong regret, which is 0 only when both arms pulled are the Condorcet winner; and the less well-studied weak regret, which is 0 if either arm pu
Strong NP-Hardness for Sparse Optimization with Concave Penalty Functions  Yichen Chen 1 Dongdong Ge 2 Mengdi Wang 1 Zizhuo Wang 3 Yinyu Ye 4 Hao Yin 4  Abstract  Consider the regularized sparse minimization problem, which involves empirical sums of loss functions for n data points (each of dimension d) and a nonconvex sparsity penalty. We prove that ﬁnding an O(nc1dc2)-optimal solution to the regularized sparse optimization problem is strongly NP-hard for any c1, c2 ∈ [0, 1) such that c1 + c2 <
Learning to Learn without Gradient Descent by Gradient Descent  Yutian Chen 1 Matthew W. Hoffman 1 Sergio G´omez Colmenarejo 1 Misha Denil 1 Timothy P. Lillicrap 1  Matt Botvinick 1 Nando de Freitas 1  Abstract  We learn recurrent neural network optimizers trained on simple synthetic functions by gradi- ent descent. We show that these learned optimiz- ers exhibit a remarkable degree of transfer in that they can be used to efﬁciently optimize a broad range of derivative-free black-box functions, 
Identiﬁcation and Model Testing in Linear Structural Equation Models using  Auxiliary Variables  Bryant Chen 1 Daniel Kumor 2 Elias Bareinboim 2  Abstract  We developed a novel approach to identiﬁca- tion and model testing in linear structural equa- tion models (SEMs) based on auxiliary variables (AVs), which generalizes a widely-used family of methods known as instrumental variables. The identiﬁcation problem is concerned with the con- ditions under which causal parameters can be uniquely estim
Toward Efﬁcient and Accurate Covariance Matrix  Estimation on Compressed Data  Xixian Chen 1 2 Michael R. Lyu 1 2 Irwin King 1 2  Abstract  Estimating covariance matrices is a fundamen- tal technique in various domains, most notably in machine learning and signal processing. To tackle the challenges of extensive communica- tion costs, large storage capacity requirements, and high processing time complexity when han- dling massive high-dimensional and distributed data, we propose an efﬁcient and 
Online Partial Least Square Optimization:  Dropping Convexity for Better Efﬁciency and Scalability  Zhehui Chen 1 Lin F. Yang 2 Chris J. Li 3 Tuo Zhao 1  Abstract  Multiview representation learning is popular for latent factor analysis. Many existing approaches formulate the multiview representation learning as convex optimization problems, where global optima can be obtained by certain algorithms in polynomial time. However, many evidences have corroborated that heuristic nonconvex ap- proaches
Learning to Aggregate Ordinal Labels by Maximizing Separating Width  Guangyong Chen 1 Shengyu Zhang 1 Di Lin 2 Hui Huang 2 Pheng Ann Heng 1 3  Abstract  While crowdsourcing has been a cost and time efﬁcient method to label massive samples, one critical issue is quality control, for which the key challenge is to infer the ground truth from noisy or even adversarial data by various users. A large class of crowdsourcing problems, such as those involving age, grade, level, or stage, have an or- dina
Nearly Optimal Robust Matrix Completion  Yeshwanth Cherapanamjeri 1 Kartik Gupta 1 Prateek Jain 1  Abstract  In this paper, we consider the problem of Ro- bust Matrix Completion (RMC) where the goal is to recover a low-rank matrix by observing a small number of its entries out of which a few can be arbitrarily corrupted. We propose a simple projected gradient descent-based method to estimate the low-rank matrix that alternately performs a projected gradient descent step and cleans up a few of th
Algorithms for (cid:96)p Low-Rank Approximation  Flavio Chierichetti 1 Sreenivas Gollapudi 2 Ravi Kumar 2 Silvio Lattanzi 3 Rina Panigrahy 2  David P. Woodruff 4  Abstract  We consider the problem of approximating a given matrix by a low-rank matrix so as to mini- mize the entry-wise (cid:96)p-approximation error, for any p ≥ 1; the case p = 2 is the classical SVD problem. We obtain the ﬁrst provably good ap- proximation algorithms for this version of low- rank approximation that work for every 
MEC: Memory-efﬁcient Convolution for Deep Neural Network  Minsik Cho 1 Daniel Brand 1  Abstract  Convolution is a critical component in modern deep neural networks, thus several algorithms for convolution have been developed. Direct con- volution is simple but suffers from poor per- formance. As an alternative, multiple indirect methods have been proposed including im2col- based convolution, FFT-based convolution, or Winograd-based algorithm. However, all these indirect methods have high memory-
On Relaxing Determinism in Arithmetic Circuits  Arthur Choi 1 Adnan Darwiche 1  Abstract  The past decade has seen a signiﬁcant inter- est in learning tractable probabilistic represen- tations. Arithmetic circuits (ACs) were among the ﬁrst proposed tractable representations, with some subsequent representations being instances of ACs with weaker or stronger properties. In this paper, we provide a formal basis under which variants on ACs can be compared, and where the precise roles and semantics 
Improving Stochastic Policy Gradients in Continuous Control with Deep  Reinforcement Learning using the Beta Distribution  Po-Wei Chou 1 Daniel Maturana 1 Sebastian Scherer 1  Abstract  Recently, reinforcement learning with deep neu- ral networks has achieved great success in chal- lenging continuous control problems such as 3D locomotion and robotic manipulation. However, in real-world control problems, the actions one can take are bounded by physical constraints, which introduces a bias when t
On Kernelized Multi-armed Bandits  Sayak Ray Chowdhury 1 Aditya Gopalan 1  Abstract  We consider the stochastic bandit problem with a continuous set of arms, with the expected re- ward function over the arms assumed to be ﬁxed but unknown. We provide two new Gaussian process-based algorithms for continuous bandit optimization – Improved GP-UCB (IGP-UCB) and GP-Thomson sampling (GP-TS), and derive corresponding regret bounds. Speciﬁcally, the bounds hold when the expected reward function belongs 
Parseval Networks: Improving Robustness to Adversarial Examples  Moustapha Cisse 1 Piotr Bojanowski 1 Edouard Grave 1 Yann Dauphin 1 Nicolas Usunier 1  Abstract  We introduce Parseval networks, a form of deep neural networks in which the Lipschitz constant of linear, convolutional and aggregation layers is constrained to be smaller than 1. Parseval networks are empirically and theoretically mo- tivated by an analysis of the robustness of the predictions made by deep neural networks when their in
Deep Latent Dirichlet Allocation with Topic-Layer-Adaptive  Stochastic Gradient Riemannian MCMC  Yulai Cong 1 Bo Chen 1 Hongwei Liu 1 Mingyuan Zhou 2  Abstract  It is challenging to develop stochastic gradient based scalable inference for deep discrete latent variable models (LVMs), due to the difﬁculties in not only computing the gradients, but also adapting the step sizes to different latent factors and hidden layers. For the Poisson gamma be- lief network (PGBN), a recently proposed deep disc
AdaNet: Adaptive Structural Learning of Artiﬁcial Neural Networks  Corinna Cortes 1 Xavier Gonzalvo 1 Vitaly Kuznetsov 1 Mehryar Mohri 2 1 Scott Yang 2  Abstract  We present new algorithms for adaptively learn- ing artiﬁcial neural networks. Our algorithms (ADANET) adaptively learn both the structure of the network and its weights. They are based on a solid theoretical analysis, including data-dependent generalization guarantees that we prove and discuss in detail. We report the re- sults of lar
Random Feature Expansions for Deep Gaussian Processes  Kurt Cutajar 1 Edwin V. Bonilla 2 Pietro Michiardi 1 Maurizio Filippone 1  Abstract  The composition of multiple Gaussian Processes as a Deep Gaussian Process (DGP) enables a deep probabilistic nonparametric approach to ﬂexibly tackle complex machine learning problems with sound quantiﬁcation of uncertainty. Existing in- ference approaches for DGP models have lim- ited scalability and are notoriously cumbersome to construct. In this work we 
Soft-DTW: a Differentiable Loss Function for Time-Series  Marco Cuturi 1 Mathieu Blondel 2  Abstract  We propose in this paper a differentiable learning loss between time series, building upon the cel- ebrated dynamic time warping (DTW) discrep- ancy. Unlike the Euclidean distance, DTW can compare time series of variable size and is ro- bust to shifts or dilatations across the time di- mension. To compute DTW, one typically solves a minimal-cost alignment problem between two time series using dy
Understanding Synthetic Gradients and Decoupled Neural Interfaces  Wojciech Marian Czarnecki 1 Grzegorz Swirszcz 1 Max Jaderberg 1 Simon Osindero 1 Oriol Vinyals 1  Koray Kavukcuoglu 1  Abstract  When training neural networks, the use of Syn- thetic Gradients (SG) allows layers or modules to be trained without update locking – without waiting for a true error gradient to be backprop- agated – resulting in Decoupled Neural Inter- faces (DNIs). This unlocked ability of being able to update parts o
Stochastic Generative Hashing  Bo Dai* 1 Ruiqi Guo* 2 Sanjiv Kumar 2 Niao He 3 Le Song 1  Abstract  Learning-based binary hashing has become a powerful paradigm for fast search and retrieval in massive databases. However, due to the re- quirement of discrete outputs for the hash func- tions, learning such functions is known to be very challenging. In addition, the objective func- tions adopted by existing hashing techniques are mostly chosen heuristically. In this paper, we propose a novel gener
Logarithmic Time One-Against-Some  Hal Daum´e III * 1 Nikos Karampatziakis * 2 John Langford * 2 Paul Mineiro * 2  Abstract  We create a new online reduction of multiclass classiﬁcation to binary classiﬁcation for which training and prediction time scale logarithmically with the number of classes. We show that sev- eral simple techniques give rise to an algorithm which is superior to previous logarithmic time classiﬁcation approaches while competing with one-against-all in space. The core constr
An Inﬁnite Hidden Markov Model With Similarity-Biased Transitions  Colin Reimer Dawson 1 Chaofan Huang 1 Clayton T. Morrison 2  Abstract  We describe a generalization of the Hierarchical Dirichlet Process Hidden Markov Model (HDP- HMM) which is able to encode prior informa- tion that state transitions are more likely be- tween “nearby” states. This is accomplished by deﬁning a similarity function on the state space and scaling transition probabilities by pair- wise similarities, thereby inducing
Distributed Batch Gaussian Process Optimization  Erik A. Daxberger 1 Bryan Kian Hsiang Low 2  Abstract  This paper presents a novel distributed batch Gaussian process upper conﬁdence bound (DB-GP-UCB) algorithm for performing batch Bayesian optimization (BO) of highly complex, costly-to-evaluate black-box objective functions. In contrast to existing batch BO algorithms, DB- GP-UCB can jointly optimize a batch of inputs (as opposed to selecting the inputs of a batch one at a time) while still pre
Consistency Analysis for Binary Classiﬁcation Revisited  Krzysztof Dembczy´nski 1 Wojciech Kotłowski 1 Oluwasanmi Koyejo 2 Nagarajan Natarajan 3  Abstract  Statistical learning theory is at an inﬂection point enabled by recent advances in understanding and optimizing a wide range of metrics. Of partic- ular interest are non-decomposable metrics such as the F-measure and the Jaccard measure which cannot be represented as a simple average over examples. Non-decomposability is the primary source of
iSurvive: An Interpretable, Event-time Prediction Model for mHealth  Walter H. Dempsey * 1 Alexander Moreno * 2 Christy K. Scott 3 Michael L. Dennis 3 David H. Gustafson 4  Susan A. Murphy 1 James M. Rehg 2  Abstract  An important mobile health (mHealth) task is the use of multimodal data, such as sensor streams and self-report, to construct interpretable time- to-event predictions of, for example, lapse to Interpretability of alcohol or illicit drug use. the prediction model is important for ac
Image-to-Markup Generation with Coarse-to-Fine Attention  Yuntian Deng 1 Anssi Kanervisto 2 Jeffrey Ling 1 Alexander M. Rush 1  Abstract  We present a neural encoder-decoder model to convert images into presentational markup based on a scalable coarse-to-ﬁne attention mechanism. Our method is evaluated in the context of image- to-LaTeX generation, and we introduce a new dataset of real-world rendered mathematical ex- pressions paired with LaTeX markup. We show that unlike neural OCR techniques u
RobustFill: Neural Program Learning under Noisy I/O  Jacob Devlin * 1 Jonathan Uesato * 2 Surya Bhupatiraju * 2 Rishabh Singh 1 Abdel-rahman Mohamed 1  Pushmeet Kohli 1  Abstract  The problem of automatically generating a com- puter program from some speciﬁcation has been studied since the early days of AI. Recently, two competing approaches for automatic program learning have received signiﬁcant attention: (1) neural program synthesis, where a neural net- work is conditioned on input/output (I/
Being Robust (in High Dimensions) Can Be Practical  Ilias Diakonikolas * 1 Gautam Kamath * 2 Daniel M. Kane * 3 Jerry Li * 2 Ankur Moitra * 2 Alistair Stewart * 1  Abstract  Robust estimation is much more challenging in high dimensions than it is in one dimension: Most techniques either lead to intractable opti- mization problems or estimators that can toler- ate only a tiny fraction of errors. Recent work in theoretical computer science has shown that, in appropriate distributional models, it i
Probabilistic Path Hamiltonian Monte Carlo  Vu Dinh * 1 Arman Bilge * 1 2 Cheng Zhang * 1 Frederick A. Matsen IV 1  Abstract  Hamiltonian Monte Carlo (HMC) is an efﬁcient and effective means of sampling posterior distri- butions on Euclidean space, which has been ex- tended to manifolds with boundary. However, some applications require an extension to more general spaces. For example, phylogenetic (evo- lutionary) trees are deﬁned in terms of both a discrete graph and associated continuous param
A Divergence Bound for Hybrids of MCMC and Variational Inference and an  Application to Langevin Dynamics and SGVI  Justin Domke 1  Abstract  Two popular classes of methods for approxi- mate inference are Markov chain Monte Carlo (MCMC) and variational inference. MCMC tends to be accurate if run for a long enough time, while variational inference tends to give better approximations at shorter time horizons. How- ever, the amount of time needed for MCMC to exceed the performance of variational me
Dance Dance Convolution  Chris Donahue 1 Zachary C. Lipton 2 Julian McAuley 2  Abstract  Dance Dance Revolution (DDR) is a popular rhythm-based video game. Players perform steps on a dance platform in synchronization with mu- sic as directed by on-screen step charts. While many step charts are available in standardized packs, players may grow tired of existing charts, or wish to dance to a song for which no chart ex- ists. We introduce the task of learning to chore- ograph. Given a raw audio tra
Stochastic Variance Reduction Methods for Policy Evaluation  Simon S. Du 1 Jianshu Chen 2 Lihong Li 2 Lin Xiao 2 Dengyong Zhou 2  Abstract  Policy evaluation is concerned with estimating the value function that predicts long-term val- ues of states under a given policy. It is a cru- cial step in many reinforcement-learning algo- rithms. In this paper, we focus on policy eval- uation with linear function approximation over a ﬁxed dataset. We ﬁrst transform the empiri- cal policy evaluation proble
Rule-Enhanced Penalized Regression by Column Generation  using Rectangular Maximum Agreement  Jonathan Eckstein 1 Noam Goldberg 2 Ai Kagawa 3  Abstract  We describe a procedure enhancing L1-penalized regression by adding dynamically generated rules describing multidimensional “box” sets. Our rule-adding procedure is based on the classical column generation method for high- dimensional linear programming. The pricing problem for our column generation procedure reduces to the NP-hard rectangular m
Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders  Jesse Engel * 1 Cinjon Resnick * 1 Adam Roberts 1 Sander Dieleman 2 Mohammad Norouzi 1 Douglas Eck 1  Karen Simonyan 2  Abstract  Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these ar- eas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder 
Statistical Inference for Incomplete Ranking Data:  The Case of Rank-Dependent Coarsening  Mohsen Ahmadi Fahandar 1 Eyke H¨ullermeier 1 In´es Couso 2  Abstract  We consider the problem of statistical inference for ranking data, speciﬁcally rank aggregation, un- der the assumption that samples are incomplete in the sense of not comprising all choice alternatives. In contrast to most existing methods, we explicitly model the process of turning a full ranking into an incomplete one, which we call t
Maximum Selection and Ranking under Noisy Comparisons  Moein Falahatgar 1 Alon Orlitsky 1 Venkatadheeraj Pichapati 1 Ananda Theertha Suresh 2  Abstract  We consider (✏, )-PAC maximum-selection and ranking using pairwise comparisons for general probabilistic models whose comparison proba- bilities satisfy strong stochastic transitivity and stochastic triangle inequality. Modifying the popular knockout tournament, we propose a simple maximum-selection algorithm that uses  O n ✏21 + log 1   compari
Fake News Mitigation via Point Process Based Intervention  Mehrdad Farajtabar 1 Jiachen Yang 1 Xiaojing Ye 2 Huan Xu 3 Rakshit Trivedi 1 Elias Khalil 1 Shuang Li 3  Le Song 1 Hongyuan Zha 1  Abstract  We propose the ﬁrst multistage intervention framework that tackles fake news in social net- works by combining reinforcement learning with a point process network activity model. The spread of fake news and mitigation events within the network is modeled by a multivariate Hawkes process with additi
Regret Minimization in Behaviorally-Constrained Zero-Sum Games  Gabriele Farina 1 Christian Kroer 1 Tuomas Sandholm 1  Abstract  No-regret learning has emerged as a powerful tool for solving extensive-form games. This was facilitated by the counterfactual-regret minimiza- tion (CFR) framework, which relies on the in- stantiation of regret minimizers for simplexes at each information set of the game. We use an instantiation of the CFR framework to develop algorithms for solving behaviorally-const
Coresets for Vector Summarization with Applications to Network Graphs  Dan Feldman 1 Sedat Ozer 2 Daniela Rus 2  Abstract  (cid:80)  We provide a deterministic data summarization algorithm that approximates the mean ¯p = p∈P p of a set P of n vectors in Rd, by a 1 n weighted mean ˜p of a subset of O(1/ε) vectors, i.e., independent of both n and d. We prove that the squared Euclidean distance between ¯p and ˜p is at most ε multiplied by the variance of P . We use this algorithm to maintain an app
Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks  Chelsea Finn 1 Pieter Abbeel 1 2 Sergey Levine 1  Abstract  We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is com- patible with any model trained with gradient de- scent and applicable to a variety of different learning problems, including classiﬁcation, re- gression, and reinforcement learning. The goal of meta-learning is to train a model on a vari- ety of learning tasks, such that it can
Input Switched Afﬁne Networks: An RNN Architecture Designed for  Interpretability  Jakob N. Foerster * 1 Justin Gilmer * 2 Jascha Sohl-Dickstein 3 Jan Chorowski 4 David Sussillo 3  Abstract  There exist many problem domains where the interpretability of neural network models is es- sential for deployment. Here we introduce a re- current architecture composed of input-switched afﬁne transformations – in other words an RNN without any explicit nonlinearities, but with input- dependent recurrent we
Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning  Jakob Foerster * 1 Nantas Nardelli * 1 Gregory Farquhar 1 Triantafyllos Afouras 1  Philip. H. S. Torr 1 Pushmeet Kohli 2 Shimon Whiteson 1  Abstract  Many real-world problems, such as network packet routing and urban trafﬁc control, are naturally modeled as multi-agent reinforcement learning (RL) problems. However, existing multi-agent RL methods typically scale poorly in the problem size. Therefore, a key challenge is to
Counterfactual Data-Fusion for Online Reinforcement Learners  Andrew Forney 1 Judea Pearl 1 Elias Bareinboim 2  Abstract  The Multi-Armed Bandit problem with Un- observed Confounders (MABUC) considers decision-making settings where unmeasured variables can inﬂuence both the agent’s deci- sions and received rewards (Bareinboim et al., 2015). Recent ﬁndings showed that unobserved confounders (UCs) pose a unique challenge to al- gorithms based on standard randomization (i.e., experimental data); if
Forward and Reverse Gradient-Based Hyperparameter Optimization  Luca Franceschi 1 2 Michele Donini 1 Paolo Frasconi 3 Massimiliano Pontil 1 2  Abstract  We study two procedures (reverse-mode and forward-mode) for computing the gradient of the validation error with respect to the hyperparame- ters of any iterative learning algorithm such as stochastic gradient descent. These procedures mirror two methods of computing gradients for recurrent neural networks and have different trade-offs in terms o
Learning to Detect Sepsis with a Multitask Gaussian Process RNN Classiﬁer  Joseph Futoma 1 Sanjay Hariharan 1 Katherine Heller 1  Abstract  We present a scalable end-to-end classiﬁer that uses streaming physiological and medication data to accurately predict the onset of sepsis, a life-threatening complication from infections that has high mortality and morbidity. Our pro- posed framework models the multivariate trajec- tories of continuous-valued physiological time series using multitask Gaussi
Deep Bayesian Active Learning with Image Data  Yarin Gal 1 2 Riashat Islam 1 Zoubin Ghahramani 1 3  Abstract  Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difﬁculties when used in an active learn- ing setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent ad- vances in deep learning, on the other hand, are no- 
Local-to-Global Bayesian Network Structure Learning  Tian Gao 1 Kshitij Fadnis 1 Murray Campbell 1  Abstract  We introduce a new local-to-global structure learning algorithm, called graph growing struc- ture learning (GGSL), to learn Bayesian network (BN) structures. GGSL starts at a (random) node and then gradually expands the learned structure through a series of local learning steps. At each local learning step, the proposed algorithm only needs to revisit a subset of the learned nodes, consi
Communication-efﬁcient Algorithms for  Distributed Stochastic Principal Component Analysis  Dan Garber 1 Ohad Shamir 2 Nathan Srebro 3  Abstract  We study the fundamental problem of Principal Component Analysis in a statistical distributed setting in which each machine out of m stores a sample of n points sampled i.i.d. from a sin- gle unknown distribution. We study algorithms for estimating the leading principal component of the population covariance matrix that are both communication-efﬁcient 
Differentiable Programs with Neural Libraries  Alexander L. Gaunt 1 Marc Brockschmidt 1 Nate Kushman 1 Daniel Tarlow 2  Abstract  We develop a framework for combining differen- tiable programming languages with neural net- works. Using this framework we create end-to- end trainable systems that learn to write inter- pretable algorithms with perceptual components. We explore the beneﬁts of inductive biases for strong generalization and modularity that come from the program-like structure of our m
Zonotope Hit-and-run for Efﬁcient Sampling from Projection DPPs  Guillaume Gautier 1 2 R´emi Bardenet 1 Michal Valko 2  Abstract  Determinantal point processes (DPPs) are distri- butions over sets of items that model diversity us- ing kernels. Their applications in machine learn- ing include summary extraction and recommen- dation systems. Yet, the cost of sampling from a DPP is prohibitive in large-scale applications, which has triggered an effort towards efﬁcient approximate samplers. We build
No Spurious Local Minima in Nonconvex Low Rank Problems:  A Uniﬁed Geometric Analysis  Rong Ge 1 Chi Jin 2 Yi Zheng 1  Abstract  In this paper we develop a new framework that captures the common landscape underlying the common non-convex low-rank matrix problems including matrix sensing, matrix completion and robust PCA. In particular, we show for all above problems (including asymmetric cases): 1) all lo- cal minima are also globally optimal; 2) no high- order saddle points exists. These result
Convolutional Sequence to Sequence Learning  Jonas Gehring 1 Michael Auli 1 David Grangier 1 Denis Yarats 1 Yann N. Dauphin 1  Abstract  The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training to better exploit the GPU hardware 
On Context-Dependent Clustering of Bandits  Claudio Gentile 1 Shuai Li 2 Purushottam Kar 3 Alexandros Karatzoglou 4 Giovanni Zappella 5 Evans Etrue 1  Abstract  We investigate a novel cluster-of-bandit algo- rithm CAB for collaborative recommendation tasks that implements the underlying feedback sharing mechanism by estimating user neigh- borhoods in a context-dependent manner. CAB makes sharp departures from the state of the art by incorporating collaborative effects into infer- ence, as well a
Neural Message Passing for Quantum Chemistry  Justin Gilmer 1 Samuel S. Schoenholz 1 Patrick F. Riley 2 Oriol Vinyals 3 George E. Dahl 1  Abstract  Supervised learning on molecules has incredi- ble potential to be useful in chemistry, drug dis- covery, and materials science. Luckily, sev- eral promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure
Convex Phase Retrieval without Lifting via PhaseMax  Tom Goldstein * 1 Christoph Studer * 2  Abstract  Semideﬁnite relaxation methods transform a va- riety of non-convex optimization problems into convex problems, but square the number of vari- ables. We study a new type of convex relaxation for phase retrieval problems, called PhaseMax, that convexiﬁes the underlying problem without lifting. The resulting problem formulation can be solved using standard convex optimization routines, while still
Preferential Bayesian Optimization  Javier Gonz´alez 1 Zhenwen Dai 1 Andreas Damianou 1 Neil D. Lawrence 1 2  Abstract  Bayesian optimization (BO) has emerged during the last few years as an effective approach to opti- mizing black-box functions where direct queries of the objective are expensive. In this paper we consider the case where direct access to the func- tion is not possible, but information about user preferences is. Such scenarios arise in problems where human preferences are modeled
Measuring Sample Quality with Kernels  Jackson Gorham 1 Lester Mackey 2  Abstract  Approximate Markov chain Monte Carlo (MCMC) offers the promise of more rapid sam- pling at the cost of more biased inference. Since standard MCMC diagnostics fail to detect these biases, researchers have developed computable Stein discrepancy measures that provably de- termine the convergence of a sample to its target distribution. This approach was recently combined with the theory of reproducing kernels to deﬁne
Efﬁcient softmax approximation for GPUs  ´Edouard Grave 1 Armand Joulin 1 Moustapha Ciss´e 1 David Grangier 1 Herv´e J´egou 1  Abstract  We propose an approximate strategy to efﬁciently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear depen- dency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of compu- tation time. Our approach fu
Automated Curriculum Learning for Neural Networks  Alex Graves 1 Marc G. Bellemare 1 Jacob Menick 1 R´emi Munos 1 Koray Kavukcuoglu 1  Abstract  We introduce a method for automatically select- ing the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efﬁciency. A measure of the amount that the network learns from each data sample is pro- vided as a reward signal to a nonstationary multi- armed bandit algorithm, which then determines a stochastic syl
OnCalibrationofModernNeuralNetworksChuanGuo*1GeoffPleiss*1YuSun*1KilianQ.Weinberger1AbstractConﬁdencecalibration–theproblemofpredict-ingprobabilityestimatesrepresentativeofthetruecorrectnesslikelihood–isimportantforclassiﬁcationmodelsinmanyapplications.Wediscoverthatmodernneuralnetworks,unlikethosefromadecadeago,arepoorlycalibrated.Throughextensiveexperiments,weobservethatdepth,width,weightdecay,andBatchNormal-izationareimportantfactorsinﬂuencingcalibra-tion.Weevaluatetheperformanceofvariouspost
ProtoNN: Compressed and Accurate kNN for Resource-scarce Devices  Chirag Gupta 1 Arun Sai Suggala 1 2 Ankit Goyal 1 3 Harsha Vardhan Simhadri 1  Bhargavi Paranjape 1 Ashish Kumar 1 Saurabh Goyal 4 Raghavendra Udupa 1 Manik Varma 1  Prateek Jain 1  Abstract  Several real-world applications require real-time prediction on resource-scarce devices such as an Internet of Things (IoT) sensor. Such applica- tions demand prediction models with small stor- age and computational complexity that do not com
Deep Value Networks Learn to  Evaluate and Iteratively Reﬁne Structured Outputs  Michael Gygli 1 * Mohammad Norouzi 2 Anelia Angelova 2  Abstract  We approach structured output prediction by op- timizing a deep value network (DVN) to pre- cisely estimate the task loss on different out- put conﬁgurations for a given input. Once the model is trained, we perform inference by gra- dient descent on the continuous relaxations of the output variables to ﬁnd outputs with promis- ing scores from the valu
Reinforcement Learning with Deep Energy-Based Policies  Tuomas Haarnoja * 1 Haoran Tang * 2 Pieter Abbeel 1 3 4 Sergey Levine 1  Abstract  We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learn- ing maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that ex- presses the optimal policy via a Boltzmann dis- tribution. We use the recently 
DeepBach: a Steerable Model for Bach Chorales Generation  Ga¨etan Hadjeres 1 2 Franc¸ois Pachet 1 2 Frank Nielsen 3  Abstract  This paper introduces DeepBach, a graphical model aimed at modeling polyphonic music and speciﬁcally hymn-like pieces. We claim that, af- ter being trained on the chorale harmonizations by Johann Sebastian Bach, our model is capable of generating highly convincing chorales in the style of Bach. DeepBach’s strength comes from the use of pseudo-Gibbs sampling coupled with 
Consistent On-Line Off-Policy Evaluation  Assaf Hallak 1 Shie Mannor 1  Abstract  The problem of on-line off-policy evaluation (OPE) has been actively studied in the last decade due to its importance both as a stand-alone prob- lem and as a module in a policy improvement scheme. However, most Temporal Difference (TD) based solutions ignore the discrepancy be- tween the stationary distribution of the behavior and target policies and its effect on the conver- gence limit when function approximatio
Faster Greedy MAP Inference for Determinantal Point Processes  Insu Han 1 Prabhanjan Kambadur 2 Kyoungsoo Park 1 Jinwoo Shin 1  Abstract  Determinantal point processes (DPPs) are popu- lar probabilistic models that arise in many ma- chine learning tasks, where distributions of di- verse sets are characterized by matrix determi- nants. In this paper, we develop fast algorithms to ﬁnd the most likely conﬁguration (MAP) of large-scale DPPs, which is NP-hard in general. Due to the submodular nature 
Data-Efﬁcient Policy Evaluation Through Behavior Policy Search  Josiah P. Hanna 1 Philip S. Thomas 2 3 Peter Stone 1 Scott Niekum 1  Abstract  We consider the task of evaluating a policy for a Markov decision process (MDP). The standard unbiased technique for evaluating a policy is to deploy the policy and observe its performance. We show that the data collected from deploying a different policy, commonly called the behavior policy, can be used to produce unbiased estimates with lower mean squar
Joint Dimensionality Reduction and Metric Learning: A Geometric Take  Mehrtash Harandi 1 2 Mathieu Salzmann 3 Richard Hartley 2 1  Abstract  To be tractable and robust to data noise, exist- ing metric learning algorithms commonly rely on PCA as a pre-processing step. How can we know, however, that PCA, or any other speciﬁc dimen- sionality reduction technique, is the method of choice for the problem at hand? The answer is simple: We cannot! To address this issue, in this paper, we develop a Riem
Deep IV: A Flexible Approach for Counterfactual Prediction  Jason Hartford 1 Greg Lewis 2 Kevin Leyton-Brown 1 Matt Taddy 2  Abstract  Counterfactual prediction requires understanding causal relationships between so-called treatment and outcome variables. This paper provides a recipe for augmenting deep learning methods to accurately characterize such relationships in the presence of instrument variables (IVs)—sources of treatment randomization that are conditionally independent from the outcome
Robust Guarantees of Stochastic Greedy Algorithms  Avinatan Hassidim 1 Yaron Singer 2  Abstract  In this paper we analyze the robustness of stochastic variants of the greedy algorithm for submodular maximization. Our main result shows that for maximizing a monotone submod- ular function under a cardinality constraint, itera- tively selecting an element whose marginal con- tribution is approximately maximal in expecta- tion is a sufﬁcient condition to obtain the opti- mal approximation guarantee 
Efﬁcient Regret Minimization in Non-Convex Games  Elad Hazan 1 Karan Singh 1 Cyril Zhang 1  Abstract  We consider regret minimization in repeated games with non-convex loss functions. Minimiz- ing the standard notion of regret is computation- ally intractable. Thus, we deﬁne a natural no- tion of regret which permits efﬁcient optimiza- tion and generalizes ofﬂine guarantees for con- vergence to an approximate local optimum. We give gradient-based methods that achieve optimal regret, which in tur
Kernelized Support Tensor Machines  Lifang He 1 Chun-Ta Lu 1 Guixiang Ma 1 Shen Wang 1 Linlin Shen 2 Philip S. Yu 1 3 Ann B. Ragin 4  Abstract  In the context of supervised tensor learning, pre- serving the structural information and exploit- ing the discriminative nonlinear relationships of tensor data are crucial for improving the perfor- mance of learning tasks. Based on tensor fac- torization theory and kernel methods, we pro- pose a novel Kernelized Support Tensor Ma- chine (KSTM) which int
The Sample Complexity of Online One-Class Collaborative Filtering  Reinhard Heckel 1 Kannan Ramchandran 1  Abstract  We consider the online one-class collaborative ﬁltering (CF) problem that consists of recom- mending items to users over time in an online fashion based on positive ratings only. This prob- lem arises when users respond only occasionally to a recommendation with a positive rating, and never with a negative one. We study the im- pact of the probability of a user responding to a rec
Warped Convolutions: Efﬁcient Invariance to Spatial Transformations  Jo˜ao F. Henriques 1 Andrea Vedaldi 1  Abstract  Convolutional Neural Networks (CNNs) are ex- tremely efﬁcient, since they exploit the inherent translation-invariance of natural images. How- ever, translation is just one of a myriad of use- ful spatial transformations. Can the same efﬁ- ciency be attained when considering other spa- tial invariances? Such generalized convolutions have been considered in the past, but at a high 
Parallel and Distributed Thompson Sampling for  Large-scale Accelerated Exploration of Chemical Space  Jos´e Miguel Hern´andez-Lobato * 1 James Requeima * 1 2 Edward O. Pyzer-Knapp 3 4 Al´an Aspuru-Guzik 3  Abstract  Chemical space is so large that brute force searches for new interesting molecules are in- High-throughput virtual screening feasible. via computer cluster simulations can speed up the discovery process by collecting very large amounts of data in parallel, e.g., up to hundreds or th
DARLA: Improving Zero-Shot Transfer in Reinforcement Learning  Irina Higgins * 1 Arka Pal * 1 Andrei Rusu 1 Loic Matthey 1 Christopher Burgess 1 Alexander Pritzel 1  Matthew Botvinick 1 Charles Blundell 1 Alexander Lerchner 1  Abstract  Domain adaptation is an important open prob- lem in deep reinforcement learning (RL). In many scenarios of interest data is hard to ob- tain, so agents may learn a source policy in a setting where data is readily available, with the hope that it generalises well 
SPLICE: Fully Tractable Hierarchical Extension of ICA with Pooling  Jun-ichiro Hirayama 1 2 Aapo Hyv¨arinen 3 4 Motoaki Kawanabe 2 1  Abstract  We present a novel probabilistic framework for a hierarchical extension of independent compo- nent analysis (ICA), with a particular motiva- tion in neuroscientiﬁc data analysis and model- ing. The framework incorporates a general sub- space pooling with linear ICA-like layers stacked recursively. Unlike related previous models, our generative model is f
Multilevel Clustering via Wasserstein Means  Nhat Ho 1 XuanLong Nguyen 1 Mikhail Yurochkin 1 Hung Hai Bui 2 Viet Huynh 3 Dinh Phung 3  Abstract  We propose a novel approach to the problem of multilevel clustering, which aims to simultane- ously partition data in each group and discover grouping patterns among groups in a potentially large hierarchically structured corpus of data. Our method involves a joint optimization formu- lation over several spaces of discrete probability measures, which ar
Learning Deep Latent Gaussian Models with Markov Chain Monte Carlo  Matthew D. Hoffman 1  Abstract  ﬁt  using an  Deep latent Gaussian models are powerful and popular probabilistic models of high- These models are almost dimensional data. always variational expectation- true maximization, to approximation maximum-marginal-likelihood estimation. In this paper, we propose a different approach: rather than use a variational approximation (which produces biased gradient signals), we use Markov chain
Minimizing Trust Leaks for Robust Sybil Detection  J´anos H¨oner 1 2 Shinichi Nakajima 2 3 Alexander Bauer 2 3 Klaus-Robert M¨uller 2 3 4 5 Nico G¨ornitz 2  Abstract  Sybil detection is a crucial task to protect online social networks (OSNs) against intruders who try to manipulate automatic services provided by OSNs to their customers. In this paper, we ﬁrst discuss the robustness of graph-based Sybil de- tectors SybilRank and Integro and reﬁne theoret- ically their security guarantees towards m
Prox-PDA: The Proximal Primal-Dual Algorithm for Fast Distributed  Nonconvex Optimization and Learning Over Networks  Mingyi Hong 1 Davood Hajinezhad 1 Ming-Min Zhao 2  Abstract  In this paper we consider nonconvex optimiza- tion and learning over a network of distributed nodes. We develop a Proximal Primal-Dual Al- gorithm (Prox-PDA), which enables the network nodes to distributedly and collectively compute the set of ﬁrst-order stationary solutions in a global sublinear manner [with a rate of 
Analysis and Optimization of Graph Decompositions by Lifted Multicuts  Andrea Horˇn´akov´a * 1 Jan-Hendrik Lange * 1 Bjoern Andres 1  Abstract  We study the set of all decompositions (cluster- ings) of a graph through its characterization as a set of lifted multicuts. This leads us to prac- tically relevant insights related to the deﬁnition of classes of decompositions by must-join and must-cut constraints and related to the compar- ison of clusterings by metrics. To ﬁnd optimal decompositions d
Dissipativity Theory for Nesterov’s Accelerated Method  Bin Hu 1 Laurent Lessard 1  Abstract  In this paper, we adapt the control theoretic con- cept of dissipativity theory to provide a natural understanding of Nesterov’s accelerated method. Our theory ties rigorous convergence rate anal- ysis to the physically intuitive notion of energy dissipation. Moreover, dissipativity allows one to efﬁciently construct Lyapunov functions (either numerically or analytically) by solving a small semideﬁnite 
Learning Discrete Representations via Information Maximizing  Self-Augmented Training  Weihua Hu 1 2 Takeru Miyato 3 4 Seiya Tokui 3 1 Eiichi Matsumoto 3 1 Masashi Sugiyama 2 1  Abstract  Learning discrete representations of data is a cen- tral machine learning task because of the com- pactness of the representations and ease of in- terpretation. The task includes clustering and hash learning as special cases. Deep neural net- works are promising to be used because they can model the non-lineari
State-Frequency Memory Recurrent Neural Networks  Hao Hu 1 Guo-Jun Qi 1  Abstract  Modeling temporal sequences plays a fundamen- tal role in various modern applications and has drawn more and more attentions in the machine learning community. Among those efforts on improving the capability to represent temporal data, the Long Short-Term Memory (LSTM) has achieved great success in many areas. Although the LSTM can capture long-range dependency in the time domain, it does not explicitly model the 
Deep Generative Models for Relational Data with Side Information  Changwei Hu 1 Piyush Rai 2 Lawrence Carin 3  Abstract  We present a probabilistic framework for over- lapping community discovery and link predic- tion for relational data, given as a graph. The proposed framework has: (1) a deep architecture which enables us to infer multiple layers of la- tent features/communities for each node, provid- ing superior link prediction performance on more complex networks and better interpretability
Toward Controlled Generation of Text  Zhiting Hu 1 2 Zichao Yang 1 Xiaodan Liang 1 2 Ruslan Salakhutdinov 1 Eric P. Xing 1 2  Abstract  Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual do- main. This paper aims at generating plausible text sentences, whose attributes are controlled by learning disentangled latent representations with designated semantics. We propose a new neu- ral generative model which combi
Tensor Decomposition with Smoothness  Masaaki Imaizumi 1 Kohei Hayashi 2 3  Abstract  Real data tensors are typically high dimensional; however, their intrinsic information is preserved in low-dimensional space, which motivates the use of tensor decompositions such as Tucker decomposition. Frequently, real data tensors smooth in addition to being low dimensional, which implies that adjacent elements are similar or continuously changing. These elements typi- cally appear as spatial or temporal da
Variational Inference for Sparse and Undirected Models  John Ingraham 1 Debora Marks 1  Abstract  Undirected graphical models are applied in ge- nomics, protein structure prediction, and neuro- science to identify sparse interactions that under- lie discrete data. Although Bayesian methods for inference would be favorable in these contexts, they are rarely used because they require dou- bly intractable Monte Carlo sampling. Here, we develop a framework for scalable Bayesian in- ference of discre
Fairness in Reinforcement Learning ⇤  Shahin Jabbari Matthew Joseph Michael Kearns Jamie Morgenstern Aaron Roth 1  Abstract  We initiate the study of fairness in reinforcement learning, where the actions of a learning algo- rithm may affect its environment and future re- wards. Our fairness constraint requires that an algorithm never prefers one action over another if the long-term (discounted) reward of choosing the latter action is higher. Our ﬁrst result is neg- ative: despite the fact that f
Decoupled Neural Interfaces using Synthetic Gradients  ˆAB  AB  hA→B  B  SB  MA→B  A  ˆAB  AB  ˆAB  B B  SB  MA→B  ˆAB  hA→B  A  ……  fi+2  i+1  fi+1  i  fi  F N  i+1  F i  1  ……  (b)  Max Jaderberg 1 Wojciech Marian Czarnecki 1 Simon Osindero 1 Oriol Vinyals 1 Alex Graves 1 David Silver 1  Koray Kavukcuoglu 1  Abstract  Training directed neural networks typically re- quires forward-propagating data through a com- putation graph, followed by backpropagating er- ror signal, to produce weight updat
Scalable Generative Models for Multi-label Learning with Missing Labels  Vikas Jain 1 * Nirbhay Modhe 1 * Piyush Rai 1  Abstract  We present a scalable, generative framework for multi-label learning with missing labels. Our framework consists of a latent factor model for the binary label matrix, which is coupled with an exposure model to account for label missing- ness (i.e., whether a zero in the label matrix is indeed a zero or denotes a missing observation). The underlying latent factor model
Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models  with KL-control  Natasha Jaques 1 2 Shixiang Gu 1 3 4 Dzmitry Bahdanau 1 5 Jos´e Miguel Hern´andez-Lobato 3  Richard E. Turner 3 Douglas Eck 1  Abstract  This paper proposes a general method for im- proving the structure and quality of sequences generated by a recurrent neural network (RNN), while maintaining information originally learned from data, as well as sample diversity. An RNN is ﬁrst pre-trained on data using maxim
Bayesian Optimization with Tree-structured Dependencies  Rodolphe Jenatton 1 Cedric Archambeau 1 Javier Gonzalez 2 Matthias Seeger 1  Abstract  Bayesian optimization has been successfully used to optimize complex black-box functions whose evaluations are expensive. In many applications, like in deep learning and predictive analytics, the optimization domain is itself complex and struc- tured. In this work, we focus on use cases where this domain exhibits a known dependency struc- ture. The beneﬁ
Simultaneous Learning of Trees and Representations for Extreme Classiﬁcation  and Density Estimation  Yacine Jernite 1 Anna Choromanska 1 David Sontag 2  Abstract  We consider multi-class classiﬁcation where the predictor has a hierarchical structure that allows for a very large number of labels both at train and test time. The predictive power of such models can heavily depend on the structure of the tree, and although past work showed how to learn the tree structure, it expected that the featu
From Patches to Images: A Nonparametric Generative Model  Geng Ji 1 Michael C. Hughes 2 Erik B. Sudderth 1 3  Abstract  We propose a hierarchical generative model that captures the self-similar structure of image re- gions as well as how this structure is shared across image collections. Our model is based on a novel, variational interpretation of the popular expected patch log-likelihood (EPLL) method as a model for randomly positioned grids of image patches. While previous EPLL methods modeled
Density Level Set Estimation on Manifolds with DBSCAN  Heinrich Jiang 1  Abstract  We show that DBSCAN can estimate the con- nected components of the λ-density level set {x : f (x) ≥ λ} given n i.i.d. samples from an un- known density f. We characterize the regular- ity of the level set boundaries using parameter β > 0 and analyze the estimation error under the Hausdorff metric. When the data lies in RD we  obtain a rate of (cid:101)O(n−1/(2β+D)), which matches rate of (cid:101)O(n−1/(2β+d·max{1
Uniform Convergence Rates for Kernel Density Estimation  Heinrich Jiang 1  Abstract  Kernel density estimation (KDE) is a popular nonparametric density estimation method. We (1) derive ﬁnite-sample high-probability density estimation bounds for multivariate KDE under mild density assumptions which hold uniformly in x ∈ Rd and bandwidth matrices. We ap- ply these results to (2) mode, (3) density level set, and (4) class probability estimation and at- tain optimal rates up to logarithmic factors. 
Contextual Decision Processes with low Bellman rank are PAC-Learnable  Nan Jiang 1 Akshay Krishnamurthy 2 Alekh Agarwal 3 John Langford 3 Robert E. Schapire 3  Abstract  This paper studies systematic exploration for re- inforcement learning (RL) with rich observations and function approximation. We introduce con- textual decision processes (CDPs), that unify most prior RL settings. Our ﬁrst contribution is a complexity measure, the Bellman rank, that we show enables tractable learning of near-op
Efﬁcient Nonmyopic Active Search  Shali Jiang 1 Gustavo Malkomes 1 Geoff Converse 2 Alyssa Shofner 3 Benjamin Moseley 1 Roman Garnett 1  Abstract  Active search is an active learning setting with the goal of identifying as many members of a given class as possible under a labeling budget. In this work, we ﬁrst establish a theoretical hardness of active search, proving that no polynomial-time policy can achieve a constant factor approxima- tion ratio with respect to the expected utility of the op
How to Escape Saddle Points Efﬁciently  Chi Jin 1 Rong Ge 2 Praneeth Netrapalli 3 Sham M. Kakade 4 Michael I. Jordan 1  Abstract  This paper shows that a perturbed form of gradi- ent descent converges to a second-order station- ary point in a number iterations which depends only poly-logarithmically on dimension (i.e., it is almost “dimension-free”). The convergence rate of this procedure matches the well-known con- vergence rate of gradient descent to ﬁrst-order stationary points, up to log fac
Tunable Efﬁcient Unitary Neural Networks (EUNN) and their application to  RNNs  Li Jing * 1 Yichen Shen * 1 Tena Dubcek 1 John Peurifoy 1 Scott Skirlo 1 Yann LeCun 2 Max Tegmark 1  Marin Soljaˇci´c 1  Abstract  Using unitary (instead of general) matrices in artiﬁcial neural networks (ANNs) is a promis- ing way to solve the gradient explosion/vanishing problem, as well as to enable ANNs to learn long-term correlations in the data. This ap- proach appears particularly promising for Recur- rent Neu
An Adaptive Test of Independence with Analytic Kernel Embeddings  Wittawat Jitkrittum 1 Zoltán Szabó 2 Arthur Gretton 1  Abstract  A new computationally efﬁcient dependence mea- sure, and an adaptive statistical test of independence, are proposed. The dependence measure is the differ- ence between analytic embeddings of the joint distri- bution and the product of the marginals, evaluated at a ﬁnite set of locations (features). These features are chosen so as to maximize a lower bound on the test
StingyCD: Safely Avoiding Wasteful Updates in Coordinate Descent  Tyler B. Johnson 1 Carlos Guestrin 1  Abstract  Coordinate descent (CD) is a scalable and simple algorithm for solving many optimization prob- lems in machine learning. Despite this fact, CD can also be very computationally waste- ful. Due to sparsity in sparse regression prob- lems, for example, often the majority of CD updates result in no progress toward the solu- tion. To address this inefﬁciency, we propose a modiﬁed CD algor
Differentially Private Chi-squared Test by Unit Circle Mechanism  Kazuya Kakizaki 1 Kazuto Fukuchi 1 Jun Sakuma 1 2 3  Abstract  This paper develops differentially private mech- anisms for χ2 test of independence. While exist- ing works put their effort into properly control- ling the type-I error, in addition to that, we in- vestigate the type-II error of differentially private mechanisms. Based on the analysis, we present unit circle mechanism: a novel differentially pri- vate mechanism based 
Video Pixel Networks  Nal Kalchbrenner 1 A¨aron van den Oord 1 Karen Simonyan 1 Ivo Danihelka 1  Oriol Vinyals 1 Alex Graves 1 Koray Kavukcuoglu 1  Abstract  We propose a probabilistic video model, the Video Pixel Network (VPN), that estimates the discrete joint distribution of the raw pixel val- ues in a video. The model and the neural ar- chitecture reﬂect the time, space and color struc- ture of video tensors and encode it as a four- dimensional dependency chain. The VPN ap- proaches the best
Adaptive Feature Selection: Computationally Efﬁcient Online Sparse Linear  Regression under RIP  Satyen Kale 1 Zohar Karnin 2 Tengyuan Liang 3 D´avid P´al 4  Abstract  Online sparse linear regression is an online prob- lem where an algorithm repeatedly chooses a subset of coordinates to observe in an adversar- ially chosen feature vector, makes a real-valued prediction, receives the true label, and incurs the squared loss. The goal is to design an online learning algorithm with sublinear regret 
Recursive Partitioning for Personalization using Observational Data  Nathan Kallus 1  Abstract  We study the problem of learning to choose from m discrete treatment options (e.g., news item or medical drug) the one with best causal effect for a particular instance (e.g., user or patient) where the training data consists of passive observations of covariates, treatment, and the outcome of the treatment. The standard approach to this prob- lem is regress and compare: split the training data by tre
Multi-ﬁdelity Bayesian Optimisation with Continuous Approximations  Kirthevasan Kandasamy 1 Gautam Dasarathy 2 Jeff Schneider 1 Barnab´as P´oczos 1  Abstract  Bandit methods for black-box optimisation, such as Bayesian optimisation, are used in a variety of applications including hyper-parameter tun- ing and experiment design. Recently, multi- ﬁdelity methods have garnered considerable atten- tion since function evaluations have become in- creasingly expensive in such applications. Multi- ﬁdelit
Learning in POMDPs with Monte Carlo Tree Search  Sammie Katt 1 Frans A. Oliehoek 2 Christopher Amato 1  Abstract  The POMDP is a powerful framework for reason- ing under outcome and information uncertainty, but constructing an accurate POMDP model is difﬁcult. Bayes-Adaptive Partially Observable Markov Decision Processes (BA-POMDPs) ex- tend POMDPs to allow the model to be learned during execution. BA-POMDPs are a Bayesian RL approach that, in principle, allows for an optimal trade-off between e
Meritocratic Fairness for Cross-Population Selection  Michael Kearns 1 Aaron Roth 1 Zhiwei Steven Wu 1  Abstract  We consider the problem of selecting a pool of individuals from several populations with incom- parable skills (e.g. soccer players, mathemati- cians, and singers) in a fair manner. The quality of an individual is deﬁned to be their relative rank (by cumulative distribution value) within their own population, which permits cross-population comparisons. We study algorithms which at- t
On Approximation Guarantees for Greedy Low Rank Optimization  Rajiv Khanna 1 Ethan R. Elenberg 1 Alexandros G. Dimakis 1 Joydeep Ghosh 1 Sahand Negahban 2  Abstract  We provide new approximation guarantees for greedy low rank matrix estimation under standard assumptions of restricted strong convexity and smoothness. Our novel analysis also uncovers previously unknown connections between the low rank estimation and combinatorial optimization, so much so that our bounds are reminiscent of cor- res
Graph-based Isometry Invariant Representation Learning  Renata Khasanova 1 Pascal Frossard 1  Abstract  Learning transformation invariant representa- tions of visual data is an important problem in computer vision. Deep convolutional networks have demonstrated remarkable results for image and video classiﬁcation tasks. However, they have achieved only limited success in the classi- ﬁcation of images that undergo geometric trans- formations. In this work we present a novel Transformation Invarian
Learning to Discover Cross-Domain Relations  with Generative Adversarial Networks  Taeksoo Kim 1 Moonsu Cha 1 Hyunsoo Kim 1 Jung Kwon Lee 1 Jiwon Kim 1  Abstract  While humans easily recognize relations be- tween data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the rela- tions. To avoid costly pairing, we address the task of discovering cross-domain relations when given un
SplitNet: Learning to Semantically Split Deep Networks  for Parameter Reduction and Model Parallelization  Juyong Kim * 1 Yookoon Park * 1 Gunhee Kim 1 Sung Ju Hwang 2 3  Abstract  We propose a novel deep neural network that is both lightweight and effectively structured for model parallelization. Our network, which we name as SplitNet, automatically learns to split the network weights into either a set or a hier- archy of multiple groups that use disjoint sets of features, by learning both the 
Cost-Optimal Learning of Causal Graphs  Murat Kocaoglu 1 Alex Dimakis 1 Sriram Vishwanath 1  Abstract  We consider the problem of learning a causal graph over a set of variables with interventions. We study the cost-optimal causal graph learn- ing problem: For a given skeleton (undirected version of the causal graph), design the set of interventions with minimum total cost, that can uniquely identify any causal graph with the given skeleton. We show that this problem is solvable in polynomial ti
Understanding Black-box Predictions via Inﬂuence Functions  Pang Wei Koh 1 Percy Liang 1  Abstract  How can we explain the predictions of a black- box model? In this paper, we use inﬂuence func- tions — a classic technique from robust statis- tics — to trace a model’s prediction through the learning algorithm and back to its training data, thereby identifying training points most respon- sible for a given prediction. To scale up inﬂuence functions to modern machine learning settings, we develop 
Sub-sampled Cubic Regularization for Non-convex Optimization  Jonas Moritz Kohler 1 Aurelien Lucchi 1  Abstract  We consider the minimization of non-convex functions that typically arise in machine learn- ing. Speciﬁcally, we focus our attention on a variant of trust region methods known as cubic regularization. This approach is particularly at- tractive because it escapes strict saddle points and it provides stronger convergence guarantees than ﬁrst- and second-order as well as classical trust 
PixelCNN Models with Auxiliary Variables for Natural Image Modeling  Alexander Kolesnikov 1 Christoph H. Lampert 1  Abstract  We study probabilistic models of natural im- ages and extend the autoregressive family of PixelCNN architectures by incorporating auxil- iary variables. Subsequently, we describe two new generative image models that exploit differ- ent image transformations as auxiliary variables: a quantized grayscale view of the image or a multi-resolution image pyramid. The proposed mo
Active Learning for Cost-Sensitive Classiﬁcation  Akshay Krishnamurthy 1 Alekh Agarwal 2 Tzu-Kuo Huang 3 Hal Daum´e III 4 John Langford 2  Abstract  We design an active learning algorithm for cost-sensitive multiclass classiﬁcation: problems where different errors have different costs. Our algorithm, COAL, makes predictions by regress- ing to each label’s cost and predicting the small- est. On a new example, it uses a set of regressors that perform well on past data to estimate possi- ble costs 
Evaluating Bayesian Models with Posterior Dispersion Indices  Alp Kucukelbir 1 Yixin Wang 1 David M. Blei 1  Abstract  Probabilistic modeling is cyclical: we specify a model, infer its posterior, and evaluate its per- formance. Evaluation drives the cycle, as we re- vise our model based on how it performs. This requires a metric. Traditionally, predictive accu- racy prevails. Yet, predictive accuracy does not tell the whole story. We propose to evaluate a model through posterior dispersion. The 
Resource-efﬁcient Machine Learning in 2 KB RAM for the Internet of Things  Ashish Kumar 1 Saurabh Goyal 2 Manik Varma 1  Abstract  This paper develops a novel tree-based algo- rithm, called Bonsai, for efﬁcient prediction on IoT devices – such as those based on the Ar- duino Uno board having an 8 bit ATmega328P microcontroller operating at 16 MHz with no na- tive ﬂoating point support, 2 KB RAM and 32 KB read-only ﬂash. Bonsai maintains predic- tion accuracy while minimizing model size and predi
Grammar Variational Autoencoder  Matt J. Kusner * 1 2 Brooks Paige * 1 3 José Miguel Hernández-Lobato 3  Abstract  Deep generative models have been wildly suc- cessful at learning coherent latent representations for continuous data such as natural images, art- work, and audio. However, generative model- ing of discrete data such as arithmetic expres- sions and molecular structures still poses signiﬁ- cant challenges. Crucially, state-of-the-art meth- ods often produce outputs that are not valid.
Co-clustering through Optimal Transport  Charlotte Laclau 1 Ievgen Redko 2 Basarab Matei 1 Youn`es Bennani 1 Vincent Brault 3  Abstract  In this paper, we present a novel method for co-clustering, an unsupervised learning approach that aims at discovering homogeneous groups of data instances and features by grouping them si- multaneously. The proposed method uses the en- tropy regularized optimal transport between em- pirical measures deﬁned on data instances and features in order to obtain an e
Conditional Accelerated Lazy Stochastic Gradient Descent  Guanghui Lan * 1 Sebastian Pokutta * 1 Yi Zhou * 1 Daniel Zink * 1  Abstract  In this work we introduce a conditional acceler- ated lazy stochastic gradient descent algorithm with optimal number of calls to a stochastic ﬁrst- order oracle and convergence rate O( 1 "2 ) improv- ing over the projection-free, Online Frank-Wolfe based stochastic gradient descent of (Hazan and Kale, 2012) with convergence rate O( 1  "4 ).  1. Introduction The 
Consistent k-Clustering  Silvio Lattanzi 1 Sergei Vassilvitskii 2  Abstract  The study of online algorithms and competitive analysis provides a solid foundation for studying the quality of irrevocable decision making when the data arrives in an online manner. While in some scenarios the decisions are indeed irrevo- cable, there are many practical situations when changing a previous decision is not impossible, but simply expensive. In this work we for- malize this notion and introduce the consist
Deep Spectral Clustering Learning  Marc T. Law 1 Raquel Urtasun 1 Richard S. Zemel 1 2  Abstract  Clustering is the task of grouping a set of exam- ples so that similar examples are grouped into the same cluster while dissimilar examples are in different clusters. The quality of a cluster- ing depends on two problem-dependent factors which are i) the chosen similarity metric and ii) the data representation. Supervised cluster- ing approaches, which exploit labeled partitioned datasets have thus 
Coordinated Multi-Agent Imitation Learning  Hoang M. Le 1 Yisong Yue 1 Peter Carr 2 Patrick Lucey 3  Abstract  We study the problem of imitation learning from demonstrations of multiple coordinating agents. One key challenge in this setting is that learn- ing a good model of coordination can be difﬁcult, since coordination is often implicit in the demon- strations and must be inferred as a latent vari- able. We propose a joint approach that simulta- neously learns a latent coordination model alo
Bayesian inference on random simple graphs  with power law degree distributions  Juho Lee 1 Creighton Heaukulani 2 Zoubin Ghahramani 2 3 Lancelot F. James 4 Seungjin Choi 1  Abstract  (BFRY)  We present a model for random simple graphs with power law (i.e., heavy-tailed) degree dis- tributions. To attain this behavior, the edge probabilities in the graph are constructed from Bertoin–Fujita–Roynette–Yor random variables, which have been recently utilized in Bayesian statistics for the constructio
Conﬁdent Multiple Choice Learning  Kimin Lee 1 Changho Hwang 1 KyoungSoo Park 1 Jinwoo Shin 1  Abstract  Ensemble methods are arguably the most trust- worthy techniques for boosting the performance of machine learning models. Popular indepen- dent ensembles (IE) relying on na¨ıve averag- ing/voting scheme have been of typical choice for most applications involving deep neural net- works, but they do not consider advanced collab- oration among ensemble models. In this paper, we propose new ensemb
Deriving Neural Architectures from Sequence and Graph Kernels  Tao Lei* 1 Wengong Jin* 1 Regina Barzilay 1 Tommi Jaakkola 1  Abstract  The design of neural architectures for structured objects is typically guided by experimental in- sights rather than a formal process. In this work, we appeal to kernels over combinatorial struc- tures, such as sequences and graphs, to derive appropriate neural operations. We introduce a class of deep recurrent neural operations and for- mally characterize their 
Doubly Greedy Primal-Dual Coordinate Descent  for Sparse Empirical Risk Minimization  Qi Lei 1 Ian E.H. Yen 2 Chao-yuan Wu 3 Inderjit S. Dhillon 1 3 4 Pradeep Ravikumar 2  Abstract  We consider the popular problem of sparse empir- ical risk minimization with linear predictors and a large number of both features and observations. With a convex-concave saddle point objective re- formulation, we propose a Doubly Greedy Primal- Dual Coordinate Descent algorithm that is able to exploit sparsity in bo
Learning to Align the Source Code to the Compiled Object Code  Dor Levy 1 Lior Wolf 1 2  Abstract  We propose a new neural network architecture and use it for the task of statement-by-statement alignment of source code and its compiled ob- ject code. Our architecture learns the alignment between the two sequences – one being the trans- lation of the other – by mapping each statement to a context-dependent representation vector and aligning such vectors using a grid of the two se- quence domains.
Dropout Inference in Bayesian Neural Networks with Alpha-divergences  Yingzhen Li 1 Yarin Gal 1 2  Abstract  To obtain uncertainty estimates with real-world Bayesian deep learning models, practical infer- ence approximations are needed. Dropout varia- tional inference (VI) for example has been used for machine vision and medical applications, but VI can severely underestimates model un- certainty. Alpha-divergences are alternative di- vergences to VI’s KL objective, which are able to avoid VI’s 
Provable Alternating Gradient Descent for Non-negative Matrix Factorization  with Strong Correlations  Yuanzhi Li 1 Yingyu Liang 1  Abstract  Non-negative matrix factorization is a basic tool for decomposing data into the feature and weight matrices under non-negativity constraints, and in practice is often solved in the alternating min- imization framework. However, it is unclear whether such algorithms can recover the ground- truth feature matrix when the weights for differ- ent features are h
Provably Optimal Algorithms for Generalized Linear Contextual Bandits  Lihong Li 1 Yu Lu 2 Dengyong Zhou 1  Abstract  Contextual bandits are widely used in Internet services from news recommendation to adver- tising, and to Web search. Generalized linear models (logistical regression in particular) have demonstrated stronger performance than linear models in many applications where rewards are binary. However, most theoretical analyses on contextual bandits so far are on linear bandits. In this 
Fast k-Nearest Neighbour Search via Prioritized DCI  Ke Li 1 Jitendra Malik 1  Abstract  Most exact methods for k-nearest neighbour search suffer from the curse of dimensionality; that is, their query times exhibit exponential de- pendence on either the ambient or the intrinsic dimensionality. Dynamic Continuous Indexing (DCI) (Li & Malik, 2016) offers a promising way of circumventing the curse and successfully re- duces the dependence of query time on intrinsic dimensionality from exponential t
Forest-type Regression with General Losses  and Robust Forest  Alexander Hanbo Li 1 Andrew Martin 2  Abstract  This paper introduces a new general framework for forest-type regression which allows the de- velopment of robust forest regressors by select- ing from a large family of robust loss functions. In particular, when plugged in the squared error and quantile losses, it will recover the classical random forest (Breiman, 2001) and quantile ran- dom forest (Meinshausen, 2006). We then use ro- 
Stochastic Modiﬁed Equations  and Adaptive Stochastic Gradient Algorithms  Qianxiao Li 1 Cheng Tai 2 3 Weinan E 2 3 4  Abstract  We develop the method of stochastic modiﬁed equations (SME), in which stochastic gradient algorithms are approximated in the weak sense by continuous-time stochastic differential equa- tions. We exploit the continuous formulation together with optimal control theory to derive novel adaptive hyper-parameter adjustment poli- cies. Our algorithms have competitive perfor- 
Convergence Analysis of Proximal Gradient with Momentum for Nonconvex  Optimization  Qunwei Li 1 Yi Zhou 1 Yingbin Liang 1 Pramod K. Varshney 1  Abstract  Algorithm 1 APG  In this work, we investigate the accelerated prox- imal gradient method for nonconvex program- ming (APGnc). The method compares between a usual proximal gradient step and a linear ex- trapolation step, and accepts the one that has a lower function value to achieve a monotonic de- crease. In speciﬁc, under a general nonsmooth 
Exact MAP Inference by Avoiding Fractional Vertices  Erik M. Lindgren 1 Alexandros G. Dimakis 1 Adam Klivans 2  Abstract  Given a graphical model, one essential prob- lem is MAP inference, that is, ﬁnding the most likely conﬁguration of states according to the model. Although this problem is NP-hard, large instances can be solved in practice and it is a major open question is to explain why this is true. We give a natural condition under which we can provably perform MAP inference in polyno- mia
Leveraging Union of Subspace Structure to Improve Constrained Clustering  John Lipor 1 Laura Balzano 1  Abstract  Many clustering problems in computer vision and other contexts are also classiﬁcation problems, where each cluster shares a meaningful label. Sub- space clustering algorithms in particular are often applied to problems that ﬁt this description, for example with face images or handwritten digits. While it is straightforward to request human in- put on these datasets, our goal is to re
Zero-Inﬂated Exponential Family Embeddings  Li-Ping Liu 1 2 David M. Blei 1  Abstract  Word embeddings are a widely-used tool to an- alyze language, and exponential family embed- dings (Rudolph et al., 2016) generalize the tech- nique to other types of data. One challenge to ﬁtting embedding methods is sparse data, such as a document/term matrix that contains many zeros. To address this issue, practitioners typi- cally downweight or subsample the zeros, thus focusing learning on the non-zero ent
Iterative Machine Teaching  Weiyang Liu 1 Bo Dai 1 Ahmad Humayun 1 Charlene Tay 2 Chen Yu 2  Linda B. Smith 2 James M. Rehg 1 Le Song 1  Abstract  In this paper, we consider the problem of ma- chine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch al- gorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current perf
Analogical Inference for Multi-relational Embeddings  Hanxiao Liu 1 Yuexin Wu 1 Yiming Yang 1  Abstract  Large-scale multi-relational embedding refers to the task of learning the latent representations for entities and relations in large knowledge graphs. An effective and scalable solution for this prob- lem is crucial for the true success of knowledge- based inference in a broad range of applica- tions. This paper proposes a novel framework for optimizing the latent representations with re- spe
Dual Iterative Hard Thresholding: From Non-convex Sparse Minimization to  Non-smooth Concave Maximization  Bo Liu 1 Xiao-Tong Yuan 2 Lezi Wang 1 Qingshan Liu 2 Dimitris N. Metaxas 1  Abstract  Iterative Hard Thresholding (IHT) is a class of projected gradient descent methods for optimiz- ing sparsity-constrained minimization models, with the best known efﬁciency and scalability in practice. As far as we know, the existing IHT-style methods are designed for sparse min- imization in primal form. I
Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence  Labelling  Hairong Liu* 1 Zhenyao Zhu* 1 Xiangang Li 1 Sanjeev Satheesh 1  Abstract  Most existing sequence labelling models rely on a ﬁxed decomposition of a target sequence into a sequence of basic units. These methods suffer from two major drawbacks: 1) the set of basic units is ﬁxed, such as the set of words, charac- ters or phonemes in speech recognition, and 2) the decomposition of target sequences is ﬁxed. These dra
Learning Inﬁnite Layer Networks Without the Kernel Trick  Roi Livni 1 Daniel Carmon 2 Amir Globerson 2  Abstract  Inﬁnite Layer Networks (ILN) have been pro- posed as an architecture that mimics neural net- works while enjoying some of the advantages of kernel methods. ILN are networks that in- tegrate over inﬁnitely many nodes within a sin- gle hidden layer. It has been demonstrated by several authors that the problem of learning ILN can be reduced to the kernel trick, implying that whenever a 
Deep Transfer Learning with Joint Adaptation Networks  Mingsheng Long 1 Han Zhu 1 Jianmin Wang 1 Michael I. Jordan 2  Abstract  Deep networks have been successfully applied to learn transferable features for adapting models from a source domain to a different target domain. In this paper, we present joint adaptation networks (JAN), which learn a transfer network by aligning the joint distributions of multiple domain-speciﬁc layers across domains based on a joint maximum mean discrepancy (JMMD) c
Multiplicative Normalizing Flows for Variational Bayesian Neural Networks  Christos Louizos 1 2 Max Welling 1 3  Abstract  We reinterpret multiplicative noise in neural net- works as auxiliary random variables that aug- ment the approximate posterior in a variational setting for Bayesian neural networks. We show that through this interpretation it is both efﬁcient and straightforward to improve the approxima- tion by employing normalizing ﬂows (Rezende & Mohamed, 2015) while still allowing for l
How Close Are the Eigenvectors of the Sample and Actual Covariance  Matrices?  Andreas Loukas 1  Abstract  How many samples are sufﬁcient to guarantee that the eigenvectors of the sample covariance matrix are close to those of the actual covari- ance matrix? For a wide family of distributions, including distributions with ﬁnite second mo- ment and sub-gaussian distributions supported in a centered Euclidean ball, we prove that the in- ner product between eigenvectors of the sam- ple and actual c
Learning Deep Architectures via Generalized Whitened Neural Networks  Ping Luo 1 2  Abstract  Whitened Neural Network (WNN) is a recent advanced deep architecture, which improves con- vergence and generalization of canonical neural networks by whitening their internal hidden rep- resentation. However, the whitening transforma- tion increases computation time. Unlike WNN that reduced runtime by performing whitening every thousand iterations, which degenerates convergence due to the ill conditioni
Spherical Structured Feature Maps for Kernel Approximation  Yueming Lyu 1  Abstract  We propose Spherical Structured Feature (SSF) maps to approximate shift and rotation invari- ant kernels as well as bth-order arc-cosine ker- nels (Cho & Saul, 2009). We construct SSF maps based on the point set on d − 1 dimen- sional sphere Sd−1. We prove that the inner product of SSF maps are unbiased estimates for above kernels if asymptotically uniformly dis- tributed point set on Sd−1 is given. According to
Stochastic Gradient MCMC Methods for Hidden Markov Models  Yi-An Ma 1 Nicholas J. Foti 1 Emily B. Fox 1  Abstract  Stochastic gradient MCMC (SG-MCMC) algo- rithms have proven useful in scaling Bayesian inference to large datasets under an assump- tion of i.i.d data. We instead develop an SG- MCMC algorithm to learn the parameters of hid- den Markov models (HMMs) for time-dependent data. There are two challenges to applying SG- MCMC in this setting: The latent discrete states, and needing to brea
Self-Paced Co-training  Fan Ma 1 Deyu Meng * 1 Qi Xie 1 Zina Li 1 Xuanyi Dong 2  Abstract  Co-training is a well-known semi-supervised learning approach which trains classiﬁers on two different views and exchanges labels of unla- beled instances in an iterative way. During co- training process, labels of unlabeled instances in the training pool are very likely to be false es- pecially in the initial training rounds, while the standard co-training algorithm utilizes a “draw without replacement” m
Interactive Learning from Policy-Dependent Human Feedback  James MacGlashan 1 Mark K Ho 2 Robert Loftin 3 Bei Peng 4 Guan Wang 2 David L. Roberts 3  Matthew E. Taylor 4 Michael L. Littman 2  Abstract  This paper investigates the problem of interac- tively learning behaviors communicated by a hu- man teacher using positive and negative feed- back. Much previous work on this problem has made the assumption that people provide feed- back for decisions that is dependent on the be- havior they are te
A Laplacian Framework for Option Discovery in Reinforcement Learning  Marlos C. Machado 1 Marc G. Bellemare 2 Michael Bowling 1  Abstract  Representation learning and option discovery are two of the biggest challenges in reinforcement learning (RL). Proto-value functions (PVFs) are a well-known approach for representation learn- ing in MDPs. In this paper we address the op- tion discovery problem by showing how PVFs implicitly deﬁne options. We do it by introduc- ing eigenpurposes, intrinsic rew
Frame-based Data Factorizations  Sebastian Mair 1 Ahc`ene Boubekki 1 2 Ulf Brefeld 1  Abstract  Archetypal Analysis is the method of choice to compute interpretable matrix factorizations. is represented as a convex Every data point factors, combination of i.e., points on the boundary of the convex hull of the data. This renders computation inefﬁcient. In this paper, we show that the set of vertices of a convex hull, the so-called frame, can be efﬁciently computed by a quadratic program. We provi
Global optimization of Lipschitz functions  C´edric Malherbe 1 Nicolas Vayatis 1  Abstract  The goal of the paper is to design sequential strategies which lead to efﬁcient optimization of an unknown function under the only assumption that it has a ﬁnite Lipschitz constant. We ﬁrst identify sufﬁcient conditions for the consistency of generic sequential algorithms and formulate the expected minimax rate for their performance. We introduce and analyze a ﬁrst algorithm called LIPO which assumes the 
On Mixed Memberships and Symmetric Nonnegative Matrix Factorizations  Xueyu Mao 1 Purnamrita Sarkar 2 Deepayan Chakrabarti 3  Abstract  The problem of ﬁnding overlapping communi- ties in networks has gained much attention re- cently. Optimization-based approaches use non- negative matrix factorization (NMF) or variants, but the global optimum cannot be provably at- tained in general. Model-based approaches, such as the popular mixed membership stochastic blockmodel or MMSB (Airoldi et al., 2008)
Bayesian Models of Data Streams with Hierarchical Power Priors  Andr´es Masegosa 1 2 Thomas D. Nielsen 3 Helge Langseth 2 Dar´ıo Ramos-L´opez 1 Antonio Salmer´on 1  Anders L. Madsen 3 4  Abstract  Making inferences from data streams is a perva- sive problem in many modern data analysis ap- plications. But it requires to address the prob- lem of continuous model updating, and adapt to changes or drifts in the underlying data gener- ating distribution. In this paper, we approach these problems fro
Just Sort It! A Simple and Effective Approach to Active Preference Learning  Lucas Maystre 1 Matthias Grossglauser 1  Abstract  We address the problem of learning a ranking by using adaptively chosen pairwise comparisons. Our goal is to recover the ranking accurately but to sample the comparisons sparingly. If all com- parison outcomes are consistent with the ranking, the optimal solution is to use an efﬁcient sorting algorithm, such as Quicksort. But how do sorting algorithms behave if some com
ChoiceRank: Identifying Preferences from Node Trafﬁc in Networks  Lucas Maystre 1 Matthias Grossglauser 1  Abstract  Understanding how users navigate in a network is of high interest in many applications. We con- sider a setting where only aggregate node-level trafﬁc is observed and tackle the task of learn- ing edge transition probabilities. We cast it as a preference learning problem, and we study a model where choices follow Luce’s axiom. In this case, the O(n) marginal counts of node visits 
Deciding How to Decide:  Dynamic Routing in Artiﬁcial Neural Networks  Mason McGill 1 Pietro Perona 1  Abstract  We propose and systematically evaluate three strategies for training dynamically-routed artiﬁ- cial neural networks: graphs of learned trans- formations through which different input sig- nals may take different paths. Though some ap- proaches have advantages over others, the re- sulting networks are often qualitatively similar. We ﬁnd that, in dynamically-routed networks trained to c
Risk Bounds for Transferring Representations With and Without Fine-Tuning  Daniel McNamara 1 Maria-Florina Balcan 2  Abstract  A popular machine learning strategy is the trans- fer of a representation (i.e. a feature extraction function) learned on a source task to a target task. Examples include the re-use of neural network weights or word embeddings. We develop sufﬁ- cient conditions for the success of this approach. If the representation learned from the source task is ﬁxed, we identify condi
Nonnegative Matrix Factorization for Time Series Recovery  From a Few Temporal Aggregates  Jiali Mei 1 2 Yohann De Castro 1 Yannig Goude 1 2 Georges H´ebrail 2  Abstract  Motivated by electricity consumption reconstitu- tion, we propose a new matrix recovery method using nonnegative matrix factorization (NMF). The task tackled here is to reconstitute electric- ity consumption time series at a ﬁne temporal scale from measures that are temporal aggregates of individual consumption. Contrary to exi
Unifying Variational Autoencoders and Generative Adversarial Networks  Adversarial Variational Bayes:  Lars Mescheder 1  Sebastian Nowozin 2  Andreas Geiger 1 3  Abstract  Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the in- ference model. We introduce Adversarial Vari- ational Bayes (AVB), a technique for t
Efﬁcient Orthogonal Parametrisation of Recurrent Neural Networks  Using Householder Reﬂections  Zakaria Mhammedi 1 2 Andrew Hellicar 2 Ashfaqur Rahman 2 James Bailey 1  Abstract  The problem of learning long-term dependencies in sequences using Recurrent Neural Networks (RNNs) is still a major challenge. Recent meth- ods have been suggested to solve this problem by constraining the transition matrix to be uni- tary during training which ensures that its norm is equal to one and prevents explodin
Discovering Discrete Latent Topics with Neural Variational Inference  Yishu Miao 1 Edward Grefenstette 2 Phil Blunsom 1 2  Abstract  Topic models have been widely explored as prob- abilistic generative models of documents. Tra- ditional inference methods have sought closed- form derivations for updating the models, how- ever as the expressiveness of these models grows, so does the difﬁculty of performing fast and accurate inference over their parameters. This paper presents alternative neural ap
Variational Boosting: Iteratively Reﬁning Posterior Approximations  Andrew C. Miller 1 Nicholas J. Foti 2 Ryan P. Adams 1 3  Abstract  We propose a black-box variational inference method to approximate intractable distributions with an increasingly rich approximating class. Our method, variational boosting, iteratively re- ﬁnes an existing variational approximation by solving a sequence of optimization problems, al- lowing a trade-off between computation time and accuracy. We expand the variatio
Device Placement Optimization with Reinforcement Learning  Azalia Mirhoseini * 1 2 Hieu Pham * 1 2 Quoc V. Le 1 Benoit Steiner 1 Rasmus Larsen 1 Yuefeng Zhou 1  Naveen Kumar 3 Mohammad Norouzi 1 Samy Bengio 1 Jeff Dean 1  Abstract  The past few years have witnessed a growth in size and computational requirements for training and inference with neural networks. Currently, a common approach to address these requirements is to use a heterogeneous distributed environ- ment with a mixture of hardware
Tight Bounds for Approximate Carathéodory and Beyond  Vahab Mirrokni * 1 Renato Paes Leme * 1 Adrian Vladu * 2 Sam Chiu-wai Wong * 3  Abstract  We present a deterministic nearly-linear time algo- rithm for approximating any point inside a convex polytope with a sparse convex combination of the polytope’s vertices. Our result provides a con- structive proof for the Approximate Carathéodory Problem (Barman, 2015), which states that any point inside a polytope contained in the `p ball of radius D c
Deletion-Robust Submodular Maximization:  Data Summarization with “the Right to be Forgotten”  Baharan Mirzasoleiman 1 Amin Karbasi 2 Andreas Krause 1  Abstract  How can we summarize a dynamic data stream when elements selected for the summary can be deleted at any time? This is an important chal- lenge in online services, where the users gener- ating the data may decide to exercise their right to restrict the service provider from using (part of) their data due to privacy concerns. Moti- vated 
Prediction and Control with Temporal Segment Models  Nikhil Mishra 1 Pieter Abbeel 1 2 Igor Mordatch 2  Abstract  We introduce a method for learning the dynamics of complex nonlinear systems based on deep gen- erative models over temporal segments of states and actions. Unlike dynamics models that oper- ate over individual discrete timesteps, we learn the distribution over future state trajectories con- ditioned on past state, past action, and planned future action trajectories, as well as a lat
Improving Gibbs Sampler Scan Quality with DoGS  Ioannis Mitliagkas 1 Lester Mackey 2  Abstract  The pairwise inﬂuence matrix of Dobrushin has long been used as an analytical tool to bound the rate of convergence of Gibbs sampling. In this work, we use Dobrushin inﬂuence as the basis of a practical tool to certify and efﬁciently im- prove the quality of a discrete Gibbs sampler. Our Dobrushin-optimized Gibbs samplers (DoGS) of- fer customized variable selection orders for a given sampling budget 
Differentially Private Submodular Maximization:  Data Summarization in Disguise  Marko Mitrovic 1 Mark Bun 1 2 Andreas Krause 3 Amin Karbasi 1 Abstract  Many data summarization applications are cap- tured by the general framework of submodular maximization. As a consequence, a wide range of efﬁcient approximation algorithms have been developed. However, when such applications in- volve sensitive data about individuals, their pri- vacy concerns are not automatically addressed. To remedy this prob
Active Learning for Top-K Rank Aggregation from Noisy Comparisons  Soheil Mohajer 1 Changho Suh 2 Adel Elmahdy 1  Abstract  We explore an active top-K ranking problem based on pairwise comparisons that are collected possibly in a sequential manner as per our de- sign choice. We consider two settings: (1) top-K sorting in which the goal is to recover the top-K items in order out of n items; (2) top-K partition- ing where only the set of top-K items is desired. Under a fairly general model which s
Variational Dropout Sparsiﬁes Deep Neural Networks  Dmitry Molchanov 1 2 * Arsenii Ashukha 3 4 * Dmitry Vetrov 3 1  Abstract  We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report ﬁrst experimental results with indi- vidual dropout rates per weight. Interestingly, it leads
Regularising Non-linear Models Using Feature Side-information  Amina Mollaysa * 1 Pablo Strasser * 1 Alexandros Kalousis 1  Abstract  Very often features come with their own vectorial descriptions which provide detailed information about their properties. We refer to these vecto- rial descriptions as feature side-information. In the standard learning scenario, input is repre- sented as a vector of features and the feature side- information is most often ignored or used only for feature selection
Coupling Distributed and Symbolic Execution for Natural Language Queries  Lili Mou 1 Zhengdong Lu 2 Hang Li 3 Zhi Jin 1  Abstract  Building neural networks to query a knowledge base (a table) with natural language is an emerg- ing research topic in deep learning. An execu- tor for table querying typically requires multi- ple steps of execution because queries may have complicated structures. In previous studies, re- searchers have developed either fully distributed executors or symbolic executor
McGan: Mean and Covariance Feature Matching GAN  Youssef Mroueh * 1 2 Tom Sercu * 1 2 Vaibhava Goel 2  Abstract  We introduce new families of Integral Probabil- ity Metrics (IPM) for training Generative Adver- sarial Networks (GAN). Our IPMs are based on matching statistics of distributions embedded in a ﬁnite dimensional feature space. Mean and co- variance feature matching IPMs allow for sta- ble training of GANs, which we will call Mc- Gan. McGan minimizes a meaningful loss be- tween distribu
Sequence to Better Sequence: Continuous Revision of Combinatorial Structures  Jonas Mueller 1 David Gifford 1 Tommi Jaakkola 1  Abstract  We present a model that, after learning on ob- servations of (sequence, outcome) pairs, can be efﬁciently used to revise a new sequence in order to improve its associated outcome. Our frame- work requires neither example improvements, nor additional evaluation of outcomes for pro- posed revisions. To avoid combinatorial-search over sequence elements, we specif
Variants of RMSProp and Adagrad with Logarithmic Regret Bounds  Mahesh Chandra Mukkamala 1 2 Matthias Hein 1  Abstract  √  Adaptive gradient methods have become recently very popular, in particular as they have been shown to be useful in the training of deep neu- ral networks. In this paper we have analyzed RMSProp, originally proposed for the training of deep neural networks, in the context of on- T -type re- line convex optimization and show gret bounds. Moreover, we propose two vari- ants SC-
Meta Networks  Tsendsuren Munkhdalai 1 Hong Yu 1  Abstract  Neural networks have been successfully applied in applications with a large amount of labeled data. However, the task of rapid generalization on new concepts with small training data while preserving performances on previously learned ones still presents a signiﬁcant challenge to neu- ral network models. In this work, we intro- duce a novel meta learning method, Meta Net- works (MetaNet), that learns a meta-level knowl- edge across task
Understanding the Representation and Computation of Multilayer  Perceptrons: A Case Study in Speech Recognition  Tasha Nagamine 1 Nima Mesgarani 1  Abstract  Despite the recent success of deep learning, the nature of the transformations they apply to the input features remains poorly understood. This study provides an empirical framework to study the encoding properties of node activations in various layers of the network, and to construct the exact function applied to each data point in the for
Adaptive Sampling Probabilities for Non-Smooth Optimization  Hongseok Namkoong 1 Aman Sinha 2 Steve Yadlowsky 2 John C. Duchi 2 3  Abstract  Standard forms of coordinate and stochastic gra- dient methods do not adapt to structure in data; their good behavior under random sampling is predicated on uniformity in data. When gradi- ents in certain blocks of features (for coordinate descent) or examples (for SGD) are larger than others, there is a natural structure that can be ex- ploited for quicker
Delta Networks for Optimized Recurrent Network Computation  Daniel Neil 1 Jun Haeng Lee 2 Tobi Delbruck 1 Shih-Chii Liu 1  Abstract  Many neural networks exhibit stability in their activation patterns over time in response to in- puts from sensors operating under real-world conditions. By capitalizing on this property of natural signals, we propose a Recurrent Neural Network (RNN) architecture called a delta net- work in which each neuron transmits its value only when the change in its activatio
Post-Inference Prior Swapping  Willie Neiswanger 1 Eric Xing 2  Abstract  While Bayesian methods are praised for their ability to incorporate useful prior knowledge, in practice, convenient priors that allow for compu- tationally cheap or tractable inference are com- monly used. In this paper, we investigate the fol- lowing question: for a given model, is it possible to compute an inference result with any conve- nient false prior, and afterwards, given any tar- get prior of interest, quickly tr
The Loss Surface of Deep and Wide Neural Networks  Quynh Nguyen 1 Matthias Hein 1  Abstract  While the optimization problem behind deep neural networks is highly non-convex, it is fre- quently observed in practice that training deep networks seems possible without getting stuck in suboptimal points. It has been argued that this is the case as all local minima are close to be- ing globally optimal. We show that this is (al- most) true, in fact almost all local minima are globally optimal, for a f
SARAH: A Novel Method for Machine Learning Problems  Using Stochastic Recursive Gradient  Lam M. Nguyen 1 Jie Liu 1 Katya Scheinberg 1 2 Martin Tak´aˇc 1  Abstract  In this paper, we propose a StochAstic Recur- sive grAdient algoritHm (SARAH), as well as its practical variant SARAH+, as a novel approach to the ﬁnite-sum minimization problems. Dif- ferent from the vanilla SGD and other modern stochastic methods such as SVRG, S2GD, SAG and SAGA, SARAH admits a simple recursive framework for updati
Composing Tree Graphical Models with Persistent Homology Features  for Clustering Mixed-Type Data  Xiuyan Ni 1 Novi Quadrianto 2 3 Yusu Wang 4 Chao Chen 1  Abstract  Clustering data with both continuous and dis- crete attributes is a challenging task. Existing methods often lack a principled probabilistic for- mulation. In this paper, we propose a cluster- ing method based on a tree-structured graphi- cal model to describe the generation process of mixed-type data. Our tree-structured model fac-
Multichannel End-to-end Speech Recognition  Tsubasa Ochiai 1 Shinji Watanabe 2 Takaaki Hori 2 John R. Hershey 2  Abstract  The ﬁeld of speech recognition is in the midst of a paradigm shift: end-to-end neural networks are challenging the dominance of hidden Markov models as a core technology. Using an attention mechanism in a recurrent encoder-decoder archi- tecture solves the dynamic time alignment prob- lem, allowing joint end-to-end training of the acoustic and language modeling components. I
Conditional Image Synthesis with Auxiliary Classiﬁer GANs  Augustus Odena 1 Christopher Olah 1 Jonathon Shlens 1  Abstract  In this paper we introduce new methods for the improved training of generative adversarial net- works (GANs) for image synthesis. We con- struct a variant of GANs employing label condi- tioning that results in 128 × 128 resolution im- age samples exhibiting global coherence. We expand on previous work for image quality as- sessment to provide two new analyses for assess- in
Nystr¨om Method with Kernel K-means++ Samples as Landmarks  Dino Oglic 1 2 Thomas G¨artner 2  Abstract  We investigate, theoretically and empirically, the effectiveness of kernel K-means++ samples as landmarks in the Nystr¨om method for low-rank approximation of kernel matrices. Previous em- pirical studies (Zhang et al., 2008; Kumar et al., 2012) observe that the landmarks obtained using (kernel) K-means clustering deﬁne a good low- rank approximation of kernel matrices. However, the existing w
Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning  Junhyuk Oh 1 Satinder Singh 1 Honglak Lee 1 2 Pushmeet Kohli 3  Abstract  As a step towards developing zero-shot task gen- eralization capabilities in reinforcement learning (RL), we introduce a new RL problem where the agent should learn to execute sequences of in- structions after learning useful skills that solve subtasks. In this problem, we consider two types of generalizations: to previously unseen instruc- tions an
The Statistical Recurrent Unit  Junier B. Oliva 1 Barnab´as P´oczos 1 Jeff Schneider 1  Abstract  Sophisticated gated recurrent neural network ar- chitectures like LSTMs and GRUs have been shown to be highly effective in a myriad of appli- cations. We develop an un-gated unit, the statisti- cal recurrent unit (SRU), that is able to learn long term dependencies in data by only keeping mov- ing averages of statistics. The SRU’s architec- ture is simple, un-gated, and contains a compara- ble number
Deep Decentralized Multi-task Multi-Agent Reinforcement Learning  under Partial Observability  Shayegan Omidshaﬁei 1 Jason Pazis 1 Christopher Amato 2 Jonathan P. How 1 John Vian 3  Abstract  Many real-world tasks involve multiple agents with partial observability and limited communi- cation. Learning is challenging in these settings due to local viewpoints of agents, which perceive the world as non-stationary due to concurrently- exploring teammates. Approaches that learn spe- cialized policies
Algebraic Variety Models for High-Rank Matrix Completion  Greg Ongie 1 Rebecca Willett 2 Robert D. Nowak 2 Laura Balzano 1  Abstract  We consider a generalization of low-rank matrix completion to the case where the data belongs to an algebraic variety, i.e., each data point is a solu- tion to a system of polynomial equations. In this case the original matrix is possibly high-rank, but it becomes low-rank after mapping each col- umn to a higher dimensional space of monomial features. Many well-st
Why is Posterior Sampling Better than Optimism for Reinforcement Learning?  Ian Osband 1 2 Benjamin Van Roy 1  Abstract  reinforcement  Computational results demonstrate that posterior sampling for learning (PSRL) dramatically outperforms existing algorithms driven by optimism, such as UCRL2. We pro- vide insight into the extent of this performance boost and the phenomenon that drives it. We leverage this insight to establish an ˜O(HpSAT ) Bayesian regret bound for PSRL in ﬁnite-horizon episodic
Bidirectional Learning for Time-series Models with Hidden Units  Takayuki Osogami 1 Hiroshi Kajino 1 Taro Sekiyama 1  Abstract  Hidden units can play essential roles in modeling time-series having long-term dependency or non- linearity but make it difﬁcult to learn associated parameters. Here we propose a way to learn such a time-series model by training a backward model for the time-reversed time-series, where the back- ward model has a common set of parameters as the original (forward) model. 
Count-BasedExplorationwithNeuralDensityModelsGeorgOstrovski1MarcG.Bellemare1A¨aronvandenOord1R´emiMunos1AbstractBellemareetal.(2016)introducedthenotionofapseudo-count,derivedfromadensitymodel,togeneralizecount-basedexplorationtonon-tabularreinforcementlearning.Thispseudo-countwasusedtogenerateanexplorationbonusforaDQNagentandcombinedwithamixedMonteCarloupdatewassufﬁcienttoachievestateoftheartontheAtari2600gameMon-tezuma’sRevenge.Weconsidertwoquestionsleftopenbytheirwork:First,howimportantisthequ
Dictionary Learning Based on Sparse Distribution Tomography  Pedram Pad * 1 Farnood Salehi * 2 Elisa Celis 2 Patrick Thiran 2 Michael Unser 1  Abstract  We propose a new statistical dictionary learning algorithm for sparse signals that is based on an α-stable innovation model. The parameters of the underlying model—that is, the atoms of the dictionary, the sparsity index α and the disper- sion of the transform-domain coefﬁcients—are recovered using a new type of probability distri- bution tomogr
Stochastic Bouncy Particle Sampler  Ari Pakman * 1 Dar Gilboa * 1 David Carlson 2 Liam Paninski 1  Abstract  We introduce a stochastic version of the non- reversible, rejection-free Bouncy Particle Sam- pler (BPS), a Markov process whose sample tra- jectories are piecewise linear, to efﬁciently sam- ple Bayesian posteriors in big datasets. We prove that in the BPS no bias is introduced by noisy evaluations of the log-likelihood gradient. On the other hand, we argue that efﬁciency considera- tion
A Birth-Death Process for Feature Allocation  Konstantina Palla 1 David Knowles 2 Zoubin Ghahramani 3 4  Abstract  We propose a Bayesian nonparametric prior over feature allocations for sequential data, the birth- death feature allocation process (BDFP). The BDFP models the evolution of the feature al- location of a set of N objects across a covari- ate (e.g. time) by creating and deleting features. A BDFP is exchangeable, projective, stationary and reversible, and its equilibrium distribution i
Prediction under Uncertainty in Sparse Spectrum Gaussian Processes  with Applications to Filtering and Control  Yunpeng Pan 1 2 Xinyan Yan 1 3 Evangelos A. Theodorou 1 2 Byron Boots 1 3  Abstract  ization,  Sparse Spectrum Gaussian Processes (SSGPs) are a powerful tool for scaling Gaussian pro- cesses (GPs) to large datasets. Existing SSGP algorithms for regression assume deterministic inputs, precluding their use in many real-world robotics and engineering applications where ac- counting for in
Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence  and Cluster Recovery  Ashkan Panahi 1 Devdatt Dubhashi 2 Fredrik D. Johansson 3 Chiranjib Bhattacharyya 4  Abstract  Standard clustering methods such as K–means, Gaussian mixture models, and hierarchical clus- tering, are beset by local minima, which are sometimes drastically suboptimal. Moreover the number of clusters K must be known in advance. The recently introduced sum–of–norms (SON) or Clusterpath convex relaxation 
Curiosity-driven Exploration by Self-supervised Prediction  Deepak Pathak 1 Pulkit Agrawal 1 Alexei A. Efros 1 Trevor Darrell 1  Abstract  In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent al- together. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent’s ability to pre- dict the consequenc
Asynchronous Distributed Variational Gaussian Process for Regression  Hao Peng 1 Shandian Zhe 1 Xiao Zhang 1 Yuan Qi 2  Abstract  Gaussian processes (GPs) are powerful non- parametric function estimators. However, their applications are largely limited by the expensive computational cost of the inference procedures. Existing stochastic or distributed synchronous variational inferences, although have alleviated this issue by scaling up GPs to millions of sam- ples, are still far from satisfactory
Geometry of Neural Network Loss Surfaces via Random Matrix Theory  Jeffrey Pennington 1 Yasaman Bahri 1  Abstract  Understanding the geometry of neural network loss surfaces is important for the development of improved optimization algorithms and for build- ing a theoretical understanding of why deep learning works. In this paper, we study the ge- ometry in terms of the distribution of eigenvalues of the Hessian matrix at critical points of varying energy. We introduce an analytical framework an
Multi-task Learning with Labeled and Unlabeled Tasks  Anastasia Pentina 1 Christoph H. Lampert 1  Abstract  In multi-task learning, a learner is given a col- lection of prediction tasks and needs to solve all of them. In contrast to previous work, which re- quired that annotated training data is available for all tasks, we consider a new setting, in which for some tasks, potentially most of them, only un- labeled training data is provided. Consequently, to solve all tasks, information must be tr
Robust Adversarial Reinforcement Learning  Lerrel Pinto 1 James Davidson 2 Rahul Sukthankar 3 Abhinav Gupta 1 3  Abstract  Deep neural networks coupled with fast simula- tion and improved computation have led to re- cent successes in the ﬁeld of reinforcement learn- ing (RL). However, most current RL-based ap- proaches fail to generalize since: (a) the gap be- tween simulation and real world is so large that policy-learning approaches fail to transfer; (b) even if policy learning is done in real
Neural Episodic Control  Alexander Pritzel 1 Demis Hassabis 1 Daan Wierstra 1 Charles Blundell 1  Benigno Uria 1  Sriram Srinivasan 1  Adri`a Puigdom`enech Badia 1  Oriol Vinyals 1  Abstract  Deep reinforcement learning methods attain super-human performance in a wide range of en- vironments. Such methods are grossly inefﬁcient, often taking orders of magnitudes more data than humans to achieve reasonable performance. We propose Neural Episodic Control: a deep rein- forcement learning agent that
Online and Linear-Time Attention by Enforcing Monotonic Alignments  Colin Raffel 1 Minh-Thang Luong 1 Peter J. Liu 1 Ron J. Weiss 1 Douglas Eck 1  Abstract  Recurrent neural network models with an atten- tion mechanism have proven to be extremely effective on a wide variety of sequence-to- sequence problems. However, the fact that soft attention mechanisms perform a pass over the entire input sequence when producing each el- ement in the output sequence precludes their use in online settings and
On the Expressive Power of Deep Neural Networks  Maithra Raghu 1 2 Ben Poole 3 Jon Kleinberg 1 Surya Ganguli 3 Jascha Sohl Dickstein 2  Abstract  We propose a new approach to the problem of neural network expressivity, which seeks to char- acterize how structural properties of a neural net- work family affect the functions it is able to com- pute. Our approach is based on an interrelated set of measures of expressivity, uniﬁed by the novel notion of trajectory length, which mea- sures how the ou
Estimating the unseen from multiple populations  Aditi Raghunathan 1 Gregory Valiant 1 James Zou 1 2  Abstract  Given samples from a distribution, how many new elements should we expect to ﬁnd if we con- tinue sampling this distribution? This is an im- portant and actively studied problem, with many applications ranging from unseen species estima- tion to genomics. We generalize this extrapola- tion and related unseen estimation problems to the multiple population setting, where population j has
Coherence Pursuit: Fast, Simple, and Robust Subspace Recovery  Mostafa Rahmani 1 George Atia 1  Abstract  dimensional subspace by solving  A remarkably simple, yet powerful, algorithm termed Coherence Pursuit for robust Principal Component Analysis (PCA) is presented. In the proposed approach, an outlier is set apart from an inlier by comparing their coherence with the rest of the data points. As inliers lie in a low dimen- sional subspace, they are likely to have strong mutual coherence provide
Innovation Pursuit: A New Approach to the Subspace Clustering Problem  Mostafa Rahmani 1 George Atia 1  Abstract  This paper presents a new scalable approach, termed Innovation Pursuit (iPursuit), to the prob- lem of subspace clustering. iPursuit rests on a new geometrical idea whereby each subspace is identiﬁed based on its novelty with respect to the other subspaces. The subspaces are identiﬁed consecutively by solving a series of simple lin- ear optimization problems, each searching for a dir
High Dimensional Bayesian Optimization with Elastic Gaussian Process  Santu Rana * 1 Cheng Li * 1 Sunil Gupta 1 Vu Nguyen 1 Svetha Venkatesh 1  Abstract  Bayesian optimization is an efﬁcient way to op- timize expensive black-box functions such as de- signing a new product with highest quality or tuning hyperparameter of a machine learning al- gorithm. However, it has a serious limitation when the parameter space is high-dimensional as Bayesian optimization crucially depends on solving a global o
Equivariance Through Parameter-Sharing  Siamak Ravanbakhsh 1 Jeff Schneider 1 Barnab´as P´oczos 1  Abstract  We propose to study equivariance in deep neu- ral networks through parameter symmetries. In particular, given a group (cid:71) that acts discretely on the input and output of a standard neural net-  work layer φW∶ (cid:82)M → (cid:82)N , we show that φW  is equivariant with respect to (cid:71)-action iff (cid:71) ex- plains the symmetries of the network parameters W. Inspired by this obse
Large-Scale Evolution of Image Classiﬁers  Esteban Real 1 Sherry Moore 1 Andrew Selle 1 Saurabh Saxena 1 Yutaka Leon Suematsu 2 Jie Tan 1 Quoc V. Le 1 Alexey Kurakin 1  Abstract  Neural networks have proven effective at solv- ing difﬁcult problems but designing their archi- tectures can be challenging, even for image clas- siﬁcation problems alone. Our goal is to min- imize human participation, so we employ evo- lutionary algorithms to discover such networks automatically. Despite signiﬁcant com
Parallel Multiscale Autoregressive Density Estimation  Scott Reed 1 A¨aron van den Oord 1 Nal Kalchbrenner 1 Sergio G´omez Colmenarejo 1 Ziyu Wang 1  Yutian Chen 1 Dan Belov 1 Nando de Freitas 1  Abstract  PixelCNN achieves state-of-the-art results in density estimation for natural images. Although training is fast, inference is costly, requiring one network evaluation per pixel; O(N) for N pix- els. This can be sped up by caching activations, but still involves generating each pixel sequen- tia
Active Learning for Accurate Estimation of Linear Models  Carlos Riquelme 1 Mohammad Ghavamzadeh 2 Alessandro Lazaric 3  Abstract  We explore the sequential decision-making prob- lem where the goal is to estimate a number of lin- ear models uniformly well, given a shared budget of random contexts independently sampled from a known distribution. For each incoming context, the decision-maker selects one of the linear mod- els and receives an observation that is corrupted by the unknown noise level
Cognitive Psychology for Deep Neural Networks:  A Shape Bias Case Study  Samuel Ritter * 1 David G.T. Barrett * 1 Adam Santoro 1 Matt M. Botvinick 1  Abstract  Deep neural networks (DNNs) have advanced performance on a wide range of complex tasks, rapidly outpacing our understanding of the na- ture of their solutions. While past work sought to advance our understanding of these models, none has made use of the rich history of problem descriptions, theories, and experimental methods developed by 
Pain-Free Random Differential Privacy with Sensitivity Sampling  Benjamin I. P. Rubinstein 1 Francesco Ald`a 2  Abstract  Popular approaches to differential privacy, such as the Laplace and exponential mechanisms, cal- ibrate randomised smoothing through global sen- sitivity of the target non-private function. Bound- ing such sensitivity is often a prohibitively com- plex analytic calculation. As an alternative, we propose a straightforward sampler for estimat- ing sensitivity of non-private mec
Enumerating Distinct Decision Trees  Salvatore Ruggieri 1  Abstract  The search space for the feature selection prob- lem in decision tree learning is the lattice of sub- sets of the available features. We provide an exact enumeration procedure of the subsets that lead to all and only the distinct decision trees. The procedure can be adopted to prune the search space of complete and heuristics search methods in wrapper models for feature selection. Based on this, we design a computational optimi
Bayesian Boolean Matrix Factorisation  Tammo Rukat 1 Chris C. Holmes 1 2 Michalis K. Titsias 3 Christopher Yau 4  Abstract  Boolean matrix factorisation aims to decompose a binary data matrix into an approximate Boolean product of two low rank, binary matrices: one containing meaningful patterns, the other quanti- fying how the observations can be expressed as a combination of these patterns. We introduce the OrMachine, a probabilistic generative model for Boolean matrix factorisation and derive
Depth-Width Tradeoffs in Approximating Natural Functions with Neural  Networks  Itay Safran 1 Ohad Shamir 1  Abstract  We provide several new depth-based separation results for feed-forward neural networks, proving that various types of simple and natural functions can be better approximated using deeper net- works than shallower ones, even if the shallower networks are much larger. This includes indi- cators of balls and ellipses; non-linear functions which are radial with respect to the L1 nor
Asymmetric Tri-training for Unsupervised Domain Adaptation  Kuniaki Saito 1 Yoshitaka Ushiku 1 Tatsuya Harada 1 2  Abstract  It is important to apply models trained on a large number of labeled samples to different domains because collecting many labeled samples in var- ious domains is expensive. To learn discrimi- native representations for the target domain, we assume that artiﬁcially labeling the target sam- ples can result in a good representation. Tri- training leverages three classiﬁers eq
Semi-Supervised Classiﬁcation  Based on Classiﬁcation from Positive and Unlabeled Data  Tomoya Sakai 1 2 Marthinus Christoffel du Plessis Gang Niu 1 Masashi Sugiyama 2 1  Abstract  Most of the semi-supervised classiﬁcation meth- ods developed so far use unlabeled data for reg- ularization purposes under particular distribu- tional assumptions such as the cluster assump- tion. In contrast, recently developed methods of classiﬁcation from positive and unlabeled data (PU classiﬁcation) use unlabele
Analytical Guarantees on Numerical Precision of Deep Neural Networks  Charbel Sakr Yongjune Kim Naresh Shanbhag  Abstract  The acclaimed successes of neural networks of- ten overshadow their tremendous complexity. We focus on numerical precision - a key param- eter deﬁning the complexity of neural networks. First, we present theoretical bounds on the ac- curacy in presence of limited precision. Inter- estingly, these bounds can be computed via the back-propagation algorithm. Hence, by com- binin
Hierarchy Through Composition with Multitask LMDPs  Andrew M. Saxe 1 Adam C. Earle 2 Benjamin Rosman 2 3  Abstract  Hierarchical architectures are critical to the scalability of reinforcement learning methods. Most current hierarchical frameworks execute ac- tions serially, with macro-actions comprising se- quences of primitive actions. We propose a novel alternative to these control hierarchies based on concurrent execution of many actions in parallel. Our scheme exploits the guaranteed concurr
Optimal Algorithms for Smooth and Strongly Convex  Distributed Optimization in Networks  Kevin Scaman 1 Francis Bach 2 S´ebastien Bubeck 3 Yin Tat Lee 3 Laurent Massouli´e 1  Abstract  √  In this paper, we determine the optimal conver- gence rates for strongly convex and smooth dis- tributed optimization in two settings: central- ized and decentralized communications over a network. For centralized (i.e. master/slave) al- gorithms, we show that distributing Nesterov’s is optimal and accelerated 
Adapting Kernel Representations Online Using Submodular Maximization  Matthew Schlegel 1 Yangchen Pan 1 Jiecao Chen 1 Martha White 1  Abstract  Kernel representations provide a nonlinear repre- sentation, through similarities to prototypes, but require only simple linear learning algorithms given those prototypes. In a continual learning setting, with a constant stream of observations, it is critical to have an efﬁcient mechanism for sub-selecting prototypes amongst observations. In this work, w
Developing Bug-Free Machine Learning Systems With Formal Mathematics  Daniel Selsam 1 Percy Liang 1 David L. Dill 1  Abstract  Standard methodology: test it empirically  Noisy data, non-convex objectives, model mis- speciﬁcation, and numerical instability can all cause undesired behaviors in machine learning systems. As a result, detecting actual imple- mentation errors can be extremely difﬁcult. We demonstrate a methodology in which developers use an interactive proof assistant to both imple- m
Identifying Best Interventions through Online Importance Sampling  Rajat Sen * 1 Karthikeyan Shanmugam * 2 Alexandros G. Dimakis 1 Sanjay Shakkottai 1  Abstract  Motivated by applications in computational ad- vertising and systems biology, we consider the problem of identifying the best out of several possible soft interventions at a source node V in an acyclic causal directed graph, to maximize the expected value of a target node Y (located downstream of V ). Our setting imposes a ﬁxed total bu
Failures of Gradient-Based Deep Learning  Shai Shalev-Shwartz 1 Ohad Shamir 2 Shaked Shammah 1  Abstract  In recent years, Deep Learning has become the go-to solution for a broad range of applications, often outperforming state-of-the-art. However, it is important, for both theoreticians and practi- tioners, to gain a deeper understanding of the dif- ﬁculties and limitations associated with common approaches and algorithms. We describe four types of simple problems, for which the gradient- based
Estimating individual treatment effect: generalization bounds and algorithms  Uri Shalit * 1 Fredrik D. Johansson * 2 David Sontag 2 3  Abstract  There is intense interest in applying machine learning to problems of causal inference in ﬁelds such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analy- sis and family of algorithms for predicting indi- vidual treatment effect (I
Online Learning with Local Permutations and Delayed Feedback  Ohad Shamir * 1 Liran Szlak * 1  Abstract  We propose an Online Learning with Local Per- mutations (OLLP) setting, in which the learner is allowed to slightly permute the order of the loss functions generated by an adversary. On one hand, this models natural situations where the ex- act order of the learner’s responses is not crucial, and on the other hand, might allow better learn- ing and regret performance, by mitigating highly adv
Orthogonalized ALS: A Theoretically Principled Tensor Decomposition  Algorithm for Practical Use  Vatsal Sharan 1 Gregory Valiant 1  Abstract  The popular Alternating Least Squares (ALS) al- gorithm for tensor decomposition is efﬁcient and easy to implement, but often converges to poor local optima—particularly when the weights of the factors are non-uniform. We propose a mod- iﬁcation of the ALS approach that is as efﬁ- cient as standard ALS, but provably recovers the true factors with random i
Differentially Private Ordinary Least Squares  Or Sheffet 1  Abstract  Linear regression is one of the most prevalent techniques in machine learning; however, it is also common to use linear regression for its ex- planatory capabilities rather than label predic- tion. Ordinary Least Squares (OLS) is often used in statistics to establish a correlation between an attribute (e.g. gender) and a label (e.g. in- come) in the presence of other (potentially corre- lated) features. OLS assumes a particul
On the Iteration Complexity of Support Recovery  via Hard Thresholding Pursuit  Jie Shen 1 Ping Li 1  Abstract  2010; Blumensath & Davies, 2009; Bouchot et al., 2016).  Recovering the support of a sparse signal from its compressed samples has been one of the most important problems in high dimensional statis- tics. In this paper, we present a novel analysis for the hard thresholding pursuit (HTP) algorithm, showing that it exactly recovers the support of an arbitrary s-sparse signal within O (sκ
GSOS: Gauss-Seidel Operator Splitting Algorithm for Multi-Term Nonsmooth Convex Composite Optimization  Li Shen 1 Wei Liu 1 Ganzhao Yuan 2 Shiqian Ma 3  Abstract  In this paper, we propose a fast Gauss-Seidel Operator Splitting (GSOS) algorithm for ad- dressing multi-term nonsmooth convex compos- ite optimization, which has wide applications in machine learning, signal processing and statistic- s. The proposed GSOS algorithm inherits the ad- vantage of the Gauss-Seidel technique to acceler- ate 
World of Bits: An Open-Domain Platform for Web-Based Agents  Tianlin (Tim) Shi 1 2 Andrej Karpathy 2 Linxi (Jim) Fan 1 Jonathan Hernandez 2 Percy Liang 1  Abstract  While simulated game environments have greatly accelerated research in reinforcement learning, existing environments lack the open-domain re- alism of tasks in computer vision or natural lan- guage processing, which operate on artifacts cre- ated by humans in natural, organic settings. To foster reinforcement learning research in suc
Learning Important Features Through Propagating Activation Differences  Avanti Shrikumar 1 Peyton Greenside 1 Anshul Kundaje 1  Abstract  The purported “black box” nature of neural networks is a barrier to adoption in applica- tions where interpretability is essential. Here we present DeepLIFT (Deep Learning Impor- tant FeaTures), a method for decomposing the output prediction of a neural network on a spe- ciﬁc input by backpropagating the contributions of all neurons in the network to every fea
Optimal Densiﬁcation for Fast and Accurate Minwise Hashing  Anshumali Shrivastava 1  Abstract  Minwise hashing is a fundamental and one of the most successful hashing algorithm in the lit- erature. Recent advances based on the idea of densiﬁcation (Shrivastava & Li, 2014a;c) have shown that it is possible to compute k min- wise hashes, of a vector with d nonzeros, in mere (d + k) computations, a signiﬁcant im- provement over the classical O(dk). These ad- vances have led to an algorithmic improv
Bottleneck Conditional Density Estimation  Rui Shu 1 Hung H. Bui 2 Mohammad Ghavamzadeh 3  Abstract  We introduce a new framework for training deep generative models for high-dimensional condi- tional density estimation. The Bottleneck Con- ditional Density Estimator (BCDE) is a vari- ant of the conditional variational autoencoder (CVAE) that employs layer(s) of stochastic vari- ables as the bottleneck between the input x and target y, where both are high-dimensional. Crucially, we propose a new
Attentive Recurrent Comparators  Pranav Shyam 1 2 Shubham Gupta 2 Ambedkar Dukkipati 2  Abstract  Rapid learning requires ﬂexible representations to quickly adopt to new evidence. We develop a novel class of models called Attentive Recurrent Comparators (ARCs) that form representations of objects by cycling through them and making observations. Using the representations extracted by ARCs, we develop a way of approximating a dynamic representation space and use it for one- shot learning. In the t
Gradient Boosted Decision Trees for High Dimensional Sparse Output  Si Si 1 Huan Zhang 2 S. Sathiya Keerthi 3 Dhruv Mahajan 4 Inderjit S. Dhillon 5 Cho-Jui Hsieh 2  Abstract  In this paper, we study the gradient boosted decision trees (GBDT) when the output space is high dimensional and sparse. For example, in multilabel classiﬁcation, the output space is a L-dimensional 0/1 vector, where L is num- ber of labels that can grow to millions and be- yond in many modern applications. We show that van
The Predictron: End-To-End Learning and Planning  David Silver * 1 Hado van Hasselt * 1 Matteo Hessel * 1 Tom Schaul * 1 Arthur Guez * 1 Tim Harley 1  Gabriel Dulac-Arnold 1 David Reichert 1 Neil Rabinowitz 1 Andre Barreto 1 Thomas Degris 1  Abstract  One of the key challenges of artiﬁcial intelli- gence is to learn models that are effective in the context of planning. In this document we intro- duce the predictron architecture. The predictron consists of a fully abstract model, represented by a
Fractional Langevin Monte Carlo: Exploring L´evy Driven Stochastic  Differential Equations for Markov Chain Monte Carlo  Umut S¸ims¸ekli 1  Abstract  Along with the recent advances in scalable Markov Chain Monte Carlo methods, sampling techniques that are based on Langevin diffu- sions have started receiving increasing attention. These so called Langevin Monte Carlo (LMC) methods are based on diffusions driven by a Brownian motion, which gives rise to Gaussian proposal distributions in the resul
Nonparanormal Information Estimation  Shashank Singh 1 Barnab´as P´oczos 1  Abstract  We study the problem of using i.i.d. samples from an unknown multivariate probability distri- bution p to estimate the mutual information of p. This problem has recently received attention in two settings: (1) where p is assumed to be Gaus- sian and (2) where p is assumed only to lie in a large nonparametric smoothness class. Estima- tors proposed for the Gaussian case converge in high dimensions when the Gauss
High-Dimensional Structured Quantile Regression  Vidyashankar Sivakumar 1 Arindam Banerjee 1  Abstract  Quantile regression aims at modeling the condi- tional median and quantiles of a response vari- able given certain predictor variables. In this work we consider the problem of linear quantile regression in high dimensions where the num- ber of predictor variables is much higher than the number of samples available for parameter estimation. We assume the true parameter to have some structure ch
Robust Budget Allocation via Continuous Submodular Functions  Matthew Staib 1 Stefanie Jegelka 1  Abstract  The optimal allocation of resources for maximiz- ing inﬂuence, spread of information or coverage, has gained attention in the past years, in particu- lar in machine learning and data mining. But in applications, the parameters of the problem are rarely known exactly, and using wrong parame- ters can lead to undesirable outcomes. We hence revisit a continuous version of the Budget Allo- cat
Probabilistic Submodular Maximization in Sub-Linear Time  Serban Stan 1 Morteza Zadimoghaddam 2 Andreas Krause 3 Amin Karbasi 1  Abstract  In this paper, we consider optimizing submodu- lar functions that are drawn from some unknown distribution. This setting arises, e.g., in recom- mender systems, where the utility of a subset of items may depend on a user-speciﬁc submodu- lar utility function. In modern applications, the ground set of items is often so large that even the widely used (lazy) gr
Approximate Steepest Coordinate Descent  Sebastian U. Stich 1 Anant Raj 2 Martin Jaggi 1  Abstract  We propose a new selection rule for the coor- dinate selection in coordinate descent methods for huge-scale optimization. The efﬁciency of this novel scheme is provably better than the efﬁ- ciency of uniformly random selection, and can reach the efﬁciency of steepest coordinate de- scent (SCD), enabling an acceleration of a factor of up to n, the number of coordinates. In many practical applicatio
Tensor Balancing on Statistical Manifold  Mahito Sugiyama 1 2 Hiroyuki Nakahara 3 Koji Tsuda 4 5 6  Abstract  We solve tensor balancing, rescaling an Nth or- der nonnegative tensor by multiplying N ten- sors of order N (cid:0) 1 so that every ﬁber sums to one. This generalizes a fundamental process of matrix balancing used to compare matrices in a wide range of applications from biology to eco- nomics. We present an efﬁcient balancing al- gorithm with quadratic convergence using New- ton’s metho
Safety-Aware Algorithms for Adversarial Contextual Bandit  Wen Sun 1 Debadeepta Dey 2 Ashish Kapoor 2  Abstract  In this work we study the safe sequential decision making problem under the setting of adversar- ial contextual bandits with sequential risk con- straints. At each round, nature prepares a context, a cost for each arm, and additionally a risk for each arm. The learner leverages the context to pull an arm and receives the corresponding cost and risk associated with the pulled arm. In a
Relative Fisher Information and Natural Gradient for Learning Large  Modular Models  Ke Sun 1 Frank Nielsen 2 3  Abstract  Fisher information and natural gradient provided deep insights and powerful tools to artiﬁcial neu- ral networks. However related analysis becomes more and more difﬁcult as the learner’s structure turns large and complex. This paper makes a pre- liminary step towards a new direction. We extract a local component from a large neural system, and deﬁne its relative Fisher infor
meProp: Sparsiﬁed Back Propagation for Accelerated Deep Learning  with Reduced Overﬁtting  Xu Sun 1 2 Xuancheng Ren 1 2 Shuming Ma 1 2 Houfeng Wang 1 2  Abstract  We propose a simple yet effective technique for neural network learning. The forward propaga- tion is computed as usual. In back propagation, only a small subset of the full gradient is com- puted to update the model parameters. The gra- dient vectors are sparsiﬁed in such a way that only the top-k elements (in terms of magnitude) are 
Differentiable Imitation Learning for Sequential Prediction  Deeply AggreVaTeD:  Wen Sun 1 Arun Venkatraman 1 Geoffrey J. Gordon 2 Byron Boots 3 J. Andrew Bagnell 1  Abstract  Recently, researchers have demonstrated state- of-the-art performance on sequential prediction problems using deep neural networks and Re- inforcement Learning (RL). For some of these problems, oracles that can demonstrate good per- formance may be available during training, but are not used by plain RL methods. To take ad
Axiomatic Attribution for Deep Networks  Mukund Sundararajan * 1 Ankur Taly * 1 Qiqi Yan * 1  Abstract  We study the problem of attributing the pre- diction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms— Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisﬁed by most known attri- bution methods, which we consider to be a fun- damental weakness of thos
Distributed Mean Estimation with Limited Communication  Ananda Theertha Suresh 1 Felix X. Yu 1 Sanjiv Kumar 1 H. Brendan McMahan 2  Abstract  Motivated by the need for distributed learning and optimization algorithms with low commu- nication cost, we study communication efﬁcient algorithms for distributed mean estimation. Un- like previous works, we make no probabilistic as- sumptions on the data. We ﬁrst show that for d dimensional data with n clients, a naive stochas- tic rounding approach yie
Selective Inference for Sparse High-Order Interaction Models  Shinya Suzumura 1 Kazuya Nakagawa 1 Yuta Umezu 1 Koji Tsuda 2 3 Ichiro Takeuchi 1 3  Abstract  Finding statistically signiﬁcant high-order inter- actions in predictive modeling is important but challenging task because the possible number of high-order interactions is extremely large (e.g., > 1017). In this paper we study feature se- lection and statistical inference for sparse high- order interaction models. Our main contribution is 
Coherent Probabilistic Forecasts for Hierarchical Time Series  Souhaib Ben Taieb 1 James W. Taylor 2 Rob J. Hyndman 1  Abstract  Many applications require forecasts for a hierar- chy comprising a set of time series along with aggregates of subsets of these series. Hierar- chical forecasting require not only good predic- tion accuracy at each level of the hierarchy, but also the coherency between different levels — the property that forecasts add up appropriately across the hierarchy. A fundament
Partitioned Tensor Factorizations for Learning Mixed Membership Models  Zilong Tan 1 Sayan Mukherjee 1  Abstract  We present an efﬁcient algorithm for learning mixed membership models when the number of variables p is much larger than the number of hidden components k. This algorithm reduces the computational complexity of state-of-the-art tensor methods, which require decomposing an  O(cid:0)p3(cid:1) tensor, to factorizing O (p/k) sub-tensors each of size O(cid:0)k3(cid:1). In addition, we add
Gradient Projection Iterative Sketch for Large-Scale Constrained  Least-Squares  Junqi Tang 1 Mohammad Golbabaee 1 Mike E. Davies 1  Abstract  We propose a randomized ﬁrst order opti- mization algorithm Gradient Projection Iterative Sketch (GPIS) and an accelerated variant for ef- ﬁciently solving large scale constrained Least Squares (LS). We provide the ﬁrst theoretical convergence analysis for both algorithms. An efﬁcient implementation using a tailored line- search scheme is also proposed. W
Neural Networks and Rational Functions  Matus Telgarsky 1  Abstract  Neural networks and rational functions efﬁ- ciently approximate each other. In more de- tail, it is shown here that for any ReLU net- work, there exists a rational function of de- gree O(poly log(1/(cid:15))) which is (cid:15)-close, and sim- ilarly for any rational function there exists a ReLU network of size O(poly log(1/(cid:15))) which is (cid:15)-close. By contrast, polynomials need de- gree Ω(poly(1/(cid:15))) to approxim
Stochastic DCA for the Large-sum of Non-convex Functions Problem and its  Application to Group Variable Selection in Classiﬁcation  Hoai An Le Thi 1 Hoai Minh Le 1 Duy Nhat Phan 1 Bach Tran 1  Abstract  In this paper, we present a stochastic version of DCA (Difference of Convex functions Algo- rithm) to solve a class of optimization problems whose objective function is a large sum of non- convex functions and a regularization term. We consider the (cid:96)2,0 regularization to deal with the grou
An Analytical Formula of Population Gradient for two-layered ReLU network  and its Applications in Convergence and Critical Point Analysis  Yuandong Tian 1  Abstract  j=1 σ(w⊺ j  work g(x; w) = PK  In this paper, we explore theoretical prop- erties of training a two-layered ReLU net- x) with cen- tered d-dimensional spherical Gaussian input x (σ=ReLU). We train our network with gradient descent on w to mimic the output of a teacher network with the same architecture and ﬁxed pa- rameters w∗. We 
Evaluating the Variance of Likelihood-Ratio Gradient Estimators  Seiya Tokui 1 2 Issei Sato 3 2  Abstract  The likelihood-ratio method is often used to es- timate gradients of stochastic computations, for which baselines are required to reduce the esti- mation variance. Many types of baselines have been proposed, although their degree of optimal- ity is not well understood. In this study, we es- tablish a novel framework of gradient estima- tion that includes most of the common gradi- ent estima
Accelerating Eulerian Fluid Simulation With Convolutional Networks  Jonathan Tompson 1 Kristofer Schlachter 2 Pablo Sprechmann 2 3 Ken Perlin 2  Abstract  Efﬁcient simulation of the Navier-Stokes equa- tions for ﬂuid ﬂow is a long standing problem in applied mathematics, for which state-of-the- art methods require large compute resources. In this work, we propose a data-driven approach that leverages the approximation power of deep- learning with the precision of standard solvers to obtain fast 
Boosted Fitted Q-Iteration  Samuele Tosatto 1 2 Matteo Pirotta 3 Carlo D’Eramo 1 Marcello Restelli 1  Abstract  This paper is about the study of B-FQI, an Ap- proximated Value Iteration (AVI) algorithm that exploits a boosting procedure to estimate the action-value function in reinforcement learning problems. B-FQI is an iterative off-line algo- rithm that, given a dataset of transitions, builds an approximation of the optimal action-value func- tion by summing the approximations of the Bell- ma
Diameter-Based Active Learning  Christopher Tosh 1 Sanjoy Dasgupta 1  Abstract  To date, the tightest upper and lower-bounds for the active learning of general concept classes have been in terms of a parameter of the learn- ing problem called the splitting index. We pro- vide, for the ﬁrst time, an efﬁcient algorithm that is able to realize this upper bound, and we empir- ically demonstrate its good performance.  1. Introduction In many situations where a classiﬁer is to be learned, it is easy t
Magnetic Hamiltonian Monte Carlo  Nilesh Tripuraneni 1 Mark Rowland 2 Zoubin Ghahramani 2 3 Richard Turner 2  Abstract  Hamiltonian Monte Carlo (HMC) exploits Hamiltonian dynamics to construct efﬁcient pro- posals for Markov chain Monte Carlo (MCMC). In this paper, we present a generalization of HMC which exploits non-canonical Hamiltonian dynamics. We refer to this algorithm as magnetic HMC, since in 3 dimensions a subset of the dy- namics map onto the mechanics of a charged par- ticle coupled 
Know-Evolve: Deep Temporal Reasoning for Dynamic Knowledge Graphs  Rakshit Trivedi 1 Hanjun Dai 1 Yichen Wang 1 Le Song 1  Abstract  The availability of large scale event data with time stamps has given rise to dynamically evolving knowledge graphs that contain temporal informa- tion for each edge. Reasoning over time in such dynamic knowledge graphs is not yet well under- stood. To this end, we present Know-Evolve, a novel deep evolutionary knowledge network that learns non-linearly evolving en
Hyperplane Clustering via Dual Principal Component Pursuit  Manolis C. Tsakiris 1 Ren´e Vidal 1  Abstract  State-of-the-art methods for clustering data drawn from a union of subspaces are based on sparse and low-rank representation theory and convex optimization algorithms. Existing re- sults guaranteeing the correctness of such meth- ods require the dimension of the subspaces to be small relative to the dimension of the ambient space. When this assumption is violated, as is, e.g., in the case o
Breaking Locality Accelerates Block Gauss-Seidel  Stephen Tu 1 Shivaram Venkataraman 1 Ashia C. Wilson 1 Alex Gittens 2 Michael I. Jordan 1 Benjamin Recht 1  Abstract  Recent work by Nesterov and Stich (2016) showed that momentum can be used to accel- erate the rate of convergence for block Gauss- Seidel in the setting where a ﬁxed partition- ing of the coordinates is chosen ahead of time. We show that this setting is too restrictive, con- structing instances where breaking locality by running n
Multilabel Classiﬁcation with Group Testing and Codes  Shashanka Ubaru 1 Arya Mazumdar 2  Abstract In recent years, the multiclass and mutlilabel classiﬁcation problems we encounter in many ap- plications have very large (103 − 106) number of classes. However, each instance belongs to only one or few classes, i.e., the label vectors are sparse. In this work, we propose a novel ap- proach based on group testing to solve such large multilabel classiﬁcation problems with sparse la- bel vectors. We 
Learning Stable Stochastic Nonlinear Dynamical Systems  Jonas Umlauft 1 Sandra Hirche 1  Abstract  A data-driven identiﬁcation of dynamical sys- tems requiring only minimal prior knowledge is promising whenever no analytically derived model structure is available, e.g., from ﬁrst prin- ciples in physics. However, meta-knowledge on the system’s behavior is often given and should be exploited: Stability as fundamental property is essential when the model is used for controller design or movement g
Learning Determinantal Point Processes with Moments and Cycles  John Urschel 1 Victor-Emmanuel Brunel 1 Ankur Moitra 1 Philippe Rigollet 1  Abstract  Determinantal Point Processes (DPPs) are a fam- ily of probabilistic models that have a repulsive behavior, and lend themselves naturally to many tasks in machine learning where returning a di- verse set of objects is important. While there are fast algorithms for sampling, marginalization and conditioning, much less is known about learn- ing the p
Automatic Discovery of the Statistical Types of Variables in a Dataset  Isabel Valera 1 Zoubin Ghahramani 1 2  Abstract  A common practice in statistics and machine learning is to assume that the statistical data types (e.g., ordinal, categorical or real-valued) of vari- ables, and usually also the likelihood model, is known. However, as the availability of real- world data increases, this assumption becomes too restrictive. Data are often heterogeneous, complex, and improperly or incompletely d
Model-Independent Online Learning for Inﬂuence Maximization  Sharan Vaswani 1 Branislav Kveton 2 Zheng Wen 2 Mohammad Ghavamzadeh 3 Laks V.S. Lakshmanan 1  Mark Schmidt 1  Abstract  We consider inﬂuence maximization (IM) in so- cial networks, which is the problem of maximiz- ing the number of users that become aware of a product by selecting a set of “seed” users to ex- pose the product to. While prior work assumes a known model of information diffusion, we pro- pose a novel parametrization that
FeUdal Networks for Hierarchical Reinforcement Learning  Alexander Sasha Vezhnevets 1 Simon Osindero 1 Tom Schaul 1 Nicolas Heess 1 Max Jaderberg 1 David Silver 1  Koray Kavukcuoglu 1  Abstract  We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learn- ing. Our approach is inspired by the feudal rein- forcement learning proposal of Dayan and Hin- ton, and gains power and efﬁcacy by decou- pling end-to-end learning across multiple levels – allowing it to util
Scalable Multi-Class Gaussian Process Classiﬁcation  using Expectation Propagation  Carlos Villacampa-Calvo * 1 Daniel Hern´andez-Lobato * 1  Abstract  This paper describes an expectation propagation (EP) method for multi-class classiﬁcation with Gaussian processes that scales well to very large datasets. In such a method the estimate of the log-marginal-likelihood involves a sum across the data instances. This enables efﬁcient train- ing using stochastic gradients and mini-batches. When this ty
Learning to Generate Long-term Future via Hierarchical Prediction  Ruben Villegas 1 * Jimei Yang 2 Yuliang Zou 1 Sungryull Sohn 1 Xunyu Lin 3 Honglak Lee 1 4  Abstract  We propose a hierarchical approach for making long-term predictions of future frames. To avoid inherent compounding errors in recursive pixel- level prediction, we propose to ﬁrst estimate high- level structure in the input frames, then predict how that structure evolves in the future, and ﬁ- nally by observing a single frame fro
On orthogonality and learning recurrent networks with long term dependencies  Eugene Vorontsov 1 2 Chiheb Trabelsi 1 2 Samuel Kadoury 1 3 Chris Pal 1 2  Abstract  It is well known that it is challenging to train deep neural networks and recurrent neural net- works for tasks that exhibit long term dependen- cies. The vanishing or exploding gradient prob- lem is a well known issue associated with these challenges. One approach to addressing vanish- ing and exploding gradients is to use either soft
Fast Bayesian Intensity Estimation for the Permanental Process  Christian J. Walder 1 2 Adrian N. Bishop 1 2 3  Abstract  The Cox process is a stochastic process which generalises the Poisson process by letting the un- derlying intensity function itself be a stochastic process. In this paper we present a fast Bayesian inference scheme for the permanental process, a Cox process under which the square root of the intensity is a Gaussian process. In particu- lar we exploit connections with reproduc
Optimal and Adaptive Off-policy Evaluation in Contextual Bandits  Yu-Xiang Wang 1 Alekh Agarwal 2 Miroslav Dudík 2  Abstract  We study the off-policy evaluation problem— estimating the value of a target policy using data collected by another policy—under the contextual bandit model. We consider the general (agnostic) setting without access to a consistent model of re- wards and establish a minimax lower bound on the mean squared error (MSE). The bound is matched up to constants by the inverse pr
Capacity Releasing Diffusion for Speed and Locality  Di Wang 1 Kimon Fountoulakis 2 Monika Henzinger 3 Michael W. Mahoney 2 Satish Rao 1  Abstract  Diffusions and related random walk procedures are of central importance in many areas of ma- chine learning, data analysis, and applied mathe- matics. Because they spread mass agnostically at each step in an iterative manner, they can some- times spread mass “too aggressively,” thereby failing to ﬁnd the “right” clusters. We introduce a novel Capacit
Sketched Ridge Regression: Optimization Perspective,  Statistical Perspective, and Model Averaging  Shusen Wang 1 Alex Gittens 2 Michael W. Mahoney 1  Abstract  We address the statistical and optimization im- pacts of using classical sketch versus Hessian sketch to solve approximately the Matrix Ridge Regression (MRR) problem. Prior research has considered the effects of classical sketch on least squares regression (LSR), a strictly simpler prob- lem. We establish that classical sketch has a sim
Robust Gaussian Graphical Model Estimation with Arbitrary Corruption  Lingxiao Wang 1 Quanquan Gu 1  Abstract  We study the problem of estimating the high- dimensional Gaussian graphical model where the data are arbitrarily corrupted. We propose a ro- bust estimator for the sparse precision matrix in the high- dimensional regime. At the core of our method is a robust covariance matrix estimator, which is based on truncated inner product. We establish the statistical guarantee of our estima- tor 
Max-value Entropy Search for Efﬁcient Bayesian Optimization  Zi Wang 1 Stefanie Jegelka 1  Abstract  Entropy Search (ES) and Predictive Entropy Search (PES) are popular and empirically suc- cessful Bayesian Optimization techniques. Both rely on a compelling information-theoretic mo- tivation, and maximize the information gained about the arg max of the unknown function; yet, both are plagued by the expensive computation for estimating entropies. We propose a new crite- rion, Max-value Entropy Se
EfﬁcientDistributedLearningwithSparsityJialeiWang1MladenKolar1NathanSrebro2TongZhang3AbstractWeproposeanovel,efﬁcientapproachfordis-tributedsparselearningwithobservationsran-domlypartitionedacrossmachines.Ineachroundoftheproposedmethod,workermachinescomputethegradientofthelossonlocaldataandthemastermachinesolvesashifted‘1regular-izedlossminimizationproblem.Afteranumberofcommunicationroundsthatscalesonlylog-arithmicallywiththenumberofmachines,andindependentofotherparametersoftheproblem,thepropose
Robust Probabilistic Modeling with Bayesian Data Reweighting  Yixin Wang 1 Alp Kucukelbir 1 David M. Blei 1  Abstract  Probabilistic models analyze data by relying on a set of assumptions. Data that exhibit devia- tions from these assumptions can undermine infer- ence and prediction quality. Robust models offer protection against mismatch between a model’s assumptions and reality. We propose a way to systematically detect and mitigate mismatch of a large class of probabilistic models. The idea i
Batched High-dimensional Bayesian Optimization  via Structural Kernel Learning  Zi Wang * 1 Chengtao Li * 1 Stefanie Jegelka 1 Pushmeet Kohli 2  Abstract  Optimization of high-dimensional black-box functions is an extremely challenging problem. While Bayesian optimization has emerged as a popular approach for optimizing black-box functions, its applicability has been limited to low-dimensional problems due to its compu- tational and statistical challenges arising from high-dimensional settings. 
Tensor Decomposition via Simultaneous Power Iteration  Po-An Wang 1 Chi-Jen Lu 1  Abstract  Tensor decomposition is an important problem with many applications across several disci- plines, and a popular approach for this problem is the tensor power method. However, previous works with theoretical guarantee based on this approach can only ﬁnd the top eigenvectors one after one, unlike the case for matrices. In this paper, we show how to ﬁnd the eigenvectors si- multaneously with the help of a ne
Sequence Modeling via Segmentations  Chong Wang 1 Yining Wang 2 Po-Sen Huang 1 Abdelrahman Mohamed 3 Dengyong Zhou 1 Li Deng 4  Abstract  Segmental structure is a common pattern in many types of sequences such as phrases in human languages. In this paper, we present a proba- bilistic model for sequences via their segmenta- tions. The probability of a segmented sequence is calculated as the product of the probabilities of all its segments, where each segment is mod- eled using existing tools such
Variational Policy for Guiding Point Processes  Yichen Wang 1 Grady Williams 2 Evangelos Theodorou 2 Le Song 1  Abstract  Temporal point processes have been widely ap- plied to model event sequence data generated by online users. In this paper, we consider the prob- lem of how to design the optimal control policy for point processes, such that the stochastic sys- tem driven by the point process is steered to a target state. In particular, we exploit the key in- sight to view the stochastic optim
ExploitingStrongConvexityfromDatawithPrimal-DualFirst-OrderAlgorithmsJialeiWang1LinXiao2AbstractWeconsiderempiricalriskminimizationoflin-earpredictorswithconvexlossfunctions.Suchproblemscanbereformulatedasconvex-concavesaddlepointproblemsandsolvedbyprimal-dualﬁrst-orderalgorithms.However,primal-dualal-gorithmsoftenrequireexplicitstronglyconvexregularizationinordertoobtainfastlinearcon-vergence,andtherequireddualproximalmap-pingmaynotadmitclosed-formorefﬁcientso-lution.Inthispaper,wedevelopbothba
Beyond Filters: Compact Feature Map for Portable Deep Model  Yunhe Wang 1 Chang Xu 2 Chao Xu 1 Dacheng Tao 2  Abstract  Convolutional neural networks (CNNs) have shown extraordinary performance in a number of applications, but they are usually of heavy design for the accuracy reason. Beyond compressing the ﬁlters in CNNs, this paper focuses on the redun- dancy in the feature maps derived from the large number of ﬁlters in a layer. We propose to ex- tract intrinsic representation of the feature m
A Uniﬁed Variance Reduction-Based Framework for Nonconvex Low-Rank  Matrix Recovery  Lingxiao Wang * 1 Xiao Zhang * 1 Quanquan Gu 1  Abstract  We propose a generic framework based on a new stochastic variance-reduced gradient descent algorithm for accelerating nonconvex low-rank matrix recovery. Starting from an appropriate initial estimator, our proposed algorithm per- forms projected gradient descent based on a novel semi-stochastic gradient speciﬁcally de- signed for low-rank matrix recovery.
Source-Target Similarity Modelings for Multi-Source Transfer Gaussian  Process Regression  Pengfei Wei 1 2 Ramon Sagarna 1 2 Yiping Ke 1 2 Yew-Soon Ong 1 2 Chi-Keong Goh 3 2  Abstract  A key challenge in multi-source transfer learn- ing is to capture the diverse inter-domain sim- ilarities. In this paper, we study different ap- proaches based on Gaussian process models to solve the multi-source transfer regression prob- lem. Precisely, we ﬁrst investigate the feasibility and performance of a fam
Latent Intention Dialogue Models  Tsung-Hsien Wen 1 * Yishu Miao 2 * Phil Blunsom 2 Steve Young 1  Abstract  Developing a dialogue agent that is capable of making autonomous decisions and communicat- ing by natural language is one of the long-term goals of machine learning research. Traditional approaches either rely on hand-crafting a small state-action set for applying reinforcement learn- ing that is not scalable or constructing determin- istic models for learning dialogue sentences that fail
Unifying Task Speciﬁcation in Reinforcement Learning  Martha White 1  Abstract  Reinforcement learning tasks are typically speci- ﬁed as Markov decision processes. This formal- ism has been highly successful, though speciﬁca- tions often couple the dynamics of the environ- ment and the learning objective. This lack of mod- ularity can complicate generalization of the task speciﬁcation, as well as obfuscate connections between different task settings, such as episodic and continuing. In this work
Learned Optimizers that Scale and Generalize  Olga Wichrowska 1 Niru Maheswaranathan 2 3 Matthew W. Hoffman 4 Sergio G´omez Colmenarejo 4  Misha Denil 4 Nando de Freitas 4 Jascha Sohl-Dickstein 1  Abstract  Learning to learn has emerged as an important di- rection for achieving artiﬁcial intelligence. Two of the primary barriers to its adoption are an in- ability to scale to larger problems and a limited ability to generalize to new tasks. We intro- duce a learned gradient descent optimizer that
Exact Inference for Integer Latent-Variable Models  Kevin Winner 1 Debora Sujono 1 Dan Sheldon 1 2  Abstract  Graphical models with latent count variables arise in a number of areas. However, standard inference algorithms do not apply to these mod- els due to the inﬁnite support of the latent vari- ables. Winner & Sheldon (2016) recently devel- oped a new technique using probability generat- ing functions (PGFs) to perform efﬁcient, exact inference for certain Poisson latent variable mod- els. H
Tensor Belief Propagation  Andrew Wrigley 1 Wee Sun Lee 2 Nan Ye 3  Abstract  We propose a new approximate inference algo- rithm for graphical models, tensor belief prop- agation, based on approximating the messages passed in the junction tree algorithm. Our al- gorithm represents the potential functions of the graphical model and all messages on the junction tree compactly as mixtures of rank-1 tensors. Us- ing this representation, we show how to perform the operations required for inference on
A Uniﬁed View of Multi-Label Performance Measures  Xi-Zhu Wu 1 Zhi-Hua Zhou 1  Abstract  Multi-label classiﬁcation deals with the problem where each instance is associated with multiple class labels. Because evaluation in multi-label classiﬁcation is more complicated than single- label setting, a number of performance measures have been proposed. It is noticed that an algo- rithm usually performs differently on different measures. Therefore, it is important to under- stand which algorithms perfo
Dual Supervised Learning  Yingce Xia 1 Tao Qin 2 Wei Chen 2 Jiang Bian 2 Nenghai Yu 1 Tie-Yan Liu 2  Abstract  Many supervised learning tasks are emerged in dual forms, e.g., English-to-French translation vs. French-to-English translation, speech recog- nition vs. text to speech, and image classiﬁcation vs. image generation. Two dual tasks have intrin- sic connections with each other due to the prob- abilistic correlation between their models. This connection is, however, not effectively utilize
Learning Latent Space Models with Angular Constraints  Pengtao Xie 1 2 Yuntian Deng 3 Yi Zhou 4 Abhimanu Kumar 5 Yaoliang Yu 6 James Zou 7 Eric P. Xing 2  Abstract  The large model capacity of latent space mod- els (LSMs) enables them to achieve great per- formance on various applications, but meanwhile renders LSMs to be prone to overﬁtting. Several recent studies investigate a new type of regular- ization approach, which encourages components in LSMs to be diverse, for the sake of alleviating 
Uncorrelation and Evenness: a New Diversity-Promoting Regularizer  Pengtao Xie 1 2 Aarti Singh 1 Eric P. Xing 2  Abstract  Latent space models (LSMs) provide a princi- pled and effective way to extract hidden patterns from observed data. To cope with two challenges in LSMs: (1) how to capture infrequent pat- terns when pattern frequency is imbalanced and (2) how to reduce model size without sacriﬁcing their expressiveness, several studies have been proposed to “diversify” LSMs, which design reg-
Stochastic Convex Optimization:  Faster Local Growth Implies Faster Global Convergence  Yi Xu 1 Qihang Lin 2 Tianbao Yang 1  Abstract  2  In this paper, a new theory is developed for ﬁrst- order stochastic convex optimization, showing that the global convergence rate is sufﬁciently quantiﬁed by a local growth rate of the objec- tive function in a neighborhood of the optimal solutions. In particular, if the objective func- tion F (w) in the (cid:15)-sublevel set grows as fast as (cid:107)w − w∗(c
Learning Hawkes Processes from Short Doubly-Censored Event Sequences  Hongteng Xu 1 Dixin Luo 2 Hongyuan Zha 1  Abstract  Many real-world applications require robust al- gorithms to learn point processes based on a type of incomplete data — the so-called short doubly- censored (SDC) event sequences. We study this critical problem of quantitative asynchronous event sequence analysis under the framework of Hawkes processes by leveraging the idea of data synthesis. Given SDC event sequences ob- ser
High-dimensional Non-Gaussian Single Index Models via Thresholded Score  Function Estimation  Zhuoran Yang 1 Krishnakumar Balasubramanian 1 Han Liu 1  Abstract  We consider estimating the parametric compo- nent of single index models in high dimensions. Compared with existing work, we do not require the covariate to be normally distributed. Utilizing Stein’s Lemma, we propose estimators based on the score function of the covariate. Moreover, to handle score function and response variables that a
Towards K-means-friendly Spaces: Simultaneous Deep  Learning and Clustering  Bo Yang 1 Xiao Fu 1 Nicholas D. Sidiropoulos 1 Mingyi Hong 2  Abstract  Most learning approaches treat dimensionality reduction (DR) and clustering separately (i.e., se- quentially), but recent research has shown that optimizing the two tasks jointly can substantially improve the performance of both. The premise behind the latter genre is that the data samples are obtained via linear transformation of latent repre- sent
On The Projection Operator to A Three-view Cardinality Constrained Set  Haichuan Yang 1 Shupeng Gui 1 Chuyang Ke 1 Daniel Stefankovic 1 Ryohei Fujimaki 2 Ji Liu 1  Abstract  The cardinality constraint is an intrinsic way to restrict the solution structure in many domains, for example, sparse learning, feature selection, and compressed sensing. To solve a cardinal- ity constrained problem, the key challenge is to solve the projection onto the cardinality con- straint set, which is NP-hard in gene
Improved Variational Autoencoders for Text Modeling using Dilated  Convolutions  Zichao Yang 1 Zhiting Hu 1 Ruslan Salakhutdinov 1 Taylor Berg-Kirkpatrick 1  Abstract  Recent work on generative text modeling has found that variational autoencoders (VAE) with LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning informa- tion from the e
Tensor-Train Recurrent Neural Networks for Video Classiﬁcation  Yinchong Yang 1 2 Denis Krompass 2 Volker Tresp 1 2  Abstract  The Recurrent Neural Networks and their vari- ants have shown promising performances in se- quence modeling tasks such as Natural Language Processing. These models, however, turn out to be impractical and difﬁcult to train when exposed to very high-dimensional inputs due to the large input-to-hidden weight matrix. This may have prevented RNNs’ large-scale application in 
A Richer Theory of Convex Constrained Optimization  with Reduced Projections and Improved Rates  Tianbao Yang 1 Qihang Lin 1 Lijun Zhang 2  Abstract  This paper focuses on convex constrained opti- mization problems, where the solution is subject to a convex inequality constraint. In particular, we aim at challenging problems for which both projection into the constrained domain and a lin- ear optimization under the inequality constraint are time-consuming, which render both projected gradient me
Sparse + Group-Sparse Dirty Models: Statistical Guarantees without  Unreasonable Conditions and a Case for Non-Convexity  Eunho Yang 1 2 Aur´elie C. Lozano 3  Abstract  Imposing sparse + group-sparse superposition structures in high-dimensional parameter estima- tion is known to provide ﬂexible regularization that is more realistic for many real-world prob- lems. For example, such a superposition en- ables partially-shared support sets in multi-task learning, thereby striking the right balance b
Scalable Bayesian Rule Lists  Hongyu Yang 1 Cynthia Rudin 2 Margo Seltzer 3  Abstract  We present an algorithm for building probabilis- tic rule lists that is two orders of magnitude faster than previous work. Rule list algorithms are competitors for decision tree algorithms. They are associative classiﬁers, in that they are built from pre-mined association rules. They have a logical structure that is a sequence of IF-THEN rules, identical to a decision list or one-sided decision tree. Instead o
Approximate Newton Methods and Their Local Convergence  Haishan Ye 1 Luo Luo 1 Zhihua Zhang 2  Abstract  Many machine learning models are reformulated as optimization problems. Thus, it is important to solve a large-scale optimization problem in big data applications. Recently, stochastic sec- ond order methods have emerged to attract much attention for optimization due to their efﬁciency at each iteration, rectiﬁed a weakness in the ordi- nary Newton method of suffering a high cost in each iter
A Simulated Annealing Based Inexact Oracle  for Wasserstein Loss Minimization  Jianbo Ye 1 James Z. Wang 1 Jia Li 2  Abstract  Learning under a Wasserstein loss, a.k.a. Wasser- stein loss minimization (WLM), is an emerging research topic for gaining insights from a large set of structured objects. Despite being concep- tually simple, WLM problems are computation- ally challenging because they involve minimiz- ing over functions of quantities (i.e. Wasserstein distances) that themselves require n
Latent Feature Lasso  Ian E.H. Yen 1 Wei-Cheng Lee 2 Sung-En Chang 2 Arun S. Suggala 1 Shou-De Lin 2 Pradeep Ravikumar 1  Abstract  The latent feature model (LFM), proposed in (Grifﬁths & Ghahramani, 2005), but possibly with earlier origins, is a generalization of a mix- ture model, where each instance is generated not from a single latent class but from a combina- tion of latent features. Thus, each instance has an associated latent binary feature incidence vec- tor indicating the presence or a
Combined Group and Exclusive Sparsity for Deep Neural Networks  Jaehong Yoon 1 Sung Ju Hwang 1 2  Abstract  The number of parameters in a deep neural net- work is usually very large, which helps with its learning capacity but also hinders its scalabil- ity and practicality due to memory/time inefﬁ- ciency and overﬁtting. To resolve this issue, we propose a sparsity regularization method that exploits both positive and negative correlations among the features to enforce the network to be sparse, 
Joint Clustering and Non-Linear Dynamic Modeling of Sequential Data  Latent LSTM Allocation  Manzil Zaheer 1 Amr Ahmed 2 Alexander J Smola 1  Abstract  Recurrent neural networks, such as long-short term memory (LSTM) networks, are power- ful tools for modeling sequential data like user browsing history (Tan et al., 2016; Korpusik et al., 2016) or natural language text (Mikolov et al., 2010). However, to generalize across dif- ferent user types, LSTMs require a large num- ber of parameters, notwi
Canopy — Fast Sampling with Cover Trees  Manzil Zaheer 1 2 * Satwik Kottur 1 * Amr Ahmed 3 Jos´e Moura 1 Alex Smola 1 2  Abstract  Hierarchical Bayesian models often capture distri- butions over a very large number of distinct atoms. The need for these models arises when organizing huge amount of unsupervised data, for instance, features extracted using deep convnets that can be exploited to organize abundant unlabeled im- ages. Inference for hierarchical Bayesian models in such cases can be rat
Continual Learning Through Synaptic Intelligence  Friedemann Zenke * 1 Ben Poole * 1 Surya Ganguli 1  Abstract  While deep learning has led to remarkable ad- vances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, possibly by leveraging com- plex molecular machinery to solve many tasks In this study, we introduce in- simultaneously. telligent s
Stochastic Gradient Monomial Gamma Sampler  Yizhe Zhang 1 Changyou Chen 1 Zhe Gan 1 Ricardo Henao 1 Lawrence Carin 1  Abstract  Recent advances in stochastic gradient tech- niques have made it possible to estimate poste- rior distributions from large datasets via Markov Chain Monte Carlo (MCMC). However, when the target posterior is multimodal, mixing per- formance is often poor. This results in inade- quate exploration of the posterior distribution. A framework is proposed to improve the sampli
Adversarial Feature Matching for Text Generation  Yizhe Zhang 1 Zhe Gan 1 Kai Fan 1 Zhi Chen 1 Ricardo Henao 1 Dinghan Shen 1 Lawrence Carin 1  Abstract  The Generative Adversarial Network (GAN) has achieved great success in generating realistic (real- valued) synthetic data. However, convergence issues and difﬁculties dealing with discrete data hinder the applicability of GAN to text. We pro- pose a framework for generating realistic text via adversarial training. We employ a long short- term m
Scaling Up Sparse Support Vector Machines  by Simultaneous Feature and Sample Reduction  Weizhong Zhang * 1 2 Bin Hong * 1 3 Wei Liu 2 Jieping Ye 3 Deng Cai 1 Xiaofei He 1 Jie Wang 3  Abstract  Sparse support vector machine (SVM) is a pop- ular classiﬁcation technique that can simultane- ously learn a small set of the most interpretable features and identify the support vectors. It has achieved great successes in many real-world ap- plications. However, for large-scale problems in- volving a hug
Re-revisiting Learning on Hypergraphs:  Conﬁdence Interval and Subgradient Method  Chenzi Zhang * 1 Shuguang Hu * 1 Zhihao Gavin Tang 1 T-H. Hubert Chan 1 2  Abstract  We revisit semi-supervised learning on hyper- graphs. Same as previous approaches, our method uses a convex program whose objective function is not everywhere differentiable. We exploit the non-uniqueness of the optimal solu- tions, and consider conﬁdence intervals which give the exact ranges that unlabeled vertices take in any op
ZipML: Training Linear Models with End-to-End Low Precision,  and a Little Bit of Deep Learning  Hantian Zhang 1 Jerry Li 2 Kaan Kara 1 Dan Alistarh 1 3 Ji Liu 4 Ce Zhang 1  Abstract  Recently there has been signiﬁcant interest in training machine-learning models at low preci- sion: by reducing precision, one can reduce com- putation and communication by one order of magnitude. We examine training at reduced pre- cision, both from a theoretical and practical per- spective, and ask: is it possibl
Convexiﬁed Convolutional Neural Networks  Yuchen Zhang 1 Percy Liang 1 Martin J. Wainwright 2  Abstract  We describe the class of convexiﬁed convolu- tional neural networks (CCNNs), which capture the parameter sharing of convolutional neural networks in a convex manner. By representing the nonlinear convolutional ﬁlters as vectors in a reproducing kernel Hilbert space, the CNN pa- rameters can be represented in terms of a low- rank matrix, and the rank constraint can be re- laxed so as to obtain
Projection-free Distributed Online Learning in Networks  Wenpeng Zhang 1 Peilin Zhao 2 Wenwu Zhu 1 Steven C. H. Hoi 3 Tong Zhang 4  Abstract  The conditional gradient algorithm has regained a surge of research interest in recent years due to its high efﬁciency in handling large-scale ma- chine learning problems. However, none of ex- isting studies has explored it in the distributed online learning setting, where locally light com- putation is assumed. In this paper, we ﬁll this gap by proposing 
Multi-Class Optimal Margin Distribution Machine  Teng Zhang 1 Zhi-Hua Zhou 1  Abstract  Recent studies disclose that maximizing the min- imum margin like support vector machines does not necessarily lead to better generalization per- formances, and instead, it is crucial to opti- mize the margin distribution. Although it has been shown that for binary classiﬁcation, char- acterizing the margin distribution by the ﬁrst- and second-order statistics can achieve superior performance. It still remain
Leveraging Node Attributes for Incomplete Relational Data  He Zhao 1 Lan Du 1 Wray Buntine 1  Abstract  Relational data are usually highly incomplete in practice, which inspires us to leverage side in- formation to improve the performance of com- munity detection and link prediction. This paper presents a Bayesian probabilistic approach that incorporates various kinds of node attributes en- coded in binary form in relational models with Poisson likelihood. Our method works ﬂexibly with both dire
Theoretical Properties for Neural Networks with Weight Matrices of Low  Displacement Rank  Liang Zhao 1 Siyu Liao 1 Yanzhi Wang 2 Zhe Li 2 Jian Tang 2 Bo Yuan 1  Abstract  Recently low displacement rank (LDR) matri- ces, or so-called structured matrices, have been proposed to compress large-scale neural net- works. Empirical results have shown that neu- ral networks with weight matrices of LDR ma- trices, referred as LDR neural networks, can achieve signiﬁcant reduction in space and com- putatio
Learning Hierarchical Features from Deep Generative Models  Shengjia Zhao 1 Jiaming Song 1 Stefano Ermon 1  Abstract  Deep neural networks have been shown to be very successful at learning feature hierarchies in supervised learning tasks. Generative models, on the other hand, have beneﬁted less from hierar- chical models with multiple layers of latent vari- ables. In this paper, we prove that hierarchical latent variable models do not take advantage of the hierarchical structure when trained wit
Learning Sleep Stages from Radio Signals: A Conditional Adversarial Architecture  Mingmin Zhao 1 Shichao Yue 1 Dina Katabi 1 Tommi S. Jaakkola 1 Matt T. Bianchi 2  Abstract  We focus on predicting sleep stages from radio measurements without any attached sensors on subjects. We introduce a new predictive model that combines convolutional and recurrent neu- ral networks to extract sleep-speciﬁc subject- invariant features from RF signals and capture the temporal progression of sleep. A key inno- 
Follow the Moving Leader in Deep Learning  Shuai Zheng 1 James T. Kwok 1  Abstract  Deep networks are highly nonlinear and difﬁcult to optimize. During training, the parameter iter- ate may move from one local basin to another, or the data distribution may even change. In- spired by the close connection between stochas- tic optimization and online learning, we pro- pose a variant of the follow the regularized leader (FTRL) algorithm called follow the mov- ing leader (FTML). Unlike the FTRL famil
Asynchronous Stochastic Gradient Descent with Delay Compensation  Shuxin Zheng 1 Qi Meng 2 Taifeng Wang 3 Wei Chen 3 Nenghai Yu 2 Zhi-Ming Ma 4 Tie-Yan Liu 3  Abstract  With the fast development of deep learning, it has become common to learn big neural networks using massive training data. Asynchronous Stochastic Gradient Descent (ASGD) is widely adopted to fulﬁll this task for its efﬁciency, which is, however, known to suffer from the problem of delayed gradients. That is, when a local worker 
Collect at Once, Use Effectively:  Making Non-interactive Locally Private Learning Possible  Kai Zheng * 1 Wenlong Mou * 1 Liwei Wang 1  Abstract  Non-interactive Local Differential Privacy (LDP) requires data analysts to collect data from users through noisy channel at once. In this paper, we extend the frontiers of Non-interactive LDP learning and estimation from several aspects. For learning with smooth generalized linear losses, we propose an approximate stochastic gradient oracle estimated 
Recovery Guarantees for One-hidden-layer Neural Networks∗  Kai Zhong 1 Zhao Song 2 Prateek Jain 3 Peter L. Bartlett 4 Inderjit S. Dhillon 5  Abstract  In this paper, we consider regression problems with one-hidden-layer neural networks (1NNs). We distill some properties of activation func- tions that lead to local strong convexity in the neighborhood of the ground-truth parameters for the 1NN squared-loss objective and most popu- lar nonlinear activation functions satisfy the dis- tilled propert
Stochastic Adaptive Quasi-Newton Methods for Minimizing Expected Values  Chaoxu Zhou ∗ 1 Wenbo Gao ∗ 1 Donald Goldfarb 1  Abstract  We propose a novel class of stochastic, adaptive methods for minimizing self-concordant func- tions which can be expressed as an expected value. These methods generate an estimate of the true objective function by taking the empir- ical mean over a sample drawn at each step, making the problem tractable. The use of adap- tive step sizes eliminates the need for the u
Identify the Nash Equilibrium in Static Games with Random Payoffs  Yichi Zhou 1 Jialian Li 1 Jun Zhu 1  Abstract  We study the problem on how to learn the pure Nash Equilibrium of a two-player zero-sum static game with random payoffs under unknown dis- tributions via efﬁcient payoff queries. We intro- duce a multi-armed bandit model to this problem due to its ability to ﬁnd the best arm efﬁciently among random arms and propose two algorithms for this problem—LUCB-G based on the conﬁ- dence bound
When can Multi-Site Datasets be Pooled for Regression?  Hypothesis Tests, (cid:96)2-consistency and Neuroscience Applications  Hao Henry Zhou 1 Yilin Zhang 1 Vamsi K. Ithapu 1 Sterling C. Johnson 1 2 Grace Wahba 1 Vikas Singh 1  Abstract  Many studies in biomedical and health sciences involve small sample sizes due to logistic or ﬁ- nancial constraints. Often, identifying weak (but scientiﬁcally interesting) associations between a set of predictors and a response necessitates pooling datasets fr
High-Dimensional Variance-Reduced Stochastic Gradient  Expectation-Maximization Algorithm  Rongda Zhu 1 Lingxiao Wang 2 Chengxiang Zhai 3 Quanquan Gu 2  Abstract  We propose a generic stochastic expectation- maximization (EM) algorithm for the estimation of high-dimensional latent variable models. At the core of our algorithm is a novel semi-stochastic variance-reduced gradient designed for the Q- function in the EM algorithm. Under a mild con- dition on the initialization, our algorithm is guar
Recurrent Highway Networks  Julian Georg Zilly * 1 Rupesh Kumar Srivastava * 2 Jan Koutník 2 Jürgen Schmidhuber 2  Abstract  Many sequential processing tasks require com- plex nonlinear transition functions from one step to the next. However, recurrent neural networks with "deep" transition functions remain difﬁcult to train, even when using Long Short-Term Mem- ory (LSTM) networks. We introduce a novel the- oretical analysis of recurrent networks based on Geršgorin’s circle theorem that illumin
Online Learning to Rank in Stochastic Click Models  Masrour Zoghi 1 Tomas Tunys 2 Mohammad Ghavamzadeh 3 Branislav Kveton 4 Csaba Szepesvari 5  Zheng Wen 4  Abstract  Online learning to rank is a core problem in in- formation retrieval and machine learning. Many provably efﬁcient algorithms have been recently proposed for this problem in speciﬁc click mod- els. The click model is a model of how the user interacts with a list of documents. Though these results are signiﬁcant, their impact on prac

A New Theory for Matrix Completion  Guangcan Liu∗  Qingshan Liu†  Xiao-Tong Yuan‡  B-DAT, School of Information & Control, Nanjing Univ Informat Sci & Technol  NO 219 Ningliu Road, Nanjing, Jiangsu, China, 210044  {gcliu,qsliu,xtyuan}@nuist.edu.cn  Abstract  Prevalent matrix completion theories reply on an assumption that the locations of the missing data are distributed uniformly and randomly (i.e., uniform sampling). Nevertheless, the reason for observations being missing often depends on the 
Union of Intersections (UoI) for Interpretable Data  Driven Discovery and Prediction  Kristofer E. Bouchard∗  Alejandro F. Bujan†  Farbod Roosta-Khorasani‡  Shashanka Ubaru§  Prabhat¶  Antoine M. Snijders(cid:107)  Jian-Hua Mao(cid:107)  Edward F. Chang∗∗  Michael W. Mahoney‡  Sharmodeep Bhattacharyya††  Abstract  The increasing size and complexity of scientiﬁc data could dramatically enhance discovery and prediction for basic scientiﬁc applications. Realizing this potential, however, requires n
Learning spatiotemporal piecewise-geodesic  trajectories from longitudinal manifold-valued data  Juliette Chevallier  CMAP, École polytechnique  juliette.chevallier@polytechnique.edu  Pr Stéphane Oudard Oncology Department USPC, AP-HP, HEGP  Stéphanie Allassonnière  CRC, Université Paris Descartes  stephanie.allassonniere@parisdescartes.fr  Abstract  We introduce a hierarchical model which allows to estimate a group-average piecewise-geodesic trajectory in the Riemannian space of measurements an
Deanonymization in the Bitcoin P2P Network  Giulia Fanti and Pramod Viswanath  Abstract  Recent attacks on Bitcoin’s peer-to-peer (P2P) network demonstrated that its transaction-ﬂooding protocols, which are used to ensure network consistency, may enable user deanonymization—the linkage of a user’s IP address with her pseudonym in the Bitcoin network. In 2015, the Bitcoin community responded to these attacks by changing the network’s ﬂooding mechanism to a different protocol, known as diffusion. 
A Scale Free Algorithm for Stochastic Bandits with  Bounded Kurtosis  Tor Lattimore∗  tor.lattimore@gmail.com  Abstract  Existing strategies for ﬁnite-armed stochastic bandits mostly depend on a param- eter of scale that must be known in advance. Sometimes this is in the form of a bound on the payoffs, or the knowledge of a variance or subgaussian parameter. The notable exceptions are the analysis of Gaussian bandits with unknown mean and variance by Cowan et al. [2015] and of uniform distributi
Flexpoint: An Adaptive Numerical Format for Efﬁcient Training of Deep Neural Networks  Urs Köster∗†, Tristan J. Webb∗, Xin Wang∗, Marcel Nassar∗, Arjun K. Bansal, William H. Constable, O˘guz H. Elibol, Scott Gray‡, Stewart Hall†, Luke Hornof, Amir Khosrowshahi,  Carey Kloss, Ruby J. Pai, Naveen Rao  Artiﬁcial Intelligence Products Group, Intel Corporation  Abstract  Deep neural networks are commonly developed and trained in 32-bit ﬂoating point format. Signiﬁcant gains in performance and energy 
Minimal Exploration  in Structured Stochastic Bandits  Richard Combes  Centrale-Supelec / L2S  richard.combes@supelec.fr  Stefan Magureanu  KTH, EE School / ACL  magur@kth.se  Alexandre Proutiere KTH, EE School / ACL  alepro@kth.se  Abstract  This paper introduces and addresses a wide class of stochastic bandit problems where the function mapping the arm to the corresponding reward exhibits some known structural properties. Most existing structures (e.g. linear, Lipschitz, uni- modal, combinator
Information Theoretic Properties of Markov Random  Fields, and their Algorithmic Applications  Linus Hamilton∗  Frederic Koehler †  Ankur Moitra ‡  Abstract  Markov random ﬁelds are a popular model for high-dimensional probability distri- butions. Over the years, many mathematical, statistical and algorithmic problems on them have been studied. Until recently, the only known algorithms for provably learning them relied on exhaustive search, correlation decay or various incoher- ence assumptions.
Decomposable Submodular Function Minimization  Discrete and Continuous  Alina Ene∗  Huy L. Nguy˜ên†  László A. Végh‡  Abstract  This paper investigates connections between discrete and continuous approaches for decomposable submodular function minimization. We provide improved running time estimates for the state-of-the-art continuous algorithms for the problem using combinatorial arguments. We also provide a systematic experimental comparison of the two types of methods, based on a clear distin
On Optimal Generalizability in Parametric Learning  Ahmad Beirami∗  beirami@seas.harvard.edu  Meisam Razaviyayn† razaviya@usc.edu  Shahin Shahrampour∗  shahin@seas.harvard.edu  Vahid Tarokh∗  vahid@seas.harvard.edu  Abstract  We consider the parametric learning problem, where the objective of the learner is determined by a parametric loss function. Employing empirical risk minimization with possibly regularization, the inferred parameter vector will be biased toward the training samples. Such bi
Ranking Data with Continuous Labels through Oriented Recursive Partitions  Stephan Cl´emenc¸on  Mastane Achab LTCI, T´el´ecom ParisTech, Universit´e Paris-Saclay  75013 Paris, France  first.last@telecom-paristech.fr  Abstract  We formulate a supervised learning problem, referred to as continuous ranking, where a continuous real-valued label Y is assigned to an observable r.v. X taking its values in a feature space X and the goal is to order all possible observations x in X by means of a scoring 
GibbsNet: Iterative Adversarial Inference for Deep  Graphical Models  Alex Lamb  R Devon Hjelm  Yaroslav Ganin  Joseph Paul Cohen  Aaron Courville  Yoshua Bengio  Abstract  Directed latent variable models that formulate the joint distribution as p(x, z) = p(z)p(x | z) have the advantage of fast and exact sampling. However, these models have the weakness of needing to specify p(z), often with a simple ﬁxed prior that limits the expressiveness of the model. Undirected latent variable models discar
Efﬁcient Modeling of Latent Information in Supervised Learning using Gaussian Processes  Zhenwen Dai ∗‡  zhenwend@amazon.com  Mauricio A. Álvarez †  mauricio.alvarez@sheffield.ac.uk  Neil D. Lawrence †‡ lawrennd@amazon.com  Abstract  Often in machine learning, data are collected as a combination of multiple condi- tions, e.g., the voice recordings of multiple persons, each labeled with an ID. How could we build a model that captures the latent information related to these condi- tions and genera
K-Medoids for K-Means Seeding  James Newling  Idiap Research Institue and  Franc¸ois Fleuret  Idiap Research Institue and  ´Ecole polytechnique f´ed´erale de Lausanne  ´Ecole polytechnique f´ed´erale de Lausanne  james.newling@idiap.ch  francois.fleuret@idiap.ch  Abstract  We show experimentally that the algorithm clarans of Ng and Han (1994) ﬁnds better K-medoids solutions than the Voronoi iteration algorithm of Hastie et al. (2001). This ﬁnding, along with the similarity between the Voronoi it
Riemannian approach to batch normalization  Minhyung Cho  Jaehyung Lee  Applied Research Korea, Gracenote Inc.  mhyung.cho@gmail.com  jaehyung.lee@kaist.ac.kr  Abstract  Batch Normalization (BN) has proven to be an effective algorithm for deep neural network training by normalizing the input to each neuron and reducing the internal covariate shift. The space of weight vectors in the BN layer can be naturally interpreted as a Riemannian manifold, which is invariant to linear scaling of weights. F
EEG-GRAPH: A Factor-Graph-Based Model for Capturing Spatial, Temporal, and Observational  Relationships in Electroencephalograms  Yogatheesan Varatharajah ∗ Min Jin Chong∗ Krishnakant Saboo∗  Brent Berry†  Benjamin Brinkmann†  Gregory Worrell†  Ravishankar Iyer∗  Abstract  This paper presents a probabilistic-graphical model that can be used to infer char- acteristics of instantaneous brain activity by jointly analyzing spatial and tempo- ral dependencies observed in electroencephalograms (EEG). 
Learning Non-Gaussian Multi-Index Model via  Second-Order Stein’s Method  Zhuoran Yang⇤ Krishna Balasubramanian⇤ Zhaoran Wang† Han Liu†  Abstract  We consider estimating the parametric components of semiparametric multi-index models in high dimensions. To bypass the requirements of Gaussianity or elliptical symmetry of covariates in existing methods, we propose to leverage a second-order Stein’s method with score function-based corrections. We prove that our estimator achieves a near-optimal sta
Implicit Regularization in Matrix Factorization  Suriya Gunasekar  TTI at Chicago  suriya@ttic.edu  Blake Woodworth  TTI at Chicago  blake@ttic.edu  Srinadh Bhojanapalli  TTI at Chicago  srinadh@ttic.edu  Behnam Neyshabur  TTI at Chicago  behnam@ttic.edu  Nathan Srebro TTI at Chicago  nati@ttic.edu  Abstract  We study implicit regularization when optimizing an underdetermined quadratic objective over a matrix X with gradient descent on a factorization of X. We conjecture and provide empirical an
Spectrally-normalized margin bounds  for neural networks  Peter L. Bartlett∗  Dylan J. Foster†  Matus Telgarsky‡  Abstract  This paper presents a margin-based multiclass generalization bound for neural net- works that scales with their margin-normalized spectral complexity: their Lipschitz constant, meaning the product of the spectral norms of the weight matrices, times a certain correction factor. This bound is empirically investigated for a standard AlexNet network trained with SGD on the mnis
A General Framework for Robust Interactive  Learning∗  Ehsan Emamjomeh-Zadeh†  David Kempe‡  Abstract  We propose a general framework for interactively learning models, such as (binary or non-binary) classiﬁers, orderings/rankings of items, or clusterings of data points. Our framework is based on a generalization of Angluin’s equivalence query model and Littlestone’s online learning model: in each iteration, the algorithm proposes a model, and the user either accepts it or reveals a speciﬁc mist
Understanding variable importances  in forests of randomized trees  Gilles Louppe, Louis Wehenkel, Antonio Sutera and Pierre Geurts  {g.louppe, l.wehenkel, a.sutera, p.geurts}@ulg.ac.be  Dept. of EE & CS, University of Li`ege, Belgium  Abstract  Despite growing interest and practical use in various scientiﬁc areas, variable im- portances derived from tree-based ensemble methods are not well understood from a theoretical point of view. In this work we characterize the Mean Decrease Im- purity (MD
Rapid Distance-Based Outlier Detection via Sampling  Mahito Sugiyama1 Karsten M. Borgwardt1;2  1Machine Learning and Computational Biology Research Group, MPIs T¨ubingen, Germany  2Zentrum f¨ur Bioinformatik, Eberhard Karls Universit¨at T¨ubingen, Germany fmahito.sugiyama,karsten.borgwardtg@tuebingen.mpg.de  Abstract  Distance-based approaches to outlier detection are popular in data mining, as they do not require to model the underlying probability distribution, which is particu- larly challeng
Non-Linear Domain Adaptation with Boosting  Carlos Becker∗  C. Mario Christoudias  Pascal Fua  CVLab, ´Ecole Polytechnique F´ed´erale de Lausanne, Switzerland  firstname.lastname@epfl.ch  Abstract  A common assumption in machine vision is that the training and test samples are drawn from the same distribution. However, there are many problems when this assumption is grossly violated, as in bio-medical applications where differ- ent acquisitions can generate drastic variations in the appearance o
A Gang of Bandits  Nicol`o Cesa-Bianchi  Universit`a degli Studi di Milano, Italy nicolo.cesa-bianchi@unimi.it  Claudio Gentile  University of Insubria, Italy  claudio.gentile@uninsubria.it  Giovanni Zappella  Universit`a degli Studi di Milano, Italy giovanni.zappella@unimi.it  Abstract  Multi-armed bandit problems formalize the exploration-exploitation trade-offs arising in several industrially relevant applications, such as online advertisement and, more generally, recommendation systems. In m
Unsupervised Spectral Learning of FSTs  Rapha¨el Bailly  Xavier Carreras  Ariadna Quattoni  Universitat Politecnica de Catalunya  Barcelona, 08034  rbailly,carreras,aquattoni@lsi.upc.edu  Abstract  Finite-State Transducers (FST) are a standard tool for modeling paired input- output sequences and are used in numerous applications, ranging from computa- tional biology to natural language processing. Recently Balle et al. [4] presented a spectral algorithm for learning FST from samples of aligned i
On Flat versus Hierarchical Classiﬁcation in  Large-Scale Taxonomies  Rohit Babbar, Ioannis Partalas, Eric Gaussier, Massih-Reza Amini  Université Joseph Fourier, Laboratoire Informatique de Grenoble  BP 53 - F-38041 Grenoble Cedex 9  firstname.lastname@imag.fr  Abstract  We study in this paper ﬂat and hierarchical classiﬁcation strategies in the context of large-scale taxonomies. To this end, we ﬁrst propose a multiclass, hierarchi- cal data dependent bound on the generalization error of classi
Algorithmic Stability and Uniform Generalization  Ibrahim Alabdulmohsin  King Abdullah University of Science and Technology  Thuwal 23955, Saudi Arabia  ibrahim.alabdulmohsin@kaust.edu.sa  Abstract  One of the central questions in statistical learning theory is to determine the con- ditions under which agents can learn from experience. This includes the neces- sary and sufﬁcient conditions for generalization from a given ﬁnite training set to new observations. In this paper, we prove that algori
Approximating Sparse PCA from Incomplete Data  Abhisek Kundu ∗  Petros Drineas †  Malik Magdon-Ismail ‡  Abstract  We study how well one can recover sparse principal components of a data ma- trix using a sketch formed from a few of its elements. We show that for a wide class of optimization problems, if the sketch is close (in the spectral norm) to the original data matrix, then one can recover a near optimal solution to the optimiza- tion problem by using the sketch. In particular, we use this 
HONOR: Hybrid Optimization for NOn-convex  Regularized problems  Pinghua Gong  Jieping Ye  Univeristy of Michigan, Ann Arbor, MI 48109  Univeristy of Michigan, Ann Arbor, MI 48109  gongp@umich.edu  jpye@umich.edu  Abstract  Recent years have witnessed the superiority of non-convex sparse learning formu- lations over their convex counterparts in both theory and practice. However, due to the non-convexity and non-smoothness of the regularizer, how to efﬁciently solve the non-convex optimization pr
Empirical Localization of Homogeneous Divergences  on Discrete Sample Spaces  Takashi Takenouchi  Department of Complex and Intelligent Systems  Future University Hakodate  116-2 Kamedanakano, Hakodate, Hokkaido, 040-8655, Japan  ttakashi@fun.ac.jp  Department of Computer Science and Mathematical Informatics  Takafumi Kanamori  Nagoya University  Furocho, Chikusaku, Nagoya 464-8601, Japan  kanamori@is.nagoya-u.ac.jp  Abstract  In this paper, we propose a novel parameter estimator for probabilist
Hessian-free Optimization for Learning  Deep Multidimensional Recurrent Neural Networks  Minhyung Cho  Chandra Shekhar Dhir  Jaehyung Lee  {mhyung.cho,shekhardhir}@gmail.com  Applied Research Korea, Gracenote Inc.  jaehyung.lee@kaist.ac.kr  Abstract  Multidimensional recurrent neural networks (MDRNNs) have shown a remark- able performance in the area of speech and handwriting recognition. The perfor- mance of an MDRNN is improved by further increasing its depth, and the dif- ﬁculty of learning t
Bounding the Cost of Search-Based Lifted Inference  David Smith  University of Texas At Dallas  Vibhav Gogate  University of Texas At Dallas  800 W Campbell Rd, Richardson, TX 75080  800 W Campbell Rd, Richardson, TX 75080  dbs014200@utdallas.edu  vibhav.gogate@utdallas.edu  Abstract  Recently, there has been growing interest in systematic search-based and impor- tance sampling-based lifted inference algorithms for statistical relational models (SRMs). These lifted algorithms achieve signiﬁcant 
The Brain Uses Reliability of Stimulus Information  when Making Perceptual Decisions  Sebastian Bitzer1  sebastian.bitzer@tu-dresden.de  Stefan J. Kiebel1  stefan.kiebel@tu-dresden.de  1Department of Psychology, Technische Universit¨at Dresden, 01062 Dresden, Germany  Abstract  In simple perceptual decisions the brain has to identify a stimulus based on noisy sensory samples from the stimulus. Basic statistical considerations state that the reliability of the stimulus information, i.e., the amou
Convergence Rates of Active Learning for Maximum Likelihood Estimation  Kamalika Chaudhuri ⇤  Sham M. Kakade †  Praneeth Netrapalli ‡  Sujay Sanghavi §  Abstract  An active learner is given a class of models, a large set of unlabeled examples, and the ability to interactively query labels of a subset of these examples; the goal of the learner is to learn a model in the class that ﬁts the data well. Previous theoretical work has rigorously characterized label complexity of active learning, but mo
ß´¬»®²¿¬·²¹ Ó·²·³·¦¿¬·±² º±® Î»¹®»­­·±² Ð®±¾´»³­  ©·¬¸ Ê»½¬±®óª¿´«»¼ Ñ«¬°«¬­  Ð®¿¬»»µ Ö¿·²  ß³¾«¶ Ì»©¿®·  Ó·½®±­±º¬ Î»­»¿®½¸ô ×ÒÜ×ß  Ë²·ª»®­·¬§ ±º Ó·½¸·¹¿²ô ß²² ß®¾±®ô ËÍß  °®¿¶¿·²à³·½®±­±º¬ò½±³  ¬»©¿®·¿à«³·½¸ò»¼«  ß¾­¬®¿½¬  ×² ®»¹®»­­·±² °®±¾´»³­ ·²ª±´ª·²¹ ª»½¬±®óª¿´«»¼ ±«¬°«¬­ ø±® »¯«·ª¿´»²¬´§ô ³«´¬·°´» ®»­°±²­»­÷ô ·¬ ·­ ©»´´ µ²±©² ¬¸¿¬ ¬¸» ³¿¨·³«³ ´·µ»´·¸±±¼ »­¬·³¿¬±® øÓÔÛ÷ô ©¸·½¸  ¬¸¿² ¬¸» ±®¼·²¿®§ ´»¿­¬ ­¯«¿®»­ øÑÔÍ÷ »­¬·³¿¬±®ò Ø±©»ª»®ô »¨·­¬·²¹ ´·¬»®¿¬«®» ½±³ó  Ó±®» ½®«½·¿´´§ô ½±³°«¬·²¹ ¬¸
Improved Iteration Complexity Bounds of Cyclic Block Coordinate Descent for Convex Problems  Ruoyu Sun∗, Mingyi Hong†‡  Abstract  The iteration complexity of the block-coordinate descent (BCD) type algorithm has been under extensive investigation. It was recently shown that for convex problems the classical cyclic BCGD (block coordinate gradient descent) achieves an O(1/r) complexity (r is the number of passes of all blocks). However, such bounds are at least linearly depend on K (the number of 
Distributionally Robust Logistic Regression  Soroosh Shaﬁeezadeh-Abadeh  Peyman Mohajerin Esfahani  Daniel Kuhn  ´Ecole Polytechnique F´ed´erale de Lausanne, CH-1015 Lausanne, Switzerland  {soroosh.shafiee,peyman.mohajerin,daniel.kuhn} @epfl.ch  Abstract  This paper proposes a distributionally robust approach to logistic regression. We use the Wasserstein distance to construct a ball in the space of probability distribu- tions centered at the uniform distribution on the training samples. If the 
Attractor Network Dynamics Enable Preplay and Rapid Path Planning in Maze–like Environments  Dane Corneil  Laboratory of Computational Neuroscience ´Ecole Polytechnique F´ed´erale de Lausanne  CH-1015 Lausanne, Switzerland dane.corneil@epfl.ch  Wulfram Gerstner  Laboratory of Computational Neuroscience ´Ecole Polytechnique F´ed´erale de Lausanne  CH-1015 Lausanne, Switzerland  wulfram.gerstner@epfl.ch  Abstract  Rodents navigating in a well–known environment can rapidly learn and revisit ob- ser
When are Kalman-Filter Restless Bandits Indexable?  Christopher Dance and Tomi Silander  Xerox Research Centre Europe  6 chemin de Maupertuis, Meylan, Is`ere, France {dance,silander}@xrce.xerox.com  Abstract  We study the restless bandit associated with an extremely simple scalar Kalman ﬁlter model in discrete time. Under certain assumptions, we prove that the prob- lem is indexable in the sense that the Whittle index is a non-decreasing function of the relevant belief state. In spite of the lon
Saliency, Scale and Information:  Towards a Unifying Theory  Shaﬁn Rahman  Department of Computer Science  University of Manitoba  shafin109@gmail.com  Neil D.B. Bruce  Department of Computer Science  University of Manitoba  bruce@cs.umanitoba.ca  Abstract  In this paper we present a deﬁnition for visual saliency grounded in information theory. This proposal is shown to relate to a variety of classic research contribu- tions in scale-space theory, interest point detection, bilateral ﬁltering, an
A Normative Theory of Adaptive Dimensionality  Reduction in Neural Networks  Cengiz Pehlevan  Simons Center for Data Analysis  Simons Foundation New York, NY 10010  Dmitri B. Chklovskii  Simons Center for Data Analysis  Simons Foundation New York, NY 10010  cpehlevan@simonsfoundation.org  dchklovskii@simonsfoundation.org  Abstract  To make sense of the world our brains must analyze high-dimensional datasets streamed by our sensory organs. Because such analysis begins with dimension- ality reduct
Sample Complexity of Learning Mahalanobis  Distance Metrics  Nakul Verma  Janelia Research Campus, HHMI verman@janelia.hhmi.org  Kristin Branson  Janelia Research Campus, HHMI  bransonk@janelia.hhmi.org  Abstract  Metric learning seeks a transformation of the feature space that enhances predic- tion quality for a given task. In this work we provide PAC-style sample complexity rates for supervised metric learning. We give matching lower- and upper-bounds showing that sample complexity scales with
Estimating Jaccard Index with Missing Observations:  A Matrix Calibration Approach  Wenye Li  Macao Polytechnic Institute  Macao SAR, China  wyli@ipm.edu.mo  Abstract  The Jaccard index is a standard statistics for comparing the pairwise similarity be- tween data samples. This paper investigates the problem of estimating a Jaccard index matrix when there are missing observations in data samples. Starting from a Jaccard index matrix approximated from the incomplete data, our method cali- brates t
Optimal decision-making  with time-varying evidence reliability  Jan Drugowitsch1 1D´ept. des Neurosciences Fondamentales  Rub´en Moreno-Bote2  Universit´e de Gen`eve  CH-1211 Gen`eve 4, Switzerland  jdrugo@gmail.com,  alexandre.pouget@unige.ch  Alexandre Pouget1 2Research Unit, Parc Sanitari  Sant Joan de D´eu and University of Barcelona 08950 Barcelona, Spain rmoreno@fsjd.org  Abstract  Previous theoretical and experimental work on optimal decision-making was re- stricted to the artiﬁcial sett
Provable Submodular Minimization using  Wolfe’s Algorithm  Deeparnab Chakrabarty∗  Prateek Jain∗  Pravesh Kothari†  Abstract  Owing to several applications in large scale learning and vision problems, fast submodular function minimization (SFM) has become a critical problem. Theoreti- cally, unconstrained SFM can be performed in polynomial time [10, 11]. However, these algorithms are typically not practical. In 1976, Wolfe [21] proposed an algorithm to ﬁnd the minimum Euclidean norm point in a p
BlossomTreeGraphicalModelsZheLiuDepartmentofStatisticsUniversityofChicagoJohnLaffertyDepartmentofStatisticsDepartmentofComputerScienceUniversityofChicagoAbstractWecombinetheideasbehindtreesandGaussiangraphicalmodelstoformanewnonparametricfamilyofgraphicalmodels.Ourapproachistoattachnonpara-normal“blossoms”,witharbitrarygraphs,toacollectionofnonparametrictrees.ThetreeedgesarechosentoconnectvariablesthatmostviolatejointGaussianity.Thenon-treeedgesarepartitionedintodisjointgroups,andassignedtotreen
Scalable Inference for Neuronal Connectivity from  Calcium Imaging  Alyson K. Fletcher  Sundeep Rangan  Abstract  Fluorescent calcium imaging provides a potentially powerful tool for inferring connectivity in neural circuits with up to thousands of neurons. However, a key challenge in using calcium imaging for connectivity detection is that current sys- tems often have a temporal response and frame rate that can be orders of magni- tude slower than the underlying neural spiking process. Bayesian
A Statistical Decision-Theoretic Framework for  Social Choice  Hossein Azari Souﬁani∗  David C. Parkes †  Lirong Xia‡  Abstract  In this paper, we take a statistical decision-theoretic viewpoint on social choice, putting a focus on the decision to be made on behalf of a system of agents. In our framework, we are given a statistical ranking model, a decision space, and a loss function deﬁned on (parameter, decision) pairs, and formulate social choice mechanisms as decision rules that minimize exp
Orbit Regularization  Renato Negrinho  Instituto de Telecomunicac¸ ˜oes  Instituto Superior T´ecnico 1049–001 Lisboa, Portugal  renato.negrinho@gmail.com  Andr´e F. T. Martins∗  Instituto de Telecomunicac¸ ˜oes  Instituto Superior T´ecnico 1049–001 Lisboa, Portugal  atm@priberam.pt  Abstract  We propose a general framework for regularization based on group-induced ma- jorization. In this framework, a group is deﬁned to act on the parameter space and an orbit is ﬁxed; to control complexity, the m
Unsupervised learning of an efﬁcient short-term  memory network  Pietro Vertechi  Wieland Brendel ∗  Christian K. Machens  Champalimaud Neuroscience Programme Champalimaud Centre for the Unknown  Lisbon, Portugal  first.last@neuro.fchampalimaud.org  Abstract  Learning in recurrent neural networks has been a topic fraught with difﬁculties and problems. We here report substantial progress in the unsupervised learning of recurrent networks that can keep track of an input signal. Speciﬁcally, we sho
QuantizedEstimationofGaussianSequenceModelsinEuclideanBallsYuanchengZhuJohnLaffertyDepartmentofStatisticsUniversityofChicagoAbstractAcentralresultinstatisticaltheoryisPinsker’stheorem,whichcharacterizestheminimaxrateinthenormalmeansmodelofnonparametricestimation.Inthispaper,wepresentanextensiontoPinsker’stheoremwhereestimationiscarriedoutunderstorageorcommunicationconstraints.Inparticular,weplacelimitsonthenumberofbitsusedtoencodeanestimator,andanalyzetheexcessriskintermsofthisconstraint,thesign
DeepUSPS: Deep Robust Unsupervised Saliency  Prediction With Self-Supervision  Duc Tam Nguyen ∗†‡, Maximilian Dax ∗‡, Chaithanya Kumar Mummadi †§  Thi Phuong Nhung Ngo §, Thi Hoai Phuong Nguyen ¶, Zhongyu Lou ‡, Thomas Brox †  Abstract  Deep neural network (DNN) based salient object detection in images based on high-quality labels is expensive. Alternative unsupervised approaches rely on care- ful selection of multiple handcrafted saliency methods to generate noisy pseudo- ground-truth labels. I
RSN: Randomized Subspace Newton  Robert M. Gower  LTCI, T´el´ecom Paristech, IPP, France  Dmitry Kovalev  KAUST, Saudi Arabia  gowerrobert@gmail.com  dmitry.kovalev@kaust.edu.sa  Heinrich-Heine-Universit¨at D¨usseldorf, Germany  Felix Lieder  Peter Richt´arik  KAUST, Saudi Arabia and MIPT, Russia  lieder@opt.uni-duesseldorf.de  peter.richtarik@kaust.edu.sa  Abstract  We develop a randomized Newton method capable of solving learning problems with huge dimensional feature spaces, which is a common
Explicit Disentanglement of Appearance and  Perspective in Generative Models  Nicki S. Detlefsen ∗ nsde@dtu.dk  Søren Hauberg ∗ sohau@dtu.dk  Abstract  Disentangled representation learning ﬁnds compact, independent and easy-to- interpret factors of the data. Learning such has been shown to require an inductive bias, which we explicitly encode in a generative model of images. Speciﬁcally, we propose a model with two latent spaces: one that represents spatial transformations of the input data, and
Polynomial Cost of Adaptation for X -Armed Bandits  Hédi Hadiji  Laboratoire de Mathématiques d’Orsay  Université Paris-Sud, Orsay, France hedi.hadiji@math.u-psud.fr  Abstract  In the context of stochastic continuum-armed bandits, we present an algorithm that adapts to the unknown smoothness of the objective function. We exhibit and compute a polynomial cost of adaptation to the Hölder regularity for regret minimization. To do this, we ﬁrst reconsider the recent lower bound of Locatelli and Carp
Region-speciﬁc Diffeomorphic Metric Mapping  Zhengyang Shen UNC Chapel Hill zyshen@cs.unc.edu  François-Xavier Vialard  LIGM, UPEM  francois-xavier.vialard@u-pem.fr  Marc Niethammer UNC Chapel Hill mn@cs.unc.edu  Abstract  We introduce a region-speciﬁc diffeomorphic metric mapping (RDMM) registra- tion approach. RDMM is non-parametric, estimating spatio-temporal velocity ﬁelds which parameterize the sought-for spatial transformation. Regularization of these velocity ﬁelds is necessary. In contra
Fine-grained Optimization of Deep Neural Networks  Mete Ozay∗  Abstract  In recent studies, several asymptotic upper bounds on generalization errors on deep neural networks (DNNs) are theoretically derived. These bounds are functions of several norms of weights of the DNNs, such as the Frobenius and spectral norms, and they are computed for weights grouped according to either input and output channels of the DNNs. In this work, we conjecture that if we can impose multiple constraints on weights 
Extending Stein’s unbiased risk estimator to train deep denoisers with correlated pairs of noisy images  Magauiya Zhussip  Shakarim Soltanayev  Se Young Chun  Ulsan National Institute of Science and Technology (UNIST)  {mzhussip, shakarim, sychun}@unist.ac.kr  Abstract  Recently, Stein’s unbiased risk estimator (SURE) has been applied to unsupervised training of deep neural network Gaussian denoisers that outperformed classical non-deep learning based denoisers and yielded comparable performance
Unconstrained Monotonic Neural Networks  Antoine Wehenkel University of Liège  Gilles Louppe  University of Liège  Abstract  Monotonic neural networks have recently been proposed as a way to deﬁne in- vertible transformations. These transformations can be combined into powerful autoregressive ﬂows that have been shown to be universal approximators of con- tinuous probability distributions. Architectures that ensure monotonicity typically enforce constraints on weights and activation functions, w
A New Defense Against Adversarial Images:  Turning a Weakness into a Strength  Tao Yu∗†  Shengyuan Hu∗† Chuan Guo† Wei-Lun Chao‡ Kilian Q. Weinberger†  Abstract  Natural images are virtually surrounded by low-density misclassiﬁed regions that can be efﬁciently discovered by gradient-guided search — enabling the generation of adversarial images. While many techniques for detecting these attacks have been proposed, they are easily bypassed when the adversary has full knowledge of the detection mec
CNN2: Viewpoint Generalization via  a Binocular Vision  Wei-Da Chen  Department of Computer Science National Tsing-Hua University  Taiwan, R.O.C.  wdchen@datalab.cs.nthu.edu.tw  Shan-Hung Wu  Department of Computer Science National Tsing-Hua University  Taiwan, R.O.C.  shwu@cs.nthu.edu.tw  Abstract  The Convolutional Neural Networks (CNNs) have laid the foundation for many techniques in various applications. Despite achieving remarkable performance in some tasks, the 3D viewpoint generalizabilit
GENO – GENeric Optimization for Classical Machine Learning  Sören Laue  &  Friedrich-Schiller-Universität Jena  Data Assessment Solutions GmbH  soeren.laue@uni-jena.de  Matthias Mitterreiter  Friedrich-Schiller-Universität Jena  Germany  matthias.mitterreiter@uni-jena.de  Joachim Giesen  Friedrich-Schiller-Universität Jena  Germany  joachim.giesen@uni-jena.de  Abstract  Although optimization is the longstanding algorithmic backbone of machine learn- ing, new models still require the time-consumi
SpiderBoostandMomentum:FasterStochasticVarianceReductionAlgorithmsZheWangDepartmentofECETheOhioStateUniversitywang.10982@osu.eduKaiyiJiDepartmentofECETheOhioStateUniversityji.367@osu.eduYiZhouDepartmentofECETheUniversityofUtahyi.zhou@utah.eduYingbinLiangDepartmentofECETheOhioStateUniversityliang.889@osu.eduVahidTarokhDepartmentofECEDukeUniversityvahid.tarokh@duke.eduAbstractSARAHandSPIDERaretworecentlydevelopedstochasticvariance-reducedalgorithms,andSPIDERhasbeenshowntoachieveanear-optimalﬁrst-o
Direct Estimation of Differential Functional  Graphical Models  Boxin Zhao  Department of Statistics The Unveristy of Chicago  Chicago, IL 60637  boxinz@uchicago.edu  Y. Samuel Wang  Booth School of Business The Unveristy of Chicago  Chicago, IL 60637  swang24@uchicago.edu  Mladen Kolar  Booth School of Business The Unveristy of Chicago  Chicago, IL 60637  mkolar@chicagobooth.edu  Abstract  We consider the problem of estimating the difference between two functional undirected graphical models wi
Glyce: Glyph-vectors for Chinese Character  Representations  Yuxian Meng*, Wei Wu*, Fei Wang*, Xiaoya Li*, Ping Nie, Fan Yin  Muyu Li, Qinghong Han, Xiaofei Sun and Jiwei Li  {yuxian meng, wei wu, fei wang, xiaoya li, ping nie, fan yin, muyu li, qinghong han, xiaofei sun, jiwei li}@shannonai.com  Shannon.AI  Abstract  It is intuitive that NLP tasks for logographic languages like Chinese should beneﬁt from the use of the glyph information in those languages. However, due to the lack of rich picto
Classiﬁcation-by-Components: Probabilistic  Modeling of Reasoning over a Set of Components  Sascha Saralajew1,∗ Lars Holdijk1,∗ Maike Rees1 Ebubekir Asan1 Thomas Villmann2,∗  1Dr. Ing. h.c. F. Porsche AG, Weissach, Germany,  sascha.saralajew@porsche.de  2University of Applied Sciences Mittweida, Mittweida, Germany,  thomas.villmann@hs-mittweida.de  Abstract  Neural networks are state-of-the-art classiﬁcation approaches but are generally difﬁcult to interpret. This issue can be partly alleviated 
Fooling Neural Network Interpretations via  Adversarial Model Manipulation  Juyeon Heo1∗, Sunghwan Joo1∗, and Taesup Moon1,2  1Department of Electrical and Computer Engineering, 2Department of Artiﬁcial Intelligence  Sungkyunkwan University, Suwon, Korea, 16419  heojuyeon12@gmail.com, {shjoo840, tsmoon}@skku.edu  Abstract  We ask whether the neural network interpretation methods can be fooled via adversarial model manipulation, which is deﬁned as a model ﬁne-tuning step that aims to radically al
Quality Aware Generative Adversarial Networks  Parimala Kancharla, Sumohana S. Channappayya  Department of Electrical Engineering  Indian Institute of Technology Hyderabad  {ee15m17p100001, sumohana}@iith.ac.in  Abstract  Generative Adversarial Networks (GANs) have become a very popular tool for im- plicitly learning high-dimensional probability distributions. Several improvements have been made to the original GAN formulation to address some of its shortcom- ings like mode collapse, convergence
DeepSignatureTransformsPatricBonnier1,∗PatrickKidger1,2,∗ImanolPerezArribas1,2,∗CristopherSalvi1,2,∗TerryLyons1,21MathematicalInstitute,UniversityofOxford2TheAlanTuringInstitute,BritishLibrary{bonnier,kidger,perez,salvi,tlyons}@maths.ox.ac.ukAbstractThesignatureisaninﬁnitegradedsequenceofstatisticsknowntocharacteriseastreamofdatauptoanegligibleequivalenceclass.Itisatransformwhichhaspreviouslybeentreatedasaﬁxedfeaturetransformation,ontopofwhichamodelmaybebuilt.Weproposeanovelapproachwhichcombines
Convergent Policy Optimization for Safe  Reinforcement Learning  Ming Yu ⇤  Zhuoran Yang †  Mladen Kolar ‡  Zhaoran Wang §  Abstract  We study the safe reinforcement learning problem with nonlinear function approx- imation, where policy optimization is formulated as a constrained optimization problem with both the objective and the constraint being nonconvex functions. For such a problem, we construct a sequence of surrogate convex constrained optimization problems by replacing the nonconvex fun
A Flexible Generative Framework for Graph-based  Semi-supervised Learning  Jiaqi Ma∗†  jiaqima@umich.edu  Weijing Tang∗‡  weijtang@umich.edu  Ji Zhu‡  jizhu@umich.edu  Qiaozhu Mei†§ qmei@umich.edu  Abstract  We consider a family of problems that are concerned about making predictions for the majority of unlabeled, graph-structured data samples based on a small proportion of labeled samples. Relational information among the data samples, often encoded in the graph/network structure, is shown to b
The Landscape of Non-convex Empirical Risk with  Degenerate Population Risk  Shuang Li, Gongguo Tang, and Michael B. Wakin  Department of Electrical Engineering  Colorado School of Mines  Golden, CO 80401  {shuangli,gtang,mwakin}@mines.edu  Abstract  The landscape of empirical risk has been widely studied in a series of machine learning problems, including low-rank matrix factorization, matrix sensing, matrix completion, and phase retrieval. In this work, we focus on the situation where the corr
Thompson Sampling with Information Relaxation  Penalties  Seungki Min  Columbia Business School  Costis Maglaras  Columbia Business School  Ciamac C. Moallemi  Columbia Business School  Abstract  We consider a ﬁnite-horizon multi-armed bandit (MAB) problem in a Bayesian setting, for which we propose an information relaxation sampling framework. With this framework, we deﬁne an intuitive family of control policies that include Thompson sampling (TS) and the Bayesian optimal policy as endpoints. A
Exact inference in structured prediction  Kevin Bello  Department of Computer Science  Purdue Univeristy  West Lafayette, IN 47906, USA  kbellome@purdue.edu  Jean Honorio  Department of Computer Science  Purdue Univeristy  West Lafayette, IN 47906, USA  jhonorio@purdue.edu  Abstract  Structured prediction can be thought of as a simultaneous prediction of multiple labels. This is often done by maximizing a score function on the space of labels, which decomposes as a sum of pairwise and unary pote
Shape and Time Distortion Loss for Training Deep  Time Series Forecasting Models  Vincent Le Guen 1,2  vincent.le-guen@edf.fr  Nicolas Thome 2  nicolas.thome@cnam.fr  (1) EDF R&D  6 quai Watier, 78401 Chatou, France  (2) CEDRIC, Conservatoire National des Arts et Métiers  292 rue Saint-Martin, 75003 Paris, France  Abstract  This paper addresses the problem of time series forecasting for non-stationary signals and multiple future steps prediction. To handle this challenging task, we introduce DIL
Data Cleansing for Models Trained with SGD  Satoshi Hara⇤  Atsushi Nitanda†  Takanori Maehara‡  Abstract  Data cleansing is a typical approach used to improve the accuracy of machine learning models, which, however, requires extensive domain knowledge to identify the inﬂuential instances that affect the models. In this paper, we propose an algo- rithm that can identify inﬂuential instances without using any domain knowledge. The proposed algorithm automatically cleans the data, which does not re
Curvilinear Distance Metric Learning  Shuo Chen†, Lei Luo‡∗, Jian Yang†∗, Chen Gong†, Jun Li§, Heng Huang‡  Abstract  Distance Metric Learning aims to learn an appropriate metric that faithfully mea- sures the distance between two data points. Traditional metric learning methods usually calculate the pairwise distance with ﬁxed distance functions (e.g., Eu- clidean distance) in the projected feature spaces. However, they fail to learn the underlying geometries of the sample space, and thus canno
AGeometricPerspectiveonOptimalRepresentationsforReinforcementLearningMarcG.Bellemare1,WillDabney2,RobertDadashi1,AdrienAliTaiga1,3,PabloSamuelCastro1,NicolasLeRoux1,DaleSchuurmans1,4,TorLattimore2,ClareLyle5AbstractWeproposeanewperspectiveonrepresentationlearninginreinforcementlearningbasedongeometricpropertiesofthespaceofvaluefunctions.Weleveragethisperspectivetoprovideformalevidenceregardingtheusefulnessofvaluefunctionsasauxiliarytasks.Ourformulationconsidersadaptingtherepresentationtomini-miz
Uncertainty-based Continual Learning with  Adaptive Regularization  Hongjoon Ahn1∗, Sungmin Cha2∗, Donggyu Lee2 and Taesup Moon1,2  1 Department of Artiﬁcial Intelligence, 2Department of Electrical and Computer Engineering,  Sungkyunkwan University, Suwon, Korea 16419  {hong0805, csm9493, ldk308, tsmoon}@skku.edu  Abstract  We introduce a new neural network-based continual learning algorithm, dubbed as Uncertainty-regularized Continual Learning (UCL), which builds on traditional Bayesian online 
No Press Diplomacy: Modeling Multi-Agent  Gameplay  Philip Paquette 1  pcpaquette@gmail.com  Yuchen Lu 1  luyuchen.paul@gmail.com  Steven Bocco 1  stevenbocco@gmail.com  Max O. Smith 3  max.olan.smith@gmail.com  Satya Ortiz-Gagné 1  s.ortizgagne@gmail.com  Jonathan K. Kummerfeld 3  jkummerf@umich.edu  Satinder Singh 3  baveja@umich.edu  Joelle Pineau 2  jpineau@cs.mcgill.ca  Aaron Courville 1  aaron.courville@gmail.com  Abstract  Diplomacy is a seven-player non-stochastic, non-cooperative game, 
Rethinking Deep Neural Network  Ownership Veriﬁcation: Embedding Passports to  Defeat Ambiguity Attacks  Lixin Fan1 Kam Woh Ng2 Chee Seng Chan2  1WeBank AI Lab, Shenzhen, China  2Center of Image and Signal Processing, Faculty of Comp. Sci. and Info., Tech.  University of Malaya, Kuala Lumpur, Malaysia  {lixinfan@webank.com;kamwoh@siswa.um.edu.my;cs.chan@um.edu.my}  Abstract  With substantial amount of time, resources and human (team) efforts invested to explore and develop successful deep neural
Optimal Sketching for Kronecker Product Regression  and Low Rank Approximation  Huaian Diao∗ Rajesh Jayaram† Zhao Song‡ Wen Sun§ David P. Woodruff¶  Abstract  of O((cid:80)q time is O((cid:80)q our running time is O((cid:80)q  We study the Kronecker product regression problem, in which the design matrix is a Kronecker product of two or more matrices. Formally, given Ai ∈ Rni×di for i = 1, 2, . . . , q where ni (cid:29) di for each i, and b ∈ Rn1n2···nq, let A = A1 ⊗ A2 ⊗ ··· ⊗ Aq. Then for p ∈ [
Learning Disentangled Representation for Robust  Person Re-identiﬁcation  Chanho Eom  Bumsub Ham∗  School of Electrical and Electronic Engineering, Yonsei University  cheom@yonsei.ac.kr  bumsub.ham@yonsei.ac.kr  ∗Corresponding author  Abstract  We address the problem of person re-identiﬁcation (reID), that is, retrieving person images from a large dataset, given a query image of the person of interest. A key challenge is to learn person representations robust to intra-class variations, as differ
Equipping Experts/Bandits with Long-term Memory  Kai Zheng1,2  zhengk92@pku.edu.cn  Haipeng Luo3  haipengl@usc.edu  Ilias Diakonikolas4  ilias.diakonikolas@gmail.com  Liwei Wang1,2  wanglw@cis.pku.edu.cn  Abstract  develop various algorithms with a regret bound of order O((cid:112)T (S ln T + n ln K))  We propose the ﬁrst reduction-based approach to obtaining long-term memory guarantees for online learning in the sense of Bousquet and Warmuth [8], by reducing the problem to achieving typical swi
Learning Dynamics of Attention:  Human Prior for Interpretable Machine Reasoning  Wonjae Kim  Kakao Corporation  Pangyo, Republic of Korea  dandelin.kim@kakaocorp.com  Yoonho Lee  Kakao Corporation  Pangyo, Republic of Korea eddy.l@kakaocorp.com  Abstract  Without relevant human priors, neural networks may learn uninterpretable features. We propose Dynamics of Attention for Focus Transition (DAFT) as a human prior for machine reasoning. DAFT is a novel method that regularizes attention- based re
Singleshot : a scalable Tucker tensor decomposition  Abraham Traoré LITIS EA4108  University of Rouen Normandy  abraham.traore@etu.univ-rouen.fr  Maxime Bérar LITIS EA4108  University of Rouen Normandy maxime.berar@univ-rouen.fr  Alain Rakotomamonjy  LITIS EA4108  University of Rouen Normandy  Criteo AI Lab, Criteo Paris  alain.rakoto@insa-rouen.fr  Abstract  This paper introduces a new approach for the scalable Tucker decomposition problem. Given a tensor X , the algorithm proposed, named Singl
Reliable training and estimation of variance networks  Nicki S. Detlefsen∗ †  nsde@dtu.dk  Martin Jørgensen* †  marjor@dtu.dk  Søren Hauberg † sohau@dtu.dk  Abstract  We propose and investigate new complementary methodologies for estimating predictive variance networks in regression neural networks. We derive a locally aware mini-batching scheme that results in sparse robust gradients, and we show how to make unbiased weight updates to a variance network. Further, we formulate a heuristic for ro
Copula Multi-label Learning  Weiwei Liu  School of Computer Science, Wuhan University  Wuhan, China 430072  liuweiwei863@gmail.com  Abstract  A formidable challenge in multi-label learning is to model the interdependencies between labels and features. Unfortunately, the statistical properties of existing multi-label dependency modelings are still not well understood. Copulas are a powerful tool for modeling dependence of multivariate data, and achieve great success in a wide range of application
First-order methods almost always avoid saddle  points: The case of vanishing step-sizes  Ioannis Panageas  SUTD  Singapore  Georgios Piliouras  SUTD  Singapore  ioannis@sutd.edu.sg  georgios@sutd.edu.sg  Xiao Wang  SUTD  Singapore  xiao_wang@sutd.edu.sg  Abstract  In a series of papers [17, 22, 16], it was established that some of the most commonly used ﬁrst order methods almost surely (under random initializations) and with step- size being small enough, avoid strict saddle points, as long as 
Neural Shufﬂe-Exchange Networks − Sequence  Processing in O(n log n) Time  K¯arlis Freivalds, Em¯ıls Ozolin, š, Agris Šostaks Institute of Mathematics and Computer Science  University of Latvia  Raina bulvaris 29, Riga, LV-1459, Latvia  {Karlis.Freivalds, Emils.Ozolins, Agris.Sostaks}@lumii.lv  Abstract  A key requirement in sequence to sequence processing is the modeling of long range dependencies. To this end, a vast majority of the state-of-the-art models use attention mechanism which is of O
Stochastic Proximal Langevin Algorithm:  Potential Splitting and Nonasymptotic Rates  Adil Salim  Dmitry Kovalev  Peter Richtárik∗  King Abdullah University of Science and Technology, Thuwal, Saudi Arabia  Abstract  We propose a new algorithm—Stochastic Proximal Langevin Algorithm (SPLA)—for sampling from a log concave distribution. Our method is a gen- eralization of the Langevin algorithm to potentials expressed as the sum of one stochastic smooth term and multiple stochastic nonsmooth terms. 
Blow: a single-scale hyperconditioned ﬂow for  non-parallel raw-audio voice conversion  Joan Serrà  Telefónica Research  joan.serra@telefonica.com  Santiago Pascual  Universitat Politècnica de Catalunya  santi.pascual@upc.edu  Carlos Segura  Telefónica Research  carlos.seguraperales  @telefonica.com  Abstract  End-to-end models for raw audio generation are a challenge, specially if they have to work with non-parallel data, which is a desirable setup in many situations. Voice conversion, in which
Unsupervised Emergence of Egocentric Spatial  Structure from Sensorimotor Prediction  Alban Laﬂaquière  AI Lab, SoftBank Robotics Europe  Paris, France  Michael Garcia Ortiz  AI Lab, SoftBank Robotics Europe  Paris, France  alaflaquiere@softbankrobotics.com  mgarciaortiz@softbankrobotics.com  Abstract  Despite its omnipresence in robotics application, the nature of spatial knowledge and the mechanisms that underlie its emergence in autonomous agents are still poorly understood. Recent theoretica
Non-Stationary Markov Decision Processes a Worst-Case Approach using Model-Based  Reinforcement Learning  Erwan Lecarpentier Université de Toulouse  ONERA - The French Aerospace Lab  erwan.lecarpentier@isae-supaero.fr  Emmanuel Rachelson Université de Toulouse  ISAE-SUPAERO  emmanuel.rachelson@isae-supaero.fr  Abstract  This work tackles the problem of robust planning in non-stationary stochastic environments. We study Markov Decision Processes (MDPs) evolving over time and consider Model-Based 
Coresets for Archetypal Analysis  Sebastian Mair  Leuphana University, Germany  mair@leuphana.de  Ulf Brefeld  Leuphana University, Germany  brefeld@leuphana.de  Abstract  Archetypal analysis represents instances as linear mixtures of prototypes (the archetypes) that lie on the boundary of the convex hull of the data. Archetypes are thus often better interpretable than factors computed by other matrix factorization techniques. However, the interpretability comes with high computational cost due 
GRU-ODE-Bayes: Continuous modeling of  sporadically-observed time series  Edward De Brouwer⇤†  ESAT-STADIUS KU LEUVEN  Leuven, 3001, Belgium  edward.debrouwer@esat.kuleuven.be  Adam Arany ESAT-STADIUS KU LEUVEN  Leuven, 3001, Belgium  adam.arany@esat.kuleuven.be  Jaak Simm⇤ ESAT-STADIUS KU LEUVEN  Leuven, 3001, Belgium  jaak.simm@esat.kuleuven.be  Yves Moreau ESAT-STADIUS KU LEUVEN  Leuven, 3001, Belgium  moreau@esat.kuleuven.be  Abstract  Modeling real-world multidimensional time series can be 
MAVEN: Multi-Agent Variational Exploration  Anuj Mahajan⇤†  Tabish Rashid†  Mikayel Samvelyan‡  Shimon Whiteson†  Abstract  Centralised training with decentralised execution is an important setting for coop- erative deep multi-agent reinforcement learning due to communication constraints during execution and computational tractability in training. In this paper, we analyse value-based methods that are known to have superior performance in complex environments [43]. We speciﬁcally focus on QMIX [

On Exact Computation with an Inﬁnitely Wide  Neural Net⇤  Sanjeev Arora†  Simon S. Du‡  Wei Hu§  Zhiyuan Li¶  Ruslan Salakhutdinovk  Ruosong Wang⇤⇤  Abstract  How well does a classic deep net architecture like AlexNet or VGG19 classify on a standard dataset such as CIFAR-10 when its “width”— namely, number of channels in convolutional layers, and number of nodes in fully-connected internal layers — is allowed to increase to inﬁnity? Such questions have come to the forefront in the quest to theor
The spiked matrix model with generative priors  Benjamin Aubin†, Bruno Loureiro†, Antoine Maillard(cid:63),  Florent Krzakala(cid:63), Lenka Zdeborová†  Abstract  Using a low-dimensional parametrization of signals is a generic and powerful way to enhance performance in signal processing and statistical inference. A very popular and widely explored type of dimensionality reduction is sparsity; another type is generative modelling of signal distributions. Generative models based on neural networks
Online Normalization for Training Neural Networks  Vitaliy Chiley∗  Ilya Sharapov∗  Atli Kosson  Urs Koster  Ryan Reece  Sofía Samaniego de la Fuente  Vishal Subbiah  Michael James∗ †  Cerebras Systems  175 S. San Antonio Road Los Altos, California 94022  Abstract  Online Normalization is a new technique for normalizing the hidden activations of a neural network. Like Batch Normalization, it normalizes the sample dimen- sion. While Online Normalization does not use batches, it is as accurate as 
2 Context and related work  Our object of study is the BERT model introduced in [6]. To set context and terminology, we brieﬂy describe the model’s architecture. The input to BERT is based on a sequence of tokens (words or pieces of words). The output is a sequence of vectors, one for each input token. We will often refer to these vectors as context embeddings because they include information about a token’s context. BERT’s internals consist of two parts. First, an initial embedding for each tok
Learning to Screen  Alon Cohen∗ Avinatan Hassidim† Haim Kaplan‡ Yishay Mansour§  Shay Moran¶  Abstract  Imagine a large ﬁrm with multiple departments that plans a large recruitment. Candidates arrive one-by-one, and for each candidate the ﬁrm decides, based on her data (CV, skills, experience, etc), whether to summon her for an interview. The ﬁrm wants to recruit the best candidates while minimizing the number of interviews. We model such scenarios as an assignment problem between items (candida
Who is Afraid of Big Bad Minima? Analysis of Gradient-Flow in a Spiked Matrix-Tensor Model  Stefano Sarao Mannelli†, Giulio Biroli‡, Chiara Cammarota∗,  Florent Krzakala‡, and Lenka Zdeborová†  Abstract  Gradient-based algorithms are effective for many machine learning tasks, but despite ample recent effort and some progress, it often remains unclear why they work in practice in optimising high-dimensional non-convex functions and why they ﬁnd good minima instead of being trapped in spurious one
Latent Distance Estimation for Random Geometric  Graphs  Ernesto Araya  Laboratoire de Mathématiques d’Orsay (LMO)  Université Paris-Sud  91405 Orsay Cedex, France  ernesto.araya-valdivia@u-psud.fr  Yohann De Castro  Institut Camille Jordan École Centrale de Lyon 69134 Écully, France  yohann.de-castro@ec-lyon.fr  Abstract  Random geometric graphs are a popular choice for a latent points generative model for networks. Their deﬁnition is based on a sample of n points X1, X2,··· , Xn on the Euclide
Unlocking Fairness: a Trade-off Revisited  Michael Wick, Swetasudha Panda, Jean-Baptiste Tristan  {michael.wick,swetasudha.panda,jean.baptiste.tristan}@oracle.com  Oracle Labs, Burlington, MA.  Abstract  The prevailing wisdom is that a model’s fairness and its accuracy are in tension with one another. However, there is a pernicious modeling-evaluating dualism bedeviling fair machine learning in which phenomena such as label bias are ap- propriately acknowledged as a source of unfairness when des
Modelling heterogeneous distributions with an Uncountable Mixture of Asymmetric Laplacians  Axel Brando ∗  BBVA Data & Analytics Universitat de Barcelona  Jose A. Rodríguez-Serrano† BBVA Data & Analytics  Jordi Vitrià‡  Universitat de Barcelona  Alberto Rubio  BBVA Data & Analytics  Abstract  In regression tasks, aleatoric uncertainty is commonly addressed by considering a parametric distribution of the output variable, which is based on strong assumptions such as symmetry, unimodality or by sup
Discovery of Useful Questions as Auxiliary Tasks  Vivek Veeriah1  Matteo Hessel2  Zhongwen Xu2  Richard Lewis1  Janarthanan Rajendran1  Junhyuk Oh2  Hado van Hasselt2  David Silver2  Satinder Singh1,2  Abstract  Arguably, intelligent agents ought to be able to discover their own questions so that in learning answers for them they learn unanticipated useful knowledge and skills; this departs from the focus in much of machine learning on agents learning answers to externally deﬁned questions. We p
Correlation Clustering with Local Objectives  Sanchit Kalhan  Konstantin Makarychev  Timothy Zhou  Abstract  Correlation Clustering is a powerful graph partitioning model that aims to clus- ter items based on the notion of similarity between items. An instance of the Correlation Clustering problem consists of a graph G (not necessarily complete) whose edges are labeled by a binary classiﬁer as “similar” and “dissimilar”. An objective which has received a lot of attention in literature is that of
Algorithmic Analysis and Statistical Estimation of  SLOPE via Approximate Message Passing  Zhiqi Bu∗  Jason M. Klusowski†  Cynthia Rush‡  Weijie Su§  Abstract  SLOPE is a relatively new convex optimization procedure for high-dimensional linear regression via the sorted (cid:96)1 penalty: the larger the rank of the ﬁtted coefﬁcient, the larger the penalty. This non-separable penalty renders many existing techniques invalid or inconclusive in analyzing the SLOPE solution. In this paper, we develop
Stochastic Variance Reduced Primal Dual Algorithms  for Empirical Composition Optimization  Adithya M. Devraj∗ and  Jianshu Chen†  Abstract  We consider a generic empirical composition optimization problem, where there are empirical averages present both outside and inside nonlinear loss functions. Such a problem is of interest in various machine learning applications, and cannot be directly solved by standard methods such as stochastic gradient descent. We take a novel approach to solving this 
Multi-Agent Common Knowledge  Reinforcement Learning  Christian A. Schroeder de Witt⇤†  Jakob N. Foerster⇤†  Gregory Farquhar†  Philip H. S. Torr†  Wendelin Böhmer†  Shimon Whiteson†  Abstract  Cooperative multi-agent reinforcement learning often requires decentralised poli- cies, which severely limit the agents’ ability to coordinate their behaviour. In this paper, we show that common knowledge between agents allows for complex decentralised coordination. Common knowledge arises naturally in a 
Statistical-Computational Tradeoffs in High-Dimensional Single Index  Models  Lingxiao Wang∗  Zhuoran Yang†  Zhaoran Wang‡  Abstract  We study the statistical-computational tradeoffs in a high dimensional single in- dex model Y = f (X(cid:62)β∗) + (cid:15), where f is unknown, X is a Gaussian vector and β∗ is s-sparse with unit norm. When Cov(Y, X(cid:62)β∗) (cid:54)= 0, [43] shows that the direction and support of β∗ can be recovered using a generalized version of Lasso. In this paper, we inves
Neural Proximal/Trust Region Policy Optimization  Attains Globally Optimal Policy  Boyi Liu⇤†  Qi Cai⇤‡  Zhuoran Yang§  Zhaoran Wang¶  Abstract  Proximal policy optimization and trust region policy optimization (PPO and TRPO) with actor and critic parametrized by neural networks achieve signiﬁcant empirical success in deep reinforcement learning. However, due to nonconvexity, the global convergence of PPO and TRPO remains less understood, which sepa- rates theory from practice. In this paper, we
An Embedding Framework for Consistent Polyhedral  Surrogates  Jessie Finocchiaro  jefi8453@colorado.edu  CU Boulder  Rafael Frongillo  raf@colorado.edu  CU Boulder  Bo Waggoner  bwag@colorado.edu  CU Boulder  Abstract  We formalize and study the natural approach of designing convex surrogate loss functions via embeddings for problems such as classiﬁcation or ranking. In this approach, one embeds each of the ﬁnitely many predictions (e.g. classes) as a point in Rd, assigns the original loss value
Gaussian-Based Pooling  for Convolutional Neural Networks  National Institute of Advanced Industrial Science and Technology (AIST)  Takumi Kobayashi  1-1-1 Umezono, Tsukuba, Japan takumi.kobayashi@aist.go.jp  Abstract  Convolutional neural networks (CNNs) contain local pooling to effectively down- size feature maps for increasing computation efﬁciency as well as robustness to input variations. The local pooling methods are generally formulated in a form of convex combination of local neuron acti
Neural Temporal-Difference Learning  Converges to Global Optima  Qi Cai ⇤  Zhuoran Yang †  Jason D. Lee ‡  Zhaoran Wang ⇤  Abstract  Temporal-difference learning (TD), coupled with neural networks, is among the most fundamental building blocks of deep reinforcement learning. However, due to the nonlinearity in value function approximation, such a coupling leads to non- convexity and even divergence in optimization. As a result, the global convergence of neural TD remains unclear. In this paper, 
Personalizing Many Decisions with High-Dimensional  Covariates  Nima Hamidi∗  Mohsen Bayati†  Kapil Gupta‡  Abstract  We consider the k-armed stochastic contextual bandit problem with d dimensional features, when both k and d can be large. To the best of our knowledge, all existing algorithms for this problem have a regret bound that scale as polynomials of degree at least two in k and d. The main contribution of this paper is to introduce and theoretically analyse a new algorithm (REAL-Bandit) 
Margin-Based Generalization Lower Bounds for  Boosted Classiﬁers  Allan Grønlund ‡§  Lior Kamma § Kasper Green Larsen § Alexander Mathiasen §  Jelani Nelson ∗  Abstract  Boosting is one of the most successful ideas in machine learning. The most well- accepted explanations for the low generalization error of boosting algorithms such as AdaBoost stem from margin theory. The study of margins in the context of boosting algorithms was initiated by Schapire, Freund, Bartlett and Lee (1998) and has ins
Screening Sinkhorn Algorithm for Regularized  Optimal Transport  Mokhtar Z. Alaya  LITIS EA4108  University of Rouen Normandy  mokhtarzahdi.alaya@gmail.com  Maxime Bérar LITIS EA4108  University of Rouen Normandy maxime.berar@univ-rouen.fr  Gilles Gasso LITIS EA4108  INSA, University of Rouen Normandy  gilles.gasso@insa-rouen.fr  Alain Rakotomamonjy  LITIS EA4108  University of Rouen Normandy and Criteo AI Lab, Criteo Paris alain.rakoto@insa-rouen.fr  Abstract  We introduce in this paper a novel
Random Projections and Sampling Algorithms for Clustering of High-Dimensional Polygonal Curves  Stefan Meintrup  Faculty of Computer Science  TU Dortmund University  Dortmund, Germany  Alexander Munteanu  Dortmund Data Science Center  TU Dortmund University  Dortmund, Germany  stefan.meintrup@tu-dortmund.de  alexander.munteanu@tu-dortmund.de  Chair of Efﬁcient Algorithms and Complexity Theory  Dennis Rohde  TU Dortmund University  Dortmund, Germany  dennis.rohde@tu-dortmund.de  Abstract  We stud
Recurrent Space-time Graph Neural Networks  Andrei Nicolicioiu∗, Iulia Duta∗  Bitdefender, Romania  anicolicioiu, iduta@bitdefender.com  Marius Leordeanu Bitdefender, Romania  Institute of Mathematics of the Romanian Academy  University "Politehnica" of Bucharest  marius.leordeanu@imar.ro  Abstract  Learning in the space-time domain remains a very challenging problem in machine learning and computer vision. Current computational models for understanding spatio-temporal visual data are heavily ro
Self-Supervised Deep Learning on Point Clouds by  Reconstructing Space  Jonathan Sauder  Hasso Plattner Institute  Potsdam, Germany  jonathan.sauder@student.hpi.de  Bjarne Sievers  Hasso Plattner Institute  Potsdam, Germany  bjarne.sievers@student.hpi.de  Abstract  Point clouds provide a ﬂexible and natural representation usable in countless applications such as robotics or self-driving cars. Recently, deep neural networks operating on raw point cloud data have shown promising results on supervi
Outlier-robust estimation of a sparse linear model  using (cid:96)1-penalized Huber’s M-estimator  Abstract  We study the problem of estimating a p-dimensional s-sparse vector in a linear model with Gaussian design and additive noise. In the case where the labels are contaminated by at most o adversarial outliers, we prove that the (cid:96)1-penalized Huber’s M-estimator based on n samples attains the optimal rate of convergence (s/n)1/2 + (o/n), up to a logarithmic factor. For more general desi
PrivacyAmpliﬁcationbyMixingandDiffusionMechanismsBorjaBalleGillesBartheMPIforSecurityandPrivacyIMDEASoftwareInstituteMarcoGaboardiBostonUniversityJosephGeumlekUniversityofCalifornia,SanDiegoAbstractAfundamentalresultindifferentialprivacystatesthattheprivacyguaranteesofamechanismarepreservedbyanypost-processingofitsoutput.Inthispaperweinvestigateunderwhatconditionsstochasticpost-processingcanamplifytheprivacyofamechanism.Byinterpretingpost-processingastheapplicationofaMarkovoperator,weﬁrstgivease
Non-normal Recurrent Neural Network (nnRNN): learning long time dependencies while improving  expressivity with transient dynamics  Giancarlo Kerg1,2, ∗  Kyle Goyette1,2,3,∗  Maximilian Puelma Touzel1,4  Gauthier Gidel1,2  Eugene Vorontsov1,5  Yoshua Bengio1,2,6  Guillaume Lajoie1,7  Abstract  A recent strategy to circumvent the exploding and vanishing gradient problem in RNNs, and to allow the stable propagation of signals over long time scales, is to constrain recurrent connectivity matrices t
An Inexact Augmented Lagrangian Framework for Nonconvex Optimization with Nonlinear Constraints  Mehmet Fatih Sahin  mehmet.sahin@epfl.ch  Armin Eftekhari  armin.eftekhari@epfl.ch  Ahmet Alacaoglu  ahmet.alacaoglu@epfl.ch  Fabian Latorre  fabian.latorre@epfl.ch  Volkan Cevher  volkan.cevher@epfl.ch  LIONS, Ecole Polytechnique Fédérale de Lausanne, Switzerland  Abstract  We propose a practical inexact augmented Lagrangian method (iALM) for noncon- vex problems with nonlinear constraints. We chara
Algorithm-Dependent Generalization Bounds for  Overparameterized Deep Residual Networks  Spencer Frei∗ and Yuan Cao† and Quanquan Gu‡  Abstract  The skip-connections used in residual networks have become a standard architec- ture choice in deep learning due to the increased training stability and generalization performance with this architecture, although there has been limited theoretical un- derstanding for this improvement. In this work, we analyze overparameterized deep residual networks tra
Icebreaker:  Element-wise Efﬁcient Information Acquisition with  a Bayesian Deep Latent Gaussian Model  Wenbo Gong1∗, Sebastian Tschiatschek2, Richard E. Turner12,  Sebastian Nowozin2†, José Miguel Hernández-Lobato12, Cheng Zhang2  Abstract  In this paper, we address the ice-start problem, i.e., the challenge of deploying machine learning models when only a little or no training data is initially available, and acquiring each feature element of data is associated with costs. This setting is repr

Solving a Class of Non-Convex Min-Max Games  Using Iterative First Order Methods  Maher Nouiehed nouiehed@usc.edu ∗  Maziar Sanjabi sanjabi@usc.edu †  Tianjian Huang tianjian@usc.edu ‡  Jason D. Lee  jasonlee@princeton.edu §  Meisam Razaviyayn razaviya@usc.edu ¶  Abstract  Recent applications that arise in machine learning have surged signiﬁcant interest in solving min-max saddle point games. This problem has been extensively studied in the convex-concave regime for which a global equilibrium so
An Algorithm to Learn Polytree Networks with  Hidden Nodes  Firoozeh Sepehr Department of EECS  University of Tennessee Knoxville  1520 Middle Dr, Knoxville, TN 37996  Donatello Materassi Department of EECS  University of Tennessee Knoxville  1520 Middle Dr, Knoxville, TN 37996  dawn@utk.edu  dmateras@utk.edu  Abstract  Ancestral graphs are a prevalent mathematical tool to take into account latent (hid- den) variables in a probabilistic graphical model. In ancestral graph representations, the no
Multi-Criteria Dimensionality Reduction with  Applications to Fairness  Uthaipon (Tao) Tantipongpipat⇤†  Samira Samadi ⇤‡  Mohit Singh⇤†  Jamie Morgenstern⇤  Santosh Vempala⇤‡  Abstract  Dimensionality reduction is a classical technique widely used for data analysis. One foundational instantiation is Principal Component Analysis (PCA), which minimizes the average reconstruction error. In this paper, we introduce the multi- criteria dimensionality reduction problem where we are given multiple obj
Multiple Futures Prediction  Yichuan Charlie Tang  yichuan_tang@apple.com  Ruslan Salakhutdinov  rsalakhutdinov@apple.com  Abstract  Temporal prediction is critical for making intelligent and robust decisions in com- plex dynamic environments. Motion prediction needs to model the inherently uncertain future which often contains multiple potential outcomes, due to multi- agent interactions and the latent goals of others. Towards these goals, we introduce a probabilistic framework that efﬁciently 
Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks  Aaron R. Voelker1,2  Ivana Kaji´c1  Chris Eliasmith1,2  1Centre for Theoretical Neuroscience, Waterloo, ON 2Applied Brain Research, Inc.  {arvoelke, i2kajic, celiasmith}@uwaterloo.ca  Abstract  We propose a novel memory cell for recurrent neural networks that dynamically maintains information across long windows of time using relatively few resources. The Legendre Memory Unit (LMU) is mathematically derived to or
Exploring Algorithmic Fairness in Robust Graph Covering Problems  Aida Rahmattalabi ⇤ rahmatta@usc.edu  Phebe Vayanos ⇤  phebe.vayanos@usc.edu  Anthony Fulginiti †  anthony.fulginiti@du.edu  Eric Rice ⇤  ericr@usc.edu  Bryan Wilder ‡  bwilder@g.harvard.edu  Amulya Yadav § amulya@psu.edu  Milind Tambe ‡  milind_tambe@harvard.edu  Abstract  Fueled by algorithmic advances, AI algorithms are increasingly being deployed in settings subject to unanticipated challenges with complex social effects. Moti
Stochastic Online AUC Maximization  Yiming Ying†, Longyin Wen‡, Siwei Lyu‡ †Department of Mathematics and Statistics SUNY at Albany, Albany, NY, 12222, USA  ‡Department of Computer Science  SUNY at Albany, Albany, NY, 12222, USA  Abstract  Area under ROC (AUC) is a metric which is widely used for measuring the classiﬁcation performance for imbalanced data. It is of theoretical and practical interest to develop online learning algorithms that maximizes AUC for large-scale data. A speciﬁc challeng
x2  x1  OKM OKM* OKM*+LPE OKM*+NPE     50  40  30  20  10  100  80  60  40  20  OKM* OKM*+LPE OKM*+NPE     0   0  0.2  0.4  0.6 F−Score  0.8  1  0   0  0.2  0.4  0.6 F−Score  0.8  1  
A Simple Practical Accelerated Method for Finite  Sums  Aaron Defazio  Ambiata, Sydney Australia  Abstract  We describe a novel optimization method for ﬁnite sums (such as empirical risk minimization problems) building on the recently introduced SAGA method. Our method achieves an accelerated convergence rate on strongly convex smooth prob- lems. Our method has only one parameter (a step size), and is radically simpler than other accelerated methods for ﬁnite sums. Additionally it can be applied
Joint Line Segmentation and Transcription for End-to-End Handwritten Paragraph Recognition  Théodore Bluche  A2iA SAS  39 rue de la Bienfaisance  75008 Paris tb@a2ia.com  Abstract  Ofﬂine handwriting recognition systems require cropped text line images for both training and recognition. On the one hand, the annotation of position and transcript at line level is costly to obtain. On the other hand, automatic line segmentation algorithms are prone to errors, compromising the subsequent recognition
Direct Feedback Alignment Provides Learning in  Deep Neural Networks  Arild Nøkland  Trondheim, Norway  arild.nokland@gmail.com  Abstract  Artiﬁcial neural networks are most commonly trained with the back-propagation algorithm, where the gradient for learning is provided by back-propagating the error, layer by layer, from the output layer to the hidden layers. A recently discovered method called feedback-alignment shows that the weights used for propagating the error backward don’t have to be sy
Dynamic Network Surgery for Efﬁcient DNNs  Yiwen Guo∗ Intel Labs China  yiwen.guo@intel.com  Anbang Yao  Intel Labs China  anbang.yao@intel.com  Yurong Chen Intel Labs China  yurong.chen@intel.com  Abstract  Deep learning has become a ubiquitous technology to improve machine intelligence. However, most of the existing deep models are structurally very complex, making them difﬁcult to be deployed on the mobile platforms with limited computational power. In this paper, we propose a novel network c
A Appendix: Proof of Theorem 1  We ﬁrst show that the estimate is unbiased. E⇡ `⇡(i),⇡(j)(z). Therefore,  L(z) =  1  k2  k Xi6=j2[k]  L(z) =  Indeed, for every i 6= j we can rewrite L(z) as k2  k Xi6=j2[k]  `⇡(i),⇡(j)(z) = E⇡  L⇡(z) ,  E⇡  1  which proves that the multibatch estimate is unbiased. Next, we turn to analyze the variance of the multibatch estimate. let I ⇢ [k]4 be all the indices i, j, s, t s.t. i 6= j, s 6= t, and we partition I to I1 [ I2 [ I3, where I1 is the set where i = s and 
Combinatorial Multi-Armed Bandit with General  Reward Functions  Wei Chen∗  Wei Hu†  Fu Li‡  Jian Li§  Yu Liu¶  Pinyan Lu(cid:107)  Abstract  In this paper, we study the stochastic combinatorial multi-armed bandit (CMAB) framework that allows a general nonlinear reward function, whose expected value may not depend only on the means of the input random variables but possibly on the entire distributions of these variables. Our framework enables a much larger class of reward functions such as the m
Privacy Odometers and Filters: Pay-as-you-Go  Composition  Ryan Rogers∗  Aaron Roth†  Jonathan Ullman‡  Salil Vadhan§  Abstract  In this paper we initiate the study of adaptive composition in differential privacy when the length of the composition, and the privacy parameters themselves can be chosen adaptively, as a function of the outcome of previously run analyses. This case is much more delicate than the setting covered by existing composition theorems, in which the algorithms themselves can 
Hierarchical Object Representation for Open-Ended  Object Category Learning and Recognition  S.Hamidreza Kasaei, Ana Maria Tomé, Luís Seabra Lopes  IEETA - Instituto de Engenharia Electrónica e Telemática de Aveiro  University of Aveiro, Averio, 3810-193, Portugal {seyed.hamidreza, ana, lsl}@ua.pt  Abstract  Most robots lack the ability to learn new objects from past experiences. To migrate a robot to a new environment one must often completely re-generate the knowledge- base that it is running 
Conﬂict-free Asynchronous Machine Learning  CYCLADES:  Xinghao Pan⇤, Maximilian Lam⇤,  Stephen Tu⇤, Dimitris Papailiopoulos⇤,  Ce Zhang†, Michael I. Jordan⇤‡, Kannan Ramchandran⇤, Chris Re†, Benjamin Recht⇤‡  Abstract  We present CYCLADES, a general framework for parallelizing stochastic optimiza- tion algorithms in a shared memory setting. CYCLADES is asynchronous during model updates, and requires no memory locking mechanisms, similar to HOG- WILD!-type algorithms. Unlike HOGWILD!, CYCLADES in

min z∈Z  f (z) :=  1 N  N(cid:88)  i=1  NESTT: A Nonconvex Primal-Dual Splitting Method  for Distributed and Stochastic Optimization  Davood Hajinezhad, Mingyi Hong ∗  Tuo Zhao†  Zhaoran Wang‡  Abstract  (cid:15)-stationary solution using O(((cid:80)N  We study a stochastic and distributed algorithm for nonconvex problems whose objective consists of a sum of N nonconvex Li/N-smooth functions, plus a non- smooth regularizer. The proposed NonconvEx primal-dual SpliTTing (NESTT) algorithm splits th
SynthesizingthepreferredinputsforneuronsinneuralnetworksviadeepgeneratornetworksAnhNguyenanguyen8@uwyo.eduAlexeyDosovitskiydosovits@cs.uni-freiburg.deJasonYosinskijason@geometric.aiThomasBroxbrox@cs.uni-freiburg.deJeffClunejeffclune@uwyo.eduAbstractDeepneuralnetworks(DNNs)havedemonstratedstate-of-the-artresultsonmanypatternrecognitiontasks,especiallyvisionclassiﬁcationproblems.Understandingtheinnerworkingsofsuchcomputationalbrainsisbothfascinatingbasicsciencethatisinterestinginitsownright—simila
Linear Contextual Bandits with Knapsacks  Shipra Agrawal∗  Nikhil R. Devanur†  Abstract  We consider the linear contextual bandit problem with resource consumption, in addition to reward generation. In each round, the outcome of pulling an arm is a reward as well as a vector of resource consumptions. The expected values of these outcomes depend linearly on the context of that arm. The budget/capacity constraints require that the total consumption doesn’t exceed the budget for each resource. The 
Ladder Variational Autoencoders  Casper Kaae Sønderby⇤ casperkaae@gmail.com  Tapani Raiko†  tapani.raiko@aalto.fi  Lars Maaløe‡  larsma@dtu.dk  Søren Kaae Sønderby⇤  skaaesonderby@gmail.com  Ole Winther⇤,‡ olwi@dtu.dk  Abstract  Variational autoencoders are powerful models for unsupervised learning. However deep models with several layers of dependent stochastic variables are difﬁcult to train which limits the improvements obtained using these highly expressive models. We propose a new inference
Spectral Learning of Dynamic Systems from  Nonequilibrium Data  Hao Wu and Frank Noé  Department of Mathematics and Computer Science  Freie Universität Berlin  Arnimallee 6, 14195 Berlin  {hao.wu,frank.noe}@fu-berlin.de  Abstract  Observable operator models (OOMs) and related models are one of the most im- portant and powerful tools for modeling and analyzing stochastic systems. They exactly describe dynamics of ﬁnite-rank systems and can be efﬁciently and con- sistently estimated through spectr
Fast and accurate spike sorting of high-channel count  probes with KiloSort  Marius Pachitariu1, Nick Steinmetz1, Shabnam Kadir1  Matteo Carandini1 and Kenneth Harris1 1 UCL, UK {ucgtmpa, }@ucl.ac.uk  Abstract  New silicon technology is enabling large-scale electrophysiological recordings in vivo from hundreds to thousands of channels. Interpreting these recordings re- quires scalable and accurate automated methods for spike sorting, which should minimize the time required for manual curation of
An Efﬁcient Streaming Algorithm for the Submodular Cover Problem  Ashkan Norouzi-Fard ⇤  ashkan.norouzifard@epfl.ch  Abbas Bazzi ⇤  abbas.bazzi@epfl.ch  Marwa El Halabi †  marwa.elhalabi@epfl.ch  Ilija Bogunovic †  ilija.bogunovic@epfl.ch  Ya-Ping Hsieh †  ya-ping.hsieh@epfl.ch  Volkan Cevher †  volkan.cevher@epfl.ch  Abstract  We initiate the study of the classical Submodular Cover (SC) problem in the data streaming model which we refer to as the Streaming Submodular Cover (SSC). We show that a
Interaction Networks for Learning about Objects,  Relations and Physics  Anonymous Author(s)  Afﬁliation Address email  Abstract  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  16  17 18 19 20 21 22 23 24  25 26 27 28 29 30 31  32 33 34 35 36  Reasoning about objects, relations, and physics is central to human intelligence, and a key goal of artiﬁcial intelligence. Here we introduce the interaction network, a model which can reason about how objects in complex systems interact, supporting dynamical predic
Learning in Games: Robustness of Fast Convergence  Dylan J. Foster⇤ Zhiyuan Li† Thodoris Lykouris⇤ Karthik Sridharan⇤ Éva Tardos⇤  Abstract  We show that learning algorithms satisfying a low approximate regret property experience fast convergence to approximate optimality in a large class of repeated games. Our property, which simply requires that each learner has small regret compared to a (1 + ✏)-multiplicative approximation to the best action in hindsight, is ubiquitous among learning algorit
Structure-Blind Signal Recovery  Dmitry Ostrovsky∗ Zaid Harchaoui† Anatoli Juditsky∗ Arkadi Nemirovski‡  firstname.lastname@imag.fr  Abstract  We consider the problem of recovering a signal observed in Gaussian noise. If the set of signals is convex and compact, and can be speciﬁed beforehand, one can use classical linear estimators that achieve a risk within a constant factor of the minimax risk. However, when the set is unspeciﬁed, designing an estimator that is blind to the hidden structure o
An Architecture for Deep, Hierarchical Generative  Models  Philip Bachman  phil.bachman@maluuba.com  Maluuba Research  Abstract  We present an architecture which lets us train deep, directed generative models with many layers of latent variables. We include deterministic paths between all latent variables and the generated output, and provide a richer set of connections between computations for inference and generation, which enables more effective communication of information throughout the mod
Feature selection in functional data classiﬁcation with  recursive maxima hunting  Jos´e L. Torrecilla  Computer Science Department  Universidad Aut´onoma de Madrid  28049 Madrid, Spain  Alberto Su´arez  Computer Science Department  Universidad Aut´onoma de Madrid  28049 Madrid, Spain  joseluis.torrecilla@uam.es  alberto.suarez@uam.es  Abstract  Dimensionality reduction is one of the key issues in the design of effective machine learning methods for automatic induction. In this work, we introduc
Text-Adaptive Generative Adversarial Networks:  Manipulating Images with Natural Language  Seonghyeon Nam, Yunji Kim, and Seon Joo Kim  Yonsei University  {shnnam,kim_yunji,seonjookim}@yonsei.ac.kr  Abstract  This paper addresses the problem of manipulating images using natural language description. Our task aims to semantically modify visual attributes of an object in an image according to the text describing the new visual appearance. Although existing methods synthesize images having new attr
Algorithmic Regularization in Learning Deep  Homogeneous Models: Layers are Automatically  Balanced˚  Simon S. Du:  Wei Hu;  Jason D. Lee§  Abstract  We study the implicit regularization imposed by gradient descent for learning multi-layer homogeneous functions including feed-forward fully connected and convolutional deep neural networks with linear, ReLU or Leaky ReLU activation. We rigorously prove that gradient ﬂow (i.e. gradient descent with inﬁnitesimal step size) effectively enforces the d
Multi-Task Learning as Multi-Objective Optimization  Ozan Sener Intel Labs  Vladlen Koltun  Intel Labs  Abstract  In multi-task learning, multiple tasks are solved jointly, sharing inductive bias between them. Multi-task learning is inherently a multi-objective problem because different tasks may conﬂict, necessitating a trade-off. A common compromise is to optimize a proxy objective that minimizes a weighted linear combination of per- task losses. However, this workaround is only valid when the
Which Neural Net Architectures Give Rise to  Exploding and Vanishing Gradients?  Boris Hanin  Department of Mathematics  Texas A& M University College Station, TX, USA bhanin@math.tamu.edu  Abstract  We give a rigorous analysis of the statistical behavior of gradients in a randomly initialized fully connected network N with ReLU activations. Our results show that the empirical variance of the squares of the entries in the input-output Jacobian of N is exponential in a simple architecture-depende
Removing the Feature Correlation Effect of  Multiplicative Noise  Zijun Zhang  University of Calgary  Yining Zhang  University of Calgary  zijun.zhang@ucalgary.ca  yining.zhang1@ucalgary.ca  Zongpeng Li  Wuhan University  zongpeng@whu.edu.cn  Abstract  Multiplicative noise, including dropout, is widely used to regularize deep neural networks (DNNs), and is shown to be effective in a wide range of architectures and tasks. From an information perspective, we consider injecting multiplicative noise
Learning Disentangled Joint Continuous and Discrete  Representations  Schlumberger Software Technology Innovation Center  Emilien Dupont  Menlo Park, CA, USA  dupont@slb.com  Abstract  We present a framework for learning disentangled and interpretable jointly continu- ous and discrete representations in an unsupervised manner. By augmenting the continuous latent distribution of variational autoencoders with a relaxed discrete distribution and controlling the amount of information encoded in each
TADAM: Task dependent adaptive metric for  improved few-shot learning  Boris N. Oreshkin  Element AI  boris@elementai.com  Pau Rodriguez  Element AI, CVC-UAB  Alexandre Lacoste  Element AI  pau.rodriguez@elementai.com  allac@elementai.com  Abstract  Few-shot learning has become essential for producing models that generalize from few examples. In this work, we identify that metric scaling and metric task conditioning are important to improve the performance of few-shot algorithms. Our analysis re
A ﬂexible model for training action localization  with varying levels of supervision  Guilhem Chéron∗ 1 2  Jean-Baptiste Alayrac∗ 1  Ivan Laptev1  Cordelia Schmid2  Abstract  Spatio-temporal action detection in videos is typically addressed in a fully- supervised setup with manual annotation of training videos required at every frame. Since such annotation is extremely tedious and prohibits scalability, there is a clear need to minimize the amount of manual supervision. In this work we propose a
Learning semantic similarity in a continuous space  Michel Deudon  Ecole Polytechnique  Palaiseau, France  michel.deudon@polytechnique.edu  Abstract  We address the problem of learning semantic representation of questions to measure similarity between pairs as a continuous distance metric. Our work naturally extends Word Mover’s Distance (WMD) [1] by representing text documents as normal distributions instead of bags of embedded words. Our learned metric measures the dissimilarity between two qu
Generalizing Tree Probability Estimation via  Bayesian Networks  Cheng Zhang  Frederick A. Matsen IV  Computational Biology Program  Fred Hutchinson Cancer Research Center  Computational Biology Program  Fred Hutchinson Cancer Research Center  Seattle, WA 98109  chengz23@fredhutch.org  Seattle, WA 98109  matsen@fredhutch.org  Abstract  Probability estimation is one of the fundamental tasks in statistics and machine learning. However, standard methods for probability estimation on discrete object
AlossframeworkforcalibratedanomalydetectionAdityaKrishnaMenonAustralianNationalUniversity∗aditya.menon@anu.edu.auRobertC.WilliamsonAustralianNationalUniversitybob.williamson@anu.edu.auAbstractGivensamplesfromadistribution,anomalydetectionistheproblemofdeterminingifagivenpointliesinalow-densityregion.Thispaperconcernscalibratedanomalydetection,whichisthepracticallyrelevantextensionwhereweadditionallywishtoproduceaconﬁdencescoreforapointbeinganomalous.Buildingonaclassiﬁcationframeworkforstandardan
Foreground Clustering for Joint Segmentation and  Localization in Videos and Images  Abhishek Sharma  Navinfo Europe Research, Eindhoven, NL ∗  kein.iitian@gmail.com  Abstract  This paper presents a novel framework in which video/image segmentation and localization are cast into a single optimization problem that integrates information from low level appearance cues with that of high level localization cues in a very weakly supervised manner. The proposed framework leverages two representa- tion
Algorithmic Linearly Constrained Gaussian Processes  Markus Lange-Hegermann  Department of Electrical Engineering and Computer Science  Ostwestfalen-Lippe University of Applied Sciences  Lemgo  markus.lange-hegermann@hs-owl.de  Abstract  We algorithmically construct multi-output Gaussian process priors which satisfy linear differential equations. Our approach attempts to parametrize all solutions of the equations using Gröbner bases. If successful, a push forward Gaussian process along the param
Zero-Shot Transfer with Deictic Object-Oriented  Representation in Reinforcement Learning  Oﬁr Marom1, Benjamin Rosman 1,2  1University of the Witwatersrand, Johannesburg, South Africa  2Council for Scientiﬁc and Industrial Research, Pretoria, South Africa  Abstract  Object-oriented representations in reinforcement learning have shown promise in transfer learning, with previous research introducing a propositional object- oriented framework that has provably efﬁcient learning bounds with respect
Computing Higher Order Derivatives of Matrix and  Tensor Expressions  Friedrich-Schiller-Universität Jena  Sören Laue  Germany  Matthias Mitterreiter  Friedrich-Schiller-Universität Jena  Germany  soeren.laue@uni-jena.de  matthias.mitterreiter@uni-jena.de  Joachim Giesen  Friedrich-Schiller-Universität Jena  Germany  joachim.giesen@uni-jena.de  Abstract  Optimization is an integral part of most machine learning systems and most nu- merical optimization schemes rely on the computation of derivati
Deep Reinforcement Learning of Marked Temporal Point Processes  Utkarsh Upadhyay  MPI-SWS  utkarshu@mpi-sws.org  Abir De MPI-SWS  ade@mpi-sws.org  Abstract  Manuel Gomez-Rodrizuez  MPI-SWS  manuelgr@mpi-sws.org  In a wide variety of applications, humans interact with a complex environment by means of asynchronous stochastic discrete events in continuous time. Can we design online interventions that will help humans achieve certain goals in such asyn- chronous setting? In this paper, we address t
The committee machine: Computational to statistical  gaps in learning a two-layers neural network  Benjamin Aubin(cid:63)†, Antoine Maillard†, Jean Barbier⊗♦† Florent Krzakala†, Nicolas Macris⊗, Lenka Zdeborová(cid:63)  Abstract  Heuristic tools from statistical physics have been used in the past to locate the phase transitions and compute the optimal learning and generalization errors in the teacher-student scenario in multi-layer neural networks. In this contribution, we provide a rigorous jus
Training deep learning based denoisers  without ground truth data  Shakarim Soltanayev  Se Young Chun  Department of Electrical Engineering  Ulsan National Institute of Science and Technology (UNIST), Republic of Korea  {shakarim,sychun}@unist.ac.kr  Abstract  Recently developed deep-learning-based denoisers often outperform state-of-the-art conventional denoisers, such as the BM3D. They are typically trained to minimize the mean squared error (MSE) between the output image of a deep neural netw
Re-evaluating Evaluation  David Balduzzi⇤  Karl Tuyls⇤  Julien Perolat⇤  Thore Graepel⇤  Abstract  “What we observe is not nature itself, but nature exposed to our method of ques- tioning.” – Werner Heisenberg Progress in machine learning is measured by careful evaluation on problems of outstanding common interest. However, the proliferation of benchmark suites and environments, adversarial attacks, and other complications has diluted the basic evaluation model by overwhelming researchers with c
Derivative Estimation in Random Design  Yu Liu1, Kris De Brabanter1,2∗  1Department of Computer Science, 2Department of Statistics  Abstract  We propose a nonparametric derivative estimation method for random design without having to estimate the regression function. The method is based on a variance-reducing linear combination of symmetric difference quotients. First, we discuss the special case of uniform random design and establish the estimator’s asymptotic properties. Secondly, we generaliz
BinGAN: Learning Compact Binary Descriptors  with a Regularized GAN  Maciej Zieba  Wroclaw University of  Science and Technology, Tooploox  maciej.zieba@pwr.edu.pl  Piotr Semberecki  Wroclaw University of  Science and Technology, Tooploox piotr.semberecki@pwr.edu.pl  Tarek El-Gaaly  Voyage  tarek@voyage.auto  Tomasz Trzcinski  Warsaw University of Technology,  Tooploox  t.trzcinski@ii.pw.edu.pl  Abstract  In this paper, we propose a novel regularization method for Generative Adversarial Networks
Optimal Subsampling with Inﬂuence Functions  Daniel Ting  Tableau Software Seattle, WA, USA  dting@tableau.com  Eric Brochu  Tableau Software  Vancouver, BC, Canada  ebrochu@tableau.com  Abstract  Subsampling is a common and often effective method to deal with the computa- tional challenges of large datasets. However, for most statistical models, there is no well-motivated approach for drawing a non-uniform subsample. We show that the concept of an asymptotically linear estimator and the associa

Neural Networks Trained to Solve Differential  Equations Learn General Representations  Martin Magill  U. of Ontario Inst. of Tech. martin.magill1@uoit.net  Faisal Z. Qureshi  U. of Ontario Inst. of Tech. faisal.qureshi@uoit.ca  Hendrick W. de Haan  U. of Ontario Inst. of Tech. hendrick.dehaan@uoit.ca  Abstract  We introduce a technique based on the singular vector canonical correlation anal- ysis (SVCCA) for measuring the generality of neural network layers across a continuously-parametrized se
Efﬁcient Projection onto the Perfect Phylogeny Model  Bei Jia∗  jiabe@bc.edu  Surjyendu Ray raysc@bc.edu  Boston College  Abstract  Sam Safavi  safavisa@bc.edu  José Bento  jose.bento@bc.edu  Several algorithms build on the perfect phylogeny model to infer evolutionary trees. This problem is particularly hard when evolutionary trees are inferred from the fraction of genomes that have mutations in different positions, across different samples. Existing algorithms might do extensive searches over 
Learning Conﬁdence Sets using Support Vector  Machines  Department of Mathematical Sciences  Department of Mathematical Sciences  Wenbo Wang  Binghamton University Binghamton, NY 13902  Xingye Qiao*  Binghamton University Binghamton, NY 13902  wang2@math.binghamton.edu  qiao@math.binghamton.edu  Abstract  The goal of conﬁdence-set learning in the binary classiﬁcation setting [14] is to construct two sets, each with a speciﬁc probability guarantee to cover a class. An observation outside the over
Fast Greedy MAP Inference for Determinantal Point  Process to Improve Recommendation Diversity  Laming Chen  Hulu LLC  Beijing, China  laming.chen@hulu.com  Guoxin Zhang∗  Kwai Inc.  Beijing, China  zhangguoxin@kuaishou.com  Hanning Zhou  Hulu LLC  Beijing, China  ericzhouh@gmail.com  Abstract  The determinantal point process (DPP) is an elegant probabilistic model of repul- sion with applications in various machine learning tasks including summarization and search. However, the maximum a poster
Critical initialisation for deep signal propagation in  noisy rectiﬁer neural networks  Arnu Pretorius∗  Computer Science Division  CAIR†  Stellenbosch University  Elan Van Biljon  Computer Science Division  Stellenbosch University  Steve Kroon  Computer Science Division  Stellenbosch University  Herman Kamper  Department of Electrical and Electronic Engineering  Stellenbosch University  Abstract  Stochastic regularisation is an important weapon in the arsenal of a deep learning practitioner. Ho
Memory Replay GANs: learning to generate images  from new categories without forgetting  Chenshen Wu, Luis Herranz, Xialei Liu, Yaxing Wang,  Joost van de Weijer, Bogdan Raducanu  Computer Vision Center  Universitat Autònoma de Barcelona, Spain  {chenshen, lherranz, xialei, yaxing, joost, bogdan}@cvc.uab.es  Abstract  Previous works on sequential learning address the problem of forgetting in dis- criminative models. In this paper we consider the case of generative models. In particular, we inves
Graphical Generative Adversarial Networks  Chongxuan Li∗  licx14@mails.tsinghua.edu.cn  Max Welling†  M.Welling@uva.nl  Jun Zhu∗  dcszj@mail.tsinghua.edu.cn  Bo Zhang∗  dcszb@mail.tsinghua.edu.cn  Abstract  We propose Graphical Generative Adversarial Networks (Graphical-GAN) to model structured data. Graphical-GAN conjoins the power of Bayesian networks on compactly representing the dependency structures among random variables and that of generative adversarial networks on learning expressive de
L4: Practical loss-based stepsize adaptation for deep  learning  Michal Rolínek and Georg Martius  Max-Planck-Institute for Intelligent Systems Tübingen, Germany  michal.rolinek@tuebingen.mpg.de and georg.martius@tuebingen.mpg.de  Abstract  We propose a stepsize adaptation scheme for stochastic gradient descent. It oper- ates directly with the loss function and rescales the gradient in order to make ﬁxed predicted progress on the loss. We demonstrate its capabilities by conclusively improving th
A Theory-Based Evaluation of Nearest Neighbor  Models Put Into Practice  Hendrik Fichtenberger∗  TU Dortmund  Dortmund, Germany  Dennis Rohde† TU Dortmund  Dortmund, Germany  hendrik.fichtenberger@tu-dortmund.de  dennis.rohde@cs.tu-dortmund.de  Abstract  In the k-nearest neighborhood model (k-NN), we are given a set of points P , and we shall answer queries q by returning the k nearest neighbors of q in P according to some metric. This concept is crucial in many areas of data analysis and data p
Provable Gaussian Embedding with One Observation  Ming Yu ⇤  Zhuoran Yang †  Tuo Zhao ‡ Mladen Kolar §  Zhaoran Wang ¶  Abstract  The success of machine learning methods heavily relies on having an appropriate representation for data at hand. Traditionally, machine learning approaches relied on user-deﬁned heuristics to extract features encoding structural information about data. However, recently there has been a surge in approaches that learn how to encode the data automatically in a low dimen
0.98  0.96  0.94  0.92  0.9  0.88  0.86  0.84  0.82  0.8  0.78  1  0.05 0.1 0.12 0.13 0.15  2  3  4  5  6  7  8  9  10  
Model-Agnostic Private Learning  Raef Bassily∗  Om Thakkar†  Abhradeep Thakurta‡  Abstract  We design differentially private learning algorithms that are agnostic to the learn- ing model assuming access to a limited amount of unlabeled public data. First, we provide a new differentially private algorithm for answering a sequence of m online classiﬁcation queries (given by a sequence of m unlabeled public feature vectors) based on a private training set. Our algorithm follows the paradigm of subs
Streaming Kernel PCA with ˜O(  √  n) Random Features  Enayat Ullah † enayat@jhu.edu  Poorya Mianjy † mianjy@jhu.edu  Teodor V. Marinov † tmarino2@jhu.edu  Raman Arora †  arora@cs.jhu.edu  Abstract  We study the statistical and computational aspects of kernel principal component √ analysis using random Fourier features and show that under mild assumptions, n log (n)) features sufﬁce to achieve O(1/(cid:15)2) sample complexity. Further- O( more, we give a memory efﬁcient streaming algorithm based 
Submodular Maximization via Gradient Ascent:  The Case of Deep Submodular Functions  Depts. of Electrical & Computer Engineering‡, Computer Science and Engineering$, and Genome Sciences∗  Wenruo Bai‡, William S Noble∗$, Jeff A. Bilmes‡$  Seattle, WA 98195  {wrbai,wnoble,bilmes}@uw.edu  Abstract  We study the problem of maximizing deep submodular functions (DSFs) [13, 3] subject to a matroid constraint. DSFs are an expressive class of submodular functions that include, as strict subfamilies, the 
Improved Expressivity Through Dendritic Neural  Networks  Xundong Wu  Xiangwen Liu  Wei Li  Qing Wu  School of Computer Science and Technology Hangzhou Dianzi University, Hangzhou, China  wuxundong@gmail.com, wuq@hdu.edu.cn  A typical biological neuron, such as a pyramidal neuron of the neocortex, receives thousands of afferent synaptic inputs on its dendrite tree and sends the efferent axonal output downstream. In typical artiﬁcial neural networks, dendrite trees are modeled as linear structure
Learning Conditioned Graph Structures for Interpretable Visual Question Answering  Will Norcliffe-Brown  AimBrain Ltd.  will.norcliffe@aimbrain.com  Efstathios Vafeias  AimBrain Ltd.  stathis@aimbrain.com  Sarah Parisot AimBrain Ltd.  sarah@aimbrain.com  Abstract  Visual Question answering is a challenging problem requiring a combination of concepts from Computer Vision and Natural Language Processing. Most existing approaches use a two streams strategy, computing image and question features tha
Contextual Stochastic Block Models  Yash Deshpande∗  Andrea Montanari †  Elchanan Mossel‡  Subhabrata Sen§  Abstract  We provide the ﬁrst information theoretic tight analysis for inference of latent community structure given a sparse graph along with high dimensional node covariates, correlated with the same latent communities. Our work bridges recent theoretical breakthroughs in the detection of latent community structure without nodes covariates and a large body of empirical work using diverse
Learning Plannable Representations with Causal  InfoGAN  Thanard Kurutach∗1  Aviv Tamar∗1 Ge Yang2  Stuart Russell1  Pieter Abbeel1  Abstract  In recent years, deep generative models have been shown to ‘imagine’ convincing high-dimensional observations such as images, audio, and even video, learning directly from raw data. In this work, we ask how to imagine goal-directed visual plans – a plausible sequence of observations that transition a dynamical system from its current conﬁguration to a des
Group Equivariant Capsule Networks  Jan Eric Lenssen  Matthias Fey  Pascal Libuschewski  TU Dortmund University - Computer Graphics Group  44227 Dortmund, Germany  {janeric.lenssen, matthias.fey, pascal.libuschewski}@udo.edu  Abstract  We present group equivariant capsule networks, a framework to introduce guar- anteed equivariance and invariance properties to the capsule network idea. Our work can be divided into two contributions. First, we present a generic routing by agreement algorithm deﬁn
Iterative Value-Aware Model Learning  Amir-massoud Farahmand∗ Vector Institute, Toronto, Canada  farahmand@vectorinstitute.ai  Abstract  This paper introduces a model-based reinforcement learning (MBRL) framework that incorporates the underlying decision problem in learning the transition model of the environment. This is in contrast with conventional approaches to MBRL that learn the model of the environment, for example by ﬁnding the maximum likelihood estimate, without taking into account the
Scalable Laplacian K-modes  Imtiaz Masud Ziko ∗  ÉTS Montreal  Eric Granger ÉTS Montreal  Ismail Ben Ayed ÉTS Montreal  Abstract  We advocate Laplacian K-modes for joint clustering and density mode ﬁnding, and propose a concave-convex relaxation of the problem, which yields a parallel algorithm that scales up to large datasets and high dimensions. We optimize a tight bound (auxiliary function) of our relaxation, which, at each iteration, amounts to computing an independent update for each cluste
Total stochastic gradient algorithms and applications  in reinforcement learning  Okinawa Institute of Science and Technology Graduate University  Paavo Parmas  Neural Computation Unit  Okinawa, Japan  paavo.parmas@oist.jp  Abstract  Backpropagation and the chain rule of derivatives have been prominent; however, the total derivative rule has not enjoyed the same amount of attention. In this work we show how the total derivative rule leads to an intuitive visual framework for creating gradient es
RobustnessofconditionalGANstonoisylabelsKiranKoshyThekumparampil†,AshishKhetan†,ZinanLin‡,SewoongOh††UniversityofIllinoisatUrbana-Champaign,‡CarnegieMellonUniversityAbstractWestudytheproblemoflearningconditionalgeneratorsfromnoisylabeledsam-ples,wherethelabelsarecorruptedbyrandomnoise.AstandardtrainingofconditionalGANswillnotonlyproducesampleswithwronglabels,butalsogener-atepoorqualitysamples.Weconsidertwoscenarios,dependingonwhetherthenoisemodelisknownornot.Whenthedistributionofthenoiseisknown,
Diversity-Driven Exploration Strategy for Deep  Reinforcement Learning  Zhang-Wei Hong, Tzu-Yun Shann, Shih-Yang Su, Yi-Hsiang Chang, Tsu-Jui Fu,  and Chun-Yi Lee  Department of Computer Science, National Tsing Hua University  {williamd4112,arielshann,at7788546,shawn420,rayfu1996ozig,cylee}  @gapp.nthu.edu.tw  Abstract  Efﬁcient exploration remains a challenging research problem in reinforcement learning, especially when an environment contains large state spaces, deceptive or sparse rewards. To
llllllllllllllllllllllllllDisconnect ratioTime(seconds)0.050.2750.50.00.51.01.52.02.53.03.5lllllllllllllllllMethodsTime(seconds)OursDirect_climeDirect_nodeNLL051015
Transfer of Deep Reactive Policies for MDP Planning  Aniket Bajpai, Sankalp Garg, Mausam  Indian Institute of Technology, Delhi  New Delhi, India  {quantum.computing96, sankalp2621998}@gmail.com, mausam@cse.iitd.ac.in  Abstract  Domain-independent probabilistic planners input an MDP description in a factored representation language such as PPDDL or RDDL, and exploit the speciﬁcs of the representation for faster planning. Traditional algorithms operate on each problem instance independently, and 
